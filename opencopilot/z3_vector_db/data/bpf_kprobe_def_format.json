{"void __bitmap_shift_right(unsigned long *dst, const unsigned long *src,unsigned shift, unsigned nbits)": "__bitmap_or_equal(const unsigned long  bitmap1,       const unsigned long  bitmap2,       const unsigned long  bitmap3,       unsigned int bits){unsigned int k, lim = bits  BITS_PER_LONG;unsigned long tmp;for (k = 0; k < lim; ++k) {if ((bitmap1[k] | bitmap2[k]) != bitmap3[k])return false;}if (!(bits % BITS_PER_LONG))return true;tmp = (bitmap1[k] | bitmap2[k]) ^ bitmap3[k];return (tmp & BITMAP_LAST_WORD_MASK(bits)) == 0;}void __bitmap_complement(unsigned long  dst, const unsigned long  src, unsigned int bits){unsigned int k, lim = BITS_TO_LONGS(bits);for (k = 0; k < lim; ++k)dst[k] = ~src[k];}EXPORT_SYMBOL(__bitmap_complement);     __bitmap_shift_right - logical right shift of the bits in a bitmap     @dst : destination bitmap     @src : source bitmap     @shift : shift by this many bits     @nbits : bitmap size, in bits     Shifting right (dividing) means moving bits in the MS -> LS bit   direction.  Zeros are fed into the vacated MS positions and the   LS bits shifted off the bottom are lost. ", "void __bitmap_shift_left(unsigned long *dst, const unsigned long *src,unsigned int shift, unsigned int nbits)": "__bitmap_shift_left - logical left shift of the bits in a bitmap     @dst : destination bitmap     @src : source bitmap     @shift : shift by this many bits     @nbits : bitmap size, in bits     Shifting left (multiplying) means moving bits in the LS -> MS   direction.  Zeros are fed into the vacated LS bit positions   and those MS bits shifted off the top are lost. ", "void bitmap_cut(unsigned long *dst, const unsigned long *src,unsigned int first, unsigned int cut, unsigned int nbits)": "bitmap_cut() - remove bit region from bitmap and right shift remaining bits   @dst: destination bitmap, might overlap with src   @src: source bitmap   @first: start bit of region to be removed   @cut: number of bits to remove   @nbits: bitmap size, in bits     Set the n-th bit of @dst iff the n-th bit of @src is set and   n is less than @first, or the m-th bit of @src is set for any   m such that @first <= n < nbits, and m = n + @cut.     In pictures, example for a big-endian 32-bit architecture:     The @src bitmap is::       31                                   63     |                                    |     10000000 11000001 11110010 00010101  10000000 11000001 01110010 00010101                     |  |              |                                    |                    16  14             0                                   32     if @cut is 3, and @first is 14, bits 14-16 in @src are cut and @dst is::       31                                   63     |                                    |     10110000 00011000 00110010 00010101  00010000 00011000 00101110 01000010                        |              |                                    |                        14 (bit 17     0                                   32                            from @src)     Note that @dst and @src might overlap partially or entirely.     This is implemented in the obvious way, with a shift and carry   step for each moved bit. Optimisation is left as an exercise   for the compiler. ", "unsigned long bitmap_find_next_zero_area_off(unsigned long *map,     unsigned long size,     unsigned long start,     unsigned int nr,     unsigned long align_mask,     unsigned long align_offset)": "bitmap_find_next_zero_area_off - find a contiguous aligned zero area   @map: The address to base the search on   @size: The bitmap size in bits   @start: The bitnumber to start searching at   @nr: The number of zeroed bits we're looking for   @align_mask: Alignment mask for zero area   @align_offset: Alignment offset for zero area.     The @align_mask should be one less than a power of 2; the effect is that   the bit offset of all zero areas this function finds plus @align_offset   is multiple of that power of 2. ", "int bitmap_parse_user(const char __user *ubuf,unsigned int ulen, unsigned long *maskp,int nmaskbits)": "bitmap_parse_user - convert an ASCII hex string in a user buffer into a bitmap     @ubuf: pointer to user buffer containing string.   @ulen: buffer size in bytes.  If string is smaller than this      then it must be terminated with a \\0.   @maskp: pointer to bitmap array that will contain result.   @nmaskbits: size of bitmap, in bits. ", "int bitmap_print_to_pagebuf(bool list, char *buf, const unsigned long *maskp,    int nmaskbits)": "bitmap_print_to_pagebuf - convert bitmap to list or hex format ASCII string   @list: indicates whether the bitmap must be list   @buf: page aligned buffer into which string is placed   @maskp: pointer to bitmap to convert   @nmaskbits: size of bitmap, in bits     Output format is a comma-separated list of decimal numbers and   ranges if list is specified or hex digits grouped into comma-separated   sets of 8 digitsset. Returns the number of characters written to buf.     It is assumed that @buf is a pointer into a PAGE_SIZE, page-aligned   area and that sufficient storage remains at @buf to accommodate the   bitmap_print_to_pagebuf() output. Returns the number of characters   actually printed to @buf, excluding terminating '\\0'. ", "int bitmap_print_bitmask_to_buf(char *buf, const unsigned long *maskp,int nmaskbits, loff_t off, size_t count)": "bitmap_parselist-like parser may fail parsing it.    - If printing the whole bitmap as list by parts, user must ensure the order      of calls of the function such that the offset is incremented linearly.    - If printing the whole bitmap as list by parts, user must keep bitmap      unchanged between the very first and very last call. Otherwise concatenated      result may be incorrect, and format may be broken.     Returns the number of characters actually printed to @buf ", "int bitmap_parselist_user(const char __user *ubuf,unsigned int ulen, unsigned long *maskp,int nmaskbits)": "bitmap_parselist_user() - convert user buffer's list format ASCII   string to bitmap     @ubuf: pointer to user buffer containing string.   @ulen: buffer size in bytes.  If string is smaller than this      then it must be terminated with a \\0.   @maskp: pointer to bitmap array that will contain result.   @nmaskbits: size of bitmap, in bits.     Wrapper for bitmap_parselist(), providing it with user buffer. ", "void bitmap_remap(unsigned long *dst, const unsigned long *src,const unsigned long *old, const unsigned long *new,unsigned int nbits)": "bitmap_remap - Apply map defined by a pair of bitmaps to another bitmap  @dst: remapped result  @src: subset to be remapped  @old: defines domain of map  @new: defines range of map  @nbits: number of bits in each of these bitmaps     Let @old and @new define a mapping of bit positions, such that   whatever position is held by the n-th set bit in @old is mapped   to the n-th set bit in @new.  In the more general case, allowing   for the possibility that the weight 'w' of @new is less than the   weight of @old, map the position of the n-th set bit in @old to   the position of the m-th set bit in @new, where m == n % w.     If either of the @old and @new bitmaps are empty, or if @src and   @dst point to the same location, then this routine copies @src   to @dst.     The positions of unset bits in @old are mapped to themselves   (the identify map).     Apply the above specified mapping to @src, placing the result in   @dst, clearing any bits previously set in @dst.     For example, lets say that @old has bits 4 through 7 set, and   @new has bits 12 through 15 set.  This defines the mapping of bit   position 4 to 12, 5 to 13, 6 to 14 and 7 to 15, and of all other   bit positions unchanged.  So if say @src comes into this routine   with bits 1, 5 and 7 set, then @dst should leave with bits 1,   13 and 15 set. ", "int bitmap_bitremap(int oldbit, const unsigned long *old,const unsigned long *new, int bits)": "bitmap_bitremap - Apply map defined by a pair of bitmaps to a single bit  @oldbit: bit position to be mapped  @old: defines domain of map  @new: defines range of map  @bits: number of bits in each of these bitmaps     Let @old and @new define a mapping of bit positions, such that   whatever position is held by the n-th set bit in @old is mapped   to the n-th set bit in @new.  In the more general case, allowing   for the possibility that the weight 'w' of @new is less than the   weight of @old, map the position of the n-th set bit in @old to   the position of the m-th set bit in @new, where m == n % w.     The positions of unset bits in @old are mapped to themselves   (the identify map).     Apply the above specified mapping to bit position @oldbit, returning   the new bit position.     For example, lets say that @old has bits 4 through 7 set, and   @new has bits 12 through 15 set.  This defines the mapping of bit   position 4 to 12, 5 to 13, 6 to 14 and 7 to 15, and of all other   bit positions unchanged.  So if say @oldbit is 5, then this routine   returns 13. ", "int bitmap_find_free_region(unsigned long *bitmap, unsigned int bits, int order)": "bitmap_find_free_region - find a contiguous aligned mem region  @bitmap: array of unsigned longs corresponding to the bitmap  @bits: number of bits in the bitmap  @order: region size (log base 2 of number of bits) to find     Find a region of free (zero) bits in a @bitmap of @bits bits and   allocate them (set them to one).  Only consider regions of length   a power (@order) of two, aligned to that power of two, which   makes the search algorithm much faster.     Return the bit offset in bitmap of the allocated region,   or -errno on failure. ", "void bitmap_release_region(unsigned long *bitmap, unsigned int pos, int order)": "bitmap_release_region - release allocated bitmap region  @bitmap: array of unsigned longs corresponding to the bitmap  @pos: beginning of bit region to release  @order: region size (log base 2 of number of bits) to release     This is the complement to __bitmap_find_free_region() and releases   the found region (by clearing it in the bitmap).     No return value. ", "int bitmap_allocate_region(unsigned long *bitmap, unsigned int pos, int order)": "bitmap_allocate_region - allocate bitmap region  @bitmap: array of unsigned longs corresponding to the bitmap  @pos: beginning of bit region to allocate  @order: region size (log base 2 of number of bits) to allocate     Allocate (set bits in) a specified region of a bitmap.     Return 0 on success, or %-EBUSY if specified region wasn't   free (not all bits were zero). ", "#ifdef __BIG_ENDIANvoid bitmap_copy_le(unsigned long *dst, const unsigned long *src, unsigned int nbits)": "bitmap_copy_le - copy a bitmap, putting the bits into little-endian order.   @dst:   destination buffer   @src:   bitmap to copy   @nbits: number of bits in the bitmap     Require nbits % BITS_PER_LONG == 0. ", "void bitmap_from_arr32(unsigned long *bitmap, const u32 *buf, unsigned int nbits)": "bitmap_from_arr32 - copy the contents of u32 array of bits to bitmap  @bitmap: array of unsigned longs, the destination bitmap  @buf: array of u32 (in host byte order), the source bitmap  @nbits: number of bits in @bitmap ", "void bitmap_to_arr32(u32 *buf, const unsigned long *bitmap, unsigned int nbits)": "bitmap_to_arr32 - copy the contents of bitmap to a u32 array of bits  @buf: array of u32 (in host byte order), the dest bitmap  @bitmap: array of unsigned longs, the source bitmap  @nbits: number of bits in @bitmap ", "void bitmap_from_arr64(unsigned long *bitmap, const u64 *buf, unsigned int nbits)": "bitmap_from_arr64 - copy the contents of u64 array of bits to bitmap  @bitmap: array of unsigned longs, the destination bitmap  @buf: array of u64 (in host byte order), the source bitmap  @nbits: number of bits in @bitmap ", "void bitmap_to_arr64(u64 *buf, const unsigned long *bitmap, unsigned int nbits)": "bitmap_to_arr64 - copy the contents of bitmap to a u64 array of bits  @buf: array of u64 (in host byte order), the dest bitmap  @bitmap: array of unsigned longs, the source bitmap  @nbits: number of bits in @bitmap ", "int sg_split(struct scatterlist *in, const int in_mapped_nents,     const off_t skip, const int nb_splits,     const size_t *split_sizes,     struct scatterlist **out, int *out_mapped_nents,     gfp_t gfp_mask)": "sg_splitter {struct scatterlist  in_sg0;int nents;off_t skip_sg0;unsigned int length_last_sg;struct scatterlist  out_sg;};static int sg_calculate_split(struct scatterlist  in, int nents, int nb_splits,      off_t skip, const size_t  sizes,      struct sg_splitter  splitters, bool mapped){int i;unsigned int sglen;size_t size = sizes[0], len;struct sg_splitter  curr = splitters;struct scatterlist  sg;for (i = 0; i < nb_splits; i++) {splitters[i].in_sg0 = NULL;splitters[i].nents = 0;}for_each_sg(in, sg, nents, i) {sglen = mapped ? sg_dma_len(sg) : sg->length;if (skip > sglen) {skip -= sglen;continue;}len = min_t(size_t, size, sglen - skip);if (!curr->in_sg0) {curr->in_sg0 = sg;curr->skip_sg0 = skip;}size -= len;curr->nents++;curr->length_last_sg = len;while (!size && (skip + len < sglen) && (--nb_splits > 0)) {curr++;size =  (++sizes);skip += len;len = min_t(size_t, size, sglen - skip);curr->in_sg0 = sg;curr->skip_sg0 = skip;curr->nents = 1;curr->length_last_sg = len;size -= len;}skip = 0;if (!size && --nb_splits > 0) {curr++;size =  (++sizes);}if (!nb_splits)break;}return (size || !splitters[0].in_sg0) ? -EINVAL : 0;}static void sg_split_phys(struct sg_splitter  splitters, const int nb_splits){int i, j;struct scatterlist  in_sg,  out_sg;struct sg_splitter  split;for (i = 0, split = splitters; i < nb_splits; i++, split++) {in_sg = split->in_sg0;out_sg = split->out_sg;for (j = 0; j < split->nents; j++, out_sg++) { out_sg =  in_sg;if (!j) {out_sg->offset += split->skip_sg0;out_sg->length -= split->skip_sg0;} else {out_sg->offset = 0;}sg_dma_address(out_sg) = 0;sg_dma_len(out_sg) = 0;in_sg = sg_next(in_sg);}out_sg[-1].length = split->length_last_sg;sg_mark_end(out_sg - 1);}}static void sg_split_mapped(struct sg_splitter  splitters, const int nb_splits){int i, j;struct scatterlist  in_sg,  out_sg;struct sg_splitter  split;for (i = 0, split = splitters; i < nb_splits; i++, split++) {in_sg = split->in_sg0;out_sg = split->out_sg;for (j = 0; j < split->nents; j++, out_sg++) {sg_dma_address(out_sg) = sg_dma_address(in_sg);sg_dma_len(out_sg) = sg_dma_len(in_sg);if (!j) {sg_dma_address(out_sg) += split->skip_sg0;sg_dma_len(out_sg) -= split->skip_sg0;}in_sg = sg_next(in_sg);}sg_dma_len(--out_sg) = split->length_last_sg;}}     sg_split - split a scatterlist into several scatterlists   @in: the input sg list   @in_mapped_nents: the result of a dma_map_sg(in, ...), or 0 if not mapped.   @skip: the number of bytes to skip in the input sg list   @nb_splits: the number of desired sg outputs   @split_sizes: the respective size of each output sg list in bytes   @out: an array where to store the allocated output sg lists   @out_mapped_nents: the resulting sg lists mapped number of sg entries. Might                      be NULL if sglist not already mapped (in_mapped_nents = 0)   @gfp_mask: the allocation flag     This function splits the input sg list into nb_splits sg lists, which are   allocated and stored into out.   The @in is split into :    - @out[0], which covers bytes [@skip .. @skip + @split_sizes[0] - 1] of @in    - @out[1], which covers bytes [@skip + split_sizes[0] ..                                   @skip + @split_sizes[0] + @split_sizes[1] -1]   etc ...   It will be the caller's duty to kfree() out array members.     Returns 0 upon success, or error code ", "    unlikely(val.t - m->s[2].t > win))  /* nothing left in window? ": "minmax_running_max(struct minmax  m, u32 win, u32 t, u32 meas){struct minmax_sample val = { .t = t, .v = meas };if (unlikely(val.v >= m->s[0].v) ||    found new max? ", "struct cpu_rmap *alloc_cpu_rmap(unsigned int size, gfp_t flags)": "alloc_cpu_rmap - allocate CPU affinity reverse-map   @size: Number of objects to be mapped   @flags: Allocation flags e.g. %GFP_KERNEL ", "int cpu_rmap_put(struct cpu_rmap *rmap)": "cpu_rmap_put - release ref on a cpu_rmap   @rmap: reverse-map allocated with alloc_cpu_rmap() ", "int cpu_rmap_add(struct cpu_rmap *rmap, void *obj)": "cpu_rmap_add - add object to a rmap   @rmap: CPU rmap allocated with alloc_cpu_rmap()   @obj: Object to add to rmap     Return index of object or -ENOSPC if no free entry was found ", "int cpu_rmap_update(struct cpu_rmap *rmap, u16 index,    const struct cpumask *affinity)": "cpu_rmap_update - update CPU rmap following a change of object affinity   @rmap: CPU rmap to update   @index: Index of object whose affinity changed   @affinity: New CPU affinity of object ", "void free_irq_cpu_rmap(struct cpu_rmap *rmap)": "free_irq_cpu_rmap - free a CPU affinity reverse-map used for IRQs   @rmap: Reverse-map allocated with alloc_irq_cpu_map(), or %NULL     Must be called in process context, before freeing the IRQs. ", "int irq_cpu_rmap_remove(struct cpu_rmap *rmap, int irq)": "irq_cpu_rmap_remove - remove an IRQ from a CPU affinity reverse-map   @rmap: The reverse-map   @irq: The IRQ number ", "int irq_cpu_rmap_add(struct cpu_rmap *rmap, int irq)": "irq_cpu_rmap_add - add an IRQ to a CPU affinity reverse-map   @rmap: The reverse-map   @irq: The IRQ number     This adds an IRQ affinity notifier that will update the reverse-map   automatically.     Must be called in process context, after the IRQ is allocated but   before it is bound with request_irq(). ", "int interval = READ_ONCE(rs->interval);int burst = READ_ONCE(rs->burst);unsigned long flags;int ret;if (!interval)return 1;/* * If we contend on this state's lock then almost * by definition we are too busy to print a message, * in addition to the one that will be printed by * the entity that is holding the lock already: ": "___ratelimit(struct ratelimit_state  rs, const char  func){  Paired with WRITE_ONCE() in .proc_handler().   Changing two values seperately could be inconsistent   and some message could be lost.  (See: net_ratelimit_state). ", "void string_get_size(u64 size, u64 blk_size, const enum string_size_units units,     char *buf, int len)": "string_get_size - get the size in the specified units   @size:The size to be converted in blocks   @blk_size:Size of the block (use 1 for size in bytes)   @units:units to use (powers of 1000 or 1024)   @buf:buffer to format to   @len:length of buffer     This function returns a string formatted to 3 significant figures   giving the size in the required units.  @buf should have room for   at least 9 bytes and will always be zero terminated.   ", "int parse_int_array_user(const char __user *from, size_t count, int **array)": "parse_int_array_user - Split string into a sequence of integers   @from:The user space buffer to read from   @count:The maximum number of bytes to read   @array:Returned pointer to sequence of integers     On success @array is allocated and initialized with a sequence of   integers extracted from the @from plus an additional element that   begins the sequence and specifies the integers count.     Caller takes responsibility for freeing @array when it is no longer   needed. ", "int string_unescape(char *src, char *dst, size_t size, unsigned int flags)": "string_unescape - unquote characters in the given string   @src:source buffer (escaped)   @dst:destination buffer (unescaped)   @size:size of the destination buffer (0 to unlimit)   @flags:combination of the flags.     Description:   The function unquotes characters in the given string.     Because the size of the output will be the same as or less than the size of   the input, the transformation may be performed in place.     Caller must provide valid source and destination pointers. Be aware that   destination buffer will always be NULL-terminated. Source string must be   NULL-terminated as well.  The supported flags are::    UNESCAPE_SPACE:  '\\f' - form feed  '\\n' - new line  '\\r' - carriage return  '\\t' - horizontal tab  '\\v' - vertical tab  UNESCAPE_OCTAL:  '\\NNN' - byte with octal value NNN (1 to 3 digits)  UNESCAPE_HEX:  '\\xHH' - byte with hexadecimal value HH (1 to 2 digits)  UNESCAPE_SPECIAL:  '\\\"' - double quote  '\\\\' - backslash  '\\a' - alert (BEL)  '\\e' - escape  UNESCAPE_ANY:  all previous together     Return:   The amount of the characters processed to the destination buffer excluding   trailing '\\0' is returned. ", "int string_escape_mem(const char *src, size_t isz, char *dst, size_t osz,      unsigned int flags, const char *only)": "string_escape_mem - quote characters in the given memory buffer   @src:source buffer (unescaped)   @isz:source buffer size   @dst:destination buffer (escaped)   @osz:destination buffer size   @flags:combination of the flags   @only:NULL-terminated string containing characters used to limit  the selected escape class. If characters are included in @only  that would not normally be escaped by the classes selected  in @flags, they will be copied to @dst unescaped.     Description:   The process of escaping byte buffer includes several parts. They are applied   in the following sequence.    1. The character is not matched to the one from @only string and thus     must go as-is to the output.  2. The character is matched to the printable and ASCII classes, if asked,     and in case of match it passes through to the output.  3. The character is matched to the printable or ASCII class, if asked,     and in case of match it passes through to the output.  4. The character is checked if it falls into the class given by @flags.     %ESCAPE_OCTAL and %ESCAPE_HEX are going last since they cover any     character. Note that they actually can't go together, otherwise     %ESCAPE_HEX will be ignored.     Caller must provide valid source and destination pointers. Be aware that   destination buffer will not be NULL-terminated, thus caller have to append   it if needs. The supported flags are::    %ESCAPE_SPACE: (special white space, not space itself)  '\\f' - form feed  '\\n' - new line  '\\r' - carriage return  '\\t' - horizontal tab  '\\v' - vertical tab  %ESCAPE_SPECIAL:  '\\\"' - double quote  '\\\\' - backslash  '\\a' - alert (BEL)  '\\e' - escape  %ESCAPE_NULL:  '\\0' - null  %ESCAPE_OCTAL:  '\\NNN' - byte with octal value NNN (3 digits)  %ESCAPE_ANY:  all previous together  %ESCAPE_NP:  escape only non-printable characters, checked by isprint()  %ESCAPE_ANY_NP:  all previous together  %ESCAPE_HEX:  '\\xHH' - byte with hexadecimal value HH (2 digits)  %ESCAPE_NA:  escape only non-ascii characters, checked by isascii()  %ESCAPE_NAP:  escape only non-printable or non-ascii characters  %ESCAPE_APPEND:  append characters from @only to be escaped by the given classes     %ESCAPE_APPEND would help to pass additional characters to the escaped, when   one of %ESCAPE_NP, %ESCAPE_NA, or %ESCAPE_NAP is provided.     One notable caveat, the %ESCAPE_NAP, %ESCAPE_NP and %ESCAPE_NA have the   higher priority than the rest of the flags (%ESCAPE_NAP is the highest).   It doesn't make much sense to use either of them without %ESCAPE_OCTAL   or %ESCAPE_HEX, because they cover most of the other character classes.   %ESCAPE_NAP can utilize %ESCAPE_SPACE or %ESCAPE_SPECIAL in addition to   the above.     Return:   The total size of the escaped output that would be generated for   the given input and flags. To check whether the output was   truncated, compare the return value to osz. There is room left in   dst for a '\\0' terminator if and only if ret < osz. ", "ssize_t strscpy_pad(char *dest, const char *src, size_t count)": "strscpy_pad() - Copy a C-string into a sized buffer   @dest: Where to copy the string to   @src: Where to copy the string from   @count: Size of destination buffer     Copy the string, or as much of it as fits, into the dest buffer.  The   behavior is undefined if the string buffers overlap.  The destination   buffer is always %NUL terminated, unless it's zero-sized.     If the source string is shorter than the destination buffer, zeros   the tail of the destination buffer.     For full explanation of why you may want to consider using the   'strscpy' functions please see the function docstring for strscpy().     Returns:     The number of characters copied (not including the trailing %NUL)     -E2BIG if count is 0 or @src was truncated. ", "char *skip_spaces(const char *str)": "skip_spaces - Removes leading whitespace from @str.   @str: The string to be stripped.     Returns a pointer to the first non-whitespace character in @str. ", "char *strim(char *s)": "strim - Removes leading and trailing whitespace from @s.   @s: The string to be stripped.     Note that the first trailing whitespace is replaced with a %NUL-terminator   in the given string @s. Returns a pointer to the first non-whitespace   character in @s. ", "bool sysfs_streq(const char *s1, const char *s2)": "sysfs_streq - return true if strings are equal, modulo trailing newline   @s1: one string   @s2: another string     This routine returns true iff two strings are equal, treating both   NUL and newline-then-NUL as equivalent string terminations.  It's   geared for use with sysfs input strings, which generally terminate   with newlines but are compared against values without newlines. ", "int match_string(const char * const *array, size_t n, const char *string)": "match_string - matches given string in an array   @array:array of strings   @n:number of strings in the array or -1 for NULL terminated arrays   @string:string to match with     This routine will look for a string in an array of strings up to the   n-th element in the array or until the first NULL element.     Historically the value of -1 for @n, was used to search in arrays that   are NULL terminated. However, the function does not make a distinction   when finishing the search: either @n elements have been compared OR   the first NULL element was found.     Return:   index of a @string in the @array if matches, or %-EINVAL otherwise. ", "int __sysfs_match_string(const char * const *array, size_t n, const char *str)": "__sysfs_match_string - matches given string in an array   @array: array of strings   @n: number of strings in the array or -1 for NULL terminated arrays   @str: string to match with     Returns index of @str in the @array or -EINVAL, just like match_string().   Uses sysfs_streq instead of strcmp for matching.     This routine will look for a string in an array of strings up to the   n-th element in the array or until the first NULL element.     Historically the value of -1 for @n, was used to search in arrays that   are NULL terminated. However, the function does not make a distinction   when finishing the search: either @n elements have been compared OR   the first NULL element was found. ", "char *strreplace(char *str, char old, char new)": "strreplace - Replace all occurrences of character in string.   @str: The string to operate on.   @old: The character being replaced.   @new: The character @old is replaced with.     Replaces the each @old character with a @new one in the given string @str.     Return: pointer to the string @str itself. ", "void memcpy_and_pad(void *dest, size_t dest_len, const void *src, size_t count,    int pad)": "memcpy_and_pad - Copy one buffer to another with padding   @dest: Where to copy to   @dest_len: The destination buffer size   @src: Where to copy from   @count: The number of bytes to copy   @pad: Character to use for padding if space is left in destination. ", "while (1) ": "ieee80211_get_buffered_bc(struct ieee80211_hw  hw,  struct ieee80211_vif  vif){struct ieee80211_local  local = hw_to_local(hw);struct sk_buff  skb = NULL;struct ieee80211_tx_data tx;struct ieee80211_sub_if_data  sdata;struct ps_data  ps;struct ieee80211_tx_info  info;struct ieee80211_chanctx_conf  chanctx_conf;sdata = vif_to_sdata(vif);rcu_read_lock();chanctx_conf = rcu_dereference(sdata->vif.bss_conf.chanctx_conf);if (!chanctx_conf)goto out;if (sdata->vif.type == NL80211_IFTYPE_AP) {struct beacon_data  beacon =rcu_dereference(sdata->deflink.u.ap.beacon);if (!beacon || !beacon->head)goto out;ps = &sdata->u.ap.ps;} else if (ieee80211_vif_is_mesh(&sdata->vif)) {ps = &sdata->u.mesh.ps;} else {goto out;}if (ps->dtim_count != 0 || !ps->dtim_bc_mc)goto out;   send buffered bcmc only after DTIM beacon ", "noinlineint kstrtoull(const char *s, unsigned int base, unsigned long long *res)": "_kstrtoull(const char  s, unsigned int base, unsigned long long  res){unsigned long long _res;unsigned int rv;s = _parse_integer_fixup_radix(s, &base);rv = _parse_integer(s, base, &_res);if (rv & KSTRTOX_OVERFLOW)return -ERANGE;if (rv == 0)return -EINVAL;s += rv;if ( s == '\\n')s++;if ( s)return -EINVAL; res = _res;return 0;}     kstrtoull - convert a string to an unsigned long long   @s: The start of the string. The string must be null-terminated, and may also    include a single newline before its terminating null. The first character    may also be a plus sign, but not a minus sign.   @base: The number base to use. The maximum supported base is 16. If base is    given as 0, then the base of the string is automatically detected with the    conventional semantics - If it begins with 0x the number will be parsed as a    hexadecimal (case insensitive), if it otherwise begins with 0, it will be    parsed as an octal number. Otherwise it will be parsed as a decimal.   @res: Where to write the result of the conversion on success.     Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.   Preferred over simple_strtoull(). Return code must be checked. ", "noinlineint kstrtoll(const char *s, unsigned int base, long long *res)": "kstrtoll - convert a string to a long long   @s: The start of the string. The string must be null-terminated, and may also    include a single newline before its terminating null. The first character    may also be a plus sign or a minus sign.   @base: The number base to use. The maximum supported base is 16. If base is    given as 0, then the base of the string is automatically detected with the    conventional semantics - If it begins with 0x the number will be parsed as a    hexadecimal (case insensitive), if it otherwise begins with 0, it will be    parsed as an octal number. Otherwise it will be parsed as a decimal.   @res: Where to write the result of the conversion on success.     Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.   Preferred over simple_strtoll(). Return code must be checked. ", "noinlineint kstrtouint(const char *s, unsigned int base, unsigned int *res)": "kstrtouint - convert a string to an unsigned int   @s: The start of the string. The string must be null-terminated, and may also    include a single newline before its terminating null. The first character    may also be a plus sign, but not a minus sign.   @base: The number base to use. The maximum supported base is 16. If base is    given as 0, then the base of the string is automatically detected with the    conventional semantics - If it begins with 0x the number will be parsed as a    hexadecimal (case insensitive), if it otherwise begins with 0, it will be    parsed as an octal number. Otherwise it will be parsed as a decimal.   @res: Where to write the result of the conversion on success.     Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.   Preferred over simple_strtoul(). Return code must be checked. ", "noinlineint kstrtoint(const char *s, unsigned int base, int *res)": "kstrtoint - convert a string to an int   @s: The start of the string. The string must be null-terminated, and may also    include a single newline before its terminating null. The first character    may also be a plus sign or a minus sign.   @base: The number base to use. The maximum supported base is 16. If base is    given as 0, then the base of the string is automatically detected with the    conventional semantics - If it begins with 0x the number will be parsed as a    hexadecimal (case insensitive), if it otherwise begins with 0, it will be    parsed as an octal number. Otherwise it will be parsed as a decimal.   @res: Where to write the result of the conversion on success.     Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.   Preferred over simple_strtol(). Return code must be checked. ", "noinlineint kstrtobool(const char *s, bool *res)": "kstrtobool - convert common user inputs into boolean values   @s: input string   @res: result     This routine returns 0 iff the first character is one of 'YyTt1NnFf0', or   [oO][NnFf] for \"on\" and \"off\". Otherwise it will return -EINVAL.  Value   pointed to by res is updated upon finding a match. ", "char buf[4];count = min(count, sizeof(buf) - 1);if (copy_from_user(buf, s, count))return -EFAULT;buf[count] = '\\0';return kstrtobool(buf, res);}EXPORT_SYMBOL(kstrtobool_from_user": "kstrtobool_from_user(const char __user  s, size_t count, bool  res){  Longest string needed to differentiate, newline, terminator ", "#include <linux/ctype.h>#include <linux/errno.h>#include <linux/export.h>#include <linux/kstrtox.h>#include <linux/math64.h>#include <linux/types.h>#include <linux/uaccess.h>#include \"kstrtox.h\"noinlineconst char *_parse_integer_fixup_radix(const char *s, unsigned int *base)": "fier: GPL-2.0    Convert integer string representation to an integer.   If an integer doesn't fit into specified type, -E is returned.     Integer starts with optional sign.   kstrtou () functions do not accept sign \"-\".     Radix 0 means autodetection: leading \"0x\" implies radix 16,   leading \"0\" implies radix 8, otherwise radix is 10.   Autodetection hints work after optional sign, but not before.     If -E is returned, result is not touched. ", ", size);}EXPORT_SYMBOL(_find_first_bit": "_find_first_bit(const unsigned long  addr, unsigned long size){return FIND_FIRST_BIT(addr[idx],   nop ", ", size);}EXPORT_SYMBOL(_find_first_and_bit": "_find_first_and_bit(const unsigned long  addr1,  const unsigned long  addr2,  unsigned long size){return FIND_FIRST_BIT(addr1[idx] & addr2[idx],   nop ", ", size);}EXPORT_SYMBOL(_find_first_zero_bit": "_find_first_zero_bit(const unsigned long  addr, unsigned long size){return FIND_FIRST_BIT(~addr[idx],   nop ", ", nbits, start);}EXPORT_SYMBOL(_find_next_bit": "_find_next_bit(const unsigned long  addr, unsigned long nbits, unsigned long start){return FIND_NEXT_BIT(addr[idx],   nop ", ", nbits, start);}EXPORT_SYMBOL(_find_next_and_bit": "_find_next_and_bit(const unsigned long  addr1, const unsigned long  addr2,unsigned long nbits, unsigned long start){return FIND_NEXT_BIT(addr1[idx] & addr2[idx],   nop ", ", nbits, start);}EXPORT_SYMBOL(_find_next_andnot_bit": "_find_next_andnot_bit(const unsigned long  addr1, const unsigned long  addr2,unsigned long nbits, unsigned long start){return FIND_NEXT_BIT(addr1[idx] & ~addr2[idx],   nop ", ", nbits, start);}EXPORT_SYMBOL(_find_next_or_bit": "_find_next_or_bit(const unsigned long  addr1, const unsigned long  addr2,unsigned long nbits, unsigned long start){return FIND_NEXT_BIT(addr1[idx] | addr2[idx],   nop ", ", nbits, start);}EXPORT_SYMBOL(_find_next_zero_bit": "_find_next_zero_bit(const unsigned long  addr, unsigned long nbits, unsigned long start){return FIND_NEXT_BIT(~addr[idx],   nop ", "if (strnlen(s, maxlen) < maxlen)return false;/* Don't dirty result unless string is valid MAC. ": "mac_pton(const char  s, u8  mac){size_t maxlen = 3   ETH_ALEN - 1;int i;  XX:XX:XX:XX:XX:XX ", "size_t fault_in_iov_iter_readable(const struct iov_iter *i, size_t size)": "fault_in_iov_iter_readable - fault in iov iterator for reading   @i: iterator   @size: maximum length     Fault in one or more iovecs of the given iov_iter, to a maximum length of   @size.  For each iovec, fault in each page that constitutes the iovec.     Returns the number of bytes not faulted in (like copy_to_user() and   copy_from_user()).     Always returns 0 for non-userspace iterators. ", "size_t fault_in_iov_iter_writeable(const struct iov_iter *i, size_t size)": "fault_in_iov_iter_writeable - fault in iov iterator for writing   @i: iterator   @size: maximum length     Faults in the iterator using get_user_pages(), i.e., without triggering   hardware page faults.  This is primarily useful when we already know that   some or all of the pages in @i aren't in memory.     Returns the number of bytes not faulted in, like copy_to_user() and   copy_from_user().     Always returns 0 for non-user-space iterators. ", "iov_iter_iovec_advance(i, size);} else if (iov_iter_is_bvec(i)) ": "iov_iter_bvec_advance(struct iov_iter  i, size_t size){const struct bio_vec  bvec,  end;if (!i->count)return;i->count -= size;size += i->iov_offset;for (bvec = i->bvec, end = bvec + i->nr_segs; bvec < end; bvec++) {if (likely(size < bvec->bv_len))break;size -= bvec->bv_len;}i->iov_offset = size;i->nr_segs -= bvec - i->bvec;i->bvec = bvec;}static void iov_iter_iovec_advance(struct iov_iter  i, size_t size){const struct iovec  iov,  end;if (!i->count)return;i->count -= size;size += i->iov_offset;  from beginning of current segmentfor (iov = iter_iov(i), end = iov + i->nr_segs; iov < end; iov++) {if (likely(size < iov->iov_len))break;size -= iov->iov_len;}i->iov_offset = size;i->nr_segs -= iov - iter_iov(i);i->__iov = iov;}void iov_iter_advance(struct iov_iter  i, size_t size){if (unlikely(i->count < size))size = i->count;if (likely(iter_is_ubuf(i)) || unlikely(iov_iter_is_xarray(i))) {i->iov_offset += size;i->count -= size;} else if (likely(iter_is_iovec(i) || iov_iter_is_kvec(i))) {  iovec and kvec have identical layouts ", "} else if (iov_iter_is_bvec(i)) ": "iov_iter_revert(struct iov_iter  i, size_t unroll){if (!unroll)return;if (WARN_ON(unroll > MAX_RW_COUNT))return;i->count += unroll;if (unlikely(iov_iter_is_discard(i)))return;if (unroll <= i->iov_offset) {i->iov_offset -= unroll;return;}unroll -= i->iov_offset;if (iov_iter_is_xarray(i) || iter_is_ubuf(i)) {BUG();   We should never go beyond the start of the specified  range since we might then be straying into pages that  aren't pinned.", "void iov_iter_xarray(struct iov_iter *i, unsigned int direction,     struct xarray *xarray, loff_t start, size_t count)": "iov_iter_xarray - Initialise an IO iterator to use the pages in an xarray   @i: The iterator to initialise.   @direction: The direction of the transfer.   @xarray: The xarray to access.   @start: The start file position.   @count: The size of the IO buffer in bytes.     Set up an IO iterator to either draw data out of the pages attached to an   inode or to inject data into those pages.  The pages  must  be prevented   from evaporation, either by taking a ref on them or locking them by the   caller. ", "void iov_iter_discard(struct iov_iter *i, unsigned int direction, size_t count)": "iov_iter_discard - Initialise an IO iterator that discards data   @i: The iterator to initialise.   @direction: The direction of the transfer.   @count: The size of the IO buffer in bytes.     Set up an IO iterator that just discards everything that's written to it.   It's only available as a READ iterator. ", "if (likely(iter_is_iovec(i) || iov_iter_is_kvec(i)))return iov_iter_alignment_iovec(i);if (iov_iter_is_bvec(i))return iov_iter_alignment_bvec(i);if (iov_iter_is_xarray(i))return (i->xarray_start + i->iov_offset) | i->count;return 0;}EXPORT_SYMBOL(iov_iter_alignment": "iov_iter_alignment_iovec(const struct iov_iter  i){unsigned long res = 0;size_t size = i->count;size_t skip = i->iov_offset;unsigned k;for (k = 0; k < i->nr_segs; k++, skip = 0) {const struct iovec  iov = iter_iov(i) + k;size_t len = iov->iov_len - skip;if (len) {res |= (unsigned long)iov->iov_base + skip;if (len > size)len = size;res |= len;size -= len;if (!size)break;}}return res;}static unsigned long iov_iter_alignment_bvec(const struct iov_iter  i){unsigned res = 0;size_t size = i->count;unsigned skip = i->iov_offset;unsigned k;for (k = 0; k < i->nr_segs; k++, skip = 0) {size_t len = i->bvec[k].bv_len - skip;res |= (unsigned long)i->bvec[k].bv_offset + skip;if (len > size)len = size;res |= len;size -= len;if (!size)break;}return res;}unsigned long iov_iter_alignment(const struct iov_iter  i){if (likely(iter_is_ubuf(i))) {size_t size = i->count;if (size)return ((unsigned long)i->ubuf + i->iov_offset) | size;return 0;}  iovec and kvec have identical layouts ", "if (likely(iter_is_iovec(i) || iov_iter_is_kvec(i)))return iov_npages(i, maxpages);if (iov_iter_is_bvec(i))return bvec_npages(i, maxpages);if (iov_iter_is_xarray(i)) ": "iov_iter_npages(const struct iov_iter  i, int maxpages){if (unlikely(!i->count))return 0;if (likely(iter_is_ubuf(i))) {unsigned offs = offset_in_page(i->ubuf + i->iov_offset);int npages = DIV_ROUND_UP(offs + i->count, PAGE_SIZE);return min(npages, maxpages);}  iovec and kvec have identical layouts ", "return new->__iov = kmemdup(new->__iov,   new->nr_segs * sizeof(struct iovec),   flags);return NULL;}EXPORT_SYMBOL(dup_iter": "dup_iter(struct iov_iter  new, struct iov_iter  old, gfp_t flags){ new =  old;if (iov_iter_is_bvec(new))return new->bvec = kmemdup(new->bvec,    new->nr_segs   sizeof(struct bio_vec),    flags);else if (iov_iter_is_kvec(new) || iter_is_iovec(new))  iovec and kvec have identical layout ", "for (seg = 0; seg < nr_segs; seg++) ": "import_iovec_ubuf(int type, const struct iovec __user  uvec,   struct iovec   iovp, struct iov_iter  i,   bool compat){struct iovec  iov =  iovp;ssize_t ret;if (compat)ret = copy_compat_iovec_from_user(iov, uvec, 1);elseret = copy_iovec_from_user(iov, uvec, 1);if (unlikely(ret))return ret;ret = import_ubuf(type, iov->iov_base, iov->iov_len, i);if (unlikely(ret))return ret; iovp = NULL;return i->count;}ssize_t __import_iovec(int type, const struct iovec __user  uvec, unsigned nr_segs, unsigned fast_segs, struct iovec   iovp, struct iov_iter  i, bool compat){ssize_t total_len = 0;unsigned long seg;struct iovec  iov;if (nr_segs == 1)return __import_iovec_ubuf(type, uvec, iovp, i, compat);iov = iovec_from_user(uvec, nr_segs, fast_segs,  iovp, compat);if (IS_ERR(iov)) { iovp = NULL;return PTR_ERR(iov);}    According to the Single Unix Specification we should return EINVAL if   an element length is < 0 when cast to ssize_t or if the total length   would overflow the ssize_t return value of the system call.     Linux caps all readwrite calls to MAX_RW_COUNT, and avoids the   overflow case. ", "BUG_ON(count > num_queued - dql->num_completed);completed = dql->num_completed + count;limit = dql->limit;ovlimit = POSDIFF(num_queued - dql->num_completed, limit);inprogress = num_queued - completed;prev_inprogress = dql->prev_num_queued - dql->num_completed;all_prev_completed = AFTER_EQ(completed, dql->prev_num_queued);if ((ovlimit && !inprogress) ||    (dql->prev_ovlimit && all_prev_completed)) ": "dql_completed(struct dql  dql, unsigned int count){unsigned int inprogress, prev_inprogress, limit;unsigned int ovlimit, completed, num_queued;bool all_prev_completed;num_queued = READ_ONCE(dql->num_queued);  Can't complete more than what's in queue ", "dql->limit = 0;dql->num_queued = 0;dql->num_completed = 0;dql->last_obj_cnt = 0;dql->prev_num_queued = 0;dql->prev_last_obj_cnt = 0;dql->prev_ovlimit = 0;dql->lowest_slack = UINT_MAX;dql->slack_start_time = jiffies;}EXPORT_SYMBOL(dql_reset": "dql_reset(struct dql  dql){  Reset all dynamic values ", "unsigned int cpumask_next_wrap(int n, const struct cpumask *mask, int start, bool wrap)": "cpumask_next_wrap - helper to implement for_each_cpu_wrap   @n: the cpu prior to the place to search   @mask: the cpumask pointer   @start: the start point of the iteration   @wrap: assume @n crossing @start terminates the iteration     Returns >= nr_cpu_ids on completion     Note: the @wrap argument is required for the start condition when   we cannot assume @start is set in @mask. ", "bool alloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node)": "alloc_cpumask_var_node - allocate a struct cpumask on a given node   @mask: pointer to cpumask_var_t where the cpumask is returned   @flags: GFP_ flags   @node: memory node from which to allocate or %NUMA_NO_NODE     Only defined when CONFIG_CPUMASK_OFFSTACK=y, otherwise is   a nop returning a constant 1 (in <linuxcpumask.h>)   Returns TRUE if memory allocation succeeded, FALSE otherwise.     In addition, mask will be NULL if this fails.  Note that gcc is   usually smart enough to know that mask can never be NULL if   CONFIG_CPUMASK_OFFSTACK=n, so does code elimination in that case   too. ", "void free_cpumask_var(cpumask_var_t mask)": "free_cpumask_var - frees memory allocated for a struct cpumask.   @mask: cpumask to free     This is safe on a NULL mask. ", "unsigned int cpumask_local_spread(unsigned int i, int node)": "cpumask_local_spread - select the i'th cpu based on NUMA distances   @i: index number   @node: local numa_node     Returns online CPU according to a numa aware policy; local cpus are returned   first, followed by non-local ones, then it wraps around.     For those who wants to enumerate all CPUs based on their NUMA distances,   i.e. call this function in a loop, like:     for (i = 0; i < num_online_cpus(); i++) {  cpu = cpumask_local_spread(i, node);  do_something(cpu);   }     There's a better alternative based on for_each()-like iterators:    for_each_numa_hop_mask(mask, node) {  for_each_cpu_andnot(cpu, mask, prev)  do_something(cpu);  prev = mask;  }     It's simpler and more verbose than above. Complexity of iterator-based   enumeration is O(sched_domains_numa_levels   nr_cpu_ids), while   cpumask_local_spread() when called for each cpu is   O(sched_domains_numa_levels   nr_cpu_ids   log(nr_cpu_ids)). ", "unsigned int cpumask_any_and_distribute(const struct cpumask *src1p,       const struct cpumask *src2p)": "cpumask_any_and_distribute - Return an arbitrary cpu within src1p & src2p.   @src1p: first &cpumask for intersection   @src2p: second &cpumask for intersection     Iterated calls using the same srcp1 and srcp2 will be distributed within   their intersection.     Returns >= nr_cpu_ids if the intersection is empty. ", "prev = __this_cpu_read(distribute_cpu_mask_prev);next = find_next_bit_wrap(cpumask_bits(srcp), nr_cpumask_bits, prev + 1);if (next < nr_cpu_ids)__this_cpu_write(distribute_cpu_mask_prev, next);return next;}EXPORT_SYMBOL(cpumask_any_distribute": "cpumask_any_distribute(const struct cpumask  srcp){unsigned int next, prev;  NOTE: our first selection will skip 0. ", "void __iomem *pci_iomap_range(struct pci_dev *dev,      int bar,      unsigned long offset,      unsigned long maxlen)": "pci_iomap_range - create a virtual mapping cookie for a PCI BAR   @dev: PCI device that owns the BAR   @bar: BAR number   @offset: map memory at the given offset in BAR   @maxlen: max length of the memory to map     Using this function you will get a __iomem address to your device BAR.   You can access it using ioread () and iowrite (). These functions hide   the details if this is a MMIO or PIO address space and will just do what   you expect from them in the correct way.     @maxlen specifies the maximum length to map. If you want to get access to   the complete BAR from offset to the end, pass %0 here.   ", "#if defined(ARCH_WANTS_GENERIC_PCI_IOUNMAP)void pci_iounmap(struct pci_dev *dev, void __iomem *p)": "pci_iounmap() somewhat illogically comes from libiomap.c for the   CONFIG_GENERIC_IOMAP case, because that's the code that knows about   the different IOMAP ranges.     But if the architecture does not use the generic iomap code, and if   it has _not_ defined it's own private pci_iounmap function, we define   it here.     NOTE! This default implementation assumes that if the architecture   support ioport mapping (HAS_IOPORT_MAP), the ioport mapping will   be fixed to the range [ PCI_IOBASE, PCI_IOBASE+IO_SPACE_LIMIT [,   and does not need unmapping with 'ioport_unmap()'.     If you have different rules for your architecture, you need to   implement your own pci_iounmap() that knows the rules for where   and how IO vs MEM get mapped.     This code is odd, and the ARCH_HASARCH_WANTS #define logic comes   from legacy <asm-genericio.h> header file behavior. In particular,   it would seem to make sense to do the iounmap(p) for the non-IO-space   case here regardless, but that's not what the old header file code   did. Probably incorrectly, but this is meant to be bug-for-bug   compatible. ", "const void *objagg_obj_root_priv(const struct objagg_obj *objagg_obj)": "objagg_obj_root_priv - obtains root private for an object   @objagg_obj:objagg object instance     Note: all locking must be provided by the caller.     Either the object is root itself when the private is returned   directly, or the parent is root and its private is returned   instead.     Returns a user private root pointer. ", "const void *objagg_obj_delta_priv(const struct objagg_obj *objagg_obj)": "objagg_obj_delta_priv - obtains delta private for an object   @objagg_obj:objagg object instance     Note: all locking must be provided by the caller.     Returns user private delta pointer or NULL in case the passed   object is root. ", "const void *objagg_obj_raw(const struct objagg_obj *objagg_obj)": "objagg_obj_get() by \"obj\" arg. ", "if (!objagg->hints) ": "objagg_obj_put(struct objagg  objagg,     struct objagg_obj  objagg_obj);static void objagg_obj_parent_unassign(struct objagg  objagg,       struct objagg_obj  objagg_obj){trace_objagg_obj_parent_unassign(objagg, objagg_obj, objagg_obj->parent, objagg_obj->parent->refcount);objagg->ops->delta_destroy(objagg->priv, objagg_obj->delta_priv);__objagg_obj_put(objagg, objagg_obj->parent);}static int objagg_obj_root_id_alloc(struct objagg  objagg,    struct objagg_obj  objagg_obj,    struct objagg_hints_node  hnode){unsigned int min, max;int root_id;  In case there are no hints available, the root id is invalid. ", "struct objagg *objagg_create(const struct objagg_ops *ops,     struct objagg_hints *objagg_hints, void *priv)": "objagg_create - creates a new objagg instance   @ops:user-specific callbacks   @objagg_hints:hints, can be NULL   @priv:pointer to a private data passed to the ops     Note: all locking must be provided by the caller.     The purpose of the library is to provide an infrastructure to   aggregate user-specified objects. Library does not care about the type   of the object. User fills-up ops which take care of the specific   user object manipulation.     As a very stupid example, consider integer numbers. For example   number 8 as a root object. That can aggregate number 9 with delta 1,   number 10 with delta 2, etc. This example is implemented as   a part of a testing module in test_objagg.c file.     Each objagg instance contains multiple trees. Each tree node is   represented by \"an object\". In the current implementation there can be   only roots and leafs nodes. Leaf nodes are called deltas.   But in general, this can be easily extended for intermediate nodes.   In that extension, a delta would be associated with all non-root   nodes.     Returns a pointer to newly created objagg instance in case of success,   otherwise it returns pointer error using ERR_PTR macro. ", "void objagg_destroy(struct objagg *objagg)": "objagg_destroy - destroys a new objagg instance   @objagg:objagg instance     Note: all locking must be provided by the caller. ", "const struct objagg_stats *objagg_stats_get(struct objagg *objagg)": "objagg_hints_put(objagg->hints);kfree(objagg);}EXPORT_SYMBOL(objagg_destroy);static int objagg_stats_info_sort_cmp_func(const void  a, const void  b){const struct objagg_obj_stats_info  stats_info1 = a;const struct objagg_obj_stats_info  stats_info2 = b;if (stats_info1->is_root != stats_info2->is_root)return stats_info2->is_root - stats_info1->is_root;if (stats_info1->stats.delta_user_count !=    stats_info2->stats.delta_user_count)return stats_info2->stats.delta_user_count -       stats_info1->stats.delta_user_count;return stats_info2->stats.user_count - stats_info1->stats.user_count;}     objagg_stats_get - obtains stats of the objagg instance   @objagg:objagg instance     Note: all locking must be provided by the caller.     The returned structure contains statistics of all object   currently in use, ordered by following rules:   1) Root objects are always on lower indexes than the rest.   2) Objects with higher delta user count are always on lower      indexes.   3) In case more objects have the same delta user count,      the objects are ordered by user count.     Returns a pointer to stats instance in case of success,   otherwise it returns pointer error using ERR_PTR macro. ", "void objagg_stats_put(const struct objagg_stats *objagg_stats)": "objagg_stats_put - puts stats of the objagg instance   @objagg_stats:objagg instance stats     Note: all locking must be provided by the caller. ", "struct objagg_hints *objagg_hints_get(struct objagg *objagg,      enum objagg_opt_algo_type opt_algo_type)": "objagg_hints_get - obtains hints instance   @objagg:objagg instance   @opt_algo_type:type of hints finding algorithm     Note: all locking must be provided by the caller.     According to the algo type, the existing objects of objagg instance   are going to be went-through to assemble an optimal tree. We call this   tree hints. These hints can be later on used for creation of   a new objagg instance. There, the future object creations are going   to be consulted with these hints in order to find out, where exactly   the new object should be put as a root or delta.     Returns a pointer to hints instance in case of success,   otherwise it returns pointer error using ERR_PTR macro. ", "const struct objagg_stats *objagg_hints_stats_get(struct objagg_hints *objagg_hints)": "objagg_hints_stats_get - obtains stats of the hints instance   @objagg_hints:hints instance     Note: all locking must be provided by the caller.     The returned structure contains statistics of all objects   currently in use, ordered by following rules:   1) Root objects are always on lower indexes than the rest.   2) Objects with higher delta user count are always on lower      indexes.   3) In case multiple objects have the same delta user count,      the objects are ordered by user count.     Returns a pointer to stats instance in case of success,   otherwise it returns pointer error using ERR_PTR macro. ", "#include <linux/types.h>#include <linux/module.h>#include <linux/crc16.h>/** CRC table for the CRC-16. The poly is 0x8005 (x^16 + x^15 + x^2 + 1) ": "crc16.c ", "_C,_C|_S,_C|_S,_C|_S,_C|_S,_C|_S,_C,_C,/* 8-15 ": "_ctype[] = {_C,_C,_C,_C,_C,_C,_C,_C,  0-7 ", "bool refcount_dec_if_one(refcount_t *r)": "refcount_dec_if_one - decrement a refcount if it is 1   @r: the refcount     No atomic_t counterpart, it attempts a 1 -> 0 transition and returns the   success thereof.     Like all decrement operations, it provides release memory order and provides   a control dependency.     It can be used like a try-delete operator; this explicit case is provided   and not cmpxchg in generic, because that would allow implementing unsafe   operations.     Return: true if the resulting refcount is 0, false otherwise ", "bool refcount_dec_not_one(refcount_t *r)": "refcount_dec_not_one - decrement a refcount if it is not 1   @r: the refcount     No atomic_t counterpart, it decrements unless the value is 1, in which case   it will return false.     Was often done like: atomic_add_unless(&var, -1, 1)     Return: true if the decrement operation was successful, false otherwise ", "bool refcount_dec_and_mutex_lock(refcount_t *r, struct mutex *lock)": "refcount_dec_and_mutex_lock - return holding mutex if able to decrement                                 refcount to 0   @r: the refcount   @lock: the mutex to be locked     Similar to atomic_dec_and_mutex_lock(), it will WARN on underflow and fail   to decrement when saturated at REFCOUNT_SATURATED.     Provides release memory ordering, such that prior loads and stores are done   before, and provides a control dependency such that free() must come after.   See the comment on top.     Return: true and hold mutex if able to decrement refcount to 0, false           otherwise ", "bool refcount_dec_and_lock(refcount_t *r, spinlock_t *lock)": "refcount_dec_and_lock - return holding spinlock if able to decrement                           refcount to 0   @r: the refcount   @lock: the spinlock to be locked     Similar to atomic_dec_and_lock(), it will WARN on underflow and fail to   decrement when saturated at REFCOUNT_SATURATED.     Provides release memory ordering, such that prior loads and stores are done   before, and provides a control dependency such that free() must come after.   See the comment on top.     Return: true and hold spinlock if able to decrement refcount to 0, false           otherwise ", "bool refcount_dec_and_lock_irqsave(refcount_t *r, spinlock_t *lock,   unsigned long *flags)": "refcount_dec_and_lock_irqsave - return holding spinlock with disabled                                   interrupts if able to decrement refcount to 0   @r: the refcount   @lock: the spinlock to be locked   @flags: saved IRQ-flags if the is acquired     Same as refcount_dec_and_lock() above except that the spinlock is acquired   with disabled interrupts.     Return: true and hold spinlock if able to decrement refcount to 0, false           otherwise ", "struct parman *parman_create(const struct parman_ops *ops, void *priv)": "parman_create - creates a new parman instance   @ops:caller-specific callbacks   @priv:pointer to a private data passed to the ops     Note: all locking must be provided by the caller.     Each parman instance manages an array area with chunks of entries   with the same priority. Consider following example:     item 1 with prio 10   item 2 with prio 10   item 3 with prio 10   item 4 with prio 20   item 5 with prio 20   item 6 with prio 30   item 7 with prio 30   item 8 with prio 30     In this example, there are 3 priority chunks. The order of the priorities   matters, however the order of items within a single priority chunk does not   matter. So the same array could be ordered as follows:     item 2 with prio 10   item 3 with prio 10   item 1 with prio 10   item 5 with prio 20   item 4 with prio 20   item 7 with prio 30   item 8 with prio 30   item 6 with prio 30     The goal of parman is to maintain the priority ordering. The caller   provides @ops with callbacks parman uses to move the items   and resize the array area.     Returns a pointer to newly created parman instance in case of success,   otherwise it returns NULL. ", "void parman_destroy(struct parman *parman)": "parman_destroy - destroys existing parman instance   @parman:parman instance     Note: all locking must be provided by the caller. ", "void parman_prio_init(struct parman *parman, struct parman_prio *prio,      unsigned long priority)": "parman_prio_init - initializes a parman priority chunk   @parman:parman instance   @prio:parman prio structure to be initialized   @priority:desired priority of the chunk     Note: all locking must be provided by the caller.     Before caller could add an item with certain priority, he has to   initialize a priority chunk for it using this function. ", "void parman_prio_fini(struct parman_prio *prio)": "parman_prio_fini - finalizes use of parman priority chunk   @prio:parman prio structure     Note: all locking must be provided by the caller. ", "int parman_item_add(struct parman *parman, struct parman_prio *prio,    struct parman_item *item)": "parman_item_add - adds a parman item under defined priority   @parman:parman instance   @prio:parman prio instance to add the item to   @item:parman item instance     Note: all locking must be provided by the caller.     Adds item to a array managed by parman instance under the specified priority.     Returns 0 in case of success, negative number to indicate an error. ", "void parman_item_remove(struct parman *parman, struct parman_prio *prio,struct parman_item *item)": "parman_item_remove - deletes parman item   @parman:parman instance   @prio:parman prio instance to delete the item from   @item:parman item instance     Note: all locking must be provided by the caller. ", "int packing(void *pbuf, u64 *uval, int startbit, int endbit, size_t pbuflen,    enum packing_op op, u8 quirks)": "packing.h>#include <linuxmodule.h>#include <linuxbitops.h>#include <linuxerrno.h>#include <linuxtypes.h>#include <linuxbitrev.h>static int get_le_offset(int offset){int closest_multiple_of_4;closest_multiple_of_4 = (offset  4)   4;offset -= closest_multiple_of_4;return closest_multiple_of_4 + (3 - offset);}static int get_reverse_lsw32_offset(int offset, size_t len){int closest_multiple_of_4;int word_index;word_index = offset  4;closest_multiple_of_4 = word_index   4;offset -= closest_multiple_of_4;word_index = (len  4) - word_index - 1;return word_index   4 + offset;}static void adjust_for_msb_right_quirk(u64  to_write, int  box_start_bit,       int  box_end_bit, u8  box_mask){int box_bit_width =  box_start_bit -  box_end_bit + 1;int new_box_start_bit, new_box_end_bit; to_write >>=  box_end_bit; to_write = bitrev8( to_write) >> (8 - box_bit_width); to_write <<=  box_end_bit;new_box_end_bit   = box_bit_width -  box_start_bit - 1;new_box_start_bit = box_bit_width -  box_end_bit - 1; box_mask = GENMASK_ULL(new_box_start_bit, new_box_end_bit); box_start_bit = new_box_start_bit; box_end_bit   = new_box_end_bit;}     packing - Convert numbers (currently u64) between a packed and an unpacked       format. Unpacked means laid out in memory in the CPU's native       understanding of integers, while packed means anything else that       requires translation.     @pbuf: Pointer to a buffer holding the packed value.   @uval: Pointer to an u64 holding the unpacked value.   @startbit: The index (in logical notation, compensated for quirks) where        the packed value starts within pbuf. Must be larger than, or        equal to, endbit.   @endbit: The index (in logical notation, compensated for quirks) where      the packed value ends within pbuf. Must be smaller than, or equal      to, startbit.   @pbuflen: The length in bytes of the packed buffer pointed to by @pbuf.   @op: If PACK, then uval will be treated as const pointer and copied (packed)  into pbuf, between startbit and endbit.  If UNPACK, then pbuf will be treated as const pointer and the logical  value between startbit and endbit will be copied (unpacked) to uval.   @quirks: A bit mask of QUIRK_LITTLE_ENDIAN, QUIRK_LSW32_IS_FIRST and      QUIRK_MSB_ON_THE_RIGHT.     Return: 0 on success, EINVAL or ERANGE if called incorrectly. Assuming     correct usage, return code may be discarded.     If op is PACK, pbuf is modified.     If op is UNPACK, uval is modified. ", "                        return 0;                a++;                b++;                len--;        }}EXPORT_SYMBOL(ucs2_strncmp": "ucs2_strncmp(const ucs2_char_t  a, const ucs2_char_t  b, size_t len){        while (1) {                if (len == 0)                        return 0;                if ( a <  b)                        return -1;                if ( a >  b)                        return 1;                if ( a == 0)   implies  b == 0 ", "#include <crypto/hash.h>#include <linux/err.h>#include <linux/init.h>#include <linux/kernel.h>#include <linux/module.h>#include <linux/crc32c.h>static struct crypto_shash *tfm;u32 crc32c(u32 crc, const void *address, unsigned int length)": "crc32c, but hopefully may be able to use this one are:    netsctp (please add all your doco to here if you change to              use this one!)    <endoflist>     Copyright (c) 2004 Cisco Systems, Inc. ", "int param_set_dyndbg_classes(const char *instr, const struct kernel_param *kp)": "param_set_dyndbg_classes - class FOO >control   @instr: string echo>d to sysfs, input depends on map_type   @kp:    kp->arg has state: bitslvl, map, map_type     Enabledisable prdbgs by their class, as given in the arguments to   DECLARE_DYNDBG_CLASSMAP.  For LEVEL map-types, enforce relative   levels by bitpos.     Returns: 0 or <0 if error. ", "int param_get_dyndbg_classes(char *buffer, const struct kernel_param *kp)": "param_get_dyndbg_classes - classes reader   @buffer: string description of controlled bits -> classes   @kp:     kp->arg has state: bits, map     Reads last written state, underlying prdbg state may have been   altered by direct >control.  Displays 0x for DISJOINT, 0-N for   LEVEL Returns: #chars written or <0 on error ", "void __iomem *ioremap(phys_addr_t offset, size_t size)": "iounmap(volatile void __iomem  addr){WARN(1, \"invalid iounmap for addr 0x%llx\\n\",     (unsigned long long)(uintptr_t __force)addr);}#endif   CONFIG_INDIRECT_IOMEM_FALLBACK ", "#define MAKE_OP(op, sz) \\u##sz __raw_read ## op(const volatile void __iomem *addr)\\": "memcpy_toio(volatile void __iomem  addr, const void  buffer,     size_t size){WARN(1, \"Invalid memcpy_toio at address 0x%llx\\n\",     (unsigned long long)(uintptr_t __force)addr);}#endif   CONFIG_INDIRECT_IOMEM_FALLBACK ", "int idr_alloc_cyclic(struct idr *idr, void *ptr, int start, int end, gfp_t gfp)": "idr_alloc_cyclic() - Allocate an ID cyclically.   @idr: IDR handle.   @ptr: Pointer to be associated with the new ID.   @start: The minimum ID (inclusive).   @end: The maximum ID (exclusive).   @gfp: Memory allocation flags.     Allocates an unused ID in the range specified by @nextid and @end.  If   @end is <= 0, it is treated as one larger than %INT_MAX.  This allows   callers to use @start + N as @end as long as N is within integer range.   The search for an unused ID will start at the last ID allocated and will   wrap around to @start if no free IDs are found before reaching @end.     The caller should provide their own locking to ensure that two   concurrent modifications to the IDR are not possible.  Read-only   accesses to the IDR may be done under the RCU read lock or may   exclude simultaneous writers.     Return: The newly allocated ID, -ENOMEM if memory allocation failed,   or -ENOSPC if no free IDs could be found. ", "int idr_for_each(const struct idr *idr,int (*fn)(int id, void *p, void *data), void *data)": "idr_for_each() - Iterate through all stored pointers.   @idr: IDR handle.   @fn: Function to be called for each pointer.   @data: Data passed to callback function.     The callback function will be called for each entry in @idr, passing   the ID, the entry and @data.     If @fn returns anything other than %0, the iteration stops and that   value is returned from this function.     idr_for_each() can be called concurrently with idr_alloc() and   idr_remove() if protected by RCU.  Newly added entries may not be   seen and deleted entries may be seen, but adding and removing entries   will not cause other entries to be skipped, nor spurious ones to be seen. ", "void *idr_get_next_ul(struct idr *idr, unsigned long *nextid)": "idr_get_next_ul() - Find next populated entry.   @idr: IDR handle.   @nextid: Pointer to an ID.     Returns the next populated entry in the tree with an ID greater than   or equal to the value pointed to by @nextid.  On exit, @nextid is updated   to the ID of the found value.  To use in a loop, the value pointed to by   nextid must be incremented by the user. ", "void *idr_replace(struct idr *idr, void *ptr, unsigned long id)": "idr_replace() - replace pointer for given ID.   @idr: IDR handle.   @ptr: New pointer to associate with the ID.   @id: ID to change.     Replace the pointer registered with an ID and return the old value.   This function can be called under the RCU read lock concurrently with   idr_alloc() and idr_remove() (as long as the ID being removed is not   the one being replaced!).     Returns: the old value on success.  %-ENOENT indicates that @id was not   found.  %-EINVAL indicates that @ptr was not valid. ", "/* * Developer's notes: * * The IDA uses the functionality provided by the XArray to store bitmaps in * each entry.  The XA_FREE_MARK is only cleared when all bits in the bitmap * have been set. * * I considered telling the XArray that each slot is an order-10 node * and indexing by bit number, but the XArray can't allow a single multi-index * entry in the head, which would significantly increase memory consumption * for the IDA.  So instead we divide the index by the number of bits in the * leaf bitmap before doing a radix tree lookup. * * As an optimisation, if there are only a few low bits set in any given * leaf, instead of allocating a 128-byte bitmap, we store the bits * as a value entry.  Value entries never have the XA_FREE_MARK cleared * because we can always convert them into a bitmap entry. * * It would be possible to optimise further; once we've run out of a * single 128-byte bitmap, we currently switch to a 576-byte node, put * the 128-byte bitmap in the first entry and then start allocating extra * 128-byte entries.  We could instead use the 512 bytes of the node's * data as a bitmap before moving to that scheme.  I do not believe this * is a worthwhile optimisation; Rasmus Villemoes surveyed the current * users of the IDA and almost none of them use more than 1024 entries. * Those that do use more than the 8192 IDs that the 512 bytes would * provide. * * The IDA always uses a lock to alloc/free.  If we add a 'test_bit' * equivalent, it will still need locking.  Going to RCU lookup would require * using RCU to free bitmaps, and that's not trivial without embedding an * RCU head in the bitmap, which adds a 2-pointer overhead to each 128-byte * bitmap, which is excessive. ": "ida_destroy() can be used to dispose of an IDA without needing to   free the individual IDs in it.  You can use ida_is_empty() to find   out whether the IDA has any IDs currently allocated.     The IDA handles its own locking.  It is safe to call any of the IDA   functions without synchronisation in your code.     IDs are currently limited to the range [0-INT_MAX].  If this is an awkward   limitation, it should be quite straightforward to raise the maximum. ", "void lockref_get(struct lockref *lockref)": "lockref_get - Increments reference count unconditionally   @lockref: pointer to lockref structure     This operation is only valid if you already hold a reference   to the object, so you know the count cannot be zero. ", "int lockref_get_not_zero(struct lockref *lockref)": "lockref_get_not_zero - Increments count unless the count is 0 or dead   @lockref: pointer to lockref structure   Return: 1 if count updated successfully or 0 if count was zero ", "int lockref_put_not_zero(struct lockref *lockref)": "lockref_put_not_zero - Decrements count unless count <= 1 before decrement   @lockref: pointer to lockref structure   Return: 1 if count updated successfully or 0 if count would become zero ", "int lockref_put_return(struct lockref *lockref)": "lockref_put_return - Decrement reference count if possible   @lockref: pointer to lockref structure     Decrement the reference count and return the new value.   If the lockref was dead or locked, return an error. ", "int lockref_put_or_lock(struct lockref *lockref)": "lockref_put_or_lock - decrements count unless count <= 1 before decrement   @lockref: pointer to lockref structure   Return: 1 if count updated successfully or 0 if count <= 1 and lock taken ", "void lockref_mark_dead(struct lockref *lockref)": "lockref_mark_dead - mark lockref dead   @lockref: pointer to lockref structure ", "int lockref_get_not_dead(struct lockref *lockref)": "lockref_get_not_dead - Increments count unless the ref is dead   @lockref: pointer to lockref structure   Return: 1 if count updated successfully or 0 if lockref was dead ", "void crc8_populate_msb(u8 table[CRC8_TABLE_SIZE], u8 polynomial)": "crc8.h>#include <linuxprintk.h>     crc8_populate_msb - fill crc table for given polynomial in reverse bit order.     @table:table to be filled.   @polynomial:polynomial for which table is to be filled. ", "void crc8_populate_lsb(u8 table[CRC8_TABLE_SIZE], u8 polynomial)": "crc8_populate_lsb - fill crc table for given polynomial in regular bit order.     @table:table to be filled.   @polynomial:polynomial for which table is to be filled. ", "w.s.high =    uu.s.high >> 31;w.s.low = uu.s.high >> -bm;} else ": "__ashrdi3(long long u, word_type b){DWunion uu, w;word_type bm;if (b == 0)return u;uu.ll = u;bm = 32 - b;if (bm <= 0) {  w.s.high = 1..1 or 0..0 ", "int kobject_set_name_vargs(struct kobject *kobj, const char *fmt,  va_list vargs)": "kobject_set_name_vargs() - Set the name of a kobject.   @kobj: struct kobject to set the name of   @fmt: format string used to build the name   @vargs: vargs to format the string. ", "if (kobj->kset) ": "kobject_add_internal(struct kobject  kobj){int error = 0;struct kobject  parent;if (!kobj)return -ENOENT;if (!kobj->name || !kobj->name[0]) {WARN(1,     \"kobject: (%p): attempted to be registered with empty name!\\n\",     kobj);return -EINVAL;}parent = kobject_get(kobj->parent);  join kset if set, use it as parent if we do not already have one ", "if (kobj->state_add_uevent_sent && !kobj->state_remove_uevent_sent) ": "kobject_del(struct kobject  kobj){struct kernfs_node  sd;const struct kobj_type  ktype;sd = kobj->sd;ktype = get_ktype(kobj);if (ktype)sysfs_remove_groups(kobj, ktype->default_groups);  send \"remove\" if the caller did not do it but sent \"add\" ", "void kobject_get_ownership(const struct kobject *kobj, kuid_t *uid, kgid_t *gid)": "kobject_get_ownership() - Get sysfs ownership data for @kobj.   @kobj: kobject in question   @uid: kernel user ID for sysfs objects   @gid: kernel group ID for sysfs objects     Returns initial uidgid pair that should be used when creating sysfs   representation of given kobject. Normally used to adjust ownership of   objects in a container. ", "if (error == -EEXIST)pr_err(\"%s failed for %s with -EEXIST, don't try to register things with the same name in the same directory.\\n\",       __func__, kobject_name(kobj));elsepr_err(\"%s failed for %s (error: %d parent: %s)\\n\",       __func__, kobject_name(kobj), error,       parent ? kobject_name(parent) : \"'none'\");} elsekobj->state_in_sysfs = 1;return error;}/** * kobject_set_name_vargs() - Set the name of a kobject. * @kobj: struct kobject to set the name of * @fmt: format string used to build the name * @vargs: vargs to format the string. ": "kobject_put(parent);kobj->parent = NULL;  be noisy on error issues ", "int kset_register(struct kset *k)": "kset_register() - Initialize and add a kset.   @k: kset.     NOTE: On error, the kset.kobj.name allocated by() kobj_set_name()   is freed, it can not be used any more. ", "void kset_unregister(struct kset *k)": "kset_unregister() - Remove a kset.   @k: kset. ", "int hex_to_bin(unsigned char ch)": "hex_to_bin - convert a hex digit to its real value   @ch: ascii character represents hex digit     hex_to_bin() converts one hex digit to its actual value or -1 in case of bad   input.     This function is used to load cryptographic keys, so it is coded in such a   way that there are no conditions or memory accesses that depend on data.     Explanation of the logic:   (ch - '9' - 1) is negative if ch <= '9'   ('0' - 1 - ch) is negative if ch >= '0'   we \"and\" these two values, so the result is negative if ch is in the range  '0' ... '9'   we are only interested in the sign, so we do a shift \">> 8\"; note that right  shift of a negative value is implementation-defined, so we cast the  value to (unsigned) before the shift --- we have 0xffffff if ch is in  the range '0' ... '9', 0 otherwise   we \"and\" this value with (ch - '0' + 1) --- we have a value 1 ... 10 if ch is  in the range '0' ... '9', 0 otherwise   we add this value to -1 --- we have a value 0 ... 9 if ch is in the range '0'  ... '9', -1 otherwise   the next line is similar to the previous one, but we need to decode both  uppercase and lowercase letters, so we use (ch & 0xdf), which converts  lowercase to uppercase ", "int hex2bin(u8 *dst, const char *src, size_t count)": "hex2bin - convert an ascii hexadecimal string to its binary representation   @dst: binary result   @src: ascii hexadecimal string   @count: result length     Return 0 on success, -EINVAL in case of bad input. ", "char *bin2hex(char *dst, const void *src, size_t count)": "bin2hex - convert binary data to an ascii hexadecimal string   @dst: ascii hexadecimal result   @src: binary data   @count: binary data length ", "int hex_dump_to_buffer(const void *buf, size_t len, int rowsize, int groupsize,       char *linebuf, size_t linebuflen, bool ascii)": "hex_dump_to_buffer - convert a blob of data to \"hex ASCII\" in memory   @buf: data blob to dump   @len: number of bytes in the @buf   @rowsize: number of bytes to print per line; must be 16 or 32   @groupsize: number of bytes to print at a time (1, 2, 4, 8; default = 1)   @linebuf: where to put the converted data   @linebuflen: total size of @linebuf, including space for terminating NUL   @ascii: include ASCII after the hex output     hex_dump_to_buffer() works on one \"line\" of output at a time, i.e.,   16 or 32 bytes of input data converted to hex + ASCII output.     Given a buffer of u8 data, hex_dump_to_buffer() converts the input data   to a hex + ASCII dump at the supplied memory location.   The converted output is always NUL-terminated.     E.g.:     hex_dump_to_buffer(frame->data, frame->len, 16, 1,  linebuf, sizeof(linebuf), true);     example output buffer:   40 41 42 43 44 45 46 47 48 49 4a 4b 4c 4d 4e 4f  @ABCDEFGHIJKLMNO     Return:   The amount of bytes placed in the buffer without terminating NUL. If the   output was truncated, then the return value is the number of bytes   (excluding the terminating NUL) which would have been written to the final   string if enough space had been available. ", "void print_hex_dump(const char *level, const char *prefix_str, int prefix_type,    int rowsize, int groupsize,    const void *buf, size_t len, bool ascii)": "print_hex_dump - print a text hex dump to syslog for a binary blob of data   @level: kernel log level (e.g. KERN_DEBUG)   @prefix_str: string to prefix each line with;    caller supplies trailing spaces for alignment if desired   @prefix_type: controls whether prefix of an offset, address, or none    is printed (%DUMP_PREFIX_OFFSET, %DUMP_PREFIX_ADDRESS, %DUMP_PREFIX_NONE)   @rowsize: number of bytes to print per line; must be 16 or 32   @groupsize: number of bytes to print at a time (1, 2, 4, 8; default = 1)   @buf: data blob to dump   @len: number of bytes in the @buf   @ascii: include ASCII after the hex output     Given a buffer of u8 data, print_hex_dump() prints a hex + ASCII dump   to the kernel log at the specified kernel log level, with an optional   leading prefix.     print_hex_dump() works on one \"line\" of output at a time, i.e.,   16 or 32 bytes of input data converted to hex + ASCII output.   print_hex_dump() iterates over the entire input @buf, breaking it into   \"line size\" chunks to format and print.     E.g.:     print_hex_dump(KERN_DEBUG, \"raw data: \", DUMP_PREFIX_ADDRESS,      16, 1, frame->data, frame->len, true);     Example output using %DUMP_PREFIX_OFFSET and 1-byte mode:   0009ab42: 40 41 42 43 44 45 46 47 48 49 4a 4b 4c 4d 4e 4f  @ABCDEFGHIJKLMNO   Example output using %DUMP_PREFIX_ADDRESS and 4-byte mode:   ffffffff88089af0: 73727170 77767574 7b7a7978 7f7e7d7c  pqrstuvwxyz{|}~. ", "u64 siphash_1u64(const u64 first, const siphash_key_t *key)": "siphash_1u64 - compute 64-bit siphash PRF value of a u64   @first: first u64   @key: the siphash key ", "u64 siphash_2u64(const u64 first, const u64 second, const siphash_key_t *key)": "siphash_2u64 - compute 64-bit siphash PRF value of 2 u64   @first: first u64   @second: second u64   @key: the siphash key ", "u64 siphash_3u64(const u64 first, const u64 second, const u64 third, const siphash_key_t *key)": "siphash_3u64 - compute 64-bit siphash PRF value of 3 u64   @first: first u64   @second: second u64   @third: third u64   @key: the siphash key ", "u64 siphash_4u64(const u64 first, const u64 second, const u64 third, const u64 forth, const siphash_key_t *key)": "siphash_4u64 - compute 64-bit siphash PRF value of 4 u64   @first: first u64   @second: second u64   @third: third u64   @forth: forth u64   @key: the siphash key ", "u32 hsiphash_1u32(const u32 first, const hsiphash_key_t *key)": "hsiphash_1u32 - compute 64-bit hsiphash PRF value of a u32   @first: first u32   @key: the hsiphash key ", "u32 hsiphash_2u32(const u32 first, const u32 second, const hsiphash_key_t *key)": "hsiphash_2u32 - compute 32-bit hsiphash PRF value of 2 u32   @first: first u32   @second: second u32   @key: the hsiphash key ", "u32 hsiphash_3u32(const u32 first, const u32 second, const u32 third,  const hsiphash_key_t *key)": "hsiphash_3u32 - compute 32-bit hsiphash PRF value of 3 u32   @first: first u32   @second: second u32   @third: third u32   @key: the hsiphash key ", "u32 hsiphash_4u32(const u32 first, const u32 second, const u32 third,  const u32 forth, const hsiphash_key_t *key)": "hsiphash_4u32 - compute 32-bit hsiphash PRF value of 4 u32   @first: first u32   @second: second u32   @third: third u32   @forth: forth u32   @key: the hsiphash key ", "int textsearch_register(struct ts_ops *ops)": "textsearch_register - register a textsearch module   @ops: operations lookup table     This function must be called by textsearch modules to announce   their presence. The specified &@ops must have %name set to a   unique identifier and the callbacks find(), init(), get_pattern(),   and get_pattern_len() must be implemented.     Returns 0 or -EEXISTS if another module has already registered   with same name. ", "int textsearch_unregister(struct ts_ops *ops)": "textsearch_unregister - unregister a textsearch module   @ops: operations lookup table     This function must be called by textsearch modules to announce   their disappearance for examples when the module gets unloaded.   The &ops parameter must be the same as the one during the   registration.     Returns 0 on success or -ENOENT if no matching textsearch   registration was found. ", "/* ========================================================================== ": "textsearch_destroy().     (8) Core notifies the algorithm to destroy algorithm specific         allocations. (Optional)     USAGE       Before a search can be performed, a configuration must be created     by calling textsearch_prepare() specifying the searching algorithm,     the pattern to look for and flags. As a flag, you can set TS_IGNORECASE     to perform case insensitive matching. But it might slow down     performance of algorithm, so you should use it at own your risk.     The returned configuration may then be used for an arbitrary     amount of times and even in parallel as long as a separate struct     ts_state variable is provided to every instance.       The actual search is performed by either calling     textsearch_find_continuous() for linear data or by providing     an own get_next_block() implementation and     calling textsearch_find(). Both functions return     the position of the first occurrence of the pattern or UINT_MAX if     no match was found. Subsequent occurrences can be found by calling     textsearch_next() regardless of the linearity of the data.       Once you're done using a configuration it must be given back via     textsearch_destroy.     EXAMPLE::       int pos;     struct ts_config  conf;     struct ts_state state;     const char  pattern = \"chicken\";     const char  example = \"We dance the funky chicken\";       conf = textsearch_prepare(\"kmp\", pattern, strlen(pattern),                               GFP_KERNEL, TS_AUTOLOAD);     if (IS_ERR(conf)) {         err = PTR_ERR(conf);         goto errout;     }       pos = textsearch_find_continuous(conf, &state, example, strlen(example));     if (pos != UINT_MAX)         panic(\"Oh my god, dancing chickens at %d\\n\", pos);       textsearch_destroy(conf); ", "bool __pure glob_match(char const *pat, char const *str)": "glob_match - Shell-style pattern matching, like !fnmatch(pat, str, 0)   @pat: Shell-style pattern to match, e.g. \" .[ch]\".   @str: String to match.  The pattern must match the entire string.     Perform shell-style glob matching, returning true (1) if the match   succeeds, or false (0) if it fails.  Equivalent to !fnmatch(@pat, @str, 0).     Pattern metacharacters are ?,  , [ and \\.   (And, inside character classes, !, - and ].)     This is small and simple implementation intended for device blacklists   where a string is matched against a number of patterns.  Thus, it   does not preprocess the patterns.  It is non-recursive, and run-time   is at most quadratic: strlen(@str) strlen(@pat).     An example of the worst case is glob_match(\" aaaaa\", \"aaaaaaaaaa\");   it takes 6 passes over the pattern before matching the string.     Like !fnmatch(@pat, @str, 0) and unlike the shell, this does NOT   treat  or leading . specially; it isn't actually used for pathnames.     Note that according to glob(7) (and unlike bash), character classes   are complemented by a leading !; this does not support the regex-style   [^a-z] syntax.     An opening bracket without a matching close is matched literally. ", "static inline u32 __pure crc32_le_generic(u32 crc, unsigned char const *p,  size_t len, const u32 (*tab)[256],  u32 polynomial)": "crc32_le_generic() - Calculate bitwise little-endian Ethernet AUTODIN II  CRC32CRC32C   @crc: seed value for computation.  ~0 for Ethernet, sometimes 0 for other   uses, or the previous crc32crc32c value if computing incrementally.   @p: pointer to buffer over which CRC32CRC32C is run   @len: length of buffer @p   @tab: little-endian Ethernet table   @polynomial: CRC32CRC32c LE polynomial ", "static u32 __attribute_const__ gf2_multiply(u32 x, u32 y, u32 modulus)": "crc32_be_base(u32, unsigned char const  , size_t) __alias(crc32_be);    This multiplies the polynomials x and y modulo the given modulus.   This follows the \"little-endian\" CRC convention that the lsbit   represents the highest power of x, and the msbit represents x^0. ", "kmsan_check_memory(&val, sizeof(val));IO_COND(addr, outb(val,port), writeb(val, addr));}void iowrite16(u16 val, void __iomem *addr)": "iowrite8(u8 val, void __iomem  addr){  Make sure uninitialized memory isn't copied to devices. ", "kmsan_check_memory(&val, sizeof(val));IO_COND(addr, outw(val,port), writew(val, addr));}void iowrite16be(u16 val, void __iomem *addr)": "iowrite16(u16 val, void __iomem  addr){  Make sure uninitialized memory isn't copied to devices. ", "kmsan_check_memory(&val, sizeof(val));IO_COND(addr, pio_write16be(val,port), mmio_write16be(val, addr));}void iowrite32(u32 val, void __iomem *addr)": "iowrite16be(u16 val, void __iomem  addr){  Make sure uninitialized memory isn't copied to devices. ", "kmsan_check_memory(&val, sizeof(val));IO_COND(addr, outl(val,port), writel(val, addr));}void iowrite32be(u32 val, void __iomem *addr)": "iowrite32(u32 val, void __iomem  addr){  Make sure uninitialized memory isn't copied to devices. ", "kmsan_check_memory(&val, sizeof(val));IO_COND(addr, pio_write32be(val,port), mmio_write32be(val, addr));}EXPORT_SYMBOL(iowrite8);EXPORT_SYMBOL(iowrite16);EXPORT_SYMBOL(iowrite16be);EXPORT_SYMBOL(iowrite32);EXPORT_SYMBOL(iowrite32be": "iowrite32be(u32 val, void __iomem  addr){  Make sure uninitialized memory isn't copied to devices. ", "kmsan_check_memory(&val, sizeof(val));IO_COND(addr, pio_write64_lo_hi(val, port),writeq(val, addr));}void iowrite64_hi_lo(u64 val, void __iomem *addr)": "iowrite64_lo_hi(u64 val, void __iomem  addr){  Make sure uninitialized memory isn't copied to devices. ", "kmsan_check_memory(&val, sizeof(val));IO_COND(addr, pio_write64_hi_lo(val, port),writeq(val, addr));}void iowrite64be_lo_hi(u64 val, void __iomem *addr)": "iowrite64_hi_lo(u64 val, void __iomem  addr){  Make sure uninitialized memory isn't copied to devices. ", "kmsan_check_memory(&val, sizeof(val));IO_COND(addr, pio_write64be_lo_hi(val, port),mmio_write64be(val, addr));}void iowrite64be_hi_lo(u64 val, void __iomem *addr)": "iowrite64be_lo_hi(u64 val, void __iomem  addr){  Make sure uninitialized memory isn't copied to devices. ", "kmsan_check_memory(&val, sizeof(val));IO_COND(addr, pio_write64be_hi_lo(val, port),mmio_write64be(val, addr));}EXPORT_SYMBOL(iowrite64_lo_hi);EXPORT_SYMBOL(iowrite64_hi_lo);EXPORT_SYMBOL(iowrite64be_lo_hi);EXPORT_SYMBOL(iowrite64be_hi_lo": "iowrite64be_hi_lo(u64 val, void __iomem  addr){  Make sure uninitialized memory isn't copied to devices. ", "kmsan_unpoison_memory(dst, count);}void ioread16_rep(const void __iomem *addr, void *dst, unsigned long count)": "ioread8_rep(const void __iomem  addr, void  dst, unsigned long count){IO_COND(addr, insb(port,dst,count), mmio_insb(addr, dst, count));  KMSAN must treat values read from devices as initialized. ", "kmsan_unpoison_memory(dst, count * 2);}void ioread32_rep(const void __iomem *addr, void *dst, unsigned long count)": "ioread16_rep(const void __iomem  addr, void  dst, unsigned long count){IO_COND(addr, insw(port,dst,count), mmio_insw(addr, dst, count));  KMSAN must treat values read from devices as initialized. ", "kmsan_unpoison_memory(dst, count * 4);}EXPORT_SYMBOL(ioread8_rep);EXPORT_SYMBOL(ioread16_rep);EXPORT_SYMBOL(ioread32_rep": "ioread32_rep(const void __iomem  addr, void  dst, unsigned long count){IO_COND(addr, insl(port,dst,count), mmio_insl(addr, dst, count));  KMSAN must treat values read from devices as initialized. ", "kmsan_check_memory(src, count);IO_COND(addr, outsb(port, src, count), mmio_outsb(addr, src, count));}void iowrite16_rep(void __iomem *addr, const void *src, unsigned long count)": "iowrite8_rep(void __iomem  addr, const void  src, unsigned long count){  Make sure uninitialized memory isn't copied to devices. ", "kmsan_check_memory(src, count * 2);IO_COND(addr, outsw(port, src, count), mmio_outsw(addr, src, count));}void iowrite32_rep(void __iomem *addr, const void *src, unsigned long count)": "iowrite16_rep(void __iomem  addr, const void  src, unsigned long count){  Make sure uninitialized memory isn't copied to devices. ", "kmsan_check_memory(src, count * 4);IO_COND(addr, outsl(port, src,count), mmio_outsl(addr, src, count));}EXPORT_SYMBOL(iowrite8_rep);EXPORT_SYMBOL(iowrite16_rep);EXPORT_SYMBOL(iowrite32_rep": "iowrite32_rep(void __iomem  addr, const void  src, unsigned long count){  Make sure uninitialized memory isn't copied to devices. ", "}EXPORT_SYMBOL(ioport_map": "ioport_map(unsigned long port, unsigned int nr){return (void __iomem  ) (unsigned long) port;}void ioport_unmap(void __iomem  addr){  Nothing to do ", "}EXPORT_SYMBOL(ioport_map);EXPORT_SYMBOL(ioport_unmap": "ioport_unmap(void __iomem  addr){  Nothing to do ", ", iounmap(addr));}EXPORT_SYMBOL(pci_iounmap": "pci_iounmap(struct pci_dev  dev, void __iomem   addr){IO_COND(addr,   nothing ", "unsafe_get_user(c, (unsigned long __user *)(src+res), byte_at_a_time);/* * Note that we mask out the bytes following the NUL. This is * important to do because string oblivious code may read past * the NUL. For those routines, we don't want to give them * potentially random bytes after the NUL in `src`. * * One example of such code is BPF map keys. BPF treats map keys * as an opaque set of bytes. Without the post-NUL mask, any BPF * maps keyed by strings returned from strncpy_from_user() may * have multiple entries for semantically identical strings. ": "strncpy_from_user(char  dst, const char __user  src,unsigned long count, unsigned long max){const struct word_at_a_time constants = WORD_AT_A_TIME_CONSTANTS;unsigned long res = 0;if (IS_UNALIGNED(src, dst))goto byte_at_a_time;while (max >= sizeof(unsigned long)) {unsigned long c, data, mask;  Fall back to byte-at-a-time if we get a page fault ", "struct xxh32_state state;memset(&state, 0, sizeof(state));state.v1 = seed + PRIME32_1 + PRIME32_2;state.v2 = seed + PRIME32_2;state.v3 = seed + 0;state.v4 = seed - PRIME32_1;memcpy(statePtr, &state, sizeof(state));}EXPORT_SYMBOL(xxh32_reset": "xxh32_reset(struct xxh32_state  statePtr, const uint32_t seed){  use a local state for memcpy() to avoid strict-aliasing warnings ", "struct xxh64_state state;memset(&state, 0, sizeof(state));state.v1 = seed + PRIME64_1 + PRIME64_2;state.v2 = seed + PRIME64_2;state.v3 = seed + 0;state.v4 = seed - PRIME64_1;memcpy(statePtr, &state, sizeof(state));}EXPORT_SYMBOL(xxh64_reset": "xxh64_reset(struct xxh64_state  statePtr, const uint64_t seed){  use a local state for memcpy() to avoid strict-aliasing warnings ", "memcpy((uint8_t *)(state->mem32) + state->memsize, input, len);state->memsize += (uint32_t)len;return 0;}if (state->memsize) ": "xxh32_update(struct xxh32_state  state, const void  input, const size_t len){const uint8_t  p = (const uint8_t  )input;const uint8_t  const b_end = p + len;if (input == NULL)return -EINVAL;state->total_len_32 += (uint32_t)len;state->large_len |= (len >= 16) | (state->total_len_32 >= 16);if (state->memsize + len < 16) {   fill in tmp buffer ", " + PRIME32_5;}h32 += state->total_len_32;while (p + 4 <= b_end) ": "xxh32_digest(const struct xxh32_state  state){const uint8_t  p = (const uint8_t  )state->mem32;const uint8_t  const b_end = (const uint8_t  )(state->mem32) +state->memsize;uint32_t h32;if (state->large_len) {h32 = xxh_rotl32(state->v1, 1) + xxh_rotl32(state->v2, 7) +xxh_rotl32(state->v3, 12) + xxh_rotl32(state->v4, 18);} else {h32 = state->v3   == seed ", "memcpy(((uint8_t *)state->mem64) + state->memsize, input, len);state->memsize += (uint32_t)len;return 0;}if (state->memsize) ": "xxh64_update(struct xxh64_state  state, const void  input, const size_t len){const uint8_t  p = (const uint8_t  )input;const uint8_t  const b_end = p + len;if (input == NULL)return -EINVAL;state->total_len += len;if (state->memsize + len < 32) {   fill in tmp buffer ", "size_t memweight(const void *ptr, size_t bytes)": "memweight - count the total number of bits set in memory area   @ptr: pointer to the start of the area   @bytes: the size of the area ", "static inline int any_tag_set(const struct radix_tree_node *node,unsigned int tag)": "radix_tree_preload, radix_tree_preloads) = {.lock = INIT_LOCAL_LOCK(lock),};EXPORT_PER_CPU_SYMBOL_GPL(radix_tree_preloads);static inline struct radix_tree_node  entry_to_node(void  ptr){return (void  )((unsigned long)ptr & ~RADIX_TREE_INTERNAL_NODE);}static inline void  node_to_entry(void  ptr){return (void  )((unsigned long)ptr | RADIX_TREE_INTERNAL_NODE);}#define RADIX_TREE_RETRYXA_RETRY_ENTRYstatic inline unsigned longget_slot_offset(const struct radix_tree_node  parent, void __rcu   slot){return parent ? slot - parent->slots : 0;}static unsigned int radix_tree_descend(const struct radix_tree_node  parent,struct radix_tree_node   nodep, unsigned long index){unsigned int offset = (index >> parent->shift) & RADIX_TREE_MAP_MASK;void __rcu   entry = rcu_dereference_raw(parent->slots[offset]); nodep = (void  )entry;return offset;}static inline gfp_t root_gfp_mask(const struct radix_tree_root  root){return root->xa_flags & (__GFP_BITS_MASK & ~GFP_ZONEMASK);}static inline void tag_set(struct radix_tree_node  node, unsigned int tag,int offset){__set_bit(offset, node->tags[tag]);}static inline void tag_clear(struct radix_tree_node  node, unsigned int tag,int offset){__clear_bit(offset, node->tags[tag]);}static inline int tag_get(const struct radix_tree_node  node, unsigned int tag,int offset){return test_bit(offset, node->tags[tag]);}static inline void root_tag_set(struct radix_tree_root  root, unsigned tag){root->xa_flags |= (__force gfp_t)(1 << (tag + ROOT_TAG_SHIFT));}static inline void root_tag_clear(struct radix_tree_root  root, unsigned tag){root->xa_flags &= (__force gfp_t)~(1 << (tag + ROOT_TAG_SHIFT));}static inline void root_tag_clear_all(struct radix_tree_root  root){root->xa_flags &= (__force gfp_t)((1 << ROOT_TAG_SHIFT) - 1);}static inline int root_tag_get(const struct radix_tree_root  root, unsigned tag){return (__force int)root->xa_flags & (1 << (tag + ROOT_TAG_SHIFT));}static inline unsigned root_tags_get(const struct radix_tree_root  root){return (__force unsigned)root->xa_flags >> ROOT_TAG_SHIFT;}static inline bool is_idr(const struct radix_tree_root  root){return !!(root->xa_flags & ROOT_IS_IDR);}    Returns 1 if any slot in the node has this tag set.   Otherwise returns 0. ", "local_lock(&radix_tree_preloads.lock);return 0;}EXPORT_SYMBOL(radix_tree_maybe_preload": "radix_tree_maybe_preload(gfp_t gfp_mask){if (gfpflags_allow_blocking(gfp_mask))return __radix_tree_preload(gfp_mask, RADIX_TREE_PRELOAD_SIZE);  Preloading doesn't help anything with this gfp mask, skip it ", "int radix_tree_insert(struct radix_tree_root *root, unsigned long index,void *item)": "radix_tree_insert    -    insert into a radix tree  @root:radix tree root  @index:index key  @item:item to insert    Insert an item into the radix tree at position @index. ", "void __rcu **radix_tree_lookup_slot(const struct radix_tree_root *root,unsigned long index)": "radix_tree_replace_slot, otherwise it must be called  exclusive from other writers. Any dereference of the slot must be done  using radix_tree_deref_slot. ", "void *__radix_tree_lookup(const struct radix_tree_root *root,  unsigned long index, struct radix_tree_node **nodep,  void __rcu ***slotp)": "radix_tree_lookup-lookup an item in a radix tree  @root:radix tree root  @index:index key  @nodep:returns node  @slotp:returns slot    Lookup and return the item at position @index in the radix  tree @root.    Until there is more than one item in the tree, no nodes are  allocated and @root->xa_head is used as a direct slot instead of  pointing to a node, in which case  @nodep will be NULL. ", "void *radix_tree_tag_set(struct radix_tree_root *root,unsigned long index, unsigned int tag)": "radix_tree_tag_set - set a tag on a radix tree node  @root:radix tree root  @index:index key  @tag:tag index    Set the search tag (which must be < RADIX_TREE_MAX_TAGS)  corresponding to @index in the radix tree.  From  the root all the way down to the leaf node.    Returns the address of the tagged item.  Setting a tag on a not-present  item is a bug. ", "void *radix_tree_tag_clear(struct radix_tree_root *root,unsigned long index, unsigned int tag)": "radix_tree_tag_clear - clear a tag on a radix tree node  @root:radix tree root  @index:index key  @tag:tag index    Clear the search tag (which must be < RADIX_TREE_MAX_TAGS)  corresponding to @index in the radix tree.  If this causes  the leaf node to have no tags set then clear the tag in the  next-to-leaf node, etc.    Returns the address of the tagged item on success, else NULL.  ie:  has the same return value and semantics as radix_tree_lookup(). ", "int radix_tree_tag_get(const struct radix_tree_root *root,unsigned long index, unsigned int tag)": "radix_tree_tag_get - get a tag on a radix tree node   @root:radix tree root   @index:index key   @tag:tag index (< RADIX_TREE_MAX_TAGS)     Return values:      0: tag not present or not set    1: tag set     Note that the return value of this function may not be relied on, even if   the RCU lock is held, unless tag modification and node deletion are excluded   from concurrency. ", "void __rcu **radix_tree_next_chunk(const struct radix_tree_root *root,     struct radix_tree_iter *iter, unsigned flags)": "radix_tree_next_chunk - find next chunk of slots for iteration     @root:radix tree root   @iter:iterator state   @flags:RADIX_TREE_ITER_  flags and tag index   Returns:pointer to chunk first slot, or NULL if iteration is over ", "void radix_tree_replace_slot(struct radix_tree_root *root,     void __rcu **slot, void *item)": "radix_tree_gang_lookup_tag_slot().  Caller must hold tree write locked   across slot lookup and replacement.     NOTE: This cannot be used to switch between non-entries (empty slots),   regular entries, and value entries, as that requires accounting   inside the radix tree node. When switching from one type of entry or   deleting, use __radix_tree_lookup() and __radix_tree_replace() or   radix_tree_iter_replace(). ", "void radix_tree_iter_delete(struct radix_tree_root *root,struct radix_tree_iter *iter, void __rcu **slot)": "radix_tree_delete(struct radix_tree_root  root,struct radix_tree_node  node, void __rcu   slot){void  old = rcu_dereference_raw( slot);int values = xa_is_value(old) ? -1 : 0;unsigned offset = get_slot_offset(node, slot);int tag;if (is_idr(root))node_tag_set(root, node, IDR_FREE, offset);elsefor (tag = 0; tag < RADIX_TREE_MAX_TAGS; tag++)node_tag_clear(root, node, tag, offset);replace_slot(slot, NULL, node, -1, values);return node && delete_node(root, node);}     radix_tree_iter_delete - delete the entry at this iterator position   @root: radix tree root   @iter: iterator state   @slot: pointer to slot     Delete the entry at the position currently pointed to by the iterator.   This may result in the current node being freed; if it is, the iterator   is advanced so that it will not reference the freed memory.  This   function may be called without any locking if there are no other threads   which can access this tree. ", "void *radix_tree_delete_item(struct radix_tree_root *root,     unsigned long index, void *item)": "radix_tree_delete_item - delete an item from a radix tree   @root: radix tree root   @index: index key   @item: expected item     Remove @item at @index from the radix tree rooted at @root.     Return: the deleted entry, or %NULL if it was not present   or the entry at the given @index was not @item. ", "int radix_tree_tagged(const struct radix_tree_root *root, unsigned int tag)": "radix_tree_tagged - test whether any items in the tree are tagged  @root:radix tree root  @tag:tag to test ", "void idr_preload(gfp_t gfp_mask)": "idr_preload - preload for idr_alloc()   @gfp_mask: allocation mask to use for preloading     Preallocate memory to use for the next call to idr_alloc().  This function   returns with preemption disabled.  It will be enabled by idr_preload_end(). ", "void idr_destroy(struct idr *idr)": "idr_destroy - release all internal memory from an IDR   @idr: idr handle     After this function is called, the IDR is empty, and may be reused or   the data structure containing it may be freed.     A typical clean-up sequence for objects stored in an idr tree will use   idr_for_each() to free all objects, if necessary, then idr_destroy() to   free the memory used to keep track of those objects. ", "__attribute__((nonnull(2,3,4)))static struct list_head *merge(void *priv, list_cmp_func_t cmp,struct list_head *a, struct list_head *b)": "list_sort.h>#include <linuxlist.h>    Returns a list organized in an intermediate format suited   to chaining of merge() calls: null-terminated, no reserved or   sentinel head node, \"prev\" links not maintained. ", "void argv_free(char **argv)": "argv_free - free an argv   @argv - the argument vector to be freed     Frees an argv and the strings it points to. ", "char **argv_split(gfp_t gfp, const char *str, int *argcp)": "argv_split - split a string at whitespace, returning an argv   @gfp: the GFP mask used to allocate memory   @str: the string to be split   @argcp: returned argument count     Returns an array of pointers to strings which are split out from   @str.  This is performed by strictly splitting on white-space; no   quote processing is performed.  Multiple whitespace characters are   considered to be a single argument separator.  The returned array   is always NULL-terminated.  Returns NULL on memory allocation   failure.     The source string at `str' may be undergoing concurrent alteration via   userspace sysctl activity (at least).  The argv_split() implementation   attempts to handle this gracefully by taking a local copy to work on. ", "void *xa_load(struct xarray *xa, unsigned long index)": "xa_load() - Load an entry from an XArray.   @xa: XArray.   @index: index into array.     Context: Any context.  Takes and releases the RCU lock.   Return: The entry at @index in @xa. ", "void *__xa_erase(struct xarray *xa, unsigned long index)": "xa_erase() - Erase this entry from the XArray while locked.   @xa: XArray.   @index: Index into array.     After this function returns, loading from @index will return %NULL.   If the index is part of a multi-index entry, all indices will be erased   and none of the entries will be part of a multi-index entry.     Context: Any context.  Expects xa_lock to be held on entry.   Return: The entry which used to be at this index. ", "void *__xa_store(struct xarray *xa, unsigned long index, void *entry, gfp_t gfp)": "xa_store() - Store this entry in the XArray.   @xa: XArray.   @index: Index into array.   @entry: New entry.   @gfp: Memory allocation flags.     You must already be holding the xa_lock when calling this function.   It will drop the lock if needed to allocate memory, and then reacquire   it afterwards.     Context: Any context.  Expects xa_lock to be held on entry.  May   release and reacquire xa_lock if @gfp flags permit.   Return: The old entry at this index or xa_err() if an error happened. ", "void *__xa_cmpxchg(struct xarray *xa, unsigned long index,void *old, void *entry, gfp_t gfp)": "__xa_cmpxchg() - Store this entry in the XArray.   @xa: XArray.   @index: Index into array.   @old: Old value to test against.   @entry: New entry.   @gfp: Memory allocation flags.     You must already be holding the xa_lock when calling this function.   It will drop the lock if needed to allocate memory, and then reacquire   it afterwards.     Context: Any context.  Expects xa_lock to be held on entry.  May   release and reacquire xa_lock if @gfp flags permit.   Return: The old entry at this index or xa_err() if an error happened. ", "int __xa_insert(struct xarray *xa, unsigned long index, void *entry, gfp_t gfp)": "__xa_insert() - Store this entry in the XArray if no entry is present.   @xa: XArray.   @index: Index into array.   @entry: New entry.   @gfp: Memory allocation flags.     Inserting a NULL entry will store a reserved entry (like xa_reserve())   if no entry is present.  Inserting will fail if a reserved entry is   present, even though loading from this index will return NULL.     Context: Any context.  Expects xa_lock to be held on entry.  May   release and reacquire xa_lock if @gfp flags permit.   Return: 0 if the store succeeded.  -EBUSY if another entry was present.   -ENOMEM if memory could not be allocated. ", "void *xa_store_range(struct xarray *xa, unsigned long first,unsigned long last, void *entry, gfp_t gfp)": "xa_store_range() - Store this entry at a range of indices in the XArray.   @xa: XArray.   @first: First index to affect.   @last: Last index to affect.   @entry: New entry.   @gfp: Memory allocation flags.     After this function returns, loads from any index between @first and @last,   inclusive will return @entry.   Storing into an existing multi-index entry updates the entry of every index.   The marks associated with @index are unaffected unless @entry is %NULL.     Context: Process context.  Takes and releases the xa_lock.  May sleep   if the @gfp flags permit.   Return: %NULL on success, xa_err(-EINVAL) if @entry cannot be stored in   an XArray, or xa_err(-ENOMEM) if memory allocation failed. ", "int xa_get_order(struct xarray *xa, unsigned long index)": "xa_get_order() - Get the order of an entry.   @xa: XArray.   @index: Index of the entry.     Return: A number between 0 and 63 indicating the order of the entry. ", "int __xa_alloc(struct xarray *xa, u32 *id, void *entry,struct xa_limit limit, gfp_t gfp)": "__xa_alloc() - Find somewhere to store this entry in the XArray.   @xa: XArray.   @id: Pointer to ID.   @limit: Range for allocated ID.   @entry: New entry.   @gfp: Memory allocation flags.     Finds an empty entry in @xa between @limit.min and @limit.max,   stores the index into the @id pointer, then stores the entry at   that index.  A concurrent lookup will not see an uninitialised @id.     Context: Any context.  Expects xa_lock to be held on entry.  May   release and reacquire xa_lock if @gfp flags permit.   Return: 0 on success, -ENOMEM if memory could not be allocated or   -EBUSY if there are no free entries in @limit. ", "int __xa_alloc_cyclic(struct xarray *xa, u32 *id, void *entry,struct xa_limit limit, u32 *next, gfp_t gfp)": "__xa_alloc_cyclic() - Find somewhere to store this entry in the XArray.   @xa: XArray.   @id: Pointer to ID.   @entry: New entry.   @limit: Range of allocated ID.   @next: Pointer to next ID to allocate.   @gfp: Memory allocation flags.     Finds an empty entry in @xa between @limit.min and @limit.max,   stores the index into the @id pointer, then stores the entry at   that index.  A concurrent lookup will not see an uninitialised @id.   The search for an empty entry will start at @next and will wrap   around if necessary.     Context: Any context.  Expects xa_lock to be held on entry.  May   release and reacquire xa_lock if @gfp flags permit.   Return: 0 if the allocation succeeded without wrapping.  1 if the   allocation succeeded after wrapping, -ENOMEM if memory could not be   allocated or -EBUSY if there are no free entries in @limit. ", "void __xa_set_mark(struct xarray *xa, unsigned long index, xa_mark_t mark)": "xa_set_mark() - Set this mark on this entry while locked.   @xa: XArray.   @index: Index of entry.   @mark: Mark number.     Attempting to set a mark on a %NULL entry does not succeed.     Context: Any context.  Expects xa_lock to be held on entry. ", "void __xa_clear_mark(struct xarray *xa, unsigned long index, xa_mark_t mark)": "xa_clear_mark() - Clear this mark on this entry while locked.   @xa: XArray.   @index: Index of entry.   @mark: Mark number.     Context: Any context.  Expects xa_lock to be held on entry. ", "bool xa_get_mark(struct xarray *xa, unsigned long index, xa_mark_t mark)": "xa_get_mark() - Inquire whether this mark is set on this entry.   @xa: XArray.   @index: Index of entry.   @mark: Mark number.     This function uses the RCU read lock, so the result may be out of date   by the time it returns.  If you need the result to be stable, use a lock.     Context: Any context.  Takes and releases the RCU lock.   Return: True if the entry at @index has this mark set, false if it doesn't. ", "void *xa_find(struct xarray *xa, unsigned long *indexp,unsigned long max, xa_mark_t filter)": "xa_find() - Search the XArray for an entry.   @xa: XArray.   @indexp: Pointer to an index.   @max: Maximum index to search to.   @filter: Selection criterion.     Finds the entry in @xa which matches the @filter, and has the lowest   index that is at least @indexp and no more than @max.   If an entry is found, @indexp is updated to be the index of the entry.   This function is protected by the RCU read lock, so it may not find   entries which are being simultaneously added.  It will not return an   %XA_RETRY_ENTRY; if you need to see retry entries, use xas_find().     Context: Any context.  Takes and releases the RCU lock.   Return: The entry, if found, otherwise %NULL. ", "void *xa_find_after(struct xarray *xa, unsigned long *indexp,unsigned long max, xa_mark_t filter)": "xa_find_after() - Search the XArray for a present entry.   @xa: XArray.   @indexp: Pointer to an index.   @max: Maximum index to search to.   @filter: Selection criterion.     Finds the entry in @xa which matches the @filter and has the lowest   index that is above @indexp and no more than @max.   If an entry is found, @indexp is updated to be the index of the entry.   This function is protected by the RCU read lock, so it may miss entries   which are being simultaneously added.  It will not return an   %XA_RETRY_ENTRY; if you need to see retry entries, use xas_find().     Context: Any context.  Takes and releases the RCU lock.   Return: The pointer, if found, otherwise %NULL. ", "unsigned int xa_extract(struct xarray *xa, void **dst, unsigned long start,unsigned long max, unsigned int n, xa_mark_t filter)": "xa_extract() - Copy selected entries from the XArray into a normal array.   @xa: The source XArray to copy from.   @dst: The buffer to copy entries into.   @start: The first index in the XArray eligible to be selected.   @max: The last index in the XArray eligible to be selected.   @n: The maximum number of entries to copy.   @filter: Selection criterion.     Copies up to @n entries that match @filter from the XArray.  The   copied entries will have indices between @start and @max, inclusive.     The @filter may be an XArray mark value, in which case entries which are   marked with that mark will be copied.  It may also be %XA_PRESENT, in   which case all entries which are not %NULL will be copied.     The entries returned may not represent a snapshot of the XArray at a   moment in time.  For example, if another thread stores to index 5, then   index 10, calling xa_extract() may return the old contents of index 5   and the new contents of index 10.  Indices not modified while this   function is running will not be skipped.     If you need stronger guarantees, holding the xa_lock across calls to this   function will prevent concurrent modification.     Context: Any context.  Takes and releases the RCU lock.   Return: The number of entries copied. ", "void xa_destroy(struct xarray *xa)": "xa_destroy() - Free all internal data structures.   @xa: XArray.     After calling this function, the XArray is empty and has freed all memory   allocated for its internal data structures.  You are responsible for   freeing the objects referenced by the XArray.     Context: Any context.  Takes and releases the xa_lock, interrupt-safe. ", "u32 prandom_u32_state(struct rnd_state *state)": "prandom_u32_state - seeded pseudo-random number generator.  @state: pointer to state structure holding seeded state.    This is used for pseudo-randomness with no outside seeding.  For more random results, use get_random_u32(). ", "void prandom_bytes_state(struct rnd_state *state, void *buf, size_t bytes)": "prandom_bytes_state - get the requested number of pseudo-random bytes    @state: pointer to state structure holding seeded state.  @buf: where to copy the pseudo-random bytes to  @bytes: the requested number of bytes    This is used for pseudo-randomness with no outside seeding.  For more random results, use get_random_bytes(). ", "void irq_poll_sched(struct irq_poll *iop)": "irq_poll_sched - Schedule a run of the iopoll handler   @iop:      The parent iopoll structure     Description:       Add this irq_poll structure to the pending poll list and trigger the       raise of the blk iopoll softirq.  ", "static void __irq_poll_complete(struct irq_poll *iop)": "irq_poll_complete - Mark this @iop as un-polled again   @iop:      The parent iopoll structure     Description:       See irq_poll_complete(). This function must be called with interrupts       disabled.  ", "void irq_poll_disable(struct irq_poll *iop)": "irq_poll_disable - Disable iopoll on this @iop   @iop:      The parent iopoll structure     Description:       Disable io polling and wait for any pending callbacks to have completed.  ", "void irq_poll_enable(struct irq_poll *iop)": "irq_poll_enable - Enable iopoll on this @iop   @iop:      The parent iopoll structure     Description:       Enable iopoll on this @iop. Note that the handler run will not be       scheduled, it will only mark it as active.  ", "void irq_poll_init(struct irq_poll *iop, int weight, irq_poll_fn *poll_fn)": "irq_poll_init - Initialize this @iop   @iop:      The parent iopoll structure   @weight:   The default weight (or command completion budget)   @poll_fn:  The handler to invoke     Description:       Initialize and enable this irq_poll structure.  ", "static int get_range(char **str, int *pint, int n)": "get_options. ", "unsigned long long memparse(const char *ptr, char **retptr)": "memparse - parse a string with mem suffixes into a number  @ptr: Where parse begins  @retptr: (output) Optional pointer to next char after parse completes    Parses a string into a number.  The number stored at @ptr is  potentially suffixed with K, M, G, T, P, E. ", "if (**val == '\"') ": "next_arg(char  args, char   param, char   val){unsigned int i, equals = 0;int in_quote = 0, quoted = 0;if ( args == '\"') {args++;in_quote = 1;quoted = 1;}for (i = 0; args[i]; i++) {if (isspace(args[i]) && !in_quote)break;if (equals == 0) {if (args[i] == '=')equals = i;}if (args[i] == '\"')in_quote = !in_quote;} param = args;if (!equals) val = NULL;else {args[equals] = '\\0'; val = args + equals + 1;  Don't include quotes in value. ", "void generate_random_uuid(unsigned char uuid[16])": "generate_random_uuid - generate a random UUID   @uuid: where to put the generated UUID     Random UUID interface     Used to create a Boot ID or a filesystem UUIDGUID, but can be   useful for other kernel drivers. ", "guid[7] = (guid[7] & 0x0F) | 0x40;/* Set the GUID variant to DCE ": "generate_random_guid(unsigned char guid[16]){get_random_bytes(guid, 16);  Set GUID version to 4 --- truly random generation ", "bool uuid_is_valid(const char *uuid)": "uuid_is_valid - checks if a UUID string is valid   @uuid:UUID string to check     Description:   It checks if the UUID string is following the format:  xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx     where x is a hex digit.     Return: true if input is valid UUID string. ", "struct scatterlist *sg_next(struct scatterlist *sg)": "sg_next - return the next scatterlist entry in a list   @sg:The current sg entry     Description:     Usually the next entry will be @sg@ + 1, but if this sg element is part     of a chained scatterlist, it could jump to the start of a new     scatterlist array.    ", "int sg_nents(struct scatterlist *sg)": "sg_nents - return total count of entries in scatterlist   @sg:The scatterlist     Description:   Allows to know how many entries are in sg, taking into account   chaining as well    ", "int sg_nents_for_len(struct scatterlist *sg, u64 len)": "sg_nents_for_len - return total count of entries in scatterlist                      needed to satisfy the supplied length   @sg:The scatterlist   @len:The total required length     Description:   Determines the number of entries in sg that are required to meet   the supplied length, taking into account chaining as well     Returns:     the number of sg entries needed, negative error on failure    ", "struct scatterlist *sg_last(struct scatterlist *sgl, unsigned int nents)": "sg_last - return the last scatterlist entry in a list   @sgl:First entry in the scatterlist   @nents:Number of entries in the scatterlist     Description:     Should only be used casually, it (currently) scans the entire list     to get the last entry.       Note that the @sgl@ pointer passed in need not be the first one,     the important bit is that @nents@ denotes the number of entries that     exist from @sgl@.    ", "void sg_init_table(struct scatterlist *sgl, unsigned int nents)": "sg_init_table - Initialize SG table   @sgl:   The SG table   @nents:   Number of entries in table     Notes:     If this is part of a chained sg table, sg_mark_end() should be     used only on the last table part.    ", "void sg_init_one(struct scatterlist *sg, const void *buf, unsigned int buflen)": "sg_init_one - Initialize a single entry sg list   @sg: SG entry   @buf: Virtual address for IO   @buflen: IO length    ", "void __sg_free_table(struct sg_table *table, unsigned int max_ents,     unsigned int nents_first_chunk, sg_free_fn *free_fn,     unsigned int num_ents)": "__sg_alloc_table().  The @max_ents value must be identical to      that previously used with __sg_alloc_table().    ", "void sg_free_append_table(struct sg_append_table *table)": "sg_free_append_table - Free a previously allocated append sg table.   @table: The mapped sg append table header    ", "static struct scatterlist *sg_kmalloc(unsigned int nents, gfp_t gfp_mask)": "sg_alloc_table() is to use these kmallockfree   helpers. ", "int sg_alloc_append_table_from_pages(struct sg_append_table *sgt_append,struct page **pages, unsigned int n_pages, unsigned int offset,unsigned long size, unsigned int max_segment,unsigned int left_pages, gfp_t gfp_mask)": "sg_alloc_append_table_from_pages - Allocate and initialize an append sg                                      table from an array of pages   @sgt_append:  The sg append table to use   @pages:       Pointer to an array of page pointers   @n_pages:     Number of pages in the pages array   @offset:      Offset from start of the first page to the start of a buffer   @size:        Number of valid bytes in the buffer (after offset)   @max_segment: Maximum size of a scatterlist element in bytes   @left_pages:  Left pages caller have to set after this call   @gfp_mask: GFP allocation mask     Description:      In the first call it allocate and initialize an sg table from a list of      pages, else reuse the scatterlist from sgt_append. Contiguous ranges of      the pages are squashed into a single scatterlist entry up to the maximum      size specified in @max_segment.  A user may provide an offset at a start      and a size of valid data in a buffer specified by the page array. The      returned sg table is released by sg_free_append_table     Returns:     0 on success, negative error on failure     Notes:     If this function returns non-0 (eg failure), the caller must call     sg_free_append_table() to cleanup any leftover allocations.       In the fist call, sgt_append must by initialized. ", "int sg_alloc_table_from_pages_segment(struct sg_table *sgt, struct page **pages,unsigned int n_pages, unsigned int offset,unsigned long size, unsigned int max_segment,gfp_t gfp_mask)": "sg_alloc_table_from_pages_segment - Allocate and initialize an sg table from                                       an array of pages and given maximum                                       segment.   @sgt: The sg table header to use   @pages: Pointer to an array of page pointers   @n_pages: Number of pages in the pages array   @offset:      Offset from start of the first page to the start of a buffer   @size:        Number of valid bytes in the buffer (after offset)   @max_segment: Maximum size of a scatterlist element in bytes   @gfp_mask: GFP allocation mask      Description:      Allocate and initialize an sg table from a list of pages. Contiguous      ranges of the pages are squashed into a single scatterlist node up to the      maximum size specified in @max_segment. A user may provide an offset at a      start and a size of valid data in a buffer specified by the page array.        The returned sg table is released by sg_free_table.      Returns:     0 on success, negative error on failure ", "struct scatterlist *sgl_alloc_order(unsigned long long length,    unsigned int order, bool chainable,    gfp_t gfp, unsigned int *nent_p)": "sgl_alloc_order - allocate a scatterlist and its pages   @length: Length in bytes of the scatterlist. Must be at least one   @order: Second argument for alloc_pages()   @chainable: Whether or not to allocate an extra element in the scatterlist  for scatterlist chaining purposes   @gfp: Memory allocation flags   @nent_p: [out] Number of entries in the scatterlist that have pages     Returns: A pointer to an initialized scatterlist or %NULL upon failure. ", "void sgl_free_n_order(struct scatterlist *sgl, int nents, int order)": "sgl_free_n_order - free a scatterlist and its pages   @sgl: Scatterlist with one or more elements   @nents: Maximum number of elements to free   @order: Second argument for __free_pages()     Notes:   - If several scatterlists have been chained and each chain element is     freed separately then it's essential to set nents correctly to avoid that a     page would get freed twice.   - All pages in a chained scatterlist can be freed at once by setting @nents     to a high number. ", "struct scatterlist *sgl_alloc(unsigned long long length, gfp_t gfp,      unsigned int *nent_p)": "sgl_free_order(sgl, order);return NULL;}sg_set_page(sg, page, elem_len, 0);length -= elem_len;sg = sg_next(sg);}WARN_ONCE(length, \"length = %lld\\n\", length);if (nent_p) nent_p = nent;return sgl;}EXPORT_SYMBOL(sgl_alloc_order);     sgl_alloc - allocate a scatterlist and its pages   @length: Length in bytes of the scatterlist   @gfp: Memory allocation flags   @nent_p: [out] Number of entries in the scatterlist     Returns: A pointer to an initialized scatterlist or %NULL upon failure. ", "void sg_miter_start(struct sg_mapping_iter *miter, struct scatterlist *sgl,    unsigned int nents, unsigned int flags)": "sg_miter_start - start mapping iteration over a sg list   @miter: sg mapping iter to be started   @sgl: sg list to iterate over   @nents: number of sg entries     Description:     Starts mapping iterator @miter.     Context:     Don't care. ", "bool sg_miter_skip(struct sg_mapping_iter *miter, off_t offset)": "sg_miter_next(), this     stops @miter.     Context:     Don't care.     Returns:     true if @miter contains the valid mapping.  false if end of sg     list is reached. ", "bool sg_miter_next(struct sg_mapping_iter *miter)": "sg_miter_stop(miter);while (offset) {off_t consumed;if (!sg_miter_get_next_page(miter))return false;consumed = min_t(off_t, offset, miter->__remaining);miter->__offset += consumed;miter->__remaining -= consumed;offset -= consumed;}return true;}EXPORT_SYMBOL(sg_miter_skip);     sg_miter_next - proceed mapping iterator to the next mapping   @miter: sg mapping iter to proceed     Description:     Proceeds @miter to the next mapping.  @miter should have been started     using sg_miter_start().  On successful return, @miter->page,     @miter->addr and @miter->length point to the current mapping.     Context:     May sleep if !SG_MITER_ATOMIC.     Returns:     true if @miter contains the next mapping.  false if end of sg     list is reached. ", "size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents, void *buf,      size_t buflen, off_t skip, bool to_buffer)": "sg_copy_buffer - Copy data between a linear buffer and an SG list   @sgl: The SG list   @nents: Number of SG entries   @buf: Where to copy from   @buflen: The number of bytes to copy   @skip: Number of bytes to skip before copying   @to_buffer: transfer direction (true == from an sg list to a   buffer, false == from a buffer to an sg list)     Returns the number of copied bytes.    ", "size_t sg_copy_from_buffer(struct scatterlist *sgl, unsigned int nents,   const void *buf, size_t buflen)": "sg_copy_from_buffer - Copy from a linear buffer to an SG list   @sgl: The SG list   @nents: Number of SG entries   @buf: Where to copy from   @buflen: The number of bytes to copy     Returns the number of copied bytes.    ", "size_t sg_copy_to_buffer(struct scatterlist *sgl, unsigned int nents, void *buf, size_t buflen)": "sg_copy_to_buffer - Copy from an SG list to a linear buffer   @sgl: The SG list   @nents: Number of SG entries   @buf: Where to copy to   @buflen: The number of bytes to copy     Returns the number of copied bytes.    ", "size_t sg_pcopy_from_buffer(struct scatterlist *sgl, unsigned int nents,    const void *buf, size_t buflen, off_t skip)": "sg_pcopy_from_buffer - Copy from a linear buffer to an SG list   @sgl: The SG list   @nents: Number of SG entries   @buf: Where to copy from   @buflen: The number of bytes to copy   @skip: Number of bytes to skip before copying     Returns the number of copied bytes.    ", "size_t sg_pcopy_to_buffer(struct scatterlist *sgl, unsigned int nents,  void *buf, size_t buflen, off_t skip)": "sg_pcopy_to_buffer - Copy from an SG list to a linear buffer   @sgl: The SG list   @nents: Number of SG entries   @buf: Where to copy to   @buflen: The number of bytes to copy   @skip: Number of bytes to skip before copying     Returns the number of copied bytes.    ", "size_t sg_zero_buffer(struct scatterlist *sgl, unsigned int nents,       size_t buflen, off_t skip)": "sg_zero_buffer - Zero-out a part of a SG list   @sgl: The SG list   @nents: Number of SG entries   @buflen: The number of bytes to zero out   @skip: Number of bytes to skip before zeroing     Returns the number of bytes zeroed.  ", "ret = stmp_clear_poll_bit(reset_addr, STMP_MODULE_SFTRST);if (unlikely(ret))goto error;/* clear CLKGATE ": "stmp_reset_block(void __iomem  reset_addr){int ret;int timeout = 0x400;  clear and poll SFTRST ", "if (!handle)return 0;parts.extra = extra_bits;return parts.handle;}EXPORT_SYMBOL(stack_depot_set_extra_bits": "stack_depot_set_extra_bits(depot_stack_handle_t handle, unsigned int extra_bits){union handle_parts parts = { .handle = handle };  Don't set extra bits on empty handles. ", "void *mtree_load(struct maple_tree *mt, unsigned long index)": "mtree_load() - Load a value stored in a maple tree   @mt: The maple tree   @index: The index to load     Return: the entry or %NULL ", "int mtree_store_range(struct maple_tree *mt, unsigned long index,unsigned long last, void *entry, gfp_t gfp)": "mtree_store_range() - Store an entry at a given range.   @mt: The maple tree   @index: The start of the range   @last: The end of the range   @entry: The entry to store   @gfp: The GFP_FLAGS to use for allocations     Return: 0 on success, -EINVAL on invalid request, -ENOMEM if memory could not   be allocated. ", "int mtree_insert_range(struct maple_tree *mt, unsigned long first,unsigned long last, void *entry, gfp_t gfp)": "mtree_insert_range() - Insert an entry at a give range if there is no value.   @mt: The maple tree   @first: The start of the range   @last: The end of the range   @entry: The entry to store   @gfp: The GFP_FLAGS to use for allocations.     Return: 0 on success, -EEXISTS if the range is occupied, -EINVAL on invalid   request, -ENOMEM if memory could not be allocated. ", "if (mas_nomem(&mas, gfp))goto retry;if (mas_is_err(&mas))ret = xa_err(mas.node);else*startp = mas.index;unlock:mtree_unlock(mt);return ret;}EXPORT_SYMBOL(mtree_alloc_range": "mtree_alloc_range(struct maple_tree  mt, unsigned long  startp,void  entry, unsigned long size, unsigned long min,unsigned long max, gfp_t gfp){int ret = 0;MA_STATE(mas, mt, 0, 0);if (!mt_is_alloc(mt))return -EINVAL;if (WARN_ON_ONCE(mt_is_reserved(entry)))return -EINVAL;mtree_lock(mt);retry:ret = mas_empty_area(&mas, min, max, size);if (ret)goto unlock;mas_insert(&mas, entry);    mas_nomem() may release the lock, causing the allocated area   to be unavailable, so try to allocate a free area again. ", "if (mas_nomem(&mas, gfp))goto retry;if (mas_is_err(&mas))ret = xa_err(mas.node);else*startp = mas.index;unlock:mtree_unlock(mt);return ret;}EXPORT_SYMBOL(mtree_alloc_rrange": "mtree_alloc_rrange(struct maple_tree  mt, unsigned long  startp,void  entry, unsigned long size, unsigned long min,unsigned long max, gfp_t gfp){int ret = 0;MA_STATE(mas, mt, 0, 0);if (!mt_is_alloc(mt))return -EINVAL;if (WARN_ON_ONCE(mt_is_reserved(entry)))return -EINVAL;mtree_lock(mt);retry:ret = mas_empty_area_rev(&mas, min, max, size);if (ret)goto unlock;mas_insert(&mas, entry);    mas_nomem() may release the lock, causing the allocated area   to be unavailable, so try to allocate a free area again. ", "void *mtree_erase(struct maple_tree *mt, unsigned long index)": "mtree_erase() - Find an index and erase the entire range.   @mt: The maple tree   @index: The index to erase     Erasing is the same as a walk to an entry then a store of a NULL to that   ENTIRE range.  In fact, it is implemented as such using the advanced API.     Return: The entry stored at the @index or %NULL ", "void mtree_destroy(struct maple_tree *mt)": "mtree_destroy() - Destroy a maple tree   @mt: The maple tree     Frees all resources used by the tree.  Handles locking. ", "void *mt_find(struct maple_tree *mt, unsigned long *index, unsigned long max)": "mt_find() - Search from the start up until an entry is found.   @mt: The maple tree   @index: Pointer which contains the start location of the search   @max: The maximum value to check     Handles locking.  @index will be incremented to one beyond the range.     Return: The entry at or after the @index or %NULL ", "void *mt_find_after(struct maple_tree *mt, unsigned long *index,    unsigned long max)": "mt_find_after() - Search from the start up until an entry is found.   @mt: The maple tree   @index: Pointer which contains the start location of the search   @max: The maximum value to check     Handles locking, detects wrapping on index == 0     Return: The entry at or after the @index or %NULL ", "errseq_t errseq_set(errseq_t *eseq, int err)": "errseq_set - set a errseq_t for later reporting   @eseq: errseq_t field that should be set   @err: error to set (must be between -1 and -MAX_ERRNO)     This function sets the error in @eseq, and increments the sequence counter   if the last sequence was sampled at some point in the past.     Any error set will always overwrite an existing error.     Return: The previous value, primarily for debugging purposes. The   return value should not be used as a previously sampled value in later   calls as it will not have the SEEN flag set. ", "errseq_t errseq_sample(errseq_t *eseq)": "errseq_sample() - Grab current errseq_t value.   @eseq: Pointer to errseq_t to be sampled.     This function allows callers to initialise their errseq_t variable.   If the error has been \"seen\", new callers will not see an old error.   If there is an unseen error in @eseq, the caller of this function will   see it the next time it checks for an error.     Context: Any context.   Return: The current errseq value. ", "int errseq_check(errseq_t *eseq, errseq_t since)": "errseq_check() - Has an error occurred since a particular sample point?   @eseq: Pointer to errseq_t value to be checked.   @since: Previously-sampled errseq_t from which to check.     Grab the value that eseq points to, and see if it has changed @since   the given value was sampled. The @since value is not advanced, so there   is no need to mark the value as seen.     Return: The latest error set in the errseq_t or 0 if it hasn't changed. ", "int errseq_check_and_advance(errseq_t *eseq, errseq_t *since)": "errseq_check_and_advance() - Check an errseq_t and advance to current value.   @eseq: Pointer to value being checked and reported.   @since: Pointer to previously-sampled errseq_t to check against and advance.     Grab the eseq value, and see whether it matches the value that @since   points to. If it does, then just return 0.     If it doesn't, then the value has changed. Set the \"seen\" flag, and try to   swap it into place as the new eseq value. Then, set that value as the new   \"since\" value, and return whatever the error portion is set to.     Note that no locking is provided here for concurrent updates to the \"since\"   value. The caller must provide that if necessary. Because of this, callers   may want to do a lockless errseq_check before taking the lock and calling   this.     Return: Negative errno if one has been stored, or 0 if no new error has   occurred. ", "struct lru_cache *lc_create(const char *name, struct kmem_cache *cache,unsigned max_pending_changes,unsigned e_count, size_t e_size, size_t e_off)": "lc_seq_dump_details   @cache: cache root pointer   @max_pending_changes: maximum changes to accumulate until a transaction is required   @e_count: number of elements allowed to be active simultaneously   @e_size: size of the tracked objects   @e_off: offset to the &struct lc_element member in a tracked object     Returns a pointer to a newly initialized struct lru_cache on success,   or NULL on (allocation) failure. ", "void lc_reset(struct lru_cache *lc)": "lc_reset - does a full reset for @lc and the hash table slots.   @lc: the lru cache to operate on     It is roughly the equivalent of re-allocating a fresh lru_cache object,   basically a short cut to lc_destroy(lc); lc = lc_create(...); ", "void lc_destroy(struct lru_cache *lc)": "lc_destroy - frees memory allocated by lc_create()   @lc: the lru cache to destroy ", "void lc_del(struct lru_cache *lc, struct lc_element *e)": "lc_del - removes an element from the cache   @lc: The lru_cache object   @e: The element to remove     @e must be unused (refcnt == 0). Moves @e from \"lru\" to \"free\" list,   sets @e->enr to %LC_FREE. ", "struct lc_element *lc_try_get(struct lru_cache *lc, unsigned int enr)": "lc_try_get - get element by label, if present; do not change the active set   @lc: the lru cache to operate on   @enr: the label to look up     Finds an element in the cache, increases its usage count,   \"touches\" and returns it.     Return values:    NULL       The cache was marked %LC_STARVING,       or the requested label was not in the active set      pointer to the element with the REQUESTED element number.       In this case, it can be used right away ", "if (e->lc_new_number != enr)continue;if (e->lc_new_number == e->lc_number || include_changing)return e;break;}return NULL;}/** * lc_find - find element by label, if present in the hash table * @lc: The lru_cache object * @enr: element number * * Returns the pointer to an element, if the element with the requested * \"label\" or element number is present in the hash table, * or NULL if not found. Does not change the refcnt. * Ignores elements that are \"about to be used\", i.e. not yet in the active * set, but still pending transaction commit. ": "lc_find(struct lru_cache  lc, unsigned int enr,bool include_changing){struct lc_element  e;BUG_ON(!lc);BUG_ON(!lc->nr_elements);hlist_for_each_entry(e, lc_hash_slot(lc, enr), colision) {  \"about to be changed\" elements, pending transaction commit,   are hashed by their \"new number\". \"Normal\" elements have   lc_number == lc_new_number. ", "seq_printf(seq, \"\\t%s: used:%u/%u hits:%lu misses:%lu starving:%lu locked:%lu changed:%lu\\n\",   lc->name, lc->used, lc->nr_elements,   lc->hits, lc->misses, lc->starving, lc->locked, lc->changed);}static struct hlist_head *lc_hash_slot(struct lru_cache *lc, unsigned int enr)": "lc_get are   (starving + hits + misses)   misses include \"locked\" count (update from an other thread in   progress) and \"changed\", when this in fact lead to an successful   update of the cache. ", "unsigned int lc_put(struct lru_cache *lc, struct lc_element *e)": "lc_put - give up refcnt of @e   @lc: the lru cache to operate on   @e: the element to put     If refcnt reaches zero, the element is moved to the lru list,   and a %LC_STARVING (if set) is cleared.   Returns the new (post-decrement) refcnt. ", "struct lc_element *lc_get(struct lru_cache *lc, unsigned int enr)": "lc_committed(lc) and lc_unlock(), to finish the change.     NOTE: The user needs to check the lc_number on EACH use, so he recognizes         any cache set change. ", "struct lc_element *lc_element_by_index(struct lru_cache *lc, unsigned i)": "lc_element_by_index   @lc: the lru cache to operate on   @i: the index of the element to return ", "return 0 == val;}/** * lc_create - prepares to track objects in an active set * @name: descriptive name only used in lc_seq_printf_stats and lc_seq_dump_details * @cache: cache root pointer * @max_pending_changes: maximum changes to accumulate until a transaction is required * @e_count: number of elements allowed to be active simultaneously * @e_size: size of the tracked objects * @e_off: offset to the &struct lc_element member in a tracked object * * Returns a pointer to a newly initialized struct lru_cache on success, * or NULL on (allocation) failure. ": "lc_try_lock(struct lru_cache  lc){unsigned long val;do {val = cmpxchg(&lc->flags, 0, LC_LOCKED);} while (unlikely (val == LC_PARANOIA));  Spin until no-one is inside a PARANOIA_ENTRY()RETURN() section. ", "bool lc_is_used(struct lru_cache *lc, unsigned int enr)": "lc_is_used - find element by label   @lc: The lru_cache object   @enr: element number     Returns true, if the element with the requested \"label\" or element number is   present in the hash table, and is used (refcnt > 0).   Also finds elements that are not _currently_ used but only \"about to be   used\", i.e. on the \"to_be_changed\" list, pending transaction commit. ", "struct lc_element *lc_get_cumulative(struct lru_cache *lc, unsigned int enr)": "lc_get_cumulative - like lc_get; also finds to-be-changed elements   @lc: the lru cache to operate on   @enr: the label to look up     Unlike lc_get this also returns the element for @enr, if it is belonging to   a pending transaction, so the return values are like for lc_get(),   plus:     pointer to an element already on the \"to_be_changed\" list.   In this case, the cache was already marked %LC_DIRTY.     Caller needs to make sure that the pending transaction is completed,   before proceeding to actually use this element. ", "__acquire(once_lock);return false;}return true;}EXPORT_SYMBOL(__do_once_start": "__do_once_start(bool  done, unsigned long  flags)__acquires(once_lock){spin_lock_irqsave(&once_lock,  flags);if ( done) {spin_unlock_irqrestore(&once_lock,  flags);  Keep sparse happy by restoring an even lock count on   this lock. In case we return here, we don't call into   __do_once_done but return early in the DO_ONCE() macro. ", "__acquire(once_lock);return false;}return true;}EXPORT_SYMBOL(__do_once_start);void __do_once_done(bool *done, struct static_key_true *once_key,    unsigned long *flags, struct module *mod)__releases(once_lock)": "__do_once_done but return early in the DO_ONCE() macro. ", "__acquire(once_mutex);return false;}return true;}EXPORT_SYMBOL(__do_once_sleepable_start": "__do_once_sleepable_start(bool  done)__acquires(once_mutex){mutex_lock(&once_mutex);if ( done) {mutex_unlock(&once_mutex);  Keep sparse happy by restoring an even lock count on   this mutex. In case we return here, we don't call into   __do_once_done but return early in the DO_ONCE_SLEEPABLE() macro. ", "struct gen_pool *gen_pool_create(int min_alloc_order, int nid)": "gen_pool_create - create a new special memory pool   @min_alloc_order: log base 2 of number of bytes each bitmap bit represents   @nid: node id of the node the pool structure should be allocated on, or -1     Create a new special memory pool that can be used to manage special purpose   memory not managed by the regular kmallockfree interface. ", "int gen_pool_add_owner(struct gen_pool *pool, unsigned long virt, phys_addr_t phys, size_t size, int nid, void *owner)": "gen_pool_first_fit;pool->data = NULL;pool->name = NULL;}return pool;}EXPORT_SYMBOL(gen_pool_create);     gen_pool_add_owner- add a new chunk of special memory to the pool   @pool: pool to add new memory chunk to   @virt: virtual starting address of memory chunk to add to pool   @phys: physical starting address of memory chunk to add to pool   @size: size in bytes of the memory chunk to add to pool   @nid: node id of the node the chunk structure and bitmap should be         allocated on, or -1   @owner: private data the publisher would like to recall at alloc time     Add a new chunk of special memory to the specified pool.     Returns 0 on success or a -ve errno on failure. ", "phys_addr_t gen_pool_virt_to_phys(struct gen_pool *pool, unsigned long addr)": "gen_pool_virt_to_phys - return the physical address of memory   @pool: pool to allocate from   @addr: starting address of memory     Returns the physical address on success, or -1 on error. ", "void gen_pool_destroy(struct gen_pool *pool)": "gen_pool_destroy - destroy a special memory pool   @pool: pool to destroy     Destroy the specified special memory pool. Verifies that there are no   outstanding allocations. ", "unsigned long gen_pool_alloc_algo_owner(struct gen_pool *pool, size_t size,genpool_algo_t algo, void *data, void **owner)": "gen_pool_alloc_algo_owner - allocate special memory from the pool   @pool: pool to allocate from   @size: number of bytes to allocate from the pool   @algo: algorithm passed from caller   @data: data passed to algorithm   @owner: optionally retrieve the chunk owner     Allocate the requested number of bytes from the specified pool.   Uses the pool allocation function (with first-fit algorithm by default).   Can not be used in NMI handler on architectures without   NMI-safe cmpxchg implementation. ", "void *gen_pool_dma_alloc(struct gen_pool *pool, size_t size, dma_addr_t *dma)": "gen_pool_dma_alloc - allocate special memory from the pool for DMA usage   @pool: pool to allocate from   @size: number of bytes to allocate from the pool   @dma: dma-view physical address return value.  Use %NULL if unneeded.     Allocate the requested number of bytes from the specified pool.   Uses the pool allocation function (with first-fit algorithm by default).   Can not be used in NMI handler on architectures without   NMI-safe cmpxchg implementation.     Return: virtual address of the allocated memory, or %NULL on failure ", "void *gen_pool_dma_alloc_algo(struct gen_pool *pool, size_t size,dma_addr_t *dma, genpool_algo_t algo, void *data)": "gen_pool_dma_alloc_algo(pool, size, dma, pool->algo, pool->data);}EXPORT_SYMBOL(gen_pool_dma_alloc);     gen_pool_dma_alloc_algo - allocate special memory from the pool for DMA   usage with the given pool algorithm   @pool: pool to allocate from   @size: number of bytes to allocate from the pool   @dma: DMA-view physical address return value. Use %NULL if unneeded.   @algo: algorithm passed from caller   @data: data passed to algorithm     Allocate the requested number of bytes from the specified pool. Uses the   given pool allocation function. Can not be used in NMI handler on   architectures without NMI-safe cmpxchg implementation.     Return: virtual address of the allocated memory, or %NULL on failure ", "void *gen_pool_dma_alloc_align(struct gen_pool *pool, size_t size,dma_addr_t *dma, int align)": "gen_pool_dma_alloc_align - allocate special memory from the pool for DMA   usage with the given alignment   @pool: pool to allocate from   @size: number of bytes to allocate from the pool   @dma: DMA-view physical address return value. Use %NULL if unneeded.   @align: alignment in bytes for starting address     Allocate the requested number bytes from the specified pool, with the given   alignment restriction. Can not be used in NMI handler on architectures   without NMI-safe cmpxchg implementation.     Return: virtual address of the allocated memory, or %NULL on failure ", "void *gen_pool_dma_zalloc(struct gen_pool *pool, size_t size, dma_addr_t *dma)": "gen_pool_first_fit_align, &data);}EXPORT_SYMBOL(gen_pool_dma_alloc_align);     gen_pool_dma_zalloc - allocate special zeroed memory from the pool for   DMA usage   @pool: pool to allocate from   @size: number of bytes to allocate from the pool   @dma: dma-view physical address return value.  Use %NULL if unneeded.     Allocate the requested number of zeroed bytes from the specified pool.   Uses the pool allocation function (with first-fit algorithm by default).   Can not be used in NMI handler on architectures without   NMI-safe cmpxchg implementation.     Return: virtual address of the allocated zeroed memory, or %NULL on failure ", "void *gen_pool_dma_zalloc_algo(struct gen_pool *pool, size_t size,dma_addr_t *dma, genpool_algo_t algo, void *data)": "gen_pool_dma_zalloc_algo(pool, size, dma, pool->algo, pool->data);}EXPORT_SYMBOL(gen_pool_dma_zalloc);     gen_pool_dma_zalloc_algo - allocate special zeroed memory from the pool for   DMA usage with the given pool algorithm   @pool: pool to allocate from   @size: number of bytes to allocate from the pool   @dma: DMA-view physical address return value. Use %NULL if unneeded.   @algo: algorithm passed from caller   @data: data passed to algorithm     Allocate the requested number of zeroed bytes from the specified pool. Uses   the given pool allocation function. Can not be used in NMI handler on   architectures without NMI-safe cmpxchg implementation.     Return: virtual address of the allocated zeroed memory, or %NULL on failure ", "void *gen_pool_dma_zalloc_align(struct gen_pool *pool, size_t size,dma_addr_t *dma, int align)": "gen_pool_dma_zalloc_align - allocate special zeroed memory from the pool for   DMA usage with the given alignment   @pool: pool to allocate from   @size: number of bytes to allocate from the pool   @dma: DMA-view physical address return value. Use %NULL if unneeded.   @align: alignment in bytes for starting address     Allocate the requested number of zeroed bytes from the specified pool,   with the given alignment restriction. Can not be used in NMI handler on   architectures without NMI-safe cmpxchg implementation.     Return: virtual address of the allocated zeroed memory, or %NULL on failure ", "void gen_pool_free_owner(struct gen_pool *pool, unsigned long addr, size_t size,void **owner)": "gen_pool_free_owner - free allocated special memory back to the pool   @pool: pool to free to   @addr: starting address of memory to free back to pool   @size: size in bytes of memory to free   @owner: private data stashed at gen_pool_add() time     Free previously allocated special memory back to the specified   pool.  Can not be used in NMI handler on architectures without   NMI-safe cmpxchg implementation. ", "void gen_pool_for_each_chunk(struct gen_pool *pool,void (*func)(struct gen_pool *pool, struct gen_pool_chunk *chunk, void *data),void *data)": "gen_pool_for_each_chunk - call func for every chunk of generic memory pool   @pool:the generic memory pool   @func:func to call   @data:additional data used by @func     Call @func for every chunk of generic memory pool.  The @func is   called with rcu_read_lock held. ", "bool gen_pool_has_addr(struct gen_pool *pool, unsigned long start,size_t size)": "gen_pool_has_addr - checks if an address falls within the range of a pool   @pool:the generic memory pool   @start:start address   @size:size of the region     Check if the range of addresses falls within the specified pool. Returns   true if the entire range is contained in the pool and false otherwise. ", "void gen_pool_set_algo(struct gen_pool *pool, genpool_algo_t algo, void *data)": "gen_pool_set_algo - set the allocation algorithm   @pool: pool to change allocation algorithm   @algo: custom algorithm function   @data: additional data used by @algo     Call @algo for each memory allocation in the pool.   If @algo is NULL use gen_pool_first_fit as default   memory allocation function. ", "unsigned long gen_pool_fixed_alloc(unsigned long *map, unsigned long size,unsigned long start, unsigned int nr, void *data,struct gen_pool *pool, unsigned long start_addr)": "gen_pool_fixed_alloc - reserve a specific region   @map: The address to base the search on   @size: The bitmap size in bits   @start: The bitnumber to start searching at   @nr: The number of zeroed bits we're looking for   @data: data for alignment   @pool: pool to get order from   @start_addr: not used in this function ", "unsigned long gen_pool_first_fit_order_align(unsigned long *map,unsigned long size, unsigned long start,unsigned int nr, void *data, struct gen_pool *pool,unsigned long start_addr)": "gen_pool_first_fit_order_align - find the first available region   of memory matching the size requirement. The region will be aligned   to the order of the size specified.   @map: The address to base the search on   @size: The bitmap size in bits   @start: The bitnumber to start searching at   @nr: The number of zeroed bits we're looking for   @data: additional data - unused   @pool: pool to find the fit region memory from   @start_addr: not used in this function ", "unsigned long gen_pool_best_fit(unsigned long *map, unsigned long size,unsigned long start, unsigned int nr, void *data,struct gen_pool *pool, unsigned long start_addr)": "gen_pool_best_fit - find the best fitting region of memory   matching the size requirement (no alignment constraint)   @map: The address to base the search on   @size: The bitmap size in bits   @start: The bitnumber to start searching at   @nr: The number of zeroed bits we're looking for   @data: additional data - unused   @pool: pool to find the fit region memory from   @start_addr: not used in this function     Iterate over the bitmap to find the smallest free region   which we can allocate the memory. ", "struct gen_pool *devm_gen_pool_create(struct device *dev, int min_alloc_order,      int nid, const char *name)": "devm_gen_pool_create - managed gen_pool_create   @dev: device that provides the gen_pool   @min_alloc_order: log base 2 of number of bytes each bitmap bit represents   @nid: node selector for allocated gen_pool, %NUMA_NO_NODE for all nodes   @name: name of a gen_pool or NULL, identifies a particular gen_pool on device     Create a new special memory pool that can be used to manage special purpose   memory not managed by the regular kmallockfree interface. The pool will be   automatically destroyed by the device management code. ", "int match_token(char *s, const match_table_t table, substring_t args[])": "match_token - Find a token (and optional args) in a string   @s: the string to examine for tokenargument pairs   @table: match_table_t describing the set of allowed option tokens and the   arguments that may be associated with them. Must be terminated with a   &struct match_token whose pattern is set to the NULL pointer.   @args: array of %MAX_OPT_ARGS &substring_t elements. Used to return match   locations.     Description: Detects which if any of a set of token strings has been passed   to it. Tokens can include up to %MAX_OPT_ARGS instances of basic c-style   format identifiers which will be taken into account when matching the   tokens, and whose locations will be returned in the @args array. ", "int match_int(substring_t *s, int *result)": "match_int - scan a decimal representation of an integer from a substring_t   @s: substring_t to be scanned   @result: resulting integer on success     Description: Attempts to parse the &substring_t @s as a decimal integer.     Return: On success, sets @result to the integer represented by the string   and returns 0. Returns -EINVAL or -ERANGE on failure. ", "int match_uint(substring_t *s, unsigned int *result)": "match_uint - scan a decimal representation of an integer from a substring_t   @s: substring_t to be scanned   @result: resulting integer on success     Description: Attempts to parse the &substring_t @s as a decimal integer.     Return: On success, sets @result to the integer represented by the string   and returns 0. Returns -EINVAL or -ERANGE on failure. ", "static int match_u64int(substring_t *s, u64 *result, int base)": "match_strlcpy(buf, s, NUMBER_BUF_LEN) >= NUMBER_BUF_LEN)return -ERANGE;ret = 0;val = simple_strtol(buf, &endp, base);if (endp == buf)ret = -EINVAL;else if (val < (long)INT_MIN || val > (long)INT_MAX)ret = -ERANGE;else result = (int) val;return ret;}     match_u64int - scan a number in the given base from a substring_t   @s: substring to be scanned   @result: resulting u64 on success   @base: base to use when converting string     Description: Given a &substring_t and a base, attempts to parse the substring   as a number in that base.     Return: On success, sets @result to the integer represented by the   string and returns 0. Returns -EINVAL or -ERANGE on failure. ", "int match_octal(substring_t *s, int *result)": "match_octal - scan an octal representation of an integer from a substring_t   @s: substring_t to be scanned   @result: resulting integer on success     Description: Attempts to parse the &substring_t @s as an octal integer.     Return: On success, sets @result to the integer represented by the string   and returns 0. Returns -EINVAL or -ERANGE on failure. ", "int match_hex(substring_t *s, int *result)": "match_hex - scan a hex representation of an integer from a substring_t   @s: substring_t to be scanned   @result: resulting integer on success     Description: Attempts to parse the &substring_t @s as a hexadecimal integer.     Return: On success, sets @result to the integer represented by the string   and returns 0. Returns -EINVAL or -ERANGE on failure. ", "bool match_wildcard(const char *pattern, const char *str)": "match_wildcard - parse if a string matches given wildcard pattern   @pattern: wildcard pattern   @str: the string to be parsed     Description: Parse the string @str to check if matches wildcard   pattern @pattern. The pattern may contain two types of wildcards:     ' ' - matches zero or more characters     '?' - matches one character     Return: If the @str matches the @pattern, return true, else return false. ", "char *match_strdup(const substring_t *s)": "match_strdup - allocate a new string with the contents of a substring_t   @s: &substring_t to copy     Description: Allocates and returns a string filled with the contents of   the &substring_t @s. The caller is responsible for freeing the returned   string with kfree().     Return: the address of the newly allocated NUL-terminated string or   %NULL on error. ", "s64 __percpu_counter_sum(struct percpu_counter *fbc)": "__percpu_counter_sum() just does the right thing when CPUs are being taken   offline. ", "static inline void debug_percpu_counter_activate(struct percpu_counter *fbc)": "percpu_counter_destroy(fbc);debug_object_free(fbc, &percpu_counter_debug_descr);return true;default:return false;}}static const struct debug_obj_descr percpu_counter_debug_descr = {.name= \"percpu_counter\",.fixup_free= percpu_counter_fixup_free,};static inline void debug_percpu_counter_activate(struct percpu_counter  fbc){debug_object_init(fbc, &percpu_counter_debug_descr);debug_object_activate(fbc, &percpu_counter_debug_descr);}static inline void debug_percpu_counter_deactivate(struct percpu_counter  fbc){debug_object_deactivate(fbc, &percpu_counter_debug_descr);debug_object_free(fbc, &percpu_counter_debug_descr);}#else  CONFIG_DEBUG_OBJECTS_PERCPU_COUNTER ", "if (abs(count - rhs) > (batch * num_online_cpus())) ": "__percpu_counter_compare(struct percpu_counter  fbc, s64 rhs, s32 batch){s64count;count = percpu_counter_read(fbc);  Check to see if rough count will be sufficient for comparison ", "size = roundup_pow_of_two(size);fifo->in = 0;fifo->out = 0;fifo->esize = esize;if (size < 2) ": "__kfifo_alloc(struct __kfifo  fifo, unsigned int size,size_t esize, gfp_t gfp_mask){    round up to the next power of 2, since our 'let the indices   wrap' technique works only in this case. ", "int strncasecmp(const char *s1, const char *s2, size_t len)": "strncasecmp - Case insensitive, length-limited string comparison   @s1: One string   @s2: The other string   @len: the maximum number of characters to compare ", ";return tmp;}EXPORT_SYMBOL(strcpy": "strcpy(char  dest, const char  src){char  tmp = dest;while (( dest++ =  src++) != '\\0')  nothing ", "if ((long)src & (sizeof(long) - 1)) ": "memcpy(dest, src, len);dest[len] = '\\0';}return ret;}EXPORT_SYMBOL(strlcpy);#endif#ifndef __HAVE_ARCH_STRSCPYssize_t strscpy(char  dest, const char  src, size_t count){const struct word_at_a_time constants = WORD_AT_A_TIME_CONSTANTS;size_t max = count;long res = 0;if (count == 0 || WARN_ON_ONCE(count > INT_MAX))return -E2BIG;#ifdef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS    If src is unaligned, don't cross a page boundary,   since we don't know if the next page is mapped. ", "char *stpcpy(char *__restrict__ dest, const char *__restrict__ src);char *stpcpy(char *__restrict__ dest, const char *__restrict__ src)": "stpcpy - copy a string from src to dest returning a pointer to the new end            of dest, including src's %NUL-terminator. May overrun dest.   @dest: pointer to end of string being copied into. Must be large enough          to receive copy.   @src: pointer to the beginning of string being copied from. Must not overlap         dest.     stpcpy differs from strcpy in a key way: the return value is a pointer   to the new %NUL-terminating character in @dest. (For strcpy, the return   value is a pointer to the start of @dest). This interface is considered   unsafe as it doesn't perform bounds checking of the inputs. As such it's   not recommended for usage. Instead, its definition is provided in case   the compiler lowers other libcalls to stpcpy. ", "BUG_ON(dsize >= count);dest += dsize;count -= dsize;if (len >= count)len = count-1;__builtin_memcpy(dest, src, len);dest[len] = 0;return res;}EXPORT_SYMBOL(strlcat": "strlcat(char  dest, const char  src, size_t count){size_t dsize = strlen(dest);size_t len = strlen(src);size_t res = dsize + len;  This would be a bug ", "int strcmp(const char *cs, const char *ct)": "strcmp - Compare two strings   @cs: One string   @ct: Another string ", "int strncmp(const char *cs, const char *ct, size_t count)": "strncmp - Compare two length-limited strings   @cs: One string   @ct: Another string   @count: The maximum number of bytes to compare ", "char *strchr(const char *s, int c)": "strchr - Find the first occurrence of a character in a string   @s: The string to be searched   @c: The character to search for     Note that the %NUL-terminator is considered part of the string, and can   be searched for. ", "char *strchrnul(const char *s, int c)": "strchrnul - Find and return a character in a string, or end of string   @s: The string to be searched   @c: The character to search for     Returns pointer to first occurrence of 'c' in s. If c is not found, then   return a pointer to the null byte at the end of s. ", "char *strrchr(const char *s, int c)": "strrchr - Find the last occurrence of a character in a string   @s: The string to be searched   @c: The character to search for ", "char *strnchrnul(const char *s, size_t count, int c)": "strnchrnul - Find and return a character in a length limited string,   or end of string   @s: The string to be searched   @count: The number of characters to be searched   @c: The character to search for     Returns pointer to the first occurrence of 'c' in s. If c is not found,   then return a pointer to the last character of the string. ", ";return sc - s;}EXPORT_SYMBOL(strnlen": "strnlen(const char  s, size_t count){const char  sc;for (sc = s; count-- &&  sc != '\\0'; ++sc)  nothing ", "size_t strspn(const char *s, const char *accept)": "strspn - Calculate the length of the initial substring of @s which only contain letters in @accept   @s: The string to be searched   @accept: The string to search for ", "size_t strcspn(const char *s, const char *reject)": "strcspn - Calculate the length of the initial substring of @s which does not contain letters in @reject   @s: The string to be searched   @reject: The string to avoid ", "char *strpbrk(const char *cs, const char *ct)": "strpbrk - Find the first occurrence of a set of characters   @cs: The string to be searched   @ct: The characters to search for ", "char *strsep(char **s, const char *ct)": "strsep - Split a string into tokens   @s: The string to be searched   @ct: The characters to search for     strsep() updates @s to point after the token, ready for the next call.     It returns empty tokens, too, behaving exactly like the libc function   of that name. In fact, it was stolen from glibc2 and de-fancy-fied.   Same semantics, slimmer shape. ;) ", "void *memset(void *s, int c, size_t count)": "memset - Fill a region of memory with the given value   @s: Pointer to the start of the area.   @c: The byte to fill the area with   @count: The size of the area.     Do not use memset() to access IO space, use memset_io() instead. ", "void *memset16(uint16_t *s, uint16_t v, size_t count)": "memset16() - Fill a memory area with a uint16_t   @s: Pointer to the start of the area.   @v: The value to fill the area with   @count: The number of values to store     Differs from memset() in that it fills with a uint16_t instead   of a byte.  Remember that @count is the number of uint16_ts to   store, not the number of bytes. ", "void *memset32(uint32_t *s, uint32_t v, size_t count)": "memset32() - Fill a memory area with a uint32_t   @s: Pointer to the start of the area.   @v: The value to fill the area with   @count: The number of values to store     Differs from memset() in that it fills with a uint32_t instead   of a byte.  Remember that @count is the number of uint32_ts to   store, not the number of bytes. ", "void *memset64(uint64_t *s, uint64_t v, size_t count)": "memset64() - Fill a memory area with a uint64_t   @s: Pointer to the start of the area.   @v: The value to fill the area with   @count: The number of values to store     Differs from memset() in that it fills with a uint64_t instead   of a byte.  Remember that @count is the number of uint64_ts to   store, not the number of bytes. ", "void *memmove(void *dest, const void *src, size_t count)": "memmove - Copy one area of memory to another   @dest: Where to copy to   @src: Where to copy from   @count: The size of the area.     Unlike memcpy(), memmove() copes with overlapping areas. ", "#undef memcmp__visible int memcmp(const void *cs, const void *ct, size_t count)": "memcmp - Compare two areas of memory   @cs: One area of memory   @ct: Another area of memory   @count: The size of the area. ", "int bcmp(const void *a, const void *b, size_t len)": "bcmp - returns 0 if and only if the buffers have identical contents.   @a: pointer to first buffer.   @b: pointer to second buffer.   @len: size of buffers.     The sign or magnitude of a non-zero return value has no particular   meaning, and architectures may implement their own more efficient bcmp(). So   while this particular implementation is a simple (tail) call to memcmp, do   not rely on anything but whether the return value is zero or non-zero. ", "void *memscan(void *addr, int c, size_t size)": "memscan - Find a character in an area of memory.   @addr: The memory area   @c: The byte to search for   @size: The size of the area.     returns the address of the first occurrence of @c, or 1 byte past   the area if @c is not found ", "char *strstr(const char *s1, const char *s2)": "strstr - Find the first substring in a %NUL terminated string   @s1: The string to be searched   @s2: The string to search for ", "char *strnstr(const char *s1, const char *s2, size_t len)": "strnstr - Find the first substring in a length-limited string   @s1: The string to be searched   @s2: The string to search for   @len: the maximum number of characters to search ", "void *memchr(const void *s, int c, size_t n)": "memchr - Find a character in an area of memory.   @s: The memory area   @c: The byte to search for   @n: The size of the area.     returns the address of the first occurrence of @c, or %NULL   if @c is not found ", "void *memchr_inv(const void *start, int c, size_t bytes)": "memchr_inv - Find an unmatching character in an area of memory.   @start: The memory area   @c: Find a character other than c   @bytes: The size of the area.     returns the address of the first character other than @c, or %NULL   if the whole buffer contains just @c. ", "int check_signature(const volatile void __iomem *io_addr,const unsigned char *signature, int length)": "check_signature-find BIOS signatures  @io_addr: mmio address to check  @signature:  signature block  @length: length of signature    Perform a signature comparison with the mmio address io_addr. This  address should have been obtained by ioremap.  Returns 1 on a match. ", "void *bsearch(const void *key, const void *base, size_t num, size_t size, cmp_func_t cmp)": "bsearch.h>#include <linuxkprobes.h>    bsearch - binary search an array of elements   @key: pointer to item being searched for   @base: pointer to first element to search   @num: number of elements   @size: size of each element   @cmp: pointer to comparison function     This function does a binary search on the given array.  The   contents of the array should already be in ascending sorted order   under the provided comparison function.     Note that the key need not have the same type as the elements in   the array, e.g. key could be a string and the comparison function   could compare the string with the struct's name field.  However, if   the key and elements in the array are of the same type, you can use   the same comparison function for both sort() and bsearch(). ", "if (atomic_add_unless(atomic, -1, 1))return 0;/* Otherwise do it the slow way ": "_atomic_dec_and_raw_lock_irqsave(atomic_t  atomic, raw_spinlock_t  lock,     unsigned long  flags){  Subtract 1 from counter unless that drops it to 0 (ie. it was 1) ", "__sum16 ip_fast_csum(const void *iph, unsigned int ihl)": "ip_compute_csum() optimized for IP headers,  which always checksum on 4 octet boundaries. ", "result += sum;if (sum > result)result += 1;return (__force __wsum)result;}EXPORT_SYMBOL(csum_partial": "csum_partial(const void  buff, int len, __wsum wsum){unsigned int sum = (__force unsigned int)wsum;unsigned int result = do_csum(buff, len);  add in old sum, and carry.. ", "x = (x & 0xffffffff) + (x >> 32);/* add up carry.. ": "csum_tcpudp_nofoldstatic inline u32 from64to32(u64 x){  add up 32-bit and 32-bit for 32+c bit ", "sibling = parent->rb_right;if (node != sibling) ": "__rb_erase_color(struct rb_node  parent, struct rb_root  root,void ( augment_rotate)(struct rb_node  old, struct rb_node  new)){struct rb_node  node = NULL,  sibling,  tmp1,  tmp2;while (true) {    Loop invariants:   - node is black (or NULL on first iteration)   - node is not the root (parent is not NULL)   - All leaf paths going through parent and node have a     black node count that is 1 lower than other leaf paths. ", "static inline void dummy_propagate(struct rb_node *node, struct rb_node *stop) ": "rb_insert_color() and rb_erase() function definitions. ", "static __always_inline void____rb_erase_color(struct rb_node *parent, struct rb_root *root,void (*augment_rotate)(struct rb_node *old, struct rb_node *new))": "rb_erase() use - we want to be able to inline   and eliminate the dummy_rotate callback there ", "if (node->rb_right) ": "rb_next(const struct rb_node  node){struct rb_node  parent;if (RB_EMPTY_NODE(node))return NULL;    If we have a right-hand child, go down and then left as far   as we can. ", "if (node->rb_left) ": "rb_prev(const struct rb_node  node){struct rb_node  parent;if (RB_EMPTY_NODE(node))return NULL;    If we have a left-hand child, go down and then right as far   as we can. ", "*new = *victim;/* Set the surrounding nodes to point to the replacement ": "rb_replace_node_rcu(struct rb_node  victim, struct rb_node  new, struct rb_root  root){struct rb_node  parent = rb_parent(victim);  Copy the pointerscolour from the victim to the replacement ", "if (parent && node == parent->rb_left && parent->rb_right) ": "rb_next_postorder(const struct rb_node  node){const struct rb_node  parent;if (!node)return NULL;parent = rb_parent(node);  If we're sitting on node, we've already seen our children ", "#include <linux/stdarg.h>#include <linux/export.h>#include <linux/slab.h>#include <linux/types.h>#include <linux/string.h>/* Simplified asprintf. ": "kasprintf.c      Copyright (C) 1991, 1992  Linus Torvalds ", "if (bf->selector & ~valid_flags_mask)return -EINVAL;/*disallow invalid bit values ": "__nla_validate_parse(const struct nlattr  head, int len, int maxtype,const struct nla_policy  policy,unsigned int validate,struct netlink_ext_ack  extack,struct nlattr   tb, unsigned int depth);static int validate_nla_bitfield32(const struct nlattr  nla,   const u32 valid_flags_mask){const struct nla_bitfield32  bf = nla_data(nla);if (!valid_flags_mask)return -EINVAL; disallow invalid bit selector ", "intnla_policy_len(const struct nla_policy *p, int n)": "nla_policy_len - Determine the max. length of a policy   @p: policy to use   @n: number of policies     Determines the max. length of the policy.  It is currently used   to allocated Netlink buffers roughly the size of the actual   message.     Returns 0 on success or a negative error code. ", "int __nla_parse(struct nlattr **tb, int maxtype,const struct nlattr *head, int len,const struct nla_policy *policy, unsigned int validate,struct netlink_ext_ack *extack)": "__nla_parse - Parse a stream of attributes into a tb buffer   @tb: destination array with maxtype+1 elements   @maxtype: maximum attribute type to be expected   @head: head of attribute stream   @len: length of attribute stream   @policy: validation policy   @validate: validation strictness   @extack: extended ACK pointer     Parses a stream of attributes and stores a pointer to each attribute in   the tb array accessible via the attribute type.   Validation is controlled by the @validate parameter.     Returns 0 on success or a negative error code. ", "struct nlattr *nla_find(const struct nlattr *head, int len, int attrtype)": "nla_find - Find a specific attribute in a stream of attributes   @head: head of attribute stream   @len: length of attribute stream   @attrtype: type of attribute to look for     Returns the first attribute in the stream matching the specified type. ", "ssize_t nla_strscpy(char *dst, const struct nlattr *nla, size_t dstsize)": "nla_strscpy - Copy string attribute payload into a sized buffer   @dst: Where to copy the string to.   @nla: Attribute to copy the string from.   @dstsize: Size of destination buffer.     Copies at most dstsize - 1 bytes into the destination buffer.   Unlike strlcpy the destination buffer is always padded out.     Return:     srclen - Returns @nla length (not including the trailing %NUL).     -E2BIG - If @dstsize is 0 or greater than U16_MAX or @nla length greater              than @dstsize. ", "char *nla_strdup(const struct nlattr *nla, gfp_t flags)": "nla_strdup - Copy string attribute payload into a newly allocated buffer   @nla: attribute to copy the string from   @flags: the type of memory to allocate (see kmalloc).     Returns a pointer to the allocated buffer or NULL on error. ", "int nla_memcpy(void *dest, const struct nlattr *src, int count)": "nla_memcpy - Copy a netlink attribute into another memory area   @dest: where to copy to memcpy   @src: netlink attribute to copy from   @count: size of the destination area     Note: The number of bytes copied is limited by the length of         attribute's payload. memcpy     Returns the number of bytes copied. ", "int nla_memcmp(const struct nlattr *nla, const void *data,     size_t size)": "nla_memcmp - Compare an attribute with sized memory area   @nla: netlink attribute   @data: memory area   @size: size of memory area ", "int nla_strcmp(const struct nlattr *nla, const char *str)": "nla_strcmp - Compare a string attribute against a string   @nla: netlink string attribute   @str: another string ", "struct nlattr *__nla_reserve(struct sk_buff *skb, int attrtype, int attrlen)": "nla_reserve - reserve room for attribute on the skb   @skb: socket buffer to reserve room on   @attrtype: attribute type   @attrlen: length of attribute payload     Adds a netlink attribute header to a socket buffer and reserves   room for the payload but does not copy it.     The caller is responsible to ensure that the skb provides enough   tailroom for the attribute header and payload. ", "struct nlattr *__nla_reserve_64bit(struct sk_buff *skb, int attrtype,   int attrlen, int padattr)": "nla_reserve_64bit - reserve room for attribute on the skb and align it   @skb: socket buffer to reserve room on   @attrtype: attribute type   @attrlen: length of attribute payload   @padattr: attribute type for the padding     Adds a netlink attribute header to a socket buffer and reserves   room for the payload but does not copy it. It also ensure that this   attribute will have a 64-bit aligned nla_data() area.     The caller is responsible to ensure that the skb provides enough   tailroom for the attribute header and payload. ", "void *__nla_reserve_nohdr(struct sk_buff *skb, int attrlen)": "nla_reserve_nohdr - reserve room for attribute without header   @skb: socket buffer to reserve room on   @attrlen: length of attribute payload     Reserves room for attribute payload without a header.     The caller is responsible to ensure that the skb provides enough   tailroom for the payload. ", "void __nla_put(struct sk_buff *skb, int attrtype, int attrlen,     const void *data)": "nla_put - Add a netlink attribute to a socket buffer   @skb: socket buffer to add attribute to   @attrtype: attribute type   @attrlen: length of attribute payload   @data: head of attribute payload     The caller is responsible to ensure that the skb provides enough   tailroom for the attribute header and payload. ", "void __nla_put_64bit(struct sk_buff *skb, int attrtype, int attrlen,     const void *data, int padattr)": "nla_put_64bit - Add a netlink attribute to a socket buffer and align it   @skb: socket buffer to add attribute to   @attrtype: attribute type   @attrlen: length of attribute payload   @data: head of attribute payload   @padattr: attribute type for the padding     The caller is responsible to ensure that the skb provides enough   tailroom for the attribute header and payload. ", "void __nla_put_nohdr(struct sk_buff *skb, int attrlen, const void *data)": "nla_put_nohdr - Add a netlink attribute without header   @skb: socket buffer to add attribute to   @attrlen: length of attribute payload   @data: head of attribute payload     The caller is responsible to ensure that the skb provides enough   tailroom for the attribute payload. ", "int nla_append(struct sk_buff *skb, int attrlen, const void *data)": "nla_append - Add a netlink attribute without header or padding   @skb: socket buffer to add attribute to   @attrlen: length of attribute payload   @data: head of attribute payload     Returns -EMSGSIZE if the tailroom of the skb is insufficient to store   the attribute payload. ", "noinlineunsigned long long simple_strtoull(const char *cp, char **endp, unsigned int base)": "simple_strtoull - convert a string to an unsigned long long   @cp: The start of the string   @endp: A pointer to the end of the parsed string will be placed here   @base: The number base to use     This function has caveats. Please use kstrtoull instead. ", "long simple_strtol(const char *cp, char **endp, unsigned int base)": "simple_strtol - convert a string to a signed long   @cp: The start of the string   @endp: A pointer to the end of the parsed string will be placed here   @base: The number base to use     This function has caveats. Please use kstrtol instead. ", "long long simple_strtoll(const char *cp, char **endp, unsigned int base)": "simple_strtoll - convert a string to a signed long long   @cp: The start of the string   @endp: A pointer to the end of the parsed string will be placed here   @base: The number base to use     This function has caveats. Please use kstrtoll instead. ", "#include <linux/stdarg.h>#include <linux/build_bug.h>#include <linux/clk.h>#include <linux/clk-provider.h>#include <linux/errname.h>#include <linux/module.h>/* for KSYM_SYMBOL_LEN ": "scnprintf and vscnprintf ", "/* vsprintf.c -- Lars Wirzenius & Linus Torvalds. ": "sprintf.c      Copyright (C) 1991, 1992  Linus Torvalds ", "int vsscanf(const char *buf, const char *fmt, va_list args)": "sscanf - Unformat a buffer into a list of arguments   @buf:input buffer   @fmt:format of buffer   @args:arguments ", "align = (sizeof(unsigned long) - 1) & (unsigned long)src;src -= align;max += align;unsafe_get_user(c, (unsigned long __user *)src, efault);c |= aligned_byte_mask(align);for (;;) ": "strnlen_user(const char __user  src, unsigned long count, unsigned long max){const struct word_at_a_time constants = WORD_AT_A_TIME_CONSTANTS;unsigned long align, res = 0;unsigned long c;    Do everything aligned. But that means that we   need to also expand the maximum.. ", "void sort_r(void *base, size_t num, size_t size,    cmp_r_func_t cmp_func,    swap_r_func_t swap_func,    const void *priv)": "sort_r - sort an array of elements   @base: pointer to data to sort   @num: number of elements   @size: size of each element   @cmp_func: pointer to comparison function   @swap_func: pointer to swap function or NULL   @priv: third argument passed to comparison function     This function does a heapsort on the given array.  You may provide   a swap_func function if you need to do something more than a memory   copy (e.g. fix up pointers or auxiliary data), but the built-in swap   avoids a slow retpoline and so is significantly faster.     Sorting time is O(n log n) both on average and worst-case. While   quicksort is slightly faster on average, it suffers from exploitable   O(n n) worst-case behavior and extra memory requirements that make   it less suitable for kernel use. ", "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt#include <linux/types.h>#include <linux/export.h>#include <linux/sort.h>/** * is_aligned - is this pointer & size okay for word-wide copying? * @base: pointer to data * @size: size of each element * @align: required alignment (typically 4 or 8) * * Returns true if elements can be copied using word loads and stores. * The size must be a multiple of the alignment, and the base address must * be if we do not have CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS. * * For some reason, gcc doesn't know to optimize \"if (a & mask || b & mask)\" * to \"if ((a | b) & mask)\", so we do that by hand. ": "sort for the Linux kernel     This performs n log2(n) + 0.37 n + o(n) comparisons on average,   and 1.5 n log2(n) + O(n) in the (very contrived) worst case.     Glibc qsort() manages n log2(n) - 1.26 n for random inputs (1.63 n   better) at the expense of stack usage and much larger code to avoid   quicksort's O(n^2) worst case. ", "asmlinkage __visible void dump_stack_lvl(const char *log_lvl)": "dump_stack_lvl - dump the current task information and its stack trace   @log_lvl: log level     Architectures can override this implementation by implementing its own. ", "#include <linux/kernel.h>#include <linux/buildid.h>#include <linux/export.h>#include <linux/sched.h>#include <linux/sched/debug.h>#include <linux/smp.h>#include <linux/atomic.h>#include <linux/kexec.h>#include <linux/utsname.h>#include <linux/stop_machine.h>static char dump_stack_arch_desc_str[128];/** * dump_stack_set_arch_desc - set arch-specific str to show with task dumps * @fmt: printf-style format string * @...: arguments for the format string * * The configured string will be printed right after utsname during task * dumps.  Usually used to add arch-specific system identifiers.  If an * arch wants to make use of such an ID string, it should initialize this * as soon as possible during boot. ": "dump_stack() function for architectures   which don't implement their own. ", "barrier_nospec();instrument_copy_from_user_before(to, from, n);res = raw_copy_from_user(to, from, n);instrument_copy_from_user_after(to, from, n, res);}if (unlikely(res))memset(to + (n - res), 0, res);return res;}EXPORT_SYMBOL(_copy_from_user": "_copy_from_user(void  to, const void __user  from, unsigned long n){unsigned long res = n;might_fault();if (!should_fail_usercopy() && likely(access_ok(from, n))) {    Ensure that bad access_ok() speculation will not   lead to nasty side effects  after  the copy is   finished: ", "int check_zeroed_user(const void __user *from, size_t size)": "check_zeroed_user: check if a userspace buffer only contains zero bytes   @from: Source address, in userspace.   @size: Size of buffer.     This is effectively shorthand for \"memchr_inv(from, 0, size) == NULL\" for   userspace addresses (and is more efficient because we don't care where the   first non-zero byte is).     Returns:      0: There were non-zero bytes present in the buffer.      1: The buffer was full of zero bytes.      -EFAULT: access to userspace failed. ", "void __iomem *devm_ioremap(struct device *dev, resource_size_t offset,   resource_size_t size)": "devm_ioremap_type {DEVM_IOREMAP = 0,DEVM_IOREMAP_UC,DEVM_IOREMAP_WC,DEVM_IOREMAP_NP,};void devm_ioremap_release(struct device  dev, void  res){iounmap( (void __iomem   )res);}static int devm_ioremap_match(struct device  dev, void  res, void  match_data){return  (void   )res == match_data;}static void __iomem  __devm_ioremap(struct device  dev, resource_size_t offset,    resource_size_t size,    enum devm_ioremap_type type){void __iomem   ptr,  addr = NULL;ptr = devres_alloc_node(devm_ioremap_release, sizeof( ptr), GFP_KERNEL,dev_to_node(dev));if (!ptr)return NULL;switch (type) {case DEVM_IOREMAP:addr = ioremap(offset, size);break;case DEVM_IOREMAP_UC:addr = ioremap_uc(offset, size);break;case DEVM_IOREMAP_WC:addr = ioremap_wc(offset, size);break;case DEVM_IOREMAP_NP:addr = ioremap_np(offset, size);break;}if (addr) { ptr = addr;devres_add(dev, ptr);} elsedevres_free(ptr);return addr;}     devm_ioremap - Managed ioremap()   @dev: Generic device to remap IO address for   @offset: Resource address to map   @size: Size of map     Managed ioremap().  Map is automatically unmapped on driver detach. ", "void __iomem *devm_ioremap_wc(struct device *dev, resource_size_t offset,      resource_size_t size)": "devm_ioremap_wc - Managed ioremap_wc()   @dev: Generic device to remap IO address for   @offset: Resource address to map   @size: Size of map     Managed ioremap_wc().  Map is automatically unmapped on driver detach. ", "void devm_iounmap(struct device *dev, void __iomem *addr)": "devm_iounmap - Managed iounmap()   @dev: Generic device to unmap for   @addr: Address to unmap     Managed iounmap().  @addr must have been mapped using devm_ioremap (). ", "void __iomem *devm_ioremap_resource(struct device *dev,    const struct resource *res)": "devm_ioremap_resource(struct device  dev, const struct resource  res,enum devm_ioremap_type type){resource_size_t size;void __iomem  dest_ptr;char  pretty_name;BUG_ON(!dev);if (!res || resource_type(res) != IORESOURCE_MEM) {dev_err(dev, \"invalid resource %pR\\n\", res);return IOMEM_ERR_PTR(-EINVAL);}if (type == DEVM_IOREMAP && res->flags & IORESOURCE_MEM_NONPOSTED)type = DEVM_IOREMAP_NP;size = resource_size(res);if (res->name)pretty_name = devm_kasprintf(dev, GFP_KERNEL, \"%s %s\",     dev_name(dev), res->name);elsepretty_name = devm_kstrdup(dev, dev_name(dev), GFP_KERNEL);if (!pretty_name) {dev_err(dev, \"can't generate pretty name for resource %pR\\n\", res);return IOMEM_ERR_PTR(-ENOMEM);}if (!devm_request_mem_region(dev, res->start, size, pretty_name)) {dev_err(dev, \"can't request region for resource %pR\\n\", res);return IOMEM_ERR_PTR(-EBUSY);}dest_ptr = __devm_ioremap(dev, res->start, size, type);if (!dest_ptr) {dev_err(dev, \"ioremap failed for resource %pR\\n\", res);devm_release_mem_region(dev, res->start, size);dest_ptr = IOMEM_ERR_PTR(-ENOMEM);}return dest_ptr;}     devm_ioremap_resource() - check, request region, and ioremap resource   @dev: generic device to handle the resource for   @res: resource to be handled     Checks that a resource is a valid memory region, requests the memory   region and ioremaps it. All operations are managed and will be undone   on driver detach.     Usage example:    res = platform_get_resource(pdev, IORESOURCE_MEM, 0);  base = devm_ioremap_resource(&pdev->dev, res);  if (IS_ERR(base))  return PTR_ERR(base);     Return: a pointer to the remapped memory or an ERR_PTR() encoded error code   on failure. ", "void __iomem *devm_of_iomap(struct device *dev, struct device_node *node, int index,    resource_size_t *size)": "devm_of_iomap - Requests a resource and maps the memory mapped IO     for a given device_node managed by a given device     Checks that a resource is a valid memory region, requests the memory   region and ioremaps it. All operations are managed and will be undone   on driver detach of the device.     This is to be used when a device requestsmaps resources described   by other device tree nodes (children or otherwise).     @dev:The device \"managing\" the resource   @node:       The device-tree node where the resource resides   @index:index of the MMIO range in the \"reg\" property   @size:Returns the size of the resource (pass NULL if not needed)     Usage example:    base = devm_of_iomap(&pdev->dev, node, 0, NULL);  if (IS_ERR(base))  return PTR_ERR(base);     Please Note: This is not a one-to-one replacement for of_iomap() because the   of_iomap() function does not track whether the region is already mapped.  If   two drivers try to map the same memory, the of_iomap() function will succeed   but the devm_of_iomap() function will return -EBUSY.     Return: a pointer to the requested and mapped memory or an ERR_PTR() encoded   error code on failure. ", "void __iomem *devm_ioport_map(struct device *dev, unsigned long port,       unsigned int nr)": "devm_ioport_map_release(struct device  dev, void  res){ioport_unmap( (void __iomem   )res);}static int devm_ioport_map_match(struct device  dev, void  res, void  match_data){return  (void   )res == match_data;}     devm_ioport_map - Managed ioport_map()   @dev: Generic device to map ioport for   @port: Port to map   @nr: Number of ports to map     Managed ioport_map().  Map is automatically unmapped on driver   detach.     Return: a pointer to the remapped memory or NULL on failure. ", "void devm_ioport_unmap(struct device *dev, void __iomem *addr)": "devm_ioport_unmap - Managed ioport_unmap()   @dev: Generic device to unmap for   @addr: Address to unmap     Managed ioport_unmap().  @addr must have been mapped using   devm_ioport_map(). ", "void __iomem * const *pcim_iomap_table(struct pci_dev *pdev)": "pcim_iomap_devres {void __iomem  table[PCIM_IOMAP_MAX];};static void pcim_iomap_release(struct device  gendev, void  res){struct pci_dev  dev = to_pci_dev(gendev);struct pcim_iomap_devres  this = res;int i;for (i = 0; i < PCIM_IOMAP_MAX; i++)if (this->table[i])pci_iounmap(dev, this->table[i]);}     pcim_iomap_table - access iomap allocation table   @pdev: PCI device to access iomap table for     Access iomap allocation table for @dev.  If iomap table doesn't   exist and @pdev is managed, it will be allocated.  All iomaps   recorded in the iomap table are automatically unmapped on driver   detach.     This function might sleep when the table is first allocated but can   be safely called without context and guaranteed to succeed once   allocated. ", "void pcim_iounmap(struct pci_dev *pdev, void __iomem *addr)": "pcim_iounmap - Managed pci_iounmap()   @pdev: PCI device to iounmap for   @addr: Address to unmap     Managed pci_iounmap().  @addr must have been mapped using pcim_iomap(). ", "int pcim_iomap_regions(struct pci_dev *pdev, int mask, const char *name)": "pcim_iomap_regions - Request and iomap PCI BARs   @pdev: PCI device to map IO resources for   @mask: Mask of BARs to request and iomap   @name: Name used when requesting regions     Request and iomap regions specified by @mask. ", "int pcim_iomap_regions_request_all(struct pci_dev *pdev, int mask,   const char *name)": "pcim_iomap_regions_request_all - Request all BARs and iomap specified ones   @pdev: PCI device to map IO resources for   @mask: Mask of BARs to iomap   @name: Name used when requesting regions     Request all PCI BARs and iomap regions specified by @mask. ", "void pcim_iounmap_regions(struct pci_dev *pdev, int mask)": "pcim_iounmap_regions - Unmap and release PCI BARs   @pdev: PCI device to map IO resources for   @mask: Mask of BARs to unmap and release     Unmap and release regions specified by @mask. ", "int devm_arch_phys_wc_add(struct device *dev, unsigned long base, unsigned long size)": "devm_arch_phys_wc_add - Managed arch_phys_wc_add()   @dev: Managed device   @base: Memory base address   @size: Size of memory range     Adds a WC MTRR using arch_phys_wc_add() and sets up a release callback.   See arch_phys_wc_add() for more information. ", "int devm_arch_io_reserve_memtype_wc(struct device *dev, resource_size_t start,    resource_size_t size)": "devm_arch_io_reserve_memtype_wc - Managed arch_io_reserve_memtype_wc()   @dev: Managed device   @start: Memory base address   @size: Size of memory range     Reserves a memory range with WC caching using arch_io_reserve_memtype_wc()   and sets up a release callback See arch_io_reserve_memtype_wc() for more   information. ", "void chacha_block_generic(u32 *state, u8 *stream, int nrounds)": "chacha_block_generic - generate one keystream block and increment block counter   @state: input state matrix (16 32-bit words)   @stream: output keystream block (64 bytes)   @nrounds: number of rounds (20 or 12; 20 is recommended)     This is the ChaCha core, a function from 64-byte strings to 64-byte strings.   The caller has already converted the endianness of the input.  This function   also handles incrementing the block counter in the input matrix. ", "void hchacha_block_generic(const u32 *state, u32 *stream, int nrounds)": "hchacha_block_generic - abbreviated ChaCha core, for XChaCha   @state: input state matrix (16 32-bit words)   @stream: output (8 32-bit words)   @nrounds: number of rounds (20 or 12; 20 is recommended)     HChaCha is the ChaCha equivalent of HSalsa and is an intermediate step   towards XChaCha (see https:cr.yp.tosnufflexsalsa-20081128.pdf).  HChaCha   skips the final addition of the initial state, and outputs only certain words   of the state.  It should not be used for streaming directly. ", "key->key.r[0] = (get_unaligned_le32(&raw_key[0])) & 0x3ffffff;key->key.r[1] = (get_unaligned_le32(&raw_key[3]) >> 2) & 0x3ffff03;key->key.r[2] = (get_unaligned_le32(&raw_key[6]) >> 4) & 0x3ffc0ff;key->key.r[3] = (get_unaligned_le32(&raw_key[9]) >> 6) & 0x3f03fff;key->key.r[4] = (get_unaligned_le32(&raw_key[12]) >> 8) & 0x00fffff;/* s = 5*r ": "poly1305_core_setkey(struct poly1305_core_key  key,  const u8 raw_key[POLY1305_BLOCK_SIZE]){  r &= 0xffffffc0ffffffc0ffffffc0fffffff ", "h0 += (get_unaligned_le32(&input[0])) & 0x3ffffff;h1 += (get_unaligned_le32(&input[3]) >> 2) & 0x3ffffff;h2 += (get_unaligned_le32(&input[6]) >> 4) & 0x3ffffff;h3 += (get_unaligned_le32(&input[9]) >> 6) & 0x3ffffff;h4 += (get_unaligned_le32(&input[12]) >> 8) | hibit;/* h *= r ": "poly1305_core_blocks(struct poly1305_state  state,  const struct poly1305_core_key  key, const void  src,  unsigned int nblocks, u32 hibit){const u8  input = src;u32 r0, r1, r2, r3, r4;u32 s1, s2, s3, s4;u32 h0, h1, h2, h3, h4;u64 d0, d1, d2, d3, d4;u32 c;if (!nblocks)return;hibit <<= 24;r0 = key->key.r[0];r1 = key->key.r[1];r2 = key->key.r[2];r3 = key->key.r[3];r4 = key->key.r[4];s1 = key->precomputed_s.r[0];s2 = key->precomputed_s.r[1];s3 = key->precomputed_s.r[2];s4 = key->precomputed_s.r[3];h0 = state->h[0];h1 = state->h[1];h2 = state->h[2];h3 = state->h[3];h4 = state->h[4];do {  h += m[i] ", "h0 = state->h[0];h1 = state->h[1];h2 = state->h[2];h3 = state->h[3];h4 = state->h[4];c = h1 >> 26;h1 = h1 & 0x3ffffff;h2 += c;c = h2 >> 26;h2 = h2 & 0x3ffffff;h3 += c;c = h3 >> 26;h3 = h3 & 0x3ffffff;h4 += c;c = h4 >> 26;h4 = h4 & 0x3ffffff;h0 += c * 5;c = h0 >> 26;h0 = h0 & 0x3ffffff;h1 += c;/* compute h + -p ": "poly1305_core_emit(const struct poly1305_state  state, const u32 nonce[4],void  dst){u8  mac = dst;u32 h0, h1, h2, h3, h4, c;u32 g0, g1, g2, g3, g4;u64 f;u32 mask;  fully carry h ", "while (size > 0) ": "__crypto_memneq_generic(const void  a, const void  b, size_t size){unsigned long neq = 0;#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)while (size >= sizeof(unsigned long)) {neq |= get_unaligned((unsigned long  )a) ^       get_unaligned((unsigned long  )b);OPTIMIZER_HIDE_VAR(neq);a += sizeof(unsigned long);b += sizeof(unsigned long);size -= sizeof(unsigned long);}#endif   CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS ", "t0 = get_unaligned_le64(&raw_key[0]);t1 = get_unaligned_le64(&raw_key[8]);key->key.r64[0] = t0 & 0xffc0fffffffULL;key->key.r64[1] = ((t0 >> 44) | (t1 << 20)) & 0xfffffc0ffffULL;key->key.r64[2] = ((t1 >> 24)) & 0x00ffffffc0fULL;/* s = 20*r ": "poly1305_core_setkey(struct poly1305_core_key  key,  const u8 raw_key[POLY1305_BLOCK_SIZE]){u64 t0, t1;  r &= 0xffffffc0ffffffc0ffffffc0fffffff ", "t0 = get_unaligned_le64(&input[0]);t1 = get_unaligned_le64(&input[8]);h0 += t0 & 0xfffffffffffULL;h1 += ((t0 >> 44) | (t1 << 20)) & 0xfffffffffffULL;h2 += (((t1 >> 24)) & 0x3ffffffffffULL) | hibit64;/* h *= r ": "poly1305_core_blocks(struct poly1305_state  state,  const struct poly1305_core_key  key, const void  src,  unsigned int nblocks, u32 hibit){const u8  input = src;u64 hibit64;u64 r0, r1, r2;u64 s1, s2;u64 h0, h1, h2;u64 c;u128 d0, d1, d2, d;if (!nblocks)return;hibit64 = ((u64)hibit) << 40;r0 = key->key.r64[0];r1 = key->key.r64[1];r2 = key->key.r64[2];h0 = state->h64[0];h1 = state->h64[1];h2 = state->h64[2];s1 = key->precomputed_s.r64[0];s2 = key->precomputed_s.r64[1];do {u64 t0, t1;  h += m[i] ", "h0 = state->h64[0];h1 = state->h64[1];h2 = state->h64[2];c = h1 >> 44;h1 &= 0xfffffffffffULL;h2 += c;c = h2 >> 42;h2 &= 0x3ffffffffffULL;h0 += c * 5;c = h0 >> 44;h0 &= 0xfffffffffffULL;h1 += c;c = h1 >> 44;h1 &= 0xfffffffffffULL;h2 += c;c = h2 >> 42;h2 &= 0x3ffffffffffULL;h0 += c * 5;c = h0 >> 44;h0 &= 0xfffffffffffULL;h1 += c;/* compute h + -p ": "poly1305_core_emit(const struct poly1305_state  state, const u32 nonce[4],void  dst){u8  mac = dst;u64 h0, h1, h2, c;u64 g0, g1, g2;u64 t0, t1;  fully carry h ", "be128 array[16 + 3] = ": "gf128mul_lle(be128  r, const be128  b){    The p array should be aligned to twice the size of its element type,   so that every evenodd pair is guaranteed to share a cacheline   (assuming a cacheline size of 32 bytes or more, which is by far the   most common). This ensures that each be128_xor() call in the loop   takes the same amount of time regardless of the value of 'ch', which   is derived from function parameter 'b', which is commonly used as a   key, e.g., for GHASH. The odd array elements are all set to zero,   making each be128_xor() a NOP if its associated bit in 'ch' is not   set, and this is equivalent to calling be128_xor() conditionally.   This approach aims to avoid leaking information about such keys   through execution time variances.     Unfortunately, __aligned(16) or higher does not work on x86 for   variables on the stack so we need to perform the alignment by hand. ", "int aesgcm_expandkey(struct aesgcm_ctx *ctx, const u8 *key,     unsigned int keysize, unsigned int authsize)": "aesgcm_expandkey - Expands the AES and GHASH keys for the AES-GCM key        schedule     @ctx:The data structure that will hold the AES-GCM key schedule   @key:The AES encryption input key   @keysize:The length in bytes of the input key   @authsize:The size in bytes of the GCM authentication tag     Returns: 0 on success, or -EINVAL if @keysize or @authsize contain values   that are not permitted by the GCM specification. ", "local_irq_save(flags);aes_encrypt(ctx, dst, src);local_irq_restore(flags);}/** * aesgcm_expandkey - Expands the AES and GHASH keys for the AES-GCM key *      schedule * * @ctx:The data structure that will hold the AES-GCM key schedule * @key:The AES encryption input key * @keysize:The length in bytes of the input key * @authsize:The size in bytes of the GCM authentication tag * * Returns: 0 on success, or -EINVAL if @keysize or @authsize contain values * that are not permitted by the GCM specification. ": "aesgcm_encrypt_block(const struct crypto_aes_ctx  ctx, void  dst, const void  src){unsigned long flags;    In AES-GCM, both the GHASH key derivation and the CTR mode   encryption operate on known plaintext, making them susceptible to   timing attacks on the encryption key. The AES library already   mitigates this risk to some extent by pulling the entire S-box into   the caches before doing any substitutions, but this strategy is more   effective when running with interrupts disabled. ", "bool __must_check aesgcm_decrypt(const struct aesgcm_ctx *ctx, u8 *dst, const u8 *src, int crypt_len, const u8 *assoc, int assoc_len, const u8 iv[GCM_AES_IV_SIZE], const u8 *authtag)": "aesgcm_decrypt - Perform AES-GCM decryption on a block of data     @ctx:The AES-GCM key schedule   @dst:Pointer to the plaintext output buffer   @src:Pointer the ciphertext (may equal @dst for decryption in place)   @crypt_len:The size in bytes of the plaintext and ciphertext.   @assoc:Pointer to the associated data,   @assoc_len:The size in bytes of the associated data   @iv:The initialization vector (IV) to use for this block of data  (must be 12 bytes in size as per the GCM spec recommendation)   @authtag:The address of the buffer in memory where the authentication  tag is stored.     Returns: true on success, or false if the ciphertext failed authentication.   On failure, no plaintext will be returned. ", "#include <asm/unaligned.h>#include <crypto/sha256_base.h>#include <linux/kernel.h>#include <linux/module.h>#include <linux/string.h>static const u32 SHA256_K[] = ": "sha256-384-512.pdf     SHA-256 code by Jean-Luc Cooke <jlcooke@certainkey.com>.     Copyright (c) Jean-Luc Cooke <jlcooke@certainkey.com>   Copyright (c) Andrew McDonald <andrew@mcdonald.org.uk>   Copyright (c) 2002 James Morris <jmorris@intercode.com.au>   Copyright (c) 2014 Red Hat Inc. ", "u8 stream[CHACHA_BLOCK_SIZE] __aligned(sizeof(long));while (bytes >= CHACHA_BLOCK_SIZE) ": "chacha_crypt_generic(u32  state, u8  dst, const u8  src,  unsigned int bytes, int nrounds){  aligned to potentially speed up crypto_xor() ", "blake2s_compress(state, state->buf, 1, state->buflen);cpu_to_le32_array(state->h, ARRAY_SIZE(state->h));memcpy(out, state->h, state->outlen);memzero_explicit(state, sizeof(*state));}EXPORT_SYMBOL(blake2s_final": "blake2s_final(struct blake2s_state  state, u8  out){WARN_ON(IS_ENABLED(DEBUG) && !out);blake2s_set_lastblock(state);memset(state->buf + state->buflen, 0,       BLAKE2S_BLOCK_SIZE - state->buflen);   Padding ", "int aes_expandkey(struct crypto_aes_ctx *ctx, const u8 *in_key,  unsigned int key_len)": "aes_expandkey - Expands the AES key as described in FIPS-197   @ctx:The location where the computed key will be stored.   @in_key:The supplied key.   @key_len:The length of the supplied key.     Returns 0 on success. The function fails only if an invalid key size (or   pointer) is supplied.   The expanded key size is 240 bytes (max of 14 rounds with a unique 16 bytes   key schedule plus a 16 bytes key which is used before the first round).   The decryption key is prepared for the \"Equivalent Inverse Cipher\" as   described in FIPS-197. The first slot (16 bytes) of each key (enc or dec) is   for the initial combination, the second slot for the first round and so on. ", "void aes_encrypt(const struct crypto_aes_ctx *ctx, u8 *out, const u8 *in)": "aes_encrypt - Encrypt a single AES block   @ctx:Context struct containing the key schedule   @out:Buffer to store the ciphertext   @in:Buffer containing the plaintext ", "void aes_decrypt(const struct crypto_aes_ctx *ctx, u8 *out, const u8 *in)": "aes_decrypt - Decrypt a single AES block   @ctx:Context struct containing the key schedule   @out:Buffer to store the plaintext   @in:Buffer containing the ciphertext ", "void sha1_transform(__u32 *digest, const char *data, __u32 *array)": "sha1_transform - single block SHA1 transform (deprecated)     @digest: 160 bit digest to update   @data:   512 bits of data to hash   @array:  16 words of workspace (see note)     This function executes SHA-1's internal compression function.  It updates the   160-bit internal state (@digest) with a single 512-bit data block (@data).     Don't use this function.  SHA-1 is no longer considered secure.  And even if   you do have to use SHA-1, this isn't the correct way to hash something with   SHA-1 as this doesn't handle padding and finalization.     Note: If the hash is security sensitive, the caller should be sure   to clear the workspace. This is left to the caller to avoid   unnecessary clears between chained hashing operations. ", "void sha1_init(__u32 *buf)": "sha1_init - initialize the vectors for a SHA1 digest   @buf: vector to initialize ", "#include <linux/module.h>#include <linux/init.h>#include <linux/zlib.h>EXPORT_SYMBOL(zlib_inflate": "zlib_inflateinflate_syms.c     Exported symbols for the inflate functionality.   ", "return (dim->steps_right > 1) && (dim->steps_left == 1);}}EXPORT_SYMBOL(dim_on_top": "dim_on_top(struct dim  dim){switch (dim->tune_state) {case DIM_PARKING_ON_TOP:case DIM_PARKING_TIRED:return true;case DIM_GOING_RIGHT:return (dim->steps_left > 1) && (dim->steps_right == 1);default:   DIM_GOING_LEFT ", "u32 delta_us = ktime_us_delta(end->time, start->time);u32 npkts = BIT_GAP(BITS_PER_TYPE(u32), end->pkt_ctr, start->pkt_ctr);u32 nbytes = BIT_GAP(BITS_PER_TYPE(u32), end->byte_ctr,     start->byte_ctr);u32 ncomps = BIT_GAP(BITS_PER_TYPE(u32), end->comp_ctr,     start->comp_ctr);if (!delta_us)return false;curr_stats->ppms = DIV_ROUND_UP(npkts * USEC_PER_MSEC, delta_us);curr_stats->bpms = DIV_ROUND_UP(nbytes * USEC_PER_MSEC, delta_us);curr_stats->epms = DIV_ROUND_UP(DIM_NEVENTS * USEC_PER_MSEC,delta_us);curr_stats->cpms = DIV_ROUND_UP(ncomps * USEC_PER_MSEC, delta_us);if (curr_stats->epms != 0)curr_stats->cpe_ratio = DIV_ROUND_DOWN_ULL(curr_stats->cpms * 100, curr_stats->epms);elsecurr_stats->cpe_ratio = 0;return true;}EXPORT_SYMBOL(dim_calc_stats": "dim_calc_stats(struct dim_sample  start, struct dim_sample  end,    struct dim_stats  curr_stats){  u32 holds up to 71 minutes, should be enough ", "if (!prev->cpms)return DIM_STATS_SAME;if (IS_SIGNIFICANT_DIFF(curr->cpms, prev->cpms))return (curr->cpms > prev->cpms) ? DIM_STATS_BETTER :DIM_STATS_WORSE;if (IS_SIGNIFICANT_DIFF(curr->cpe_ratio, prev->cpe_ratio))return (curr->cpe_ratio > prev->cpe_ratio) ? DIM_STATS_BETTER :DIM_STATS_WORSE;return DIM_STATS_SAME;}static bool rdma_dim_decision(struct dim_stats *curr_stats, struct dim *dim)": "rdma_dim_step(struct dim  dim){if (dim->tune_state == DIM_GOING_RIGHT) {if (dim->profile_ix == (RDMA_DIM_PARAMS_NUM_PROFILES - 1))return DIM_ON_EDGE;dim->profile_ix++;dim->steps_right++;}if (dim->tune_state == DIM_GOING_LEFT) {if (dim->profile_ix == 0)return DIM_ON_EDGE;dim->profile_ix--;dim->steps_left++;}return DIM_STEPPED;}static int rdma_dim_stats_compare(struct dim_stats  curr,  struct dim_stats  prev){  first stat ", "static int LZ4_decompress_safe_withPrefix64k(const char *source, char *dest,      int compressedSize, int maxOutputSize)": "LZ4_decompress_fast(const char  source, char  dest, int originalSize){return LZ4_decompress_generic(source, dest, 0, originalSize,      endOnOutputSize, decode_full_block,      withPrefix64k,      (BYTE  )dest - 64   KB, NULL, 0);}  ===== Instantiate a few more decoding cases, used more than once. ===== ", "int LZ4_decompress_safe_continue(LZ4_streamDecode_t *LZ4_streamDecode,const char *source, char *dest, int compressedSize, int maxOutputSize)": "LZ4_setStreamDecode(LZ4_streamDecode_t  LZ4_streamDecode,const char  dictionary, int dictSize){LZ4_streamDecode_t_internal  lz4sd =&LZ4_streamDecode->internal_donotuse;lz4sd->prefixSize = (size_t) dictSize;lz4sd->prefixEnd = (const BYTE  ) dictionary + dictSize;lz4sd->externalDict = NULL;lz4sd->extDictSize= 0;return 1;}     _continue() :   These decoding functions allow decompression of multiple blocks   in \"streaming\" mode.   Previously decoded blocks must still be available at the memory   position where they were decoded.   If it's not possible, save the relevant part of   decoded data into a safe buffer,   and indicate where it stands using LZ4_setStreamDecode() ", "assert(lz4sd->extDictSize == 0);result = LZ4_decompress_safe(source, dest,compressedSize, maxOutputSize);if (result <= 0)return result;lz4sd->prefixSize = result;lz4sd->prefixEnd = (BYTE *)dest + result;} else if (lz4sd->prefixEnd == (BYTE *)dest) ": "LZ4_decompress_safe_continue(LZ4_streamDecode_t  LZ4_streamDecode,const char  source, char  dest, int compressedSize, int maxOutputSize){LZ4_streamDecode_t_internal  lz4sd =&LZ4_streamDecode->internal_donotuse;int result;if (lz4sd->prefixSize == 0) {  The first call, no dictionary yet. ", "return 0;}LZ4HC_init(ctx, (const BYTE *)src);if (maxDstSize < LZ4_compressBound(srcSize))return LZ4HC_compress_generic(ctx, src, dst,srcSize, maxDstSize, compressionLevel, limitedOutput);elsereturn LZ4HC_compress_generic(ctx, src, dst,srcSize, maxDstSize, compressionLevel, noLimit);}int LZ4_compress_HC(const char *src, char *dst, int srcSize,int maxDstSize, int compressionLevel, void *wrkmem)": "LZ4_compress_HC_extStateHC(void  state,const char  src,char  dst,int srcSize,int maxDstSize,int compressionLevel){LZ4HC_CCtx_internal  ctx = &((LZ4_streamHC_t  )state)->internal_donotuse;if (((size_t)(state)&(sizeof(void  ) - 1)) != 0) {  Error : state is not aligned   for pointers (32 or 64 bits) ", "- 8 /* because 8 + MINMATCH == MFLIMIT ": "LZ4_compress_destSize_generic(LZ4_stream_t_internal   const ctx,const char   const src,char   const dst,int   const srcSizePtr,const int targetDstSize,const tableType_t tableType){const BYTE  ip = (const BYTE  ) src;const BYTE  base = (const BYTE  ) src;const BYTE  lowLimit = (const BYTE  ) src;const BYTE  anchor = ip;const BYTE   const iend = ip +  srcSizePtr;const BYTE   const mflimit = iend - MFLIMIT;const BYTE   const matchlimit = iend - LASTLITERALS;BYTE  op = (BYTE  ) dst;BYTE   const oend = op + targetDstSize;BYTE   const oMaxLit = op + targetDstSize - 2   offset ", "LZ4_resetStream(LZ4_dict);}if (dictSize < (int)HASH_UNIT) ": "LZ4_loadDict(LZ4_stream_t  LZ4_dict,const char  dictionary, int dictSize){LZ4_stream_t_internal  dict = &LZ4_dict->internal_donotuse;const BYTE  p = (const BYTE  )dictionary;const BYTE   const dictEnd = p + dictSize;const BYTE  base;if ((dict->initCheck)|| (dict->currentOffset > 1   GB)) {  Uninitialized structure, or reuse overflow ", "dictSize = 64 * KB;}if ((U32)dictSize > dict->dictSize)dictSize = dict->dictSize;memmove(safeBuffer, previousDictEnd - dictSize, dictSize);dict->dictionary = (const BYTE *)safeBuffer;dict->dictSize = (U32)dictSize;return dictSize;}EXPORT_SYMBOL(LZ4_saveDict": "LZ4_saveDict(LZ4_stream_t  LZ4_dict, char  safeBuffer, int dictSize){LZ4_stream_t_internal   const dict = &LZ4_dict->internal_donotuse;const BYTE   const previousDictEnd = dict->dictionary + dict->dictSize;if ((U32)dictSize > 64   KB) {  useless to define a dictionary > 64   KB ", "return 0;}if ((streamPtr->dictSize > 0) && (smallest > dictEnd))smallest = dictEnd;LZ4_renormDictT(streamPtr, smallest);if (acceleration < 1)acceleration = LZ4_ACCELERATION_DEFAULT;/* Check overlapping input/dictionary space ": "LZ4_compress_fast_continue(LZ4_stream_t  LZ4_stream, const char  source,char  dest, int inputSize, int maxOutputSize, int acceleration){LZ4_stream_t_internal  streamPtr = &LZ4_stream->internal_donotuse;const BYTE   const dictEnd = streamPtr->dictionary+ streamPtr->dictSize;const BYTE  smallest = (const BYTE  ) source;if (streamPtr->initCheck) {  Uninitialized structure detected ", "if (pledged_src_size == 0)pledged_src_size = ZSTD_CONTENTSIZE_UNKNOWN;if (ZSTD_isError(zstd_cctx_init(cstream, parameters, pledged_src_size)))return NULL;return cstream;}EXPORT_SYMBOL(zstd_init_cstream": "zstd_init_cstream(const zstd_parameters  parameters,unsigned long long pledged_src_size, void  workspace, size_t workspace_size){zstd_cstream  cstream;if (workspace == NULL)return NULL;cstream = ZSTD_initStaticCStream(workspace, workspace_size);if (cstream == NULL)return NULL;  0 means unknown in linux zstd API but means 0 in new zstd API ", "const struct font_desc *find_font(const char *name)": "find_font - find a font  @name: string name of a font    Find a specified font with string name @name.    Returns %NULL if no font found, or a pointer to the  specified font.   ", "const struct font_desc *get_default_font(int xres, int yres, u32 font_w, u32 font_h)": "get_default_font - get default font  @xres: screen size of X  @yres: screen size of Y        @font_w: bit array of supported widths (1 - 32)        @font_h: bit array of supported heights (1 - 32)    Get the default font for a specified screen size.  Dimensions are in pixels.    Returns %NULL if no font is found, or a pointer to the  chosen font.   ", "void rational_best_approximation(unsigned long given_numerator, unsigned long given_denominator,unsigned long max_numerator, unsigned long max_denominator,unsigned long *best_numerator, unsigned long *best_denominator)": "rational_best_approximation(31415, 10000,  (1 << 8) - 1, (1 << 5) - 1, &n, &d);     you may look at given_numerator as a fixed point number,   with the fractional part size described in given_denominator.     for theoretical background, see:   https:en.wikipedia.orgwikiContinued_fraction ", "l = fls(d - 1);/* NOTE: mlow/mhigh could overflow u64 when l == 32. This case needs to * be handled before calling \"reciprocal_value_adv\", please see the * comment at include/linux/reciprocal_div.h. ": "reciprocal_value_adv reciprocal_value_adv(u32 d, u8 prec){struct reciprocal_value_adv R;u32 l, post_shift;u64 mhigh, mlow;  ceil(log2(d)) ", "struct cordic_iq cordic_calc_iq(s32 theta)": "cordic_calc_iq() - calculates the iq coordinate for given angle     theta: angle in degrees for which iq coordinate is to be calculated   coord: function output parameter holding the iq coordinate ", "#include <linux/bitops.h>#include <linux/export.h>#include <linux/math.h>#include <linux/math64.h>#include <linux/log2.h>/* Not needed on 64bit architectures ": "__div64_32() can be overridden by linking arch-specific   assembly versions such as archppclibdiv64.S and archshlibdiv64.S   or by defining a preprocessor macro in archincludeasmdiv64.h. ", "#ifndef div64_u64_remu64 div64_u64_rem(u64 dividend, u64 divisor, u64 *remainder)": "div64_u64_rem - unsigned 64bit divide with 64bit divisor and remainder   @dividend:64bit dividend   @divisor:64bit divisor   @remainder:  64bit remainder     This implementation is a comparable to algorithm used by div64_u64.   But this operation, which includes math for calculating the remainder,   is kept distinct to avoid slowing down the div64_u64 operation on 32bit   systems. ", "if (ilog2(a) + ilog2(b) > 62) ": "mul_u64_u64_div_u64u64 mul_u64_u64_div_u64(u64 a, u64 b, u64 c){u64 res = 0, div, rem;int shift;  can a   b overflow ? ", "sz = 2 * x;if (sz < x)return false;sz = round_up(sz, BITS_PER_LONG);new = kmalloc(sizeof(*new) + bitmap_size(sz),      GFP_KERNEL | __GFP_NOWARN);if (!new)return false;mutex_lock(&lock);p = rcu_dereference_protected(primes, lockdep_is_held(&lock));if (x < p->last) ": "is_prime_number(unsigned long x){unsigned long y = int_sqrt(x);while (y > 1) {if ((x % y) == 0)break;y--;}return y == 1;}static unsigned long slow_next_prime_number(unsigned long x){while (x < ULONG_MAX && !slow_is_prime_number(++x));return x;}static unsigned long clear_multiples(unsigned long x,     unsigned long  p,     unsigned long start,     unsigned long end){unsigned long m;m = 2   x;if (m < start)m = roundup(start, x);while (m < end) {__clear_bit(m, p);m += x;}return x;}static bool expand_to_next_prime(unsigned long x){const struct primes  p;struct primes  new;unsigned long sz, y;  Betrand's Postulate (or Chebyshev's theorem) states that if n > 3,   there is always at least one prime p between n and 2n - 2.   Equivalently, if n > 1, then there is always at least one prime p   such that n < p < 2n.     http:mathworld.wolfram.comBertrandsPostulate.html   https:en.wikipedia.orgwikiBertrand's_postulate ", "unsigned long int_sqrt(unsigned long x)": "int_sqrt - computes the integer square root   @x: integer of which to calculate the sqrt     Computes: floor(sqrt(x)) ", "u32 int_sqrt64(u64 x)": "int_sqrt64 - strongly typed int_sqrt function when minimum 64 bit input   is expected.   @x: 64bit integer of which to calculate the sqrt ", "bool pldmfw_op_pci_match_record(struct pldmfw *context, struct pldmfw_record *record)": "pldmfw_op_pci_match_record - Check if a PCI device matches the record   @context: PLDM fw update structure   @record: list of records extracted from the PLDM image     Determine of the PCI device associated with this device matches the record   data provided.     Searches the descriptor TLVs and extracts the relevant descriptor data into   a pldm_pci_record_id. This is then compared against the PCI device ID   information.     Returns: true if the device matches the record, false otherwise. ", "int pldmfw_flash_image(struct pldmfw *context, const struct firmware *fw)": "pldmfw_flash_image - Write a PLDM-formatted firmware image to the device   @context: ops and data for firmware update   @fw: firmware object pointing to the relevant firmware file to program     Parse the data for a given firmware file, verifying that it is a valid PLDM   formatted image that matches this device.     Extract the device record Package Data and Component Tables and send them   to the device firmware. Extract and write the flash data for each of the   components indicated in the firmware file.     Returns: zero on success, or a negative error code on failure. ", "    if (zlib_dfltcc_support == ZLIB_DFLTCC_DISABLED ||            zlib_dfltcc_support == ZLIB_DFLTCC_DEFLATE_ONLY)        return 0;    /* Unsupported hardware ": "dfltcc_can_inflate(    z_streamp strm){    struct inflate_state  state = (struct inflate_state  )strm->state;    struct dfltcc_state  dfltcc_state = GET_DFLTCC_STATE(state);      Check for kernel dfltcc command line parameter ", "int dfltcc_can_inflate(    z_streamp strm)": "dfltcc_inflate.h\"#include <asmsetup.h>#include <linuxexport.h>#include <linuxzutil.h>    Expand. ", "    if (zlib_dfltcc_support == ZLIB_DFLTCC_DISABLED ||            zlib_dfltcc_support == ZLIB_DFLTCC_INFLATE_ONLY)        return 0;    /* Unsupported compression settings ": "dfltcc_can_deflate(    z_streamp strm){    deflate_state  state = (deflate_state  )strm->state;    struct dfltcc_deflate_state  dfltcc_state = GET_DFLTCC_DEFLATE_STATE(state);      Check for kernel dfltcc command line parameter ", "    if (zlib_dfltcc_support == ZLIB_DFLTCC_FULL_DEBUG)        dfltcc_state->level_mask = DFLTCC_LEVEL_MASK_DEBUG;    else        dfltcc_state->level_mask = DFLTCC_LEVEL_MASK;    dfltcc_state->block_size = DFLTCC_BLOCK_SIZE;    dfltcc_state->block_threshold = DFLTCC_FIRST_FHT_BLOCK_SIZE;    dfltcc_state->dht_threshold = DFLTCC_DHT_MIN_SAMPLE_SIZE;}EXPORT_SYMBOL(dfltcc_reset_deflate_state": "dfltcc_reset_deflate_state(z_streamp strm) {    deflate_state  state = (deflate_state  )strm->state;    struct dfltcc_deflate_state  dfltcc_state = GET_DFLTCC_DEFLATE_STATE(state);    dfltcc_reset_state(&dfltcc_state->common);      Initialize tuning parameters ", "int dfltcc_can_deflate(    z_streamp strm)": "dfltcc_deflate.h\"#include <asmsetup.h>#include <linuxexport.h>#include <linuxzutil.h>#define GET_DFLTCC_DEFLATE_STATE(state) ((struct dfltcc_deflate_state  )GET_DFLTCC_STATE(state))    Compress. ", "#include <linux/module.h>#include <linux/init.h>#include <linux/zlib.h>EXPORT_SYMBOL(zlib_deflate": "zlib_deflatedeflate_syms.c     Exported symbols for the deflate functionality.   ", "BUILD_BUG_ON(offsetof(struct sha256_state, state) != 0);return sha256_base_do_update(desc, data, len,(sha256_block_fn *)sha256_block_data_order);}EXPORT_SYMBOL(crypto_sha256_arm_update": "crypto_sha256_arm_update(struct shash_desc  desc, const u8  data,     unsigned int len){  make sure casting to sha256_block_fn() is safe ", "static int ncores;int zynq_cpun_start(u32 address, int cpu)": "zynq_cpun_start() because it is not in __init section. ", "if (memblock_overlaps_region(&memblock.memory,     ALIGN_DOWN(addr, pageblock_size),     pageblock_size))return 1;return 0;}EXPORT_SYMBOL(pfn_valid": "pfn_valid(unsigned long pfn){phys_addr_t addr = __pfn_to_phys(pfn);unsigned long pageblock_size = PAGE_SIZE   pageblock_nr_pages;if (__phys_to_pfn(addr) != pfn)return 0;    If address less than pageblock_size bytes away from a present   memory chunk there still will be a memory map entry for it   because we round freed memory map to the pageblock boundaries. ", "if (!PageHighMem(page)) ": "flush_dcache_page(struct address_space  mapping, struct page  page){    Writeback any data associated with the kernel mapping of this   page.  This ensures that data in the physical page is mutually   coherent with the kernels mapping. ", "struct page *empty_zero_page;EXPORT_SYMBOL(empty_zero_page": "empty_zero_page is a special page that is used for   zero-initialized data and COW. ", "VIRTUAL_BUG_ON(x < (unsigned long)KERNEL_START ||       x > (unsigned long)KERNEL_END);return __pa_symbol_nodebug(x);}EXPORT_SYMBOL(__phys_addr_symbol": "__phys_addr_symbol(unsigned long x){  This is bounds checking against the kernel image only.   __pa_symbol should only be used on kernel symbol addresses. ", "if (pfn >= 0x100000 && (paddr & ~SUPERSECTION_MASK))return NULL;#endiftype = get_mem_type(mtype);if (!type)return NULL;/* * Page align the mapping size, taking account of any offset. ": "__arm_ioremap_pfn_caller(unsigned long pfn,unsigned long offset, size_t size, unsigned int mtype, void  caller){const struct mem_type  type;int err;unsigned long addr;struct vm_struct  area;phys_addr_t paddr = __pfn_to_phys(pfn);#ifndef CONFIG_ARM_LPAE    High mappings must be supersection aligned ", "#include <linux/module.h>#include <linux/errno.h>#include <linux/mm.h>#include <linux/vmalloc.h>#include <linux/io.h>#include <linux/sizes.h>#include <linux/memblock.h>#include <asm/cp15.h>#include <asm/cputype.h>#include <asm/cacheflush.h>#include <asm/early_ioremap.h>#include <asm/mmu_context.h>#include <asm/pgalloc.h>#include <asm/tlbflush.h>#include <asm/set_memory.h>#include <asm/system_info.h>#include <asm/mach/map.h>#include <asm/mach/pci.h>#include \"mm.h\"LIST_HEAD(static_vmlist);static struct static_vm *find_static_vm_paddr(phys_addr_t paddr,size_t size, unsigned int mtype)": "ioremap.c     Re-map IO memory to kernel address space so that we can access it.     (C) Copyright 1995 1996 Linus Torvalds     Hacked for ARM by Phil Blundell <philb@gnu.org>   Hacked to allow all architectures to build, and various cleanups   by Russell King     This allows a driver to remap an arbitrary region of bus memory into   virtual space.  One should  only  use readl, writel, memcpy_toio and   so on with such remapped areas.     Because the ARM only has a 32-bit address space we can't address the   whole of the (physical) PCI space at once.  PCI huge-mode addressing   allows us to circumvent this restriction by splitting PCI space into   two 2GB chunks and mapping only one at a time into processor memory.   We use MMU protection domains to trap any attempt to access the bank   that is not currently mapped.  (This isn't fully implemented yet.) ", "static void unmap_area_sections(unsigned long virt, unsigned long size)": "iounmap and ioremap a region,   the other CPUs will not see this change until their next context switch.   Meanwhile, (eg) if an interrupt comes in on one of those other CPUs   which requires the new ioremap'd region to be referenced, the CPU will   reference the _old_ region.     Note that get_vm_area_caller() allocates a guard 4K page, so we need to   mask the size back to 1MB aligned or we will overflow in the loop below. ", "void setup_mm_for_reboot(void)": "flush_dcache_page(empty_zero_page);}    We don't need to do anything here for nommu machines. ", "inline u8 jornada_ssp_reverse(u8 byte)": "jornada_ssp_reverse - reverses input byte   @byte: input byte to reverse     we need to reverse all data we receive from the mcu due to its physical location   returns : 01110111 -> 11101110 ", "int jornada_ssp_byte(u8 byte)": "jornada_ssp_byte - waits for ready ssp bus and sends byte   @byte: input byte to transmit     waits for fifo buffer to clear and then transmits, if it doesn't then we will   timeout after <timeout> rounds. Needs mcu running before its called.     returns : %mcu output on success     : %-ETIMEDOUT on timeout ", "int jornada_ssp_inout(u8 byte)": "jornada_ssp_inout - decide if input is command or trading byte   @byte: input byte to send (may be %TXDUMMY)     returns : (jornada_ssp_byte(byte)) on success           : %-ETIMEDOUT on timeout failure ", "void jornada_ssp_start(void)": "jornada_ssp_start - enable mcu   ", "void jornada_ssp_end(void)": "jornada_ssp_end - disable mcu and turn off lock   ", "int ssp_write_word(u16 data)": "ssp_write_word - write a word to the SSP port   @data: 16-bit, MSB justified data to write.     Wait for a free entry in the SSP transmit FIFO, and write a data   word to the SSP port.  Wait for the SSP port to start sending   the data.     The caller is expected to perform the necessary locking.     Returns:     %-ETIMEDOUTtimeout occurred     0success ", "int ssp_read_word(u16 *data)": "ssp_read_word - read a word from the SSP port     Wait for a data word in the SSP receive FIFO, and return the   received data.  Data is LSB justified.     Note: Currently, if data is not expected to be received, this   function will wait for ever.     The caller is expected to perform the necessary locking.     Returns:     %-ETIMEDOUTtimeout occurred     16-bit datasuccess ", "int ssp_flush(void)": "ssp_flush - flush the transmit and receive FIFOs     Wait for the SSP to idle, and ensure that the receive FIFO   is empty.     The caller is expected to perform the necessary locking.     Returns:     %-ETIMEDOUTtimeout occurred     0success ", "void ssp_enable(void)": "ssp_enable - enable the SSP port     Turn on the SSP port. ", "void ssp_disable(void)": "ssp_disable - shut down the SSP port     Turn off the SSP port, optionally powering it down. ", "void ssp_save_state(struct ssp_state *ssp)": "ssp_save_state - save the SSP configuration   @ssp: pointer to structure to save SSP configuration     Save the configured SSP state for suspend. ", "void ssp_restore_state(struct ssp_state *ssp)": "ssp_restore_state - restore a previously saved SSP configuration   @ssp: pointer to configuration saved by ssp_save_state     Restore the SSP configuration saved previously by ssp_save_state. ", "int ssp_init(void)": "ssp_init - setup the SSP port     initialise and claim resources for the SSP port.     Returns:     %-ENODEVif the SSP port is unavailable     %-EBUSYif the resources are already in use     %0on success ", "void ssp_exit(void)": "ssp_exit - undo the effects of ssp_init     release and free resources for the SSP port. ", "ep93xx_devcfg_set_bits(EP93XX_SYSCON_DEVCFG_PONG);} else ": "ep93xx_pwm_acquire_gpio(struct platform_device  pdev){int err;if (pdev->id == 0) {err = 0;} else if (pdev->id == 1) {err = gpio_request(EP93XX_GPIO_LINE_EGPIO14,   dev_name(&pdev->dev));if (err)return err;err = gpio_direction_output(EP93XX_GPIO_LINE_EGPIO14, 0);if (err)goto fail;  PWM 1 output on EGPIO[14] ", "ep93xx_devcfg_clear_bits(EP93XX_SYSCON_DEVCFG_PONG);}}EXPORT_SYMBOL(ep93xx_pwm_release_gpio": "ep93xx_pwm_release_gpio(struct platform_device  pdev){if (pdev->id == 1) {gpio_direction_input(EP93XX_GPIO_LINE_EGPIO14);gpio_free(EP93XX_GPIO_LINE_EGPIO14);  EGPIO[14] used for GPIO ", "ep93xx_devcfg_clear_bits(EP93XX_SYSCON_DEVCFG_KEYS | EP93XX_SYSCON_DEVCFG_GONK);return 0;fail_gpio_d:gpio_free(EP93XX_GPIO_LINE_C(i));fail_gpio_c:for (--i; i >= 0; --i) ": "ep93xx_keypad_acquire_gpio(struct platform_device  pdev){int err;int i;for (i = 0; i < 8; i++) {err = gpio_request(EP93XX_GPIO_LINE_C(i), dev_name(&pdev->dev));if (err)goto fail_gpio_c;err = gpio_request(EP93XX_GPIO_LINE_D(i), dev_name(&pdev->dev));if (err)goto fail_gpio_d;}  Enable the keypad controller; GPIO ports C and D used for keypad ", "ep93xx_devcfg_set_bits(EP93XX_SYSCON_DEVCFG_KEYS |       EP93XX_SYSCON_DEVCFG_GONK);}EXPORT_SYMBOL(ep93xx_keypad_release_gpio": "ep93xx_keypad_release_gpio(struct platform_device  pdev){int i;for (i = 0; i < 8; i++) {gpio_free(EP93XX_GPIO_LINE_C(i));gpio_free(EP93XX_GPIO_LINE_D(i));}  Disable the keypad controller; GPIO ports C and D used for GPIO ", "val = __raw_readl(EP93XX_SYSCON_I2SCLKDIV);val &= ~EP93XX_I2SCLKDIV_MASK;val |= EP93XX_SYSCON_I2SCLKDIV_ORIDE | EP93XX_SYSCON_I2SCLKDIV_SPOL;ep93xx_syscon_swlocked_write(val, EP93XX_SYSCON_I2SCLKDIV);return 0;}EXPORT_SYMBOL(ep93xx_i2s_acquire": "ep93xx_i2s_acquire(void){unsigned val;ep93xx_devcfg_set_clear(EP93XX_SYSCON_DEVCFG_I2SONAC97,EP93XX_SYSCON_DEVCFG_I2S_MASK);    This is potentially racy with the clock api for i2s_mclk, sclk and    lrclk. Since the i2s driver is the only user of those clocks we   rely on it to prevent parallel use of this function and the    clock api for the i2s clocks. ", "ep93xx_devcfg_clear_bits(EP93XX_SYSCON_DEVCFG_EONIDE | EP93XX_SYSCON_DEVCFG_GONIDE | EP93XX_SYSCON_DEVCFG_HONIDE);return 0;fail_gpio_h:for (--i; i >= 0; --i)gpio_free(EP93XX_GPIO_LINE_H(i));i = 8;fail_gpio_g:for (--i; i >= 4; --i)gpio_free(EP93XX_GPIO_LINE_G(i));i = 8;fail_gpio_e:for (--i; i >= 2; --i)gpio_free(EP93XX_GPIO_LINE_E(i));gpio_free(EP93XX_GPIO_LINE_EGPIO15);fail_egpio15:gpio_free(EP93XX_GPIO_LINE_EGPIO2);return err;}EXPORT_SYMBOL(ep93xx_ide_acquire_gpio": "ep93xx_ide_acquire_gpio(struct platform_device  pdev){int err;int i;err = gpio_request(EP93XX_GPIO_LINE_EGPIO2, dev_name(&pdev->dev));if (err)return err;err = gpio_request(EP93XX_GPIO_LINE_EGPIO15, dev_name(&pdev->dev));if (err)goto fail_egpio15;for (i = 2; i < 8; i++) {err = gpio_request(EP93XX_GPIO_LINE_E(i), dev_name(&pdev->dev));if (err)goto fail_gpio_e;}for (i = 4; i < 8; i++) {err = gpio_request(EP93XX_GPIO_LINE_G(i), dev_name(&pdev->dev));if (err)goto fail_gpio_g;}for (i = 0; i < 8; i++) {err = gpio_request(EP93XX_GPIO_LINE_H(i), dev_name(&pdev->dev));if (err)goto fail_gpio_h;}  GPIO ports E[7:2], G[7:4] and H used by IDE ", "ep93xx_devcfg_set_bits(EP93XX_SYSCON_DEVCFG_EONIDE |       EP93XX_SYSCON_DEVCFG_GONIDE |       EP93XX_SYSCON_DEVCFG_HONIDE);}EXPORT_SYMBOL(ep93xx_ide_release_gpio": "ep93xx_ide_release_gpio(struct platform_device  pdev){int i;for (i = 2; i < 8; i++)gpio_free(EP93XX_GPIO_LINE_E(i));for (i = 4; i < 8; i++)gpio_free(EP93XX_GPIO_LINE_G(i));for (i = 0; i < 8; i++)gpio_free(EP93XX_GPIO_LINE_H(i));gpio_free(EP93XX_GPIO_LINE_EGPIO15);gpio_free(EP93XX_GPIO_LINE_EGPIO2);  GPIO ports E[7:2], G[7:4] and H used by GPIO ", "iowrite16(0x0000, sdev->base + SCOOP_CDR);  /* 04 ": "reset_scoop(struct device  dev){struct scoop_dev  sdev = dev_get_drvdata(dev);iowrite16(0x0100, sdev->base + SCOOP_MCR);    00 ", "udelay(DAC_BUS_FREE_TIME);/* 5.0 usec ": "locomo_m62332_senddata(struct locomo_dev  ldev, unsigned int dac_data, int channel){struct locomo  lchip = locomo_chip_driver(ldev);int i;unsigned char data;unsigned int r;void  mapbase = lchip->base;unsigned long flags;spin_lock_irqsave(&lchip->lock, flags);  Start ", "asm volatile (\"mcr p15, 3, %0, c15, c0, 6 @ l2cpselr\" : : \"r\" (addr));isb();asm volatile (\"mcr p15, 3, %0, c15, c0, 7 @ l2cpdr\" : : \"r\" (val));isb();raw_spin_unlock_irqrestore(&krait_l2_lock, flags);}EXPORT_SYMBOL(krait_set_l2_indirect_reg": "krait_set_l2_indirect_reg(u32 addr, u32 val){unsigned long flags;raw_spin_lock_irqsave(&krait_l2_lock, flags);    Select the L2 window by poking l2cpselr, then write to the window   via l2cpdr. ", "asm volatile (\"mcr p15, 3, %0, c15, c0, 6 @ l2cpselr\" : : \"r\" (addr));isb();asm volatile (\"mrc p15, 3, %0, c15, c0, 7 @ l2cpdr\" : \"=r\" (val));raw_spin_unlock_irqrestore(&krait_l2_lock, flags);return val;}EXPORT_SYMBOL(krait_get_l2_indirect_reg": "krait_get_l2_indirect_reg(u32 addr){u32 val;unsigned long flags;raw_spin_lock_irqsave(&krait_l2_lock, flags);    Select the L2 window by poking l2cpselr, then read from the window   via l2cpdr. ", "#ifdef CONFIG_ARCH_SA1100#define PARAM_BASE0xe8ffc000#define param_start(x)(void *)(x)#else#define PARAM_BASE0xa0000a00#define param_start(x)__va(x)#endif#define MAGIC_CHG(a,b,c,d) ( ( d << 24 ) | ( c << 16 )  | ( b << 8 ) | a )#define COMADJ_MAGICMAGIC_CHG('C','M','A','D')#define UUID_MAGICMAGIC_CHG('U','U','I','D')#define TOUCH_MAGICMAGIC_CHG('T','U','C','H')#define AD_MAGICMAGIC_CHG('B','V','A','D')#define PHAD_MAGICMAGIC_CHG('P','H','A','D')struct sharpsl_param_info sharpsl_param;EXPORT_SYMBOL(sharpsl_param": "sharpsl_param.h>#include <asmpage.h>    Certain hardware parameters determined at the time of device manufacture,   typically including LCD parameters are loaded by the bootloader at the   address PARAM_BASE. As the kernel will overwrite them, we need to store   them early in the boot process, then pass them to the appropriate drivers.   Not all devices use all parameters but the format is common to all. ", "unsigned int sa1111_pll_clock(struct sa1111_dev *sadev)": "sa1111_pll_clock(struct sa1111  sachip){unsigned int skcdr, fbdiv, ipdiv, opdiv;skcdr = readl_relaxed(sachip->base + SA1111_SKCDR);fbdiv = (skcdr & 0x007f) + 2;ipdiv = ((skcdr & 0x0f80) >> 7) + 2;opdiv = opdiv_table[(skcdr & 0x3000) >> 12];return 3686400   fbdiv  (ipdiv   opdiv);}    sa1111_pll_clock - return the current PLL clock frequency.  @sadev: SA1111 function block    BUG: we should look at SKCR.  We also blindly believe that  the chip is being fed with the 3.6864MHz clock.    Returns the PLL clock in Hz. ", "void sa1111_select_audio_mode(struct sa1111_dev *sadev, int mode)": "sa1111_select_audio_mode - select I2S or AC link mode  @sadev: SA1111 function block  @mode: One of %SA1111_AUDIO_ACLINK or %SA1111_AUDIO_I2S    Frob the SKCR to select AC Link mode or I2S mode for  the audio block. ", "int sa1111_set_audio_rate(struct sa1111_dev *sadev, int rate)": "sa1111_set_audio_rate - set the audio sample rate  @sadev: SA1111 SAC function block  @rate: sample rate to select ", "int sa1111_get_audio_rate(struct sa1111_dev *sadev)": "sa1111_get_audio_rate - get the audio sample rate  @sadev: SA1111 SAC function block device ", "int sa1111_enable_device(struct sa1111_dev *sadev)": "sa1111_enable_device - enable an on-chip SA1111 function block  @sadev: SA1111 function block device to enable ", "void sa1111_disable_device(struct sa1111_dev *sadev)": "sa1111_disable_device - disable an on-chip SA1111 function block  @sadev: SA1111 function block device to disable ", "if (info->dma && sachip->dev->dma_mask) ": "sa1111_bus_type;dev->dev.release = sa1111_dev_release;dev->res.start   = sachip->phys + info->offset;dev->res.end     = dev->res.start + 511;dev->res.name    = dev_name(&dev->dev);dev->res.flags   = IORESOURCE_MEM;dev->mapbase     = sachip->base + info->offset;dev->skpcr_mask  = info->skpcr_mask;for (i = 0; i < ARRAY_SIZE(info->hwirq); i++)dev->hwirq[i] = info->hwirq[i];    If the parent device has a DMA mask associated with it, and   this child supports DMA, propagate it down to the children. ", "void pcibios_fixup_bus(struct pci_bus *bus)": "pcibios_fixup_bus - Called after each bus is probed,   but before its children are examined. ", "}return 0;}static struct fiq_handler default_owner = ": "set_fiq_handler(&dfl_fiq_insn, sizeof(dfl_fiq_insn));local_fiq_enable();  FIXME: notify irq controller to standard enable FIQs ", "EXPORT_SYMBOL(__get_fiq_regs);/* defined in fiqasm.S ": "disable_fiq(int fiq){disable_irq(fiq + fiq_start);}EXPORT_SYMBOL(set_fiq_handler);EXPORT_SYMBOL(__set_fiq_regs);  defined in fiqasm.S ", "EXPORT_SYMBOL(arm_delay_ops);/* networking ": "__aeabi_ulcmp(void);extern void fpundefinstr(void);void mmioset(void  , unsigned int, size_t);void mmiocpy(void  , const void  , size_t);  platform dependent support ", "if (x->e_machine != EM_ARM)return 0;/* Make sure the entry address is reasonable ": "elf_check_arch(const struct elf32_hdr  x){unsigned int eflags;  Make sure it's an ARM executable ", "personality |= PER_LINUX;/* * APCS-26 is only valid for OABI executables ": "elf_set_personality(const struct elf32_hdr  x){unsigned int eflags = x->e_flags;unsigned int personality = current->personality & ~PER_MASK;    We only support Linux ELF executables, so always set the   personality to LINUX. ", "val = omap_readl(OCPI_PROT);val &= ~0xff;/* val &= (1 << 0); Allow access only to EMIFS ": "ocpi_enable(void){unsigned int val;if (!cpu_is_omap16xx())return -ENODEV;  Enable access for OHCI in OCPI ", "static struct omap_id omap_ids[] __initdata = ": "omap_revision;  Register values to detect the OMAP version ", "u8 omap_readb(u32 pa)": "omap_writew(0x0, MPU_PUBLIC_TIPB_CNTL);omap_writew(0x0, MPU_PRIVATE_TIPB_CNTL);}void __init omap1_init_late(void){omap_serial_wakeup_init();}    NOTE: Please use ioremap + __raw_readwrite where possible instead of these ", "reg = OMAP_TC_OCPT1_PRIOR;break;case OMAP_DMA_PORT_OCP_T2:/* FFFECCD0 ": "omap_set_dma_priority(int lch, int dst_port, int priority){unsigned long reg;u32 l;if (dma_omap1()) {switch (dst_port) {case OMAP_DMA_PORT_OCP_T1:  FFFECC00 ", "fallthrough;case OMAP_DMA_DATA_BURST_16:/* OMAP1 don't support burst 16 ": "omap_set_dma_src_burst_mode(int lch, enum omap_dma_burst_mode burst_mode){unsigned int burst = 0;u32 l;l = p->dma_read(CSDP, lch);l &= ~(0x03 << 7);switch (burst_mode) {case OMAP_DMA_DATA_BURST_DIS:break;case OMAP_DMA_DATA_BURST_4:burst = 0x2;break;case OMAP_DMA_DATA_BURST_8:    not supported by current hardware on OMAP1   w |= (0x03 << 7); ", "fallthrough;default:printk(KERN_ERR \"Invalid DMA burst mode\\n\");BUG();return;}l |= (burst << 14);p->dma_write(l, CSDP, lch);}EXPORT_SYMBOL(omap_set_dma_dest_burst_mode": "omap_set_dma_dest_burst_mode(int lch, enum omap_dma_burst_mode burst_mode){unsigned int burst = 0;u32 l;l = p->dma_read(CSDP, lch);l &= ~(0x03 << 14);switch (burst_mode) {case OMAP_DMA_DATA_BURST_DIS:break;case OMAP_DMA_DATA_BURST_4:burst = 0x2;break;case OMAP_DMA_DATA_BURST_8:burst = 0x3;break;case OMAP_DMA_DATA_BURST_16:  OMAP1 don't support burst 16 ", "break;}}if (free_ch == -1) ": "omap_request_dma(int dev_id, const char  dev_name,     void ( callback)(int lch, u16 ch_status, void  data),     void  data, int  dma_ch_out){int ch, free_ch = -1;unsigned long flags;struct omap_dma_lch  chan;WARN(strcmp(dev_name, \"DMA engine\"), \"Using deprecated platform DMA API - please update to DMA engine\");spin_lock_irqsave(&dma_chan_lock, flags);for (ch = 0; ch < dma_chan_count; ch++) {if (free_ch == -1 && dma_chan[ch].dev_id == -1) {free_ch = ch;  Exit after first free channel found ", "omap_disable_channel_irq(lch);/* Make sure the DMA transfer is stopped. ": "omap_free_dma(int lch){unsigned long flags;if (dma_chan[lch].dev_id == -1) {pr_err(\"omap_dma: trying to free unallocated DMA channel %d\\n\",       lch);return;}  Disable all DMA interrupts for the channel. ", "static void omap_clear_dma(int lch)": "omap_start_dma(). Any buffers in flight are discarded. ", "omap_disable_channel_irq(lch);l = p->dma_read(CCR, lch);if (IS_DMA_ERRATA(DMA_ERRATA_i541) &&(l & OMAP_DMA_CCR_SEL_SRC_DST_SYNC)) ": "omap_stop_dma(int lch){u32 l;  Disable all interrupts on the channel ", "if (likely(p->dma_read(CDAC, lch)))offset = p->dma_read(CSAC, lch);elseoffset = p->dma_read(CSSA, lch);}offset |= (p->dma_read(CSSA, lch) & 0xFFFF0000);return offset;}EXPORT_SYMBOL(omap_get_dma_src_pos": "omap_get_dma_src_pos(int lch){dma_addr_t offset = 0;if (dma_omap15xx())offset = p->dma_read(CPC, lch);elseoffset = p->dma_read(CSAC, lch);if (IS_DMA_ERRATA(DMA_ERRATA_3_3) && offset == 0)offset = p->dma_read(CSAC, lch);if (!dma_omap15xx()) {    CDAC == 0 indicates that the DMA transfer on the channel has   not been started (no data has been transferred so far).   Return the programmed source start address in this case. ", "if (!dma_omap15xx() && offset == 0) ": "omap_get_dma_dst_pos(int lch){dma_addr_t offset = 0;if (dma_omap15xx())offset = p->dma_read(CPC, lch);elseoffset = p->dma_read(CDAC, lch);    omap 3.23.3 erratum: sometimes 0 is returned if CSACCDAC is   read before the DMA controller finished disabling the channel. ", "int sx1_getkeylight(u8 * keylight)": "sx1_setkeylight(u8 keylight){if (keylight > SOFIA_MAX_LIGHT_VAL)keylight = SOFIA_MAX_LIGHT_VAL;return sx1_i2c_write_byte(SOFIA_I2C_ADDR, SOFIA_KEYLIGHT_REG, keylight);}  get current keylight intensity ", "int sx1_setbacklight(u8 backlight)": "sx1_getkeylight(u8   keylight){return sx1_i2c_read_byte(SOFIA_I2C_ADDR, SOFIA_KEYLIGHT_REG, keylight);}  set LCD backlight intensity ", "int sx1_getbacklight (u8 * backlight)": "sx1_setbacklight(u8 backlight){if (backlight > SOFIA_MAX_LIGHT_VAL)backlight = SOFIA_MAX_LIGHT_VAL;return sx1_i2c_write_byte(SOFIA_I2C_ADDR, SOFIA_BACKLIGHT_REG,  backlight);}  get current LCD backlight intensity ", "int sx1_setmmipower(u8 onoff)": "sx1_getbacklight (u8   backlight){return sx1_i2c_read_byte(SOFIA_I2C_ADDR, SOFIA_BACKLIGHT_REG, backlight);}  set LCD backlight power onoff ", "int sx1_setusbpower(u8 onoff)": "sx1_setmmipower(u8 onoff){int err;u8 dat = 0;err = sx1_i2c_read_byte(SOFIA_I2C_ADDR, SOFIA_POWER1_REG, &dat);if (err < 0)return err;if (onoff)dat |= SOFIA_MMILIGHT_POWER;elsedat &= ~SOFIA_MMILIGHT_POWER;return sx1_i2c_write_byte(SOFIA_I2C_ADDR, SOFIA_POWER1_REG, dat);}  set USB power onoff ", "if (fmrx(FPEXC) & FPEXC_EN)pr_crit(\"BUG: unsupported FP instruction in kernel mode\\n\");elsepr_crit(\"BUG: FP instruction issued in kernel mode with FP unit disabled\\n\");pr_crit(\"FPEXC == 0x%08x\\n\", fmrx(FPEXC));return 1;}static struct undef_hook vfp_kmode_exception_hook[] = ": "kernel_neon_begin(), something has   caused the task to be scheduled out and back in again. In this case,   rebuilding and running with CONFIG_DEBUG_ATOMIC_SLEEP enabled should   be helpful in localizing the problem. ", "fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);local_bh_enable();}EXPORT_SYMBOL(kernel_neon_end": "kernel_neon_end(void){  Disable the NEONVFP unit. ", "index = c_start(&excd);continue;}if (c_id(&excd) == 0x80) ": "ecard_readchunk(struct in_chunk_dir  cd, ecard_t  ec, int id, int num){struct ex_chunk_dir excd;int index = 16;int useld = 0;if (!ec->cid.cd)return 0;while(1) {ecard_readbytes(&excd, ec, index, 8, useld);index += 8;if (c_id(&excd) == 0) {if (!useld && ec->loader) {useld = 1;index = 0;continue;}return 0;}if (c_id(&excd) == 0xf0) {   link ", "for (i = 0x2000; i <= 0x2800; i += 0x0400)writeb(0, addr + i + 4);}/* * Probe for an expansion card. * * If bit 1 of the first byte of the card is set, then the * card does not exist. ": "ecard_bus_type;ec->dev.dma_mask = &ec->dma_mask;ec->dma_mask = (u64)0xffffffff;ec->dev.coherent_dma_mask = ec->dma_mask;if (slot < 4) {ec_set_resource(ec, ECARD_RES_MEMC,PODSLOT_MEMC_BASE + (slot << 14),PODSLOT_MEMC_SIZE);base = PODSLOT_IOC0_BASE + (slot << 14);} elsebase = PODSLOT_IOC4_BASE + ((slot - 4) << 14);#ifdef CONFIG_ARCH_RPCif (slot < 8) {ec_set_resource(ec, ECARD_RES_EASI,PODSLOT_EASI_BASE + (slot << 24),PODSLOT_EASI_SIZE);}if (slot == 8) {ec_set_resource(ec, ECARD_RES_MEMC, NETSLOT_BASE, NETSLOT_SIZE);} else#endiffor (i = 0; i <= ECARD_RES_IOCSYNC - ECARD_RES_IOCSLOW; i++)ec_set_resource(ec, i + ECARD_RES_IOCSLOW,base + (i << 19), PODSLOT_IOC_SIZE);for (i = 0; i < ECARD_NUM_RESOURCES; i++) {if (ec->resource[i].flags &&    request_resource(&iomem_resource, &ec->resource[i])) {dev_err(&ec->dev, \"resource(s) not available\\n\");ec->resource[i].end -= ec->resource[i].start;ec->resource[i].start = 0;ec->resource[i].flags = 0;}} nomem:return ec;}static ssize_t irq_show(struct device  dev, struct device_attribute  attr, char  buf){struct expansion_card  ec = ECARD_DEV(dev);return sprintf(buf, \"%u\\n\", ec->irq);}static DEVICE_ATTR_RO(irq);static ssize_t dma_show(struct device  dev, struct device_attribute  attr, char  buf){struct expansion_card  ec = ECARD_DEV(dev);return sprintf(buf, \"%u\\n\", ec->dma);}static DEVICE_ATTR_RO(dma);static ssize_t resource_show(struct device  dev, struct device_attribute  attr, char  buf){struct expansion_card  ec = ECARD_DEV(dev);char  str = buf;int i;for (i = 0; i < ECARD_NUM_RESOURCES; i++)str += sprintf(str, \"%08x %08x %08lx\\n\",ec->resource[i].start,ec->resource[i].end,ec->resource[i].flags);return str - buf;}static DEVICE_ATTR_RO(resource);static ssize_t vendor_show(struct device  dev, struct device_attribute  attr, char  buf){struct expansion_card  ec = ECARD_DEV(dev);return sprintf(buf, \"%u\\n\", ec->cid.manufacturer);}static DEVICE_ATTR_RO(vendor);static ssize_t device_show(struct device  dev, struct device_attribute  attr, char  buf){struct expansion_card  ec = ECARD_DEV(dev);return sprintf(buf, \"%u\\n\", ec->cid.product);}static DEVICE_ATTR_RO(device);static ssize_t type_show(struct device  dev, struct device_attribute  attr, char  buf){struct expansion_card  ec = ECARD_DEV(dev);return sprintf(buf, \"%s\\n\", ec->easi ? \"EASI\" : \"IOC\");}static DEVICE_ATTR_RO(type);static struct attribute  ecard_dev_attrs[] = {&dev_attr_device.attr,&dev_attr_dma.attr,&dev_attr_irq.attr,&dev_attr_resource.attr,&dev_attr_type.attr,&dev_attr_vendor.attr,NULL,};ATTRIBUTE_GROUPS(ecard_dev);int ecard_request_resources(struct expansion_card  ec){int i, err = 0;for (i = 0; i < ECARD_NUM_RESOURCES; i++) {if (ecard_resource_end(ec, i) &&    !request_mem_region(ecard_resource_start(ec, i),ecard_resource_len(ec, i),ec->dev.driver->name)) {err = -EBUSY;break;}}if (err) {while (i--)if (ecard_resource_end(ec, i))release_mem_region(ecard_resource_start(ec, i),   ecard_resource_len(ec, i));}return err;}EXPORT_SYMBOL(ecard_request_resources);void ecard_release_resources(struct expansion_card  ec){int i;for (i = 0; i < ECARD_NUM_RESOURCES; i++)if (ecard_resource_end(ec, i))release_mem_region(ecard_resource_start(ec, i),   ecard_resource_len(ec, i));}EXPORT_SYMBOL(ecard_release_resources);void ecard_setirq(struct expansion_card  ec, const struct expansion_card_ops  ops, void  irq_data){ec->irq_data = irq_data;barrier();ec->ops = ops;}EXPORT_SYMBOL(ecard_setirq);void __iomem  ecardm_iomap(struct expansion_card  ec, unsigned int res,   unsigned long offset, unsigned long maxsize){unsigned long start = ecard_resource_start(ec, res);unsigned long end = ecard_resource_end(ec, res);if (offset > (end - start))return NULL;start += offset;if (maxsize && end - start > maxsize)end = start + maxsize;return devm_ioremap(&ec->dev, start, end - start);}EXPORT_SYMBOL(ecardm_iomap);static void atomwide_3p_quirk(ecard_t  ec){void __iomem  addr = __ecard_address(ec, ECARD_IOC, ECARD_SYNC);unsigned int i;  Disable interrupts on each port ", "wb977_open();__gpio_modify_io(mask, in);/* Close up the EFER gate ": "nw_gpio_modify_io(unsigned int mask, unsigned int in){  Open up the SuperIO chip ", "unsigned long long ncycles = nsecs * lpj_fine * NDELAY_MULT;__delay(ncycles >> NDELAY_SHIFT);}EXPORT_SYMBOL(ndelay": "ndelay(unsigned long nsecs){    This doesn't bother checking for overflow, as it won't happen (it's   an hour) of delay. ", "WARN(!is_linear_mapping(x) && !is_kernel_mapping(x),     \"virt_to_phys used for non-linear address: %pK (%pS)\\n\",     (void *)x, (void *)x);return __va_to_pa_nodebug(x);}EXPORT_SYMBOL(__virt_to_phys": "__virt_to_phys(unsigned long x){    Boundary checking aginst the kernel linear mapping space. ", "VIRTUAL_BUG_ON(x < kernel_start || x > kernel_end);return __va_to_pa_nodebug(x);}EXPORT_SYMBOL(__phys_addr_symbol": "__phys_addr_symbol(unsigned long x){unsigned long kernel_start = kernel_map.virt_addr;unsigned long kernel_end = kernel_start + kernel_map.size;    Boundary checking aginst the kernel image mapping.   __pa_symbol should only be used on kernel symbol addresses. ", "void sbi_console_putchar(int ch)": "sbi_console_putchar() - Writes given character to the console device.   @ch: The data to be written to the console.     Return: None ", "int sbi_console_getchar(void)": "sbi_console_getchar() - Reads a byte from console device.     Returns the value read from console. ", "void sbi_shutdown(void)": "sbi_shutdown() - Remove all the harts from executing supervisor code.     Return: None ", "for_each_cpu(cpuid, cpu_mask) ": "sbi_send_ipi)(unsigned int cpu) __ro_after_init;static int ( __sbi_rfence)(int fid, const struct cpumask  cpu_mask,   unsigned long start, unsigned long size,   unsigned long arg4, unsigned long arg5) __ro_after_init;struct sbiret sbi_ecall(int ext, int fid, unsigned long arg0,unsigned long arg1, unsigned long arg2,unsigned long arg3, unsigned long arg4,unsigned long arg5){struct sbiret ret;register uintptr_t a0 asm (\"a0\") = (uintptr_t)(arg0);register uintptr_t a1 asm (\"a1\") = (uintptr_t)(arg1);register uintptr_t a2 asm (\"a2\") = (uintptr_t)(arg2);register uintptr_t a3 asm (\"a3\") = (uintptr_t)(arg3);register uintptr_t a4 asm (\"a4\") = (uintptr_t)(arg4);register uintptr_t a5 asm (\"a5\") = (uintptr_t)(arg5);register uintptr_t a6 asm (\"a6\") = (uintptr_t)(fid);register uintptr_t a7 asm (\"a7\") = (uintptr_t)(ext);asm volatile (\"ecall\"      : \"+r\" (a0), \"+r\" (a1)      : \"r\" (a2), \"r\" (a3), \"r\" (a4), \"r\" (a5), \"r\" (a6), \"r\" (a7)      : \"memory\");ret.error = a0;ret.value = a1;return ret;}EXPORT_SYMBOL(sbi_ecall);int sbi_err_map_linux_errno(int err){switch (err) {case SBI_SUCCESS:return 0;case SBI_ERR_DENIED:return -EPERM;case SBI_ERR_INVALID_PARAM:return -EINVAL;case SBI_ERR_INVALID_ADDRESS:return -EFAULT;case SBI_ERR_NOT_SUPPORTED:case SBI_ERR_FAILURE:default:return -ENOTSUPP;};}EXPORT_SYMBOL(sbi_err_map_linux_errno);#ifdef CONFIG_RISCV_SBI_V01static unsigned long __sbi_v01_cpumask_to_hartmask(const struct cpumask  cpu_mask){unsigned long cpuid, hartid;unsigned long hmask = 0;    There is no maximum hartid concept in RISC-V and NR_CPUS must not be   associated with hartid. As SBI v0.1 is only kept for backward compatibility   and will be removed in the future, there is no point in supporting hartid   greater than BITS_PER_LONG (32 for RV32 and 64 for RV64). Ideally, SBI v0.2   should be used for platforms with hartid greater than BITS_PER_LONG. ", "int sbi_remote_fence_i(const struct cpumask *cpu_mask)": "sbi_remote_fence_i() - Execute FENCE.I instruction on given remote harts.   @cpu_mask: A cpu mask containing all the target harts.     Return: 0 on success, appropriate linux error code otherwise. ", "int sbi_remote_sfence_vma(const struct cpumask *cpu_mask,   unsigned long start,   unsigned long size)": "sbi_remote_sfence_vma() - Execute SFENCE.VMA instructions on given remote       harts for the specified virtual address range.   @cpu_mask: A cpu mask containing all the target harts.   @start: Start of the virtual address   @size: Total size of the virtual address range.     Return: 0 on success, appropriate linux error code otherwise. ", "int sbi_remote_sfence_vma_asid(const struct cpumask *cpu_mask,unsigned long start,unsigned long size,unsigned long asid)": "sbi_remote_sfence_vma_asid() - Execute SFENCE.VMA instructions on given   remote harts for a virtual address range belonging to a specific ASID.     @cpu_mask: A cpu mask containing all the target harts.   @start: Start of the virtual address   @size: Total size of the virtual address range.   @asid: The value of address space identifier (ASID).     Return: 0 on success, appropriate linux error code otherwise. ", "int sbi_remote_hfence_gvma_vmid(const struct cpumask *cpu_mask,unsigned long start,unsigned long size,unsigned long vmid)": "sbi_remote_hfence_gvma_vmid() - Execute HFENCE.GVMA instructions on given   remote harts for a guest physical address range belonging to a specific VMID.     @cpu_mask: A cpu mask containing all the target harts.   @start: Start of the guest physical address   @size: Total size of the guest physical address range.   @vmid: The value of guest ID (VMID).     Return: 0 if success, Error otherwise. ", "int sbi_remote_hfence_vvma(const struct cpumask *cpu_mask,   unsigned long start,   unsigned long size)": "sbi_remote_hfence_vvma() - Execute HFENCE.VVMA instructions on given remote       harts for the current guest virtual address range.   @cpu_mask: A cpu mask containing all the target harts.   @start: Start of the current guest virtual address   @size: Total size of the current guest virtual address range.     Return: None ", "int sbi_remote_hfence_vvma_asid(const struct cpumask *cpu_mask,unsigned long start,unsigned long size,unsigned long asid)": "sbi_remote_hfence_vvma_asid() - Execute HFENCE.VVMA instructions on given   remote harts for current guest virtual address range belonging to a specific   ASID.     @cpu_mask: A cpu mask containing all the target harts.   @start: Start of the current guest virtual address   @size: Total size of the current guest virtual address range.   @asid: The value of address space identifier (ASID).     Return: None ", "long sbi_probe_extension(int extid)": "sbi_probe_extension() - Check if an SBI extension ID is supported or not.   @extid: The extension ID to be probed.     Return: 1 or an extension specific nonzero value if yes, 0 otherwise. ", "EXPORT_SYMBOL(acpi_pci_disabled": "acpi_pci_disabled;  skip ACPI PCI scan and IRQ initialization ", "}EXPORT_SYMBOL(__udelay": "__udelay(unsigned long usecs){__const_udelay(usecs   0x10C7UL);   2  32  1000000 (rounded up) ", "}EXPORT_SYMBOL(__ndelay": "__ndelay(unsigned long nsecs){__const_udelay(nsecs   0x5UL);   2  32  1000000000 (rounded up) ", "unsigned long start = (unsigned long)page_address(page);__flush_dcache(start, start + PAGE_SIZE);}void flush_dcache_page(struct page *page)": "flush_dcache_page(struct address_space  mapping, struct page  page){    Writeback any data associated with the kernel mapping of this   page.  This ensures that data in the physical page is mutually   coherent with the kernels mapping. ", "\"1: addi  r6,r6,-1\\n\"\"   movi  r2,-1\\n\"\"   beq   r6,r2,3f\\n\"/* Copy byte by byte for small copies and if src^dst != 0 ": "raw_copy_to_user\\n\"\"   .type raw_copy_to_user, @function\\n\"\"raw_copy_to_user:\\n\"\"   movi  r2,7\\n\"\"   mov   r3,r4\\n\"\"   bge   r2,r6,1f\\n\"\"   xor   r2,r4,r5\\n\"\"   andi  r2,r2,3\\n\"\"   movi  r7,3\\n\"\"   beq   r2,zero,4f\\n\"  Bail if we try to copy zero bytes  ", "last_addr = phys_addr + size - 1;if (!size || last_addr < phys_addr)return NULL;/* Don't allow anybody to remap normal RAM that we're using ": "ioremap(unsigned long phys_addr, unsigned long size){struct vm_struct  area;unsigned long offset;unsigned long last_addr;void  addr;  Don't allow wraparound or zero size ", "void iounmap(void __iomem *addr)": "iounmap unmaps nearly everything, so be careful   it doesn't free currently pointerpage tables anymore but it   wasn't used anyway and might be added later. ", "if (nios2_cs.timer.base)return nios2_timer_read(&nios2_cs.cs);return 0;}EXPORT_SYMBOL(get_cycles": "get_cycles(void){  Only read timer if it has been initialized ", "wfit(end);while ((get_cycles() - start) < cycles)wfet(end);} else if (arch_timer_evtstrm_available()) ": "__delay(unsigned long cycles){cycles_t start = get_cycles();if (cpus_have_const_cap(ARM64_HAS_WFXT)) {u64 end = start + cycles;    Start with WFIT. If an interrupt makes us resume   early, use a WFET loop to complete the delay. ", "asmlinkage void aes_ecb_encrypt(u8 out[], u8 const in[], u32 const rk[],int rounds, int blocks);asmlinkage void aes_ecb_decrypt(u8 out[], u8 const in[], u32 const rk[],int rounds, int blocks);asmlinkage void aes_cbc_encrypt(u8 out[], u8 const in[], u32 const rk[],int rounds, int blocks, u8 iv[]);asmlinkage void aes_cbc_decrypt(u8 out[], u8 const in[], u32 const rk[],int rounds, int blocks, u8 iv[]);asmlinkage void aes_cbc_cts_encrypt(u8 out[], u8 const in[], u32 const rk[],int rounds, int bytes, u8 const iv[]);asmlinkage void aes_cbc_cts_decrypt(u8 out[], u8 const in[], u32 const rk[],int rounds, int bytes, u8 const iv[]);asmlinkage void aes_ctr_encrypt(u8 out[], u8 const in[], u32 const rk[],int rounds, int bytes, u8 ctr[]);asmlinkage void aes_xctr_encrypt(u8 out[], u8 const in[], u32 const rk[], int rounds, int bytes, u8 ctr[], int byte_ctr);asmlinkage void aes_xts_encrypt(u8 out[], u8 const in[], u32 const rk1[],int rounds, int bytes, u32 const rk2[], u8 iv[],int first);asmlinkage void aes_xts_decrypt(u8 out[], u8 const in[], u32 const rk1[],int rounds, int bytes, u32 const rk2[], u8 iv[],int first);asmlinkage void aes_essiv_cbc_encrypt(u8 out[], u8 const in[], u32 const rk1[],      int rounds, int blocks, u8 iv[],      u32 const rk2[]);asmlinkage void aes_essiv_cbc_decrypt(u8 out[], u8 const in[], u32 const rk1[],      int rounds, int blocks, u8 iv[],      u32 const rk2[]);asmlinkage int aes_mac_update(u8 const in[], u32 const rk[], int rounds,      int blocks, u8 dg[], int enc_before,      int enc_after);struct crypto_aes_xts_ctx ": "neon_aes_xts_decrypt#define aes_mac_updateneon_aes_mac_updateMODULE_DESCRIPTION(\"AES-ECBCBCCTRXTSXCTR using ARMv8 NEON\");#endif#if defined(USE_V8_CRYPTO_EXTENSIONS) || !IS_ENABLED(CONFIG_CRYPTO_AES_ARM64_BS)MODULE_ALIAS_CRYPTO(\"ecb(aes)\");MODULE_ALIAS_CRYPTO(\"cbc(aes)\");MODULE_ALIAS_CRYPTO(\"ctr(aes)\");MODULE_ALIAS_CRYPTO(\"xts(aes)\");MODULE_ALIAS_CRYPTO(\"xctr(aes)\");#endifMODULE_ALIAS_CRYPTO(\"cts(cbc(aes))\");MODULE_ALIAS_CRYPTO(\"essiv(cbc(aes),sha256)\");MODULE_ALIAS_CRYPTO(\"cmac(aes)\");MODULE_ALIAS_CRYPTO(\"xcbc(aes)\");MODULE_ALIAS_CRYPTO(\"cbcmac(aes)\");MODULE_AUTHOR(\"Ard Biesheuvel <ard.biesheuvel@linaro.org>\");MODULE_LICENSE(\"GPL v2\");  defined in aes-modes.S ", "static u8 const rcon[] = ": "ce_aes_expandkey(struct crypto_aes_ctx  ctx, const u8  in_key,     unsigned int key_len){    The AES key schedule round constants ", "if (vm_flags & VM_MTE)prot |= PTE_ATTRINDX(MT_NORMAL_TAGGED);return __pgprot(prot);}EXPORT_SYMBOL(vm_get_page_prot": "vm_get_page_prot(unsigned long vm_flags){pteval_t prot = pgprot_val(protection_map[vm_flags &   (VM_READ|VM_WRITE|VM_EXEC|VM_SHARED)]);if (vm_flags & VM_ARM64_BTI)prot |= PTE_GP;    There are two conditions required for returning a Normal Tagged   memory type: (1) the user requested it via PROT_MTE passed to   mmap() or mprotect() and (2) the corresponding vma supports MTE. We   register (1) as VM_MTE in the vma->vm_flags and (2) as   VM_MTE_ALLOWED. Note that the latter can only be set during the   mmap() call since mprotect() does not accept MAP_  flags.   Checking for VM_MTE only is sufficient since arch_validate_flags()   does not permit (VM_MTE & !VM_MTE_ALLOWED). ", "s64 memstart_addr __ro_after_init = -1;EXPORT_SYMBOL(memstart_addr": "memstart_addr   that occur (potentially in generic code) before arm64_memblock_init()   executes, which assigns it its actual value. So use a default value   that cannot be mistaken for a real physical address. ", "if (PHYS_PFN(addr) != pfn)return 0;return memblock_is_map_memory(addr);}EXPORT_SYMBOL(pfn_is_map_memory": "pfn_is_map_memory(unsigned long pfn){phys_addr_t addr = PFN_PHYS(pfn);  avoid false positives for bogus PFNs, see comment in pfn_valid() ", "if (PageHuge(page))page = compound_head(page);if (test_bit(PG_dcache_clean, &page->flags))clear_bit(PG_dcache_clean, &page->flags);}EXPORT_SYMBOL(flush_dcache_page": "flush_dcache_page(struct page  page){    HugeTLB pages are always fully mapped and only head page will be   set PG_dcache_clean (see comments in __sync_icache_dcache()). ", "void copy_to_user_page(struct vm_area_struct *vma, struct page *page,       unsigned long uaddr, void *dst, const void *src,       unsigned long len)": "caches_clean_inval_pou(start, end);}}static void flush_ptrace_access(struct vm_area_struct  vma, unsigned long start,unsigned long end){if (vma->vm_flags & VM_EXEC)sync_icache_aliases(start, end);}    Copy user data fromto a page which is mapped into a different processes   address space.  Really, we want to allow our \"user space\" model to handle   this. ", "WARN_ON_ONCE(!try_page_mte_tagging(to));mte_copy_page_tags(kto, kfrom);set_page_mte_tagged(to);}}EXPORT_SYMBOL(copy_highpage": "copy_highpage(struct page  to, struct page  from){void  kto = page_address(to);void  kfrom = page_address(from);copy_page(kto, kfrom);if (kasan_hw_tags_enabled())page_kasan_tag_reset(to);if (system_supports_mte() && page_mte_tagged(from)) {  It's a new page, shouldn't have been tagged yet ", "VIRTUAL_BUG_ON(x < (unsigned long) KERNEL_START ||       x > (unsigned long) KERNEL_END);return __pa_symbol_nodebug(x);}EXPORT_SYMBOL(__phys_addr_symbol": "__phys_addr_symbol(unsigned long x){    This is bounds checking against the kernel image only.   __pa_symbol should only be used on kernel symbol addresses. ", "if (el2_reset_needed())hdr->__hyp_stub_vectors = __pa_symbol(__hyp_stub_vectors);elsehdr->__hyp_stub_vectors = 0;/* Save the mpidr of the cpu we called cpu_suspend() on... ": "arch_hibernation_header_save(void  addr, unsigned int max_size){struct arch_hibernate_hdr  hdr = addr;if (max_size < sizeof( hdr))return -EOVERFLOW;arch_hdr_invariants(&hdr->invariants);hdr->ttbr1_el1= __pa_symbol(swapper_pg_dir);hdr->reenter_kernel= _cpu_resume;  We can't use __hyp_get_vectors() because kvm may still be loaded ", "#if defined(CONFIG_64BIT) && defined(CONFIG_CPU_MIPSR6) && (__GNUC__ < 8)/* multiply 64-bit values, low 64-bits returned ": "__multi3 calls for mips64r6, so for   that specific case only we implement that intrinsic here.     See https:gcc.gnu.orgbugzillashow_bug.cgi?id=82981 ", ": /* no inputs ": "arch_local_irq_disable(void){preempt_disable_notrace();__asm__ __volatile__(\".setpush\\n\"\".setnoat\\n\"\"mfc0$1,$12\\n\"\"ori$1,0x1f\\n\"\"xori$1,0x1f\\n\"\".setnoreorder\\n\"\"mtc0$1,$12\\n\"\"\" __stringify(__irq_disable_hazard) \"\\n\"\".setpop\\n\":   no outputs ", ": \"memory\");preempt_enable_notrace();return flags;}EXPORT_SYMBOL(arch_local_irq_save": "arch_local_irq_save(void){unsigned long flags;preempt_disable_notrace();__asm__ __volatile__(\".setpush\\n\"\".setreorder\\n\"\".setnoat\\n\"\"mfc0%[flags], $12\\n\"\"ori$1, %[flags], 0x1f\\n\"\"xori$1, 0x1f\\n\"\".setnoreorder\\n\"\"mtc0$1, $12\\n\"\"\" __stringify(__irq_disable_hazard) \"\\n\"\".setpop\\n\": [flags] \"=r\" (flags):   no inputs ", "void __mips_set_bit(unsigned long nr, volatile unsigned long *addr)": "__mips_set_bit - Atomically set a bit in memory.  This is called by   set_bit() if it cannot find a faster solution.   @nr: the bit to set   @addr: the address to start counting from ", "void __mips_clear_bit(unsigned long nr, volatile unsigned long *addr)": "__mips_clear_bit - Clears a bit in memory.  This is called by clear_bit() if   it cannot find a faster solution.   @nr: Bit to clear   @addr: Address to start counting from ", "void __mips_change_bit(unsigned long nr, volatile unsigned long *addr)": "__mips_change_bit - Toggle a bit in memory.This is called by change_bit()   if it cannot find a faster solution.   @nr: Bit to change   @addr: Address to start counting from ", "int __mips_test_and_set_bit_lock(unsigned long nr, volatile unsigned long *addr)": "__mips_test_and_set_bit_lock - Set a bit and return its old value.  This is   called by test_and_set_bit_lock() if it cannot find a faster solution.   @nr: Bit to set   @addr: Address to count from ", "int __mips_test_and_clear_bit(unsigned long nr, volatile unsigned long *addr)": "__mips_test_and_clear_bit - Clear a bit and return its old value.  This is   called by test_and_clear_bit() if it cannot find a faster solution.   @nr: Bit to clear   @addr: Address to count from ", "int __mips_test_and_change_bit(unsigned long nr, volatile unsigned long *addr)": "__mips_test_and_change_bit - Change a bit and return its old value.This is   called by test_and_change_bit() if it cannot find a faster solution.   @nr: Bit to change   @addr: Address to count from ", "__wbflush = wbflush_kn01;break;case MACH_DS5100:/* DS5100 MIPSMATE ": "__wbflush) (void);void __init wbflush_setup(void){switch (mips_machtype) {case MACH_DS23100:case MACH_DS5000_200:  DS5000 3max ", "EXPORT_SYMBOL(shm_align_mask": "shm_align_mask = PAGE_SIZE - 1;  Sane caches ", "old_ctx = read_c0_entryhi();htw_stop();write_c0_entrylo0(0);write_c0_entrylo1(0);entry = num_wired_entries();/* * Blast 'em all away. * If there are any wired entries, fall back to iterating ": "local_flush_tlb_all(void){unsigned long flags;unsigned long old_ctx;int entry, ftlbhighset;local_irq_save(flags);  Save old context and create impossible VPN2 value ", "unsigned long flags;local_irq_save(flags);write_c0_pagemask(PM_HUGE_MASK);back_to_back_c0_hazard();mask = read_c0_pagemask();write_c0_pagemask(PM_DEFAULT_MASK);local_irq_restore(flags);}return mask == PM_HUGE_MASK;}EXPORT_SYMBOL(has_transparent_hugepage": "has_transparent_hugepage(void){static unsigned int mask = -1;if (mask == -1) {  first call comes during __init ", "BUG_ON(pref_bias_clear_store % (2 * clear_word_size));BUG_ON(PAGE_SIZE < pref_bias_clear_store);off = PAGE_SIZE - pref_bias_clear_store;if (off > 0xffff || !pref_bias_clear_store)pg_addiu(&buf, A2, A0, off);elseuasm_i_ori(&buf, A2, A0, off);if (IS_ENABLED(CONFIG_WAR_R4600_V2_HIT_CACHEOP) && cpu_is_r4600_v2_x())uasm_i_lui(&buf, AT, uasm_rel_hi(0xa0000000));off = cache_line_size ? min(8, pref_bias_clear_store / cache_line_size)* cache_line_size : 0;while (off) ": "clear_page_start;extern u32 __clear_page_end;extern u32 __copy_page_start;extern u32 __copy_page_end;void build_clear_page(void){int off;u32  buf = &__clear_page_start;struct uasm_label  l = labels;struct uasm_reloc  r = relocs;int i;static atomic_t run_once = ATOMIC_INIT(0);if (atomic_xchg(&run_once, 1)) {return;}memset(labels, 0, sizeof(labels));memset(relocs, 0, sizeof(relocs));set_prefetch_parameters();    This algorithm makes the following assumptions:     - The prefetch bias is a multiple of 2 words.     - The prefetch bias is less than one page. ", "half_clear_loop_size = min(16 * clear_word_size,   max(cache_line_size >> 1,       4 * clear_word_size));half_copy_loop_size = min(16 * copy_word_size,  max(cache_line_size >> 1,      4 * copy_word_size));}static void build_clear_store(u32 **buf, int off)": "copy_page_array. ", "if (PageHighMem(page))addr = (unsigned long)kmap_atomic(page);elseaddr = (unsigned long)page_address(page);flush_data_cache_page(addr);if (PageHighMem(page))kunmap_atomic((void *)addr);}EXPORT_SYMBOL(__flush_dcache_page": "__flush_dcache_page(struct page  page){struct address_space  mapping = page_mapping_file(page);unsigned long addr;if (mapping && !mapping_mapped(mapping)) {SetPageDcacheDirty(page);return;}    We could delay the flush for the !page_mapping case too.  But that   case is for exec envarg pages and those are %99 certainly going to   get faulted into the tlb (and thus flushed) anyways. ", "VIRTUAL_BUG_ON(x < (unsigned long)_text ||       x > (unsigned long)_end);return __pa_symbol_nodebug(x);}EXPORT_SYMBOL(__phys_addr_symbol": "__phys_addr_symbol(unsigned long x){  This is bounds checking against the kernel image only.   __pa_symbol should only be used on kernel symbol addresses. ", "void __iomem *ioremap_prot(phys_addr_t phys_addr, unsigned long size,unsigned long prot_val)": "ioremap_prot     -   map bus memory into CPU space   @phys_addr:    bus address of the memory   @size:      size of the resource to map     ioremap_prot gives the caller control over cache coherency attributes (CCA) ", "for (e1 = bcm47xx_board_list_melco_id; e1->value1; e1++) ": "bcm47xx_board_get_nvram(void){char buf1[30];char buf2[30];char buf3[30];const struct bcm47xx_board_type_list1  e1;const struct bcm47xx_board_type_list2  e2;const struct bcm47xx_board_type_list3  e3;if (bcm47xx_nvram_getenv(\"model_name\", buf1, sizeof(buf1)) >= 0) {for (e1 = bcm47xx_board_list_model_name; e1->value1; e1++) {if (!strcmp(buf1, e1->value1))return &e1->board;}}if (bcm47xx_nvram_getenv(\"hardware_version\", buf1, sizeof(buf1)) >= 0) {for (e1 = bcm47xx_board_list_hardware_version; e1->value1; e1++) {if (strstarts(buf1, e1->value1))return &e1->board;}}if (bcm47xx_nvram_getenv(\"hardware_version\", buf1, sizeof(buf1)) >= 0 &&    bcm47xx_nvram_getenv(\"boardnum\", buf2, sizeof(buf2)) >= 0) {for (e2 = bcm47xx_board_list_hw_version_num; e2->value1; e2++) {if (!strstarts(buf1, e2->value1) &&    !strcmp(buf2, e2->value2))return &e2->board;}}if (bcm47xx_nvram_getenv(\"productid\", buf1, sizeof(buf1)) >= 0) {for (e1 = bcm47xx_board_list_productid; e1->value1; e1++) {if (!strcmp(buf1, e1->value1))return &e1->board;}}if (bcm47xx_nvram_getenv(\"ModelId\", buf1, sizeof(buf1)) >= 0) {for (e1 = bcm47xx_board_list_ModelId; e1->value1; e1++) {if (!strcmp(buf1, e1->value1))return &e1->board;}}if (bcm47xx_nvram_getenv(\"melco_id\", buf1, sizeof(buf1)) >= 0 ||    bcm47xx_nvram_getenv(\"buf1falo_id\", buf1, sizeof(buf1)) >= 0) {  buffalo hardware, check id for specific hardware matches ", "[0x1b] = true,/* PCI mmio window ": "octeon_should_swizzle_table[256] = {[0x00] = true,  bootbusCF ", "int octeon_is_simulation(void)": "octeon_is_simulation - Return non-zero if we are currently running   in the Octeon simulator     Return: non-0 if running in the Octeon simulator, 0 otherwise ", "uint64_t octeon_get_clock_rate(void)": "octeon_get_clock_rate - Get the clock rate of Octeon     Return: Clock rate in HZ ", "do ": "key_alloc_serial(struct key  key){struct rb_node  parent,   p;struct key  xkey;  propose a random serial number and look for a hole for it in the   serial number tree ", "octeon_ndelay_factor = (octeon_udelay_factor * 0x10000ull) / 1000ull;preset_lpj = octeon_get_clock_rate() / HZ;if (current_cpu_type() == CPU_CAVIUM_OCTEON2) ": "__ndelay we divide by 2^16, so the factor is multiplied   by the same amount. ", "void octeon_io_clk_delay(unsigned long count)": "octeon_io_clk_delay - wait for a given number of io clock cycles to pass.     We scale the wait by the clock ratio, and then wait for the   corresponding number of core clocks.     @count: The number of clocks to wait. ", "for (lane = 0; lane < 4; lane++) ": "__cvmx_helper_errata_qlm_disable_2nd_order_cdr(int qlm){int lane;cvmx_helper_qlm_jtag_init();  We need to load all four lanes of the QLM, a total of 1072 bits ", "if ((PCI_IDSEL_CS5536 == device) && (reg < PCI_MSR_CTRL)) ": "_wrmsr() to   access the regsters PCI_MSR_ADDR, PCI_MSR_DATA_LO,   PCI_MSR_DATA_HI, which is bigger than PCI_MSR_CTRL, so, it   will not go this branch, but the others. so, no calling dead   loop here. ", "sci_handler yeeloong_report_lid_status;EXPORT_SYMBOL(yeeloong_report_lid_status": "yeeloong_report_lid_status will be implemented in yeeloong_laptop.c ", "outb(0x0B, PIC_MASTER_ISR);/* ISR register ": "mach_i8259_irq(void){int irq, isr;irq = -1;if ((LOONGSON_INTISR & LOONGSON_INTEN) & LOONGSON_INT_BIT_INT0) {raw_spin_lock(&i8259A_lock);isr = inb(PIC_MASTER_CMD) &~inb(PIC_MASTER_IMR) & ~(1 << PIC_CASCADE_IR);if (!isr)isr = (inb(PIC_SLAVE_CMD) & ~inb(PIC_SLAVE_IMR)) << 8;irq = ffs(isr) - 1;if (unlikely(irq == 7)) {    This may be a spurious interrupt.     Read the interrupt status register (ISR). If the most   significant bit is not set then there is no valid   interrupt. ", "unsigned long _loongson_uart_base;EXPORT_SYMBOL(loongson_uart_base": "loongson_uart_base;  ioremapped ", "unsigned long mips_io_port_base = -1;EXPORT_SYMBOL(mips_io_port_base": "mips_io_port_base is the begin of the address space to which x86 style   IO ports are mapped. ", "for (i = 1; i < MAX_VPES; i++) ": "vpe_alloc(void){int i;struct vpe  v;  find a vpe ", "set_c0_mvpcontrol(MVPCONTROL_VPC);settc(t->index);write_vpe_c0_vpeconf0(read_vpe_c0_vpeconf0() & ~VPECONF0_VPA);/* halt the TC ": "vpe_free(void  vpe){struct vpe  v = vpe;struct tc  t;unsigned int evpe_flags;t = list_entry(v->tc.next, struct tc, tc);if (t == NULL)return -ENOEXEC;evpe_flags = dvpe();  Put MVPE's into 'configuration state' ", "low = stack_page;if (!preemptible() && on_irq_stack(raw_smp_processor_id(), *sp)) ": "unwind_stack_by_address(unsigned long stack_page,      unsigned long  sp,      unsigned long pc,      unsigned long  ra){unsigned long low, high, irq_stack_high;struct mips_frame_info info;unsigned long size, ofs;struct pt_regs  regs;int leaf;if (!stack_page)return 0;    IRQ stacks start at IRQ_STACK_START   task stacks at THREAD_SIZE - 32 ", "EXPORT_SYMBOL(__cpu_number_map": "__cpu_number_map[NR_CPUS];     Map physical to logical ", "EXPORT_SYMBOL(__cpu_logical_map": "__cpu_logical_map[NR_CPUS];  Map logical to physical ", "if (cpu != smp_processor_id() && cpu_context(cpu, vma->vm_mm))set_cpu_context(cpu, vma->vm_mm, 1);}local_flush_tlb_page(vma, page);}preempt_enable();}static void flush_tlb_one_ipi(void *info)": "flush_tlb_page_ipi(void  info){struct flush_tlb_data  fd = info;local_flush_tlb_page(fd->vma, fd->addr1);}void flush_tlb_page(struct vm_area_struct  vma, unsigned long page){u32 old_mmid;preempt_disable();if (cpu_has_mmid) {htw_stop();old_mmid = read_c0_memorymapid();write_c0_memorymapid(cpu_asid(0, vma->vm_mm));mtc0_tlbw_hazard();ginvt_va_mmid(page);sync_ginv();write_c0_memorymapid(old_mmid);instruction_hazard();htw_start();} else if ((atomic_read(&vma->vm_mm->mm_users) != 1) ||   (current->mm != vma->vm_mm)) {struct flush_tlb_data fd = {.vma = vma,.addr1 = page,};smp_on_other_tlbs(flush_tlb_page_ipi, &fd);local_flush_tlb_page(vma, page);} else {unsigned int cpu;for_each_online_cpu(cpu) {    flush_cache_page() only does partial flushes, so   invalidate ASID without it appearing to   has_valid_asid() as if mm has been completely unused   by that CPU. ", "return (!cpu_has_rixi && exstack == EXSTACK_DEFAULT);}EXPORT_SYMBOL(mips_elf_read_implies_exec": "mips_elf_read_implies_exec(void  elf_ex, int exstack){    Set READ_IMPLIES_EXEC only on non-NX systems that   do not request a specific state via PT_GNU_STACK. ", "chan->io = (void __iomem *)(KSEG1ADDR(AU1000_DMA_PHYS_ADDR) +i * DMA_CHANNEL_LEN);chan->dev_id = dev_id;chan->dev_str = dev_str;chan->fifo_addr = dev->fifo_addr;chan->mode = dev->dma_mode;/* initialize the channel before returning ": "request_au1000_dma(int dev_id, const char  dev_str,       irq_handler_t irqhandler,       unsigned long irqflags,       void  irq_dev_id){struct dma_chan  chan;const struct dma_dev  dev;int i, ret;if (alchemy_get_cputype() == ALCHEMY_CPU_AU1100) {if (dev_id < 0 || dev_id >= (DMA_NUM_DEV + DMA_NUM_DEV_BANK2))return -EINVAL;} else {if (dev_id < 0 || dev_id >= DMA_NUM_DEV)return -EINVAL;}for (i = 0; i < NUM_AU1000_DMA_CHANNELS; i++)if (au1000_dma_table[i].dev_id < 0)break;if (i == NUM_AU1000_DMA_CHANNELS)return -ENODEV;chan = &au1000_dma_table[i];if (dev_id >= DMA_NUM_DEV) {dev_id -= DMA_NUM_DEV;dev = &dma_dev_table_bank2[dev_id];} elsedev = &dma_dev_table[dev_id];if (irqhandler) {chan->irq_dev = irq_dev_id;ret = request_irq(chan->irq, irqhandler, irqflags, dev_str,  chan->irq_dev);if (ret) {chan->irq_dev = NULL;return ret;}} else {chan->irq_dev = NULL;}  fill it in ", "if (!dbdma_initialized)return 0;stp = find_dbdev_id(srcid);if (stp == NULL)return 0;dtp = find_dbdev_id(destid);if (dtp == NULL)return 0;used = 0;/* Check to see if we can get both channels. ": "au1xxx_dbdma_chan_alloc(u32 srcid, u32 destid,       void ( callback)(int, void  ), void  callparam){unsigned longflags;u32used, chan;u32dcp;inti;dbdev_tab_t stp,  dtp;chan_tab_t ctp;au1x_dma_chan_t  cp;    We do the initialization on the first channel allocation.   We have to wait because of the interrupt handler initialization   which can't be done successfully during board set up. ", "rv = stp->dev_devwidth;stp->dev_devwidth = bits;}if (dtp->dev_flags & DEV_FLAGS_OUT) ": "au1xxx_dbdma_set_devwidth(u32 chanid, int bits){u32rv;chan_tab_t ctp;dbdev_tab_t stp,  dtp;ctp =  ((chan_tab_t   )chanid);stp = ctp->chan_src;dtp = ctp->chan_dest;rv = 0;if (stp->dev_flags & DEV_FLAGS_IN) {  Source in fifo ", "ctp = *((chan_tab_t **)chanid);stp = ctp->chan_src;dtp = ctp->chan_dest;/* * The descriptors must be 32-byte aligned.  There is a * possibility the allocation will give us such an address, * and if we try that first we are likely to not waste larger * slabs of memory. ": "au1xxx_dbdma_ring_alloc(u32 chanid, int entries){inti;u32desc_base, srcid, destid;u32cmd0, cmd1, src1, dest1;u32src0, dest0;chan_tab_t ctp;dbdev_tab_t stp,  dtp;au1x_ddma_desc_t dp;    I guess we could check this to be within the   range of the table...... ", "ctp = *(chan_tab_t **)chanid;/* * We should have multiple callers for a particular channel, * an interrupt doesn't affect this pointer nor the descriptor, * so no locking should be needed. ": "au1xxx_dbdma_put_source(u32 chanid, dma_addr_t buf, int nbytes, u32 flags){chan_tab_t ctp;au1x_ddma_desc_t dp;    I guess we could check this to be within the   range of the table...... ", "ctp = *((chan_tab_t **)chanid);/* We should have multiple callers for a particular channel, * an interrupt doesn't affect this pointer nor the descriptor, * so no locking should be needed. ": "au1xxx_dbdma_put_dest(u32 chanid, dma_addr_t buf, int nbytes, u32 flags){chan_tab_t ctp;au1x_ddma_desc_t dp;  I guess we could check this to be within the   range of the table...... ", "wmb(); /* drain writebuffer ": "au1xxx_dbdma_start(u32 chanid){chan_tab_t ctp;au1x_dma_chan_t  cp;ctp =  ((chan_tab_t   )chanid);cp = ctp->chan_ptr;cp->ddma_desptr = virt_to_phys(ctp->cur_ptr);cp->ddma_cfg |= DDMA_CFG_EN;  Enable channel ", "dp = ctp->chan_desc_base;do ": "au1xxx_dbdma_reset(u32 chanid){chan_tab_t ctp;au1x_ddma_desc_t dp;au1xxx_dbdma_stop(chanid);ctp =  ((chan_tab_t   )chanid);ctp->get_ptr = ctp->put_ptr = ctp->cur_ptr = ctp->chan_desc_base;  Run through the descriptors and reset the valid indicator. ", "if (size != roundup_pow_of_two(size))return -EINVAL;if (size < 8 * 1024 || size > 256 * 1024 * 1024)return -EINVAL;val = (base & MPI_CSBASE_BASE_MASK);/* 8k => 0 - 256M => 15 ": "bcm63xx_set_cs_base(unsigned int cs, u32 base, unsigned int size){unsigned long flags;u32 val;if (!is_valid_cs(cs))return -EINVAL;  sanity check on size ", "if (cs == MPI_CS_PCMCIA_COMMON ||    cs == MPI_CS_PCMCIA_ATTR ||    cs == MPI_CS_PCMCIA_IO)return -EINVAL;spin_lock_irqsave(&bcm63xx_cs_lock, flags);val = bcm_mpi_readl(MPI_CSCTL_REG(cs));val &= ~(MPI_CSCTL_DATA16_MASK);val &= ~(MPI_CSCTL_SYNCMODE_MASK);val &= ~(MPI_CSCTL_TSIZE_MASK);val &= ~(MPI_CSCTL_ENDIANSWAP_MASK);val |= params;bcm_mpi_writel(val, MPI_CSCTL_REG(cs));spin_unlock_irqrestore(&bcm63xx_cs_lock, flags);return 0;}EXPORT_SYMBOL(bcm63xx_set_cs_param": "bcm63xx_set_cs_param(unsigned int cs, u32 params){unsigned long flags;u32 val;if (!is_valid_cs(cs))return -EINVAL;  none of this fields apply to pcmcia ", "static void enet_misc_set(struct clk *clk, int enable)": "clk_disable_unlocked(struct clk  clk){if (clk->set && (--clk->usage) == 0)clk->set(clk, 0);}static void bcm_hwclock_set(u32 mask, int enable){u32 reg;reg = bcm_perf_readl(PERF_CKCTL_REG);if (enable)reg |= mask;elsereg &= ~mask;bcm_perf_writel(reg, PERF_CKCTL_REG);}    Ethernet MAC \"misc\" clock: dma clocks and main clock on 6348 ", "#include <linux/kernel.h>#include <linux/init.h>#include <linux/export.h>#include <linux/errno.h>#include <linux/mm.h>#include <linux/memblock.h>#include <linux/spinlock.h>#include <linux/gfp.h>#include <linux/dma-map-ops.h>#include <asm/mipsregs.h>#include <asm/jazz.h>#include <asm/io.h>#include <linux/uaccess.h>#include <asm/dma.h>#include <asm/jazzdma.h>/* * Set this to one to enable additional vdma debug code. ": "vdma_alloc() one could leave page #0 unused   and return the more usual NULL pointer as logical address. ", "status = r4030_read_reg32(JAZZ_R4030_CHNL_ENABLE + (channel << 5));if (status & 0x400)printk(\"VDMA: Channel %d: Address error!\\n\", channel);if (status & 0x200)printk(\"VDMA: Channel %d: Memory error!\\n\", channel);/* * Clear all interrupt flags ": "vdma_enable(int channel){int status;if (vdma_debug)printk(\"vdma_enable: channel %d\\n\", channel);    Check error conditions first ", "*((volatile unsigned int *) JAZZ_DUMMY_DEVICE);}EXPORT_SYMBOL(vdma_disable": "vdma_disable(int channel){if (vdma_debug) {int status =    r4030_read_reg32(JAZZ_R4030_CHNL_ENABLE +     (channel << 5));printk(\"vdma_disable: channel %d\\n\", channel);printk(\"VDMA: channel %d status: %04x (%s) mode: \"       \"%02x addr: %06x count: %06x\\n\",       channel, status,       ((status & 0x600) ? \"ERROR\" : \"OK\"),       (unsigned) r4030_read_reg32(JAZZ_R4030_CHNL_MODE +   (channel << 5)),       (unsigned) r4030_read_reg32(JAZZ_R4030_CHNL_ADDR +   (channel << 5)),       (unsigned) r4030_read_reg32(JAZZ_R4030_CHNL_COUNT +   (channel << 5)));}r4030_write_reg32(JAZZ_R4030_CHNL_ENABLE + (channel << 5),  r4030_read_reg32(JAZZ_R4030_CHNL_ENABLE +   (channel << 5)) &  ~R4030_CHNL_ENABLE);    After disabling a DMA channel a remote bus register should be   read to ensure that the current DMA acknowledge cycle is completed. ", "r4030_write_reg32(JAZZ_R4030_CHNL_MODE + (channel << 5),/*  R4030_MODE_FAST | ": "vdma_set_mode(int channel, int mode){if (vdma_debug)printk(\"vdma_set_mode: channel %d, mode 0x%x\\n\", channel,       mode);switch (channel) {case JAZZ_SCSI_DMA:  scsi ", "for (i = 0; i < 16; i++) ": "vga_console_resource =    { .name= \"vga+\",      .flags= IORESOURCE_IO,      .start= 0x3C0,      .end= 0x3DF };vga_video_type = VIDEO_TYPE_VGAC;display_desc = \"VGA+\";request_resource(&ioport_resource, &vga_console_resource);    Normalise the palette registers, to point   the 16 screen colours to the first 16   DAC entries. ", "return ip22_eeprom_read(&hpc3c0->eeprom, reg);else ": "ip22_nvram_read(int reg){if (ip22_is_fullhouse())  IP22 (Indigo2 aka FullHouse) stores env variables into   93CS56 Microwire Bus EEPROM 2048 Bit (128x16) ", "smp_wmb();}EXPORT_SYMBOL(copy_user_highpage": "copy_user_highpage(struct page  to, struct page  from,unsigned long vaddr, struct vm_area_struct  vma){void  vfrom,  vto;vto = kmap_atomic(to);if (boot_cpu_data.dcache.n_aliases && page_mapcount(from) &&    test_bit(PG_dcache_clean, &from->flags)) {vfrom = kmap_coherent(from, vaddr);copy_page(vto, vfrom);kunmap_coherent(vfrom);} else {vfrom = kmap_atomic(from);copy_page(vto, vfrom);kunmap_atomic(vfrom);}if (pages_do_alias((unsigned long)vto, vaddr & PAGE_MASK) ||    (vma->vm_flags & VM_EXEC))__flush_purge_region(vto, PAGE_SIZE);kunmap_atomic(vto);  Make sure this page is cleared on other CPU's too before using it ", "#ifdef CONFIG_CPU_SHX3/* * It's possible that this gets called early on when IRQs are * still disabled due to ioremapping by the boot CPU, so don't * even attempt IPIs unless there are other CPUs online. ": "flush_icache_range)(void  args) = cache_noop;void ( local_flush_icache_page)(void  args) = cache_noop;void ( local_flush_cache_sigtramp)(void  args) = cache_noop;void ( __flush_wback_region)(void  start, int size);EXPORT_SYMBOL(__flush_wback_region);void ( __flush_purge_region)(void  start, int size);EXPORT_SYMBOL(__flush_purge_region);void ( __flush_invalidate_region)(void  start, int size);EXPORT_SYMBOL(__flush_invalidate_region);static inline void noop__flush_region(void  start, int size){}static inline void cacheop_on_each_cpu(void ( func) (void  info), void  info,                                   int wait){preempt_disable();  Needing IPI for cross-core flush is SHX3-specific. ", "last_addr = phys_addr + size - 1;if (!size || last_addr < phys_addr)return NULL;/* * If we can't yet use the regular approach, go the fixmap route. ": "__ioremap_caller(phys_addr_t phys_addr, unsigned long size, pgprot_t pgprot, void  caller){struct vm_struct  area;unsigned long offset, last_addr, addr, orig_addr;void __iomem  mapped;mapped = __ioremap_trapped(phys_addr, size);if (mapped)return mapped;mapped = __ioremap_29bit(phys_addr, size, pgprot);if (mapped)return mapped;  Don't allow wraparound or zero size ", "#ifdef CONFIG_29BITstatic void __iomem *__ioremap_29bit(phys_addr_t offset, unsigned long size, pgprot_t prot)": "iounmap()\" do not need to do   anything but place the address in the proper segment.  This is true for P1   and P2 addresses, as well as some P3 ones.  However, most of the P3 addresses   and newer cores using extended addressing need to map through page tables, so   the ioremap() implementation becomes a bit more complicated. ", "list_for_each_entry(info, &registered_dmac_list, list) ": "get_dma_info(unsigned int chan){struct dma_info  info;    Look for each DMAC's range to determine who the owner of   the channel is. ", "int request_dma_bycap(const char **dmac, const char **caps, const char *dev_id)": "request_dma_bycap - Allocate a DMA channel based on its capabilities   @dmac: List of DMA controllers to search   @caps: List of capabilities     Search all channels of all DMA controllers to find a channel which   matches the requested capabilities. The result is the channel   number if a match is found, or %-ENODEV if no match is found.     Note that not all DMA controllers export capabilities, in which   case they can never be allocated using this API, and so   request_dma() must be used specifying the channel number. ", "if (!(info->flags & DMAC_CHANNELS_CONFIGURED)) ": "register_dmac(struct dma_info  info){unsigned int total_channels, i;INIT_LIST_HEAD(&info->list);printk(KERN_INFO \"DMA: Registering %s handler (%d channel%s).\\n\",       info->name, info->nr_channels, info->nr_channels > 1 ? \"s\" : \"\");BUG_ON((info->flags & DMAC_CHANNELS_CONFIGURED) && !info->channels);info->pdev = platform_device_register_simple(info->name, -1,     NULL, 0);if (IS_ERR(info->pdev))return PTR_ERR(info->pdev);    Don't touch pre-configured channels ", "static struct pci_channel *hose_head, **hose_tail = &hose_head;static int pci_initialized;static void pcibios_scanbus(struct pci_channel *hose)": "PCIBIOS_MIN_MEM = 0;    The PCI controller list. ", "struct sh_machine_vector sh_mv = ": "sh_mv= on the command line, prior to .machvec.init teardown. ", ": \"memory\");} else ": "arch_local_irq_restore(unsigned long flags){unsigned long __dummy0, __dummy1;if (flags == ARCH_IRQ_DISABLED) {__asm__ __volatile__ (\"stcsr, %0\\n\\t\"\"or#0xf0, %0\\n\\t\"\"ldc%0, sr\\n\\t\": \"=&z\" (__dummy0):   no inputs ", ": \"memory\");return flags;}EXPORT_SYMBOL(arch_local_save_flags": "arch_local_save_flags(void){unsigned long flags;__asm__ __volatile__ (\"stcsr, %0\\n\\t\"\"and#0xf0, %0\\n\\t\": \"=&z\" (flags):   no inputs ", "#ifdef CONFIG_CPU_SH4if ((count >= 0x20) &&     (((u32)to & 0x1f) == 0) && (((u32)from & 0x3) == 0)) ": "memcpy_fromio(void  to, const volatile void __iomem  from, unsigned long count){    Would it be worthwhile doing byte and long transfers first   to try and get aligned? ", "void sq_flush_range(unsigned long start, unsigned int len)": "sq_flush_range - Flush (prefetch) a specific SQ range   @start: the store queue address to start flushing from   @len: the length to flush     Flushes the store queue cache from @start to @start + @len in a   linear fashion. ", "__raw_writel(((map->addr >> 26) << 2) & 0x1c, SQ_QACR0);__raw_writel(((map->addr >> 26) << 2) & 0x1c, SQ_QACR1);#endifreturn 0;}/** * sq_remap - Map a physical address through the Store Queues * @phys: Physical address of mapping. * @size: Length of mapping. * @name: User invoking mapping. * @prot: Protection bits. * * Remaps the physical address @phys through the next available store queue * address of @size length. @name is logged at boot time as well as through * the sysfs interface. ": "sq_remap(struct sq_mapping  map, pgprot_t prot){#if defined(CONFIG_MMU)struct vm_struct  vma;vma = __get_vm_area_caller(map->size, VM_IOREMAP, map->sq_addr,SQ_ADDRMAX, __builtin_return_address(0));if (!vma)return -ENOMEM;vma->phys_addr = map->addr;if (ioremap_page_range((unsigned long)vma->addr,       (unsigned long)vma->addr + map->size,       vma->phys_addr, prot)) {vunmap(vma->addr);return -EAGAIN;}#else    Without an MMU (or with it turned off), this is much more   straightforward, as we can just load up each queue's QACR with   the physical address appropriately masked. ", "void sq_unmap(unsigned long vaddr)": "sq_unmap - Unmap a Store Queue allocation   @vaddr: Pre-allocated Store Queue mapping.     Unmaps the store queue allocation @map that was previously created by   sq_remap(). Also frees up the pte that was previously inserted into   the kernel page table and discards the UTLB translation. ", "kasan_map_memory((void *)KASAN_SHADOW_START, KASAN_SHADOW_SIZE);init_task.kasan_depth = 0;kasan_um_is_ready = true;}static void (*kasan_init_ptr)(void)__section(\".kasan_init\") __used= kasan_init;#endif/* allocated in paging_init, zeroed in mem_init, and unchanged thereafter ": "highmem.h>#include <linuxmm.h>#include <linuxswap.h>#include <linuxslab.h>#include <asmfixmap.h>#include <asmpage.h>#include <as-layout.h>#include <init.h>#include <kern.h>#include <kern_util.h>#include <mem_user.h>#include <os.h>#include <linuxschedtask.h>#ifdef CONFIG_KASANint kasan_um_is_ready;void kasan_init(void){    kasan_map_memory will map all of the required address space and   the host machine will allocate physical memory as necessary. ", "void free_irq_by_fd(int fd)": "um_free_irq() or deactivate_fd() instead. ", "void __init setup_physmem(unsigned long start, unsigned long reserve_end,  unsigned long len, unsigned long long highmem)": "phys_mapping(phys, &offset);err = os_map_memory((void  ) virt, fd, offset, len, r, w, x);if (err) {if (err == -ENOMEM)printk(KERN_ERR \"try increasing the host's \"       \"procsysvmmax_map_count to <physical \"       \"memory size>4096\\n\");panic(\"map_memory(0x%lx, %d, 0x%llx, %ld, %d, %d, %d) failed, \"      \"err = %d\\n\", virt, fd, offset, len, r, w, x, err);}}     setup_physmem() - Setup physical memory for UML   @start:Start address of the physical kernel memory,  i.e start address of the executable image.   @reserve_end:end address of the physical kernel memory.   @len:Length of total physical memory that should be mappedmade  available, in bytes.   @highmem:Number of highmem bytes that should be mappedmade available.     Creates an unlinked temporary file of size (len + highmem) and memory maps   it on the last executable image address (uml_reserved).     The offset is needed as the length of the total physical memory   (len + highmem) includes the size of the memory used be the executable image,   but the mapped-to address is the last address of the executable image   (uml_reserved == end address of executable image).     The memory mapped memory of the temporary file is used as backing memory   of all user space processeskernel tasks. ", "int arch_futex_atomic_op_inuser(int op, u32 oparg, int *oval, u32 __user *uaddr)": "arch_futex_atomic_op_inuser() - Atomic arithmetic operation with constant    argument and comparison of the previous    futex value with another constant.     @encoded_op:encoded operation to execute   @uaddr:pointer to user space address     Return:   0 - On success   -EFAULT - User access resulted in a page fault   -EAGAIN - Atomic operation was unable to complete due to contention   -ENOSYS - Operation not supported ", "int futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,      u32 oldval, u32 newval)": "futex_atomic_cmpxchg_inatomic() - Compare and exchange the content of the  uaddr with newval if the current value is  oldval.   @uval:pointer to store content of @uaddr   @uaddr:pointer to user space address   @oldval:old value   @newval:new value to store to @uaddr     Return:   0 - On success   -EFAULT - User access resulted in a page fault   -EAGAIN - Atomic operation was unable to complete due to contention ", "void __raw_readsw(const void __iomem *addr, void *data, int len)": "__raw_readsw - read words a short at a time   @addr:  source address   @data:  data address   @len: number of shorts to read ", "void __raw_writesw(void __iomem *addr, const void *data, int len)": "__raw_writesw - read words a short at a time   @addr:  source address   @data:  data address   @len: number of shorts to read ", "/* 64 to 33 ": "csum_tcpudp_nofold(__be32 saddr, __be32 daddr,  __u32 len, __u8 proto, __wsum sum){unsigned long result;result = (__force u64)saddr + (__force u64)daddr + (__force u64)sum + ((len + proto) << 8);  Fold down to 32-bits so we don't lose in the typedef-less    network stack.  ", "while (count--) ": "iowrite32_rep(void __iomem  port, const void  src, unsigned long count){if (unlikely((unsigned long)src & 0x3)) {while (count--) {struct S { int x __attribute__((packed)); };iowrite32(((struct S  )src)->x, port);src += 4;}} else {  Buffer 32-bit aligned.  ", "}EXPORT_SYMBOL(pci_iounmap": "pci_iounmap(struct pci_dev  dev, void __iomem   addr){  nothing to do ", "static phandle __prom_getchild(phandle node)": "prom_getchild that does not alter return values. ", "static phandle __prom_getsibling(phandle node)": "prom_getsibling that does not alter return values. ", "spin_lock_irqsave(&prom_lock, flags);ret = prom_nodeops->no_getprop(node, prop, buffer);restore_current();spin_unlock_irqrestore(&prom_lock, flags);return ret;}EXPORT_SYMBOL(prom_getproperty": "prom_getproperty(phandle node, const char  prop, char  buffer, int bufsize){int plen, ret;unsigned long flags;plen = prom_getproplen(node, prop);if((plen > bufsize) || (plen == 0) || (plen == -1))return -1;  Ok, things seem all right. ", "if(error == -1) continue;if(strcmp(nodename, promlib_buf)==0) return thisnode;}return 0;}EXPORT_SYMBOL(prom_searchsiblings": "prom_searchsiblings(phandle node_start, const char  nodename){phandle thisnode;int error;char promlib_buf[128];for(thisnode = node_start; thisnode;    thisnode=prom_getsibling(thisnode)) {error = prom_getproperty(thisnode, \"name\", promlib_buf, sizeof(promlib_buf));  Should this ever happen? ", "char *prom_nextprop(phandle node, char *oprop, char *buffer)": "prom_nextprop(phandle node, char   oprop){unsigned long flags;char  prop;spin_lock_irqsave(&prom_lock, flags);prop = prom_nodeops->no_nextprop(node, oprop);restore_current();spin_unlock_irqrestore(&prom_lock, flags);return prop;}  Return the property type string after property type 'oprop'   at node 'node' .  Returns empty string if no more   property types for this node. ", "node = prom_getchild(node);for (d = nbuf; *s != 0 && *s != '@' && *s != '/';)*d++ = *s++;*d = 0;node = prom_searchsiblings(node, nbuf);if (!node)return 0;if (*s == '@') ": "prom_finddevice(char  name){char nbuf[128];char  s = name,  d;phandle node = prom_root_node, node2;unsigned int which_io, phys_addr;struct linux_prom_registers reg[PROMREG_MAX];while ( s++) {if (! s) return node;   path '...' is legal ", "char *prom_firstprop(phandle node, char *buffer)": "prom_nextprop_name = \"nextprop\";  Return the first property type for node 'node'.   buffer should be at least 32B in length ", "     : \"r\" (mask), \"r\" (non_cpu_bits), \"r\" (&page->flags)     : \"g1\", \"g7\");}static inline void clear_dcache_dirty_cpu(struct page *page, unsigned long cpu)": "flush_dcache_page_impl(struct page  page){BUG_ON(tlb_type == hypervisor);#ifdef CONFIG_DEBUG_DCFLUSHatomic_inc(&dcpage_flushes);#endif#ifdef DCACHE_ALIASING_POSSIBLE__flush_dcache_page(page_address(page),    ((tlb_type == spitfire) &&     page_mapping_file(page) != NULL));#elseif (page_mapping_file(page) != NULL &&    tlb_type == spitfire)__flush_icache_page(__pa(page_address(page)));#endif}#define PG_dcache_dirtyPG_arch_1#define PG_dcache_cpu_shift32UL#define PG_dcache_cpu_mask\\((1UL<<ilog2(roundup_pow_of_two(NR_CPUS)))-1UL)#define dcache_dirty_cpu(page) \\(((page)->flags >> PG_dcache_cpu_shift) & PG_dcache_cpu_mask)static inline void set_dcache_dirty(struct page  page, int this_cpu){unsigned long mask = this_cpu;unsigned long non_cpu_bits;non_cpu_bits = ~(PG_dcache_cpu_mask << PG_dcache_cpu_shift);mask = (mask << PG_dcache_cpu_shift) | (1UL << PG_dcache_dirty);__asm__ __volatile__(\"1:\\n\\t\"     \"ldx[%2], %%g7\\n\\t\"     \"and%%g7, %1, %%g1\\n\\t\"     \"or%%g1, %0, %%g1\\n\\t\"     \"casx[%2], %%g7, %%g1\\n\\t\"     \"cmp%%g7, %%g1\\n\\t\"     \"bne,pn%%xcc, 1b\\n\\t\"     \" nop\"     :   no outputs ", "if (tlb_type == spitfire) ": "flush_icache_range(unsigned long start, unsigned long end){  Cheetah and Hypervisor platform cpus have coherent I-cache. ", "     : \"r\" (va),       \"i\" (ASI_DCACHE_INVALIDATE));}}EXPORT_SYMBOL(__flush_dcache_range": "__flush_dcache_range(unsigned long start, unsigned long end){unsigned long va;if (tlb_type == spitfire) {int n = 0;for (va = start; va < end; va += 32) {spitfire_put_dcache_tag(va & 0x3fe0, 0x0);if (++n >= 512)break;}} else if (tlb_type == cheetah || tlb_type == cheetah_plus) {start = __pa(start);end = __pa(end);for (va = start; va < end; va += 32)__asm__ __volatile__(\"stxa %%g0, [%0] %1\\n\\t\"     \"membar #Sync\"     :   no outputs ", "};static struct mdesc_mblock *mblocks;static int num_mblocks;static struct mdesc_mblock * __init addr_to_mblock(unsigned long addr)": "numa_cpumask_lookup_table[MAX_NUMNODES];struct mdesc_mblock {u64base;u64size;u64offset;   RA-to-PA ", "#endif /* CONFIG_DEBUG_DCFLUSH ": "PAGE_OFFSET)paddr = kaddr & mask;else {pte_t  ptep = virt_to_kpte(kaddr);paddr = pte_val( ptep) & mask;}__flush_icache_page(paddr);}}}EXPORT_SYMBOL(flush_icache_range);void mmu_info(struct seq_file  m){static const char  pgsz_strings[] = {\"8K\", \"64K\", \"512K\", \"4MB\", \"32MB\",\"256MB\", \"2GB\", \"16GB\",};int i, printed;if (tlb_type == cheetah)seq_printf(m, \"MMU Type\\t: Cheetah\\n\");else if (tlb_type == cheetah_plus)seq_printf(m, \"MMU Type\\t: Cheetah+\\n\");else if (tlb_type == spitfire)seq_printf(m, \"MMU Type\\t: Spitfire\\n\");else if (tlb_type == hypervisor)seq_printf(m, \"MMU Type\\t: Hypervisor (sun4v)\\n\");elseseq_printf(m, \"MMU Type\\t: ???\\n\");seq_printf(m, \"MMU PGSZs\\t: \");printed = 0;for (i = 0; i < ARRAY_SIZE(pgsz_strings); i++) {if (cpu_pgsz_mask & (1UL << i)) {seq_printf(m, \"%s%s\",   printed ? \",\" : \"\", pgsz_strings[i]);printed++;}}seq_putc(m, '\\n');#ifdef CONFIG_DEBUG_DCFLUSHseq_printf(m, \"DCPageFlushes\\t: %d\\n\",   atomic_read(&dcpage_flushes));#ifdef CONFIG_SMPseq_printf(m, \"DCPageFlushesXC\\t: %d\\n\",   atomic_read(&dcpage_flushes_xcall));#endif   CONFIG_SMP ", "flushi(&kvmap_linear_patch[0]);flush_all_kernel_tsbs();__flush_tlb_all();}#ifdef CONFIG_DEBUG_PAGEALLOCvoid __kernel_map_pages(struct page *page, int numpages, int enable)": "PAGE_KERNEL, use_huge);}printk(\"Allocated %ld bytes for kernel page tables.\\n\",       mem_alloced);kvmap_linear_patch[0] = 0x01000000;   nop ", "if (tlb_type == hypervisor) ": "_PAGE_EXEC_4V : _PAGE_EXEC_4U);}static void __init hypervisor_tlb_lock(unsigned long vaddr,       unsigned long pte,       unsigned long mmu){unsigned long ret = sun4v_mmu_map_perm_addr(vaddr, 0, pte, mmu);if (ret != 0) {prom_printf(\"hypervisor_tlb_lock[%lx:%x:%lx:%lx]: \"    \"errors with %lx\\n\", vaddr, 0, pte, mmu, ret);prom_halt();}}static unsigned long kern_large_tte(unsigned long paddr);static void __init remap_kernel(void){unsigned long phys_page, tte_vaddr, tte_data;int i, tlb_ent = sparc64_highest_locked_tlbent();tte_vaddr = (unsigned long) KERNBASE;phys_page = (prom_boot_mapping_phys_low >> ILOG2_4MB) << ILOG2_4MB;tte_data = kern_large_tte(phys_page);kern_locked_tte_data = tte_data;  Now lock us into the TLBs via Hypervisor or OBP. ", "static void __init reduce_memory(phys_addr_t limit_ram)": "_PAGE_CACHE_4U(_PAGE_CP_4U | _PAGE_CV_4U)#define _PAGE_CACHE_4V(_PAGE_CP_4V | _PAGE_CV_4V)#define __DIRTY_BITS_4U (_PAGE_MODIFIED_4U | _PAGE_WRITE_4U | _PAGE_W_4U)#define __DIRTY_BITS_4V (_PAGE_MODIFIED_4V | _PAGE_WRITE_4V | _PAGE_W_4V)#define __ACCESS_BITS_4U (_PAGE_ACCESSED_4U | _PAGE_READ_4U | _PAGE_R)#define __ACCESS_BITS_4V (_PAGE_ACCESSED_4V | _PAGE_READ_4V | _PAGE_R)  We need to exclude reserved regions. This exclusion will include   vmlinux and initrd. To be more precise the initrd size could be used to   compute a new lower limit because it is freed later during initialization. ", "if (vma->vm_flags & VM_SPARC_ADI) ": "copy_user_highpage(struct page  to, struct page  from,unsigned long vaddr, struct vm_area_struct  vma){char  vfrom,  vto;vfrom = kmap_atomic(from);vto = kmap_atomic(to);copy_user_page(vto, vfrom, vaddr, to);kunmap_atomic(vto);kunmap_atomic(vfrom);  If this page has ADI enabled, copy over any ADI tags   as well ", "if (adi_capable()) ": "copy_highpage(struct page  to, struct page  from){char  vfrom,  vto;vfrom = kmap_atomic(from);vto = kmap_atomic(to);copy_page(vto, vfrom);kunmap_atomic(vto);kunmap_atomic(vfrom);  If this platform is ADI enabled, copy any ADI tags   as well ", "u64 vio_vdev_node(struct mdesc_handle *hp, struct vio_dev *vdev)": "vio_vdev_node() - Find VDEV node in MD   @hp:  Handle to the MD   @vdev:  Pointer to VDEV     Find the node in the current MD which matches the given vio_dev. This   must be done dynamically since the node value can change if the MD   is updated.     NOTE: the MD must be locked, using mdesc_grab(), when calling this routine     Return: The VDEV node in MDESC ", "return;}val |= (1UL << 14UL);if (bursts & DMA_BURST8)val |= (1UL << 1UL);if (bursts & DMA_BURST16)val |= (1UL << 2UL);if (bursts & DMA_BURST32)val |= (1UL << 3UL);if (bursts & DMA_BURST64)val |= (1UL << 4UL);upa_writeq(val, cfg_reg);}EXPORT_SYMBOL(sbus_set_sbus64": "sbus_set_sbus64(struct device  dev, int bursts){struct iommu  iommu = dev->archdata.iommu;struct platform_device  op = to_platform_device(dev);const struct linux_prom_registers  regs;unsigned long cfg_reg;int slot;u64 val;regs = of_get_property(op->dev.of_node, \"reg\", NULL);if (!regs) {printk(KERN_ERR \"sbus_set_sbus64: Cannot find regs for %pOF\\n\",       op->dev.of_node);return;}slot = regs->which_io;cfg_reg = iommu->write_complete_reg;switch (slot) {case 0:cfg_reg += 0x20UL;break;case 1:cfg_reg += 0x28UL;break;case 2:cfg_reg += 0x30UL;break;case 3:cfg_reg += 0x38UL;break;case 13:cfg_reg += 0x40UL;break;case 14:cfg_reg += 0x48UL;break;case 15:cfg_reg += 0x50UL;break;default:return;}val = upa_readq(cfg_reg);if (val & (1UL << 14UL)) {  Extended transfer mode already enabled. ", "if ((insn & 0xc0800000) == 0xc0800000) ": "die_if_kernel(buffer, regs);}lvl -= 0x100;if (regs->tstate & TSTATE_PRIV) {sprintf(buffer, \"Kernel bad sw trap %lx\", lvl);die_if_kernel(buffer, regs);}if (test_thread_flag(TIF_32BIT)) {regs->tpc &= 0xffffffff;regs->tnpc &= 0xffffffff;}force_sig_fault_trapno(SIGILL, ILL_ILLTRP,       (void __user  )regs->tpc, lvl);}void bad_trap_tl1(struct pt_regs  regs, long lvl){char buffer[36];if (notify_die(DIE_TRAP_TL1, \"bad trap tl1\", regs,       0, lvl, SIGTRAP) == NOTIFY_STOP)return;dump_tl1_traplog((struct tl1_traplog  )(regs + 1));sprintf (buffer, \"Bad trap %lx at tl>0\", lvl);die_if_kernel (buffer, regs);}#ifdef CONFIG_DEBUG_BUGVERBOSEvoid do_BUG(const char  file, int line){bust_spinlocks(1);printk(\"kernel BUG at %s:%d!\\n\", file, line);}EXPORT_SYMBOL(do_BUG);#endifstatic DEFINE_SPINLOCK(dimm_handler_lock);static dimm_printer_t dimm_handler;static int sprintf_dimm(int synd_code, unsigned long paddr, char  buf, int buflen){unsigned long flags;int ret = -ENODEV;spin_lock_irqsave(&dimm_handler_lock, flags);if (dimm_handler) {ret = dimm_handler(synd_code, paddr, buf, buflen);} else if (tlb_type == spitfire) {if (prom_getunumber(synd_code, paddr, buf, buflen) == -1)ret = -EINVAL;elseret = 0;} elseret = -ENODEV;spin_unlock_irqrestore(&dimm_handler_lock, flags);return ret;}int register_dimm_printer(dimm_printer_t func){unsigned long flags;int ret = 0;spin_lock_irqsave(&dimm_handler_lock, flags);if (!dimm_handler)dimm_handler = func;elseret = -EEXIST;spin_unlock_irqrestore(&dimm_handler_lock, flags);return ret;}EXPORT_SYMBOL_GPL(register_dimm_printer);void unregister_dimm_printer(dimm_printer_t func){unsigned long flags;spin_lock_irqsave(&dimm_handler_lock, flags);if (dimm_handler == func)dimm_handler = NULL;spin_unlock_irqrestore(&dimm_handler_lock, flags);}EXPORT_SYMBOL_GPL(unregister_dimm_printer);void spitfire_insn_access_exception(struct pt_regs  regs, unsigned long sfsr, unsigned long sfar){enum ctx_state prev_state = exception_enter();if (notify_die(DIE_TRAP, \"instruction access exception\", regs,       0, 0x8, SIGTRAP) == NOTIFY_STOP)goto out;if (regs->tstate & TSTATE_PRIV) {printk(\"spitfire_insn_access_exception: SFSR[%016lx] \"       \"SFAR[%016lx], going.\\n\", sfsr, sfar);die_if_kernel(\"Iax\", regs);}if (test_thread_flag(TIF_32BIT)) {regs->tpc &= 0xffffffff;regs->tnpc &= 0xffffffff;}force_sig_fault(SIGSEGV, SEGV_MAPERR, (void __user  )regs->tpc);out:exception_exit(prev_state);}void spitfire_insn_access_exception_tl1(struct pt_regs  regs, unsigned long sfsr, unsigned long sfar){if (notify_die(DIE_TRAP_TL1, \"instruction access exception tl1\", regs,       0, 0x8, SIGTRAP) == NOTIFY_STOP)return;dump_tl1_traplog((struct tl1_traplog  )(regs + 1));spitfire_insn_access_exception(regs, sfsr, sfar);}void sun4v_insn_access_exception(struct pt_regs  regs, unsigned long addr, unsigned long type_ctx){unsigned short type = (type_ctx >> 16);unsigned short ctx  = (type_ctx & 0xffff);if (notify_die(DIE_TRAP, \"instruction access exception\", regs,       0, 0x8, SIGTRAP) == NOTIFY_STOP)return;if (regs->tstate & TSTATE_PRIV) {printk(\"sun4v_insn_access_exception: ADDR[%016lx] \"       \"CTX[%04x] TYPE[%04x], going.\\n\",       addr, ctx, type);die_if_kernel(\"Iax\", regs);}if (test_thread_flag(TIF_32BIT)) {regs->tpc &= 0xffffffff;regs->tnpc &= 0xffffffff;}force_sig_fault(SIGSEGV, SEGV_MAPERR, (void __user  ) addr);}void sun4v_insn_access_exception_tl1(struct pt_regs  regs, unsigned long addr, unsigned long type_ctx){if (notify_die(DIE_TRAP_TL1, \"instruction access exception tl1\", regs,       0, 0x8, SIGTRAP) == NOTIFY_STOP)return;dump_tl1_traplog((struct tl1_traplog  )(regs + 1));sun4v_insn_access_exception(regs, addr, type_ctx);}bool is_no_fault_exception(struct pt_regs  regs){unsigned char asi;u32 insn;if (get_user(insn, (u32 __user  )regs->tpc) == -EFAULT)return false;    Must do a little instruction decoding here in order to   decide on a course of action. The bits of interest are:    insn[31:30] = op, where 3 indicates the loadstore group    insn[24:19] = op3, which identifies individual opcodes    insn[13] indicates an immediate offset    op3[4]=1 identifies alternate space instructions    op3[5:4]=3 identifies floating point instructions    op3[2]=1 identifies stores   See \"Opcode Maps\" in the appendix of any Sparc V9   architecture spec for full details. ", "ent->err_handle = 0;wmb();put_cpu();if (local_copy.err_type == SUN4V_ERR_TYPE_SHUTDOWN_RQST) ": "trap_block[cpu];paddr = tb->resum_kernel_buf_pa + offset;ent = __va(paddr);memcpy(&local_copy, ent, sizeof(struct sun4v_error_entry));  We have a local copy now, so release the entry.  ", "if (tp->type == VIO_TYPE_CTRL &&    tp->stype == VIO_SUBTYPE_INFO &&    tp->stype_env == VIO_VER_INFO)return 0;/* Ok, now figure out which SID to use.  ": "vio_validate_sid(struct vio_driver_state  vio, struct vio_msg_tag  tp){u32 sid;  Always let VERSION+INFO packets through unchecked, they   define the new SID. ", "viodbg(HS, \"HANDSHAKE FAILURE\\n\");vio->dr_state &= ~(VIO_DR_STATE_TXREG |   VIO_DR_STATE_RXREG);dr = &vio->drings[VIO_DRIVER_RX_RING];memset(dr, 0, sizeof(*dr));kfree(vio->desc_buf);vio->desc_buf = NULL;vio->desc_buf_len = 0;vio->hs_state = VIO_HS_INVALID;return -ECONNRESET;}static int process_unknown(struct vio_driver_state *vio, void *arg)": "vio_send_sid(vio);return vio_ldc_send(vio, tag, len);}static void init_tag(struct vio_msg_tag  tag, u8 type, u8 stype, u16 stype_env){tag->type = type;tag->stype = stype;tag->stype_env = stype_env;}static int send_version(struct vio_driver_state  vio, u16 major, u16 minor){struct vio_ver_info pkt;vio->_local_sid = (u32) sched_clock();memset(&pkt, 0, sizeof(pkt));init_tag(&pkt.tag, VIO_TYPE_CTRL, VIO_SUBTYPE_INFO, VIO_VER_INFO);pkt.major = major;pkt.minor = minor;pkt.dev_class = vio->dev_class;viodbg(HS, \"SEND VERSION INFO maj[%u] min[%u] devclass[%u]\\n\",       major, minor, vio->dev_class);return send_ctrl(vio, &pkt.tag, sizeof(pkt));}static int start_handshake(struct vio_driver_state  vio){int err;viodbg(HS, \"START HANDSHAKE\\n\");vio->hs_state = VIO_HS_INVALID;err = send_version(vio,   vio->ver_table[0].major,   vio->ver_table[0].minor);if (err < 0)return err;return 0;}static void flush_rx_dring(struct vio_driver_state  vio){struct vio_dring_state  dr;u64 ident;BUG_ON(!(vio->dr_state & VIO_DR_STATE_RXREG));dr = &vio->drings[VIO_DRIVER_RX_RING];ident = dr->ident;BUG_ON(!vio->desc_buf);kfree(vio->desc_buf);vio->desc_buf = NULL;memset(dr, 0, sizeof( dr));dr->ident = ident;}void vio_link_state_change(struct vio_driver_state  vio, int event){if (event == LDC_EVENT_UP) {vio->hs_state = VIO_HS_INVALID;switch (vio->dev_class) {case VDEV_NETWORK:case VDEV_NETWORK_SWITCH:vio->dr_state = (VIO_DR_STATE_TXREQ | VIO_DR_STATE_RXREQ);break;case VDEV_DISK:vio->dr_state = VIO_DR_STATE_TXREQ;break;case VDEV_DISK_SERVER:vio->dr_state = VIO_DR_STATE_RXREQ;break;}start_handshake(vio);} else if (event == LDC_EVENT_RESET) {vio->hs_state = VIO_HS_INVALID;if (vio->dr_state & VIO_DR_STATE_RXREG)flush_rx_dring(vio);vio->dr_state = 0x00;memset(&vio->ver, 0, sizeof(vio->ver));ldc_disconnect(vio->lp);}}EXPORT_SYMBOL(vio_link_state_change);static int handshake_failure(struct vio_driver_state  vio){struct vio_dring_state  dr;  XXX Put policy here...  Perhaps start a timer to fire   XXX in 100 ms, which will bring the link up and retry   XXX the handshake. ", "#include <linux/module.h>#include <linux/sched.h>#include <linux/kernel.h>#include <linux/errno.h>#include <linux/types.h>#include <linux/ioport.h>#include <linux/mm.h>#include <linux/slab.h>#include <linux/pci.h>/* struct pci_dev ": "iounmap() by Pete Zaitcev.     20000129   <rth> zait: as long as pci_alloc_consistent produces something addressable,   things are ok.   <zaitcev> rth: no, it is relevant, because get_free_pages returns you a  pointer into the big page mapping   <rth> zait: so what?   <rth> zait: remap_it_my_way(virt_to_phys(get_free_page()))   <zaitcev> Hmm   <zaitcev> Suppose I did this remap_it_my_way(virt_to_phys(get_free_page())).  So far so good.   <zaitcev> Now, driver calls pci_free_consistent(with result of  remap_it_my_way()).   <zaitcev> How do you find the address to pass to free_pages()?   <rth> zait: walk the page tables?  It's only two or three level after all.   <rth> zait: you have to walk them anyway to remove the mapping.   <zaitcev> Hmm   <zaitcev> Sounds reasonable ", "return;}p = &__cpuid_patch;while (p < &__cpuid_patch_end) ": "sparc_cpu_model == sun4m) {  Nothing to do, this is what the unpatched code   targets. ", "floppy_irq = irq;cpu_irq = (irq & (NR_IRQS - 1));/* Dork with trap table if we get this far. ": "sparc_floppy_request_irq(unsigned int irq, irq_handler_t irq_handler){unsigned int cpu_irq;int err;err = request_irq(irq, irq_handler, 0, \"floppy\", NULL);if (err)return -1;  Save for later use in floppy interrupt handler ", "atomic_t nmi_active = ATOMIC_INIT(0);/* oprofile uses this ": "nmi_active:   >0: the NMI watchdog is active, but can be disabled   <0: the NMI watchdog has not been set up, and cannot be enabled    0: the NMI watchdog is disabled, but can be enabled ", "static HLIST_HEAD(ldc_channel_list);static int __ldc_channel_exists(unsigned long id)": "ldc_free() needs to run under a mutex so   XXX that addition and removal from the ldc_channel_list has   XXX atomicity, otherwise the __ldc_channel_exists() check is   XXX totally pointless as another thread can slip into ldc_alloc()   XXX and add a channel with the same ID.  There also needs to be   XXX a spinlock for ldc_channel_list. ", "lp->hs_state = LDC_HS_COMPLETE;}spin_unlock_irqrestore(&lp->lock, flags);return 0;out_unmap_rx:lp->flags &= ~LDC_FLAG_REGISTERED_QUEUES;sun4v_ldc_rx_qconf(lp->id, 0, 0);out_unmap_tx:sun4v_ldc_tx_qconf(lp->id, 0, 0);out_free_irqs:lp->flags &= ~LDC_FLAG_REGISTERED_IRQS;free_irq(lp->cfg.tx_irq, lp);free_irq(lp->cfg.rx_irq, lp);spin_unlock_irqrestore(&lp->lock, flags);return err;}EXPORT_SYMBOL(ldc_bind": "ldc_bind(struct ldc_channel  lp){unsigned long hv_err, flags;int err = -EINVAL;if (lp->state != LDC_STATE_INIT)return -EINVAL;spin_lock_irqsave(&lp->lock, flags);enable_irq(lp->cfg.rx_irq);enable_irq(lp->cfg.tx_irq);lp->flags |= LDC_FLAG_REGISTERED_IRQS;err = -ENODEV;hv_err = sun4v_ldc_tx_qconf(lp->id, 0, 0);if (hv_err)goto out_free_irqs;hv_err = sun4v_ldc_tx_qconf(lp->id, lp->tx_ra, lp->tx_num_entries);if (hv_err)goto out_free_irqs;hv_err = sun4v_ldc_rx_qconf(lp->id, 0, 0);if (hv_err)goto out_unmap_tx;hv_err = sun4v_ldc_rx_qconf(lp->id, lp->rx_ra, lp->rx_num_entries);if (hv_err)goto out_unmap_tx;lp->flags |= LDC_FLAG_REGISTERED_QUEUES;hv_err = sun4v_ldc_tx_get_state(lp->id,&lp->tx_head,&lp->tx_tail,&lp->chan_state);err = -EBUSY;if (hv_err)goto out_unmap_rx;lp->tx_acked = lp->tx_head;lp->hs_state = LDC_HS_OPEN;ldc_set_state(lp, LDC_STATE_BOUND);if (lp->cfg.mode == LDC_MODE_RAW) {    There is no handshake in RAW mode, so handshake   is completed. ", "int ldc_bind(struct ldc_channel *lp)": "ldc_connect() does   that. ", "sun4v_ldc_rx_get_state(lp->id,       &lp->rx_head,       &lp->rx_tail,       &lp->chan_state);ldcdbg(RX, \"RX state[0x%02lx:0x%02lx] head[0x%04lx] tail[0x%04lx]\\n\",       orig_state, lp->chan_state, lp->rx_head, lp->rx_tail);event_mask = 0;if (lp->cfg.mode == LDC_MODE_RAW &&    lp->chan_state == LDC_CHANNEL_UP) ": "ldc_set_state(lp, LDC_STATE_CONNECTED);send_rdx(lp);return LDC_EVENT_UP;}static int rx_seq_ok(struct ldc_channel  lp, u32 seqid){return lp->rcv_nxt + 1 == seqid;}static int process_rdx(struct ldc_channel  lp,       struct ldc_packet  p){ldcdbg(HS, \"GOT RDX stype[%x] seqid[%x] env[%x] ackid[%x]\\n\",       p->stype, p->seqid, p->env, p->u.r.ackid);if (p->stype != LDC_INFO ||    !(rx_seq_ok(lp, p->seqid)))return LDC_ABORT(lp);lp->rcv_nxt = p->seqid;lp->hs_state = LDC_HS_COMPLETE;ldc_set_state(lp, LDC_STATE_CONNECTED);return LDC_EVENT_UP;}static int process_control_frame(struct ldc_channel  lp, struct ldc_packet  p){switch (p->ctrl) {case LDC_VERS:return process_version(lp, p);case LDC_RTS:return process_rts(lp, p);case LDC_RTR:return process_rtr(lp, p);case LDC_RDX:return process_rdx(lp, p);default:return LDC_ABORT(lp);}}static int process_error_frame(struct ldc_channel  lp,       struct ldc_packet  p){return LDC_ABORT(lp);}static int process_data_ack(struct ldc_channel  lp,    struct ldc_packet  ack){unsigned long head = lp->tx_acked;u32 ackid = ack->u.r.ackid;while (1) {struct ldc_packet  p = lp->tx_base + (head  LDC_PACKET_SIZE);head = tx_advance(lp, head);if (p->seqid == ackid) {lp->tx_acked = head;return 0;}if (head == lp->tx_tail)return LDC_ABORT(lp);}return 0;}static void send_events(struct ldc_channel  lp, unsigned int event_mask){if (event_mask & LDC_EVENT_RESET)lp->cfg.event(lp->event_arg, LDC_EVENT_RESET);if (event_mask & LDC_EVENT_UP)lp->cfg.event(lp->event_arg, LDC_EVENT_UP);if (event_mask & LDC_EVENT_DATA_READY)lp->cfg.event(lp->event_arg, LDC_EVENT_DATA_READY);}static irqreturn_t ldc_rx(int irq, void  dev_id){struct ldc_channel  lp = dev_id;unsigned long orig_state, flags;unsigned int event_mask;spin_lock_irqsave(&lp->lock, flags);orig_state = lp->chan_state;  We should probably check for hypervisor errors here and   reset the LDC channel if we get one. ", "spinlock_tlock;struct ldc_mtable_entry*page_table;struct iommu_map_tableiommu_map_table;};struct ldc_channel ": "ldc_unmap.  ", "if (lp->hs_state == LDC_HS_COMPLETE) ": "ldc_read()   paths do all of the control frame and state management.   Just trigger the callback. ", "return orig_len - len;}EXPORT_SYMBOL(ldc_copy": "ldc_copy(struct ldc_channel  lp, int copy_dir,     void  buf, unsigned int len, unsigned long offset,     struct ldc_trans_cookie  cookies, int ncookies){unsigned int orig_len;unsigned long ra;int i;if (copy_dir != LDC_COPY_IN && copy_dir != LDC_COPY_OUT) {printk(KERN_ERR PFX \"ldc_copy: ID[%lu] Bad copy_dir[%d]\\n\",       lp->id, copy_dir);return -EINVAL;}ra = __pa(buf);if ((ra | len | offset) & (8UL - 1)) {printk(KERN_ERR PFX \"ldc_copy: ID[%lu] Unaligned buffer \"       \"ra[%lx] len[%x] offset[%lx]\\n\",       lp->id, ra, len, offset);return -EFAULT;}if (lp->hs_state != LDC_HS_COMPLETE ||    (lp->flags & LDC_FLAG_RESET)) {printk(KERN_ERR PFX \"ldc_copy: ID[%lu] Link down hs_state[%x] \"       \"flags[%x]\\n\", lp->id, lp->hs_state, lp->flags);return -ECONNRESET;}orig_len = len;for (i = 0; i < ncookies; i++) {unsigned long cookie_raddr = cookies[i].cookie_addr;unsigned long this_len = cookies[i].cookie_size;unsigned long actual_len;if (unlikely(offset)) {unsigned long this_off = offset;if (this_off > this_len)this_off = this_len;offset -= this_off;this_len -= this_off;if (!this_len)continue;cookie_raddr += this_off;}if (this_len > len)this_len = len;while (1) {unsigned long hv_err;hv_err = sun4v_ldc_copy(lp->id, copy_dir,cookie_raddr, ra,this_len, &actual_len);if (unlikely(hv_err)) {printk(KERN_ERR PFX \"ldc_copy: ID[%lu] \"       \"HV error %lu\\n\",       lp->id, hv_err);if (lp->hs_state != LDC_HS_COMPLETE ||    (lp->flags & LDC_FLAG_RESET))return -ECONNRESET;elsereturn -EFAULT;}cookie_raddr += actual_len;ra += actual_len;len -= actual_len;if (actual_len == this_len)break;this_len -= actual_len;}if (!len)break;}  It is caller policy what to do about short copies.   For example, a networking driver can declare the   packet a runt and drop it. ", "regval = sbus_readb(auxio_register);sbus_writeb(((regval | bits_on) & ~bits_off) | AUXIO_ORMEIN4M,auxio_register);break;case sun4d:break;default:panic(\"Can't set AUXIO register on this machine.\");}spin_unlock_irqrestore(&auxio_lock, flags);}EXPORT_SYMBOL(set_auxio": "set_auxio(AUXIO_LED, 0);}unsigned char get_auxio(void){if(auxio_register) return sbus_readb(auxio_register);return 0;}EXPORT_SYMBOL(get_auxio);void set_auxio(unsigned char bits_on, unsigned char bits_off){unsigned char regval;unsigned long flags;spin_lock_irqsave(&auxio_lock, flags);switch (sparc_cpu_model) {case sun4m:if(!auxio_register)break;       VME chassis sun4m, no auxio. ", "return get_area(NULL, orig_addr, len, pgoff, flags);}flags &= ~MAP_SHARED;align_goal = PAGE_SIZE;if (len >= (4UL * 1024 * 1024))align_goal = (4UL * 1024 * 1024);else if (len >= (512UL * 1024))align_goal = (512UL * 1024);else if (len >= (64UL * 1024))align_goal = (64UL * 1024);do ": "get_fb_unmapped_area(struct file  filp, unsigned long orig_addr, unsigned long len, unsigned long pgoff, unsigned long flags){unsigned long align_goal, addr = -ENOMEM;unsigned long ( get_area)(struct file  , unsigned long,  unsigned long, unsigned long, unsigned long);get_area = current->mm->get_unmapped_area;if (flags & MAP_FIXED) {  Ok, don't mess with it. ", "return cpumask_first(cpu_online_mask);}static int _map_to_cpu(unsigned int index)": "map_to_cpu(unsigned int index){int i, end, cpu_rover;cpu_rover = 0;end = index % num_online_cpus();for (i = 0; i < num_possible_cpus(); i++) {if (cpu_online(cpu_rover)) {if (cpu_rover >= end)return cpu_rover;cpu_rover++;}}  Impossible, since num_online_cpus() <= num_possible_cpus() ", "if ((dev->class >> 8) == PCI_CLASS_STORAGE_IDE)pci_set_master(dev);dev->current_state = PCI_UNKNOWN;/* unknown power state ": "pci_domain_nr(bus),dev->bus->number, PCI_SLOT(devfn), PCI_FUNC(devfn));  I have seen IDE devices which will not respond to   the bmdma simplex check reads if bus mastering is   disabled. ", "for (i = 0; md_node_ops_table[i].name != NULL; i++) ": "mdesc_get_node_ops(const char  node_name,       mdesc_node_info_get_f  get_info_f,       mdesc_node_info_rel_f  rel_info_f,       mdesc_node_match_f  match_f){int i;if (get_info_f) get_info_f = NULL;if (rel_info_f) rel_info_f = NULL;if (match_f) match_f = NULL;if (!node_name)return;for (i = 0; md_node_ops_table[i].name != NULL; i++) {if (strcmp(md_node_ops_table[i].name, node_name) == 0) {if (get_info_f) get_info_f = md_node_ops_table[i].get_info;if (rel_info_f) rel_info_f = md_node_ops_table[i].rel_info;if (match_f) match_f = md_node_ops_table[i].node_match;break;}}}static void mdesc_handle_init(struct mdesc_handle  hp,      unsigned int handle_size,      void  base){BUG_ON(((unsigned long)&hp->mdesc) & (16UL - 1));memset(hp, 0, handle_size);INIT_LIST_HEAD(&hp->list);hp->self_base = base;refcount_set(&hp->refcnt, 1);hp->handle_size = handle_size;}static struct mdesc_handle   __init mdesc_memblock_alloc(unsigned int mdesc_size){unsigned int handle_size, alloc_size;struct mdesc_handle  hp;unsigned long paddr;handle_size = (sizeof(struct mdesc_handle) -       sizeof(struct mdesc_hdr) +       mdesc_size);alloc_size = PAGE_ALIGN(handle_size);paddr = memblock_phys_alloc(alloc_size, PAGE_SIZE);hp = NULL;if (paddr) {hp = __va(paddr);mdesc_handle_init(hp, handle_size, hp);}return hp;}static void __init mdesc_memblock_free(struct mdesc_handle  hp){unsigned int alloc_size;unsigned long start;BUG_ON(refcount_read(&hp->refcnt) != 0);BUG_ON(!list_empty(&hp->list));alloc_size = PAGE_ALIGN(hp->handle_size);start = __pa(hp);memblock_free_late(start, alloc_size);}static struct mdesc_mem_ops memblock_mdesc_ops = {.alloc = mdesc_memblock_alloc,.free  = mdesc_memblock_free,};static struct mdesc_handle  mdesc_kmalloc(unsigned int mdesc_size){unsigned int handle_size;struct mdesc_handle  hp;unsigned long addr;void  base;handle_size = (sizeof(struct mdesc_handle) -       sizeof(struct mdesc_hdr) +       mdesc_size);base = kmalloc(handle_size + 15, GFP_KERNEL | __GFP_RETRY_MAYFAIL);if (!base)return NULL;addr = (unsigned long)base;addr = (addr + 15UL) & ~15UL;hp = (struct mdesc_handle  ) addr;mdesc_handle_init(hp, handle_size, base);return hp;}static void mdesc_kfree(struct mdesc_handle  hp){BUG_ON(refcount_read(&hp->refcnt) != 0);BUG_ON(!list_empty(&hp->list));kfree(hp->self_base);}static struct mdesc_mem_ops kmalloc_mdesc_memops = {.alloc = mdesc_kmalloc,.free  = mdesc_kfree,};static struct mdesc_handle  mdesc_alloc(unsigned int mdesc_size,struct mdesc_mem_ops  mops){struct mdesc_handle  hp = mops->alloc(mdesc_size);if (hp)hp->mops = mops;return hp;}static void mdesc_free(struct mdesc_handle  hp){hp->mops->free(hp);}static struct mdesc_handle  cur_mdesc;static LIST_HEAD(mdesc_zombie_list);static DEFINE_SPINLOCK(mdesc_lock);struct mdesc_handle  mdesc_grab(void){struct mdesc_handle  hp;unsigned long flags;spin_lock_irqsave(&mdesc_lock, flags);hp = cur_mdesc;if (hp)refcount_inc(&hp->refcnt);spin_unlock_irqrestore(&mdesc_lock, flags);return hp;}EXPORT_SYMBOL(mdesc_grab);void mdesc_release(struct mdesc_handle  hp){unsigned long flags;spin_lock_irqsave(&mdesc_lock, flags);if (refcount_dec_and_test(&hp->refcnt)) {list_del_init(&hp->list);hp->mops->free(hp);}spin_unlock_irqrestore(&mdesc_lock, flags);}EXPORT_SYMBOL(mdesc_release);static DEFINE_MUTEX(mdesc_mutex);static struct mdesc_notifier_client  client_list;void mdesc_register_notifier(struct mdesc_notifier_client  client){bool supported = false;u64 node;int i;mutex_lock(&mdesc_mutex);  check to see if the node is supported for registration ", "mdesc_get_node_ops(node_name, &get_info_func, NULL, NULL);/* If we didn't find a get_info_func, the node name is not supported ": "mdesc_get_node_info(struct mdesc_handle  hp, u64 node,const char  node_name, union md_node_info  node_info){mdesc_node_info_get_f get_info_func;int rv;if (hp == NULL || node == MDESC_NODE_NULL ||    node_name == NULL || node_info == NULL)return -EINVAL;  Find the get_info op for the given node name ", "idp = mdesc_get_property(md, node, \"id\", NULL);name = mdesc_get_property(md, node, \"name\", NULL);parent_cfg_hdlp = parent_cfg_handle(md, node);if (!idp || !name || !parent_cfg_hdlp)return -1;node_info->vdev_port.id = *idp;node_info->vdev_port.name = kstrdup_const(name, GFP_KERNEL);if (!node_info->vdev_port.name)return -1;node_info->vdev_port.parent_cfg_hdl = *parent_cfg_hdlp;return 0;}static void rel_vdev_port_node_info(union md_node_info *node_info)": "mdesc_arc_target(hp, a);id = mdesc_get_property(hp, target,\"cfg-handle\", NULL);if (id)break;}return id;}static int get_vdev_port_node_info(struct mdesc_handle  md, u64 node,   union md_node_info  node_info){const u64  parent_cfg_hdlp;const char  name;const u64  idp;    Virtual device nodes are distinguished by:   1. \"id\" property   2. \"name\" property   3. parent node \"cfg-handle\" property ", "#include <linux/kernel.h>#include <linux/types.h>#include <linux/init.h>#include <linux/export.h>#include <linux/etherdevice.h>#include <asm/oplib.h>#include <asm/idprom.h>struct idprom *idprom;EXPORT_SYMBOL(idprom": "idprom.c: Routines to load the idprom into kernel addresses and             interpret the data contained within.     Copyright (C) 1995 David S. Miller (davem@caip.rutgers.edu) ", "__wsumcsum_partial_copy_nocheck(const void *src, void *dst, int len)": "csum_partial_copy_nocheck - Copy and checksum.   @src: source address   @dst: destination address   @len: number of bytes to be copied.   @sum: initial sum that is added into the result (32bit unfolded)     Returns an 32bit unfolded checksum of the buffer. ", "#include <linux/export.h>#include <linux/sched.h>#include <linux/timex.h>#include <linux/preempt.h>#include <linux/delay.h>#include <asm/processor.h>#include <asm/delay.h>#include <asm/timer.h>#include <asm/mwait.h>#ifdef CONFIG_SMP# include <asm/smp.h>#endifstatic void delay_loop(u64 __loops);/* * Calibration and selection of the delay mechanism happens only once * during boot. ": "__delay function must _NOT_ be inlined as its execution time  depends wildly on alignment on many x86 processors. The additional  jump magic is needed to get the timing stable on all the CPU's  we have to worry about. ", "\"movl %%ecx,%%edx\\n\"\"1:\\tmovl %6,%%edi\\n\\t\"\"movl %%esi,%%eax\\n\\t\"\"movl %%edx,%%ecx\\n\\t\"\"repe\\n\\t\"\"cmpsb\\n\\t\"\"je 2f\\n\\t\"/* also works for empty string, see above ": "strstr(const char  cs, const char  ct){intd0, d1;register char  __res;__asm__ __volatile__(\"movl %6,%%edi\\n\\t\"\"repne\\n\\t\"\"scasb\\n\\t\"\"notl %%ecx\\n\\t\"\"decl %%ecx\\n\\t\"  NOTE! This also sets Z if searchstring='' ", "if (unlikely(1 & (unsigned long)from)) ": "memcpy_fromio(void  to, const volatile void __iomem  from, size_t n){if (unlikely(!n))return;  Align any unaligned source IO ", "kmsan_check_memory(from, n);/* Align any unaligned destination IO ": "memcpy_toio(volatile void __iomem  to, const void  from, size_t n){if (unlikely(!n))return;  Make sure uninitialized memory isn't copied to devices. ", "memset((void *)a, b, c);}}EXPORT_SYMBOL(memset_io": "memset_io(volatile void __iomem  a, int b, size_t c){volatile char __iomem  mem = a;int i;for (i = 0; i < c; ++i)writeb(b, &mem[i]);}void memcpy_fromio(void  to, const volatile void __iomem  from, size_t n){if (cc_platform_has(CC_ATTR_GUEST_UNROLL_STRING_IO))unrolled_memcpy_fromio(to, from, n);elsestring_memcpy_fromio(to, from, n);}EXPORT_SYMBOL(memcpy_fromio);void memcpy_toio(volatile void __iomem  to, const void  from, size_t n){if (cc_platform_has(CC_ATTR_GUEST_UNROLL_STRING_IO))unrolled_memcpy_toio(to, from, n);elsestring_memcpy_toio(to, from, n);}EXPORT_SYMBOL(memcpy_toio);void memset_io(volatile void __iomem  a, int b, size_t c){if (cc_platform_has(CC_ATTR_GUEST_UNROLL_STRING_IO)) {unrolled_memset_io(a, b, c);} else {    TODO: memset can mangle the IO patterns quite a bit.   perhaps it would be better to use a dumb one: ", "unsigned longclear_user(void __user *to, unsigned long n)": "clear_user(addr,size)\\do {\\int __d0;\\might_fault();\\__asm__ __volatile__(\\ASM_STAC \"\\n\"\\\"0:rep; stosl\\n\"\\\"movl %2,%0\\n\"\\\"1:rep; stosb\\n\"\\\"2: \" ASM_CLAC \"\\n\"\\_ASM_EXTABLE_TYPE_REG(0b, 2b, EX_TYPE_UCOPY_LEN4, %2)\\_ASM_EXTABLE_UA(1b, 2b)\\: \"=&c\"(size), \"=&D\" (__d0)\\: \"r\"(size & 3), \"0\"(size  4), \"1\"(addr), \"a\"(0));\\} while (0)     clear_user - Zero a block of memory in user space.   @to:   Destination address, in user space.   @n:    Number of bytes to zero.     Zero a block of memory in user space.     Return: number of bytes that could not be cleared.   On success, this will be zero. ", "unsigned long__clear_user(void __user *to, unsigned long n)": "__clear_user: - Zero a block of memory in user space, with less checking.   @to:   Destination address, in user space.   @n:    Number of bytes to zero.     Zero a block of memory in user space.  Caller must check   the specified block with access_ok() before calling this function.     Returns number of bytes that could not be cleared.   On success, this will be zero. ", "if (len == 40) ": "csum_partial(const void  buff, int len, __wsum sum){u64 temp64 = (__force u64)sum;unsigned odd;odd = 1 & (unsigned long) buff;if (unlikely(odd)) {if (unlikely(len == 0))return sum;temp64 = ror32((__force u32)sum, 8);temp64 += ( (unsigned char  )buff << 8);len--;buff++;}    len == 40 is the hot case due to IPv6 headers, but annotating it likely()   has noticeable negative affect on codegen for all other cases with   minimal performance benefit here. ", "ptep = alloc_p2m_pmd(addr, pte_pg);if (!ptep)return -ENOMEM;}if (p2m_top_mfn && pfn < MAX_P2M_PFN) ": "xen_alloc_p2m_entry(unsigned long pfn){unsigned topidx;unsigned long  top_mfn_p,  mid_mfn;pte_t  ptep,  pte_pg;unsigned int level;unsigned long flags;unsigned long addr = (unsigned long)(xen_p2m_addr + pfn);unsigned long p2m_pfn;ptep = lookup_address(addr, &level);BUG_ON(!ptep || level != PG_LEVEL_4K);pte_pg = (pte_t  )((unsigned long)ptep & ~(PAGE_SIZE - 1));if (pte_pg == p2m_missing_pte || pte_pg == p2m_identity_pte) {  PMD level is missing, allocate a new one ", "BUILD_BUG_ON(SZ_4K / BLAKE2S_BLOCK_SIZE < 8);if (!static_branch_likely(&blake2s_use_ssse3) || !may_use_simd()) ": "blake2s_compress_ssse3(struct blake2s_state  state,       const u8  block, const size_t nblocks,       const u32 inc);asmlinkage void blake2s_compress_avx512(struct blake2s_state  state,const u8  block, const size_t nblocks,const u32 inc);static __ro_after_init DEFINE_STATIC_KEY_FALSE(blake2s_use_ssse3);static __ro_after_init DEFINE_STATIC_KEY_FALSE(blake2s_use_avx512);void blake2s_compress(struct blake2s_state  state, const u8  block,      size_t nblocks, const u32 inc){  SIMD disables preemption, so relax after processing each page. ", "max_level = cpuid_eax(0x80000000);if (max_level < IBS_CPUID_FEATURES)return IBS_CAPS_DEFAULT;caps = cpuid_eax(IBS_CPUID_FEATURES);if (!(caps & IBS_CAPS_AVAIL))/* cpuid flags not valid ": "get_ibs_caps(void){u32 caps;unsigned int max_level;if (!boot_cpu_has(X86_FEATURE_IBS))return 0;  check IBS cpuid feature flags ", "void __init setup_node_to_cpumask_map(void)": "cpumask_of_node() is not valid until after this is done.   (Use CONFIG_DEBUG_PER_CPU_MAPS to check this.) ", "EXPORT_SYMBOL(__default_kernel_pte_mask": "__default_kernel_pte_mask __read_mostly = DEFAULT_PTE_MASK;EXPORT_SYMBOL_GPL(__supported_pte_mask);  Used in PAGE_KERNEL_  macros which are reasonably used out-of-tree: ", "return ret;}EXPORT_SYMBOL(register_kmmio_probe": "register_kmmio_probe(struct kmmio_probe  p){unsigned long flags;int ret = 0;unsigned long size = 0;unsigned long addr = p->addr & PAGE_MASK;const unsigned long size_lim = p->len + (p->addr & ~PAGE_MASK);unsigned int l;pte_t  pte;local_irq_save(flags);arch_spin_lock(&kmmio_lock);if (get_kmmio_probe(addr)) {ret = -EEXIST;goto out;}pte = lookup_address(addr, &l);if (!pte) {ret = -EINVAL;goto out;}kmmio_count++;list_add_rcu(&p->list, &kmmio_probes);while (size < size_lim) {if (add_kmmio_fault_page(addr + size))pr_err(\"Unable to set page fault.\\n\");size += page_level_size(l);}out:arch_spin_unlock(&kmmio_lock);local_irq_restore(flags);    XXX: What should I do here?   Here was a call to global_flush_tlb(), but it does not exist   anymore. It seems it's not needed after all. ", "call_rcu(&drelease->rcu, remove_kmmio_fault_pages);}EXPORT_SYMBOL(unregister_kmmio_probe": "unregister_kmmio_probe(struct kmmio_probe  p){unsigned long flags;unsigned long size = 0;unsigned long addr = p->addr & PAGE_MASK;const unsigned long size_lim = p->len + (p->addr & ~PAGE_MASK);struct kmmio_fault_page  release_list = NULL;struct kmmio_delayed_release  drelease;unsigned int l;pte_t  pte;pte = lookup_address(addr, &l);if (!pte)return;local_irq_save(flags);arch_spin_lock(&kmmio_lock);while (size < size_lim) {release_kmmio_fault_page(addr + size, &release_list);size += page_level_size(l);}list_del_rcu(&p->list);kmmio_count--;arch_spin_unlock(&kmmio_lock);local_irq_restore(flags);if (!release_list)return;drelease = kmalloc(sizeof( drelease), GFP_ATOMIC);if (!drelease) {pr_crit(\"leaking kmmio_fault_page objects.\\n\");return;}drelease->release_list = release_list;    This is not really RCU here. We have just disarmed a set of   pages so that they cannot trigger page faults anymore. However,   we cannot remove the pages from kmmio_page_table,   because a probe hit might be in flight on another CPU. The   pages are collected into a list, and they will be removed from   kmmio_page_table when it is certain that no probe hit related to   these pages can be in flight. RCU grace period sounds like a   good choice.     If we removed the pages too early, kmmio page fault handler might   not find the respective kmmio_fault_page and determine it's not   a kmmio fault, when it actually is. This would lead to madness. ", "if (vm_flags & VM_PKEY_BIT0)val |= _PAGE_PKEY_BIT0;if (vm_flags & VM_PKEY_BIT1)val |= _PAGE_PKEY_BIT1;if (vm_flags & VM_PKEY_BIT2)val |= _PAGE_PKEY_BIT2;if (vm_flags & VM_PKEY_BIT3)val |= _PAGE_PKEY_BIT3;#endifval = __sme_set(val);if (val & _PAGE_PRESENT)val &= __supported_pte_mask;return __pgprot(val);}EXPORT_SYMBOL(vm_get_page_prot": "vm_get_page_prot(unsigned long vm_flags){unsigned long val = pgprot_val(protection_map[vm_flags &      (VM_READ|VM_WRITE|VM_EXEC|VM_SHARED)]);#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS    Take the 4 protection key bits out of the vma->vm_flags value and   turn them in to the bits that we can put in to a pte.     Only override these if Protection Keys are available (which is only   on 64-bit). ", "if (unlikely(x > y)) ": "__virt_addr_valid(unsigned long x){unsigned long y = x - __START_KERNEL_map;  use the carry flag to determine if x was < __START_KERNEL_map ", "VIRTUAL_BUG_ON(y >= KERNEL_IMAGE_SIZE);return y + phys_base;}EXPORT_SYMBOL(__phys_addr_symbol": "__phys_addr_symbol(unsigned long x){unsigned long y = x - __START_KERNEL_map;  only check upper bounds since lower bounds will trigger carry ", "struct ioremap_desc ": "ioremap() behavior. ", "enum page_cache_mode pcm = _PAGE_CACHE_MODE_UC_MINUS;return __ioremap_caller(phys_addr, size, pcm,__builtin_return_address(0), false);}EXPORT_SYMBOL(ioremap);/** * ioremap_uc     -   map bus memory into CPU space as strongly uncachable * @phys_addr:    bus address of the memory * @size:      size of the resource to map * * ioremap_uc performs a platform specific sequence of operations to * make bus memory CPU accessible via the readb/readw/readl/writeb/ * writew/writel functions and the other mmio helpers. The returned * address is not guaranteed to be usable directly as a virtual * address. * * This version of ioremap ensures that the memory is marked with a strong * preference as completely uncachable on the CPU when possible. For non-PAT * systems this ends up setting page-attribute flags PCD=1, PWT=1. For PAT * systems this will set the PAT entry for the pages as strong UC.  This call * will honor existing caching rules from things like the PCI bus. Note that * there are other caches and buffers on many busses. In particular driver * authors should read up on PCI writes. * * It's useful if some control registers are in such an area and * write combining or read caching is not desirable: * * Must be freed with iounmap. ": "ioremap_wc(), we will use   UC MINUS. Drivers that are certain they need or can already   be converted over to strong UC can use ioremap_uc(). ", "void __iomem *ioremap_wt(resource_size_t phys_addr, unsigned long size)": "ioremap_wt-map memory into CPU space write through   @phys_addr:bus address of the memory   @size:size of the resource to map     This version of ioremap ensures that the memory is marked write through.   Write through stores data into memory while keeping the cache up-to-date.     Must be freed with iounmap. ", "void __iomem *ioremap(resource_size_t phys_addr, unsigned long size)": "iounmap. ", "static inline unsigned long fix_addr(unsigned long addr)": "set_memory_uc() is a fine way to encourage a   speculative access. So we cheat and flip the top bit of the address. This   works fine for the code that updates the page tables. But at the end of the   process we need to flush the TLB and cache and the non-canonical address   causes a #GP fault when used by the INVLPG and CLFLUSH instructions.     But in the common case we already have a canonical address. This code   will fix the top bit if needed and is a no-op otherwise. ", "return change_page_attr_clear(&addr, numpages,      __pgprot(_PAGE_CACHE_MASK), 0);}int set_memory_wb(unsigned long addr, int numpages)": "set_memory_wb(unsigned long addr, int numpages){  WB cache mode is hard wired to all cache attribute bits being 0 ", "retval = cpa_clear_pages_array(pages, numpages,__pgprot(_PAGE_CACHE_MASK));if (retval)return retval;for (i = 0; i < numpages; i++) ": "set_pages_array_wb(struct page   pages, int numpages){int retval;unsigned long start;unsigned long end;int i;  WB cache mode is hard wired to all cache attribute bits being 0 ", "static void __save_processor_state(struct saved_context *ctxt)": "save_processor_state() - Save CPU registers before creating a                               hibernation image and before restoring                               the memory state from it   @ctxt: Structure to store the registers contents in.     NOTE: If there is a CPU register the modification of which by the   boot kernel (ie. the kernel used for loading the hibernation image)   might affect the operations of the restored target kernel (ie. the one   saved in the hibernation image), then its contents must be saved by this   function.  In other words, if kernel A is hibernated and different   kernel B is used for loading the hibernation image into memory, the   kernel A's __save_processor_state() function must save all registers   needed by kernel A, so that it can operate correctly after the resume   regardless of what kernel B does in the meantime. ", "static void notrace __restore_processor_state(struct saved_context *ctxt)": "restore_processor_state() - Restore the contents of CPU registers saved                                 by __save_processor_state()   @ctxt: Structure to load the registers contents from.     The asm code that gets us here will have restored a usable GDT, although   it will be pointing to the wrong alias. ", "for (bank = 0; bank < 2; ++bank)scx200_gpio_shadow[bank] = inl(scx200_gpio_base + 0x10 * bank);}static int scx200_probe(struct pci_dev *pdev, const struct pci_device_id *ent)": "scx200_cb_base = 0;static struct pci_device_id scx200_tbl[] = {{ PCI_VDEVICE(NS, PCI_DEVICE_ID_NS_SCx200_BRIDGE) },{ PCI_VDEVICE(NS, PCI_DEVICE_ID_NS_SC1100_BRIDGE) },{ PCI_VDEVICE(NS, PCI_DEVICE_ID_NS_SCx200_XBUS)   },{ PCI_VDEVICE(NS, PCI_DEVICE_ID_NS_SC1100_XBUS)   },{ },};MODULE_DEVICE_TABLE(pci,scx200_tbl);static int scx200_probe(struct pci_dev  , const struct pci_device_id  );static struct pci_driver scx200_pci_driver = {.name = \"scx200\",.id_table = scx200_tbl,.probe = scx200_probe,};static DEFINE_MUTEX(scx200_gpio_config_lock);static void scx200_init_shadow(void){int bank;  read the current values driven on the GPIO signals ", "if (port == BT_MBI_UNIT_GFX) ": "iosf_mbi_modify(u8 port, u8 opcode, u32 offset, u32 mdr, u32 mask){u32 mcr, mcrx;u32 value;unsigned long flags;int ret;  Access to the GFX unit is handled by GPU code ", "return mbi_pdev;}EXPORT_SYMBOL(iosf_mbi_available": "iosf_mbi_available(void){  Mbi isn't hot-pluggable. No remove routine is provided ", "mutex_lock(&iosf_mbi_pmic_access_mutex);while (iosf_mbi_pmic_i2c_access_count != 0) ": "iosf_mbi_punit_acquire(void){  Wait for any I2C PMIC accesses from in kernel drivers to finish. ", "cpu_latency_qos_update_request(&iosf_mbi_pm_qos, 0);/* host driver writes to side band semaphore register ": "iosf_mbi_block_punit_i2c_access(void){unsigned long start, end;int ret = 0;u32 sem;if (WARN_ON(!mbi_pdev || !iosf_mbi_sem_address))return -ENXIO;mutex_lock(&iosf_mbi_pmic_access_mutex);while (iosf_mbi_pmic_punit_access_count != 0) {mutex_unlock(&iosf_mbi_pmic_access_mutex);wait_event(iosf_mbi_pmic_access_waitq,   iosf_mbi_pmic_punit_access_count == 0);mutex_lock(&iosf_mbi_pmic_access_mutex);}if (iosf_mbi_pmic_i2c_access_count > 0)goto success;blocking_notifier_call_chain(&iosf_mbi_pmic_bus_access_notifier,     MBI_PMIC_BUS_ACCESS_BEGIN, NULL);    Disallow the CPU to enter C6 or C7 state, entering these states   requires the P-Unit to talk to the PMIC and if this happens while   we're holding the semaphore, the SoC hangs. ", "iosf_mbi_punit_acquire();ret = blocking_notifier_chain_register(&iosf_mbi_pmic_bus_access_notifier, nb);iosf_mbi_punit_release();return ret;}EXPORT_SYMBOL(iosf_mbi_register_pmic_bus_access_notifier": "iosf_mbi_register_pmic_bus_access_notifier(struct notifier_block  nb){int ret;  Wait for the bus to go inactive before registering ", "iosf_mbi_punit_acquire();ret = iosf_mbi_unregister_pmic_bus_access_notifier_unlocked(nb);iosf_mbi_punit_release();return ret;}EXPORT_SYMBOL(iosf_mbi_unregister_pmic_bus_access_notifier);void iosf_mbi_assert_punit_acquired(void)": "iosf_mbi_assert_punit_acquired();return blocking_notifier_chain_unregister(&iosf_mbi_pmic_bus_access_notifier, nb);}EXPORT_SYMBOL(iosf_mbi_unregister_pmic_bus_access_notifier_unlocked);int iosf_mbi_unregister_pmic_bus_access_notifier(struct notifier_block  nb){int ret;  Wait for the bus to go inactive before unregistering ", ": \"a\" (loops));}EXPORT_SYMBOL(__delay": "__delay(unsigned long loops){asm volatile(\"test %0,%0\\n\"\"jz 3f\\n\"\"jmp 1f\\n\"\".align 16\\n\"\"1: jmp 2f\\n\"\".align 16\\n\"\"2: dec %0\\n\"\" jnz 2b\\n\"\"3: dec %0\\n\":   we don't need output ", "if (start < BIOS_END)start = BIOS_END;}return start;}EXPORT_SYMBOL(pcibios_align_resource": "pcibios_align_resource(void  data, const struct resource  res,resource_size_t size, resource_size_t align){struct pci_dev  dev = data;resource_size_t start = res->start;if (res->flags & IORESOURCE_IO) {if (skip_isa_ioresource_align(dev))return start;if (start & 0x300)start = (start + 0x3ff) & ~0x3ff;} else if (res->flags & IORESOURCE_MEM) {  The low 1MB range is reserved for ISA cards ", "ret = paravirt_patch_call(insn_buff, paravirt_BUG, addr, len);else if (opfunc == _paravirt_nop)ret = 0;else/* Otherwise call the function. ": "pv_ops + type);unsigned ret;if (opfunc == NULL)  If there's no function, patch it with paravirt_BUG() ", "/* * outb_pic - this has to work on a wide range of PC hardware. ": "legacy_pic = &null_legacy_pic;}raw_spin_unlock_irqrestore(&i8259A_lock, flags);return nr_legacy_irqs();}static void init_8259A(int auto_eoi){unsigned long flags;i8259A_auto_eoi = auto_eoi;raw_spin_lock_irqsave(&i8259A_lock, flags);outb(0xff, PIC_MASTER_IMR);  mask all of 8259A-1 ", "WARN_ON_ONCE(type == NMI_SERR && !list_empty(&desc->head));WARN_ON_ONCE(type == NMI_IO_CHECK && !list_empty(&desc->head));/* * some handlers need to be executed first otherwise a fake * event confuses some handlers (kdump uses this flag) ": "__register_nmi_handler(unsigned int type, struct nmiaction  action){struct nmi_desc  desc = nmi_to_desc(type);unsigned long flags;if (WARN_ON_ONCE(!action->handler || !list_empty(&action->list)))return -EINVAL;raw_spin_lock_irqsave(&desc->lock, flags);    Indicate if there are multiple registrations on the   internal NMI handler call chains (SERR and IO_CHECK). ", "if (sp[0] >> 22)return sp[0];if (sp[1] >> 22)return sp[1];#endif}return pc;}EXPORT_SYMBOL(profile_pc": "profile_pc(struct pt_regs  regs){unsigned long pc = instruction_pointer(regs);if (!user_mode(regs) && in_lock_functions(pc)) {#ifdef CONFIG_FRAME_POINTERreturn  (unsigned long  )(regs->bp + sizeof(long));#elseunsigned long  sp = (unsigned long  )regs->sp;    Return address is either directly at stack pointer   or above a saved flags. Eflags has bits 22-31 zero,   kernel addresses don't. ", "EXPORT_SYMBOL(cpu_khz": "cpu_khz;  TSC clocks  usec, not used here ", "struct system_counterval_t convert_art_ns_to_tsc(u64 art_ns)": "convert_art_ns_to_tsc() - Convert ART in nanoseconds to TSC.   @art_ns: ART (Always Running Timer) in unit of nanoseconds     PTM requires all timestamps to be in units of nanoseconds. When user   software requests a cross-timestamp, this function converts system timestamp   to TSC.     This is valid when CPU feature flag X86_FEATURE_TSC_KNOWN_FREQ is set   indicating the tsc_khz is derived from CPUID[15H]. Drivers should check   that this flag is set before conversion to TSC is attempted.     Return:   struct system_counterval_t - system counter value with the pointer to the  corresponding clocksource  @cycles:System counter value  @cs:Clocksource corresponding to system counter value. Used  by timekeeping code to verify comparability of two cycle  values. ", "int topology_phys_to_logical_pkg(unsigned int phys_pkg)": "topology_phys_to_logical_pkg - Map a physical package id to a logical   @phys_pkg:The physical package id to map     Returns logical package id or -1 if not found ", "spin_lock(&rtc_lock);CMOS_WRITE(0x00, 0x8f);spin_unlock(&rtc_lock);/* * Switch to the trampoline page table. ": "machine_real_restart(unsigned int type){local_irq_disable();    Write zero to CMOS register number 0x0f, which the BIOS POST   routine will recognize as telling it to do a proper reboot.  (Well   that's what this book in front of me says -- it may only apply to   the Phoenix BIOS though, it's not clear).  At the same time,   disable NMIs by setting the top bit in the CMOS address register,   as we're about to do peculiar things to the CPU.  I'm not sure if   `outb_p' is needed instead of just `outb'.  Use it to be on the   safe side.  (Yes, CMOS_WRITE does outb_p's. -  Paul G.) ", "void native_io_delay(void)": "native_io_delay to be a constant. ", "irq = pin_2_irq(i, ioapic_idx, mp_irqs[i].dstirq, 0);if (irq > 0 && !IO_APIC_IRQ(irq))continue;if (pin == (mp_irqs[i].srcbusirq & 3)) ": "IO_APIC_get_PCI_irq_vector(int bus, int slot, int pin){int irq, i, best_ioapic = -1, best_idx = -1;apic_printk(APIC_DEBUG,    \"querying PCI -> IRQ mapping bus:%d, slot:%d, pin:%d.\\n\",    bus, slot, pin);if (test_bit(bus, mp_bus_not_pci)) {apic_printk(APIC_VERBOSE,    \"PCI BIOS passed nonexistent PCI bus %d!\\n\", bus);return -1;}for (i = 0; i < mp_irq_entries; i++) {int lbus = mp_irqs[i].srcbus;int ioapic_idx, found = 0;if (bus != lbus || mp_irqs[i].irqtype != mp_INT ||    slot != ((mp_irqs[i].srcbusirq >> 2) & 0x1f))continue;for_each_ioapic(ioapic_idx)if (mpc_ioapic_id(ioapic_idx) == mp_irqs[i].dstapic ||    mp_irqs[i].dstapic == MP_APIC_ALL) {found = 1;break;}if (!found)continue;  Skip ISA IRQs ", "return ~0ul;}EXPORT_SYMBOL(uv_undefined": "uv_undefined_panic = 1;unsigned long uv_undefined(char  str){if (likely(!disable_uv_undefined_panic))panic(\"UV: error: undefined MMR: %s\\n\", str);elsepr_crit(\"UV: error: undefined MMR: %s\\n\", str);  Cause a machine fault: ", "WARN_ONCE(bits_missing, \"CR0 WP bit went missing!?\\n\");}}EXPORT_SYMBOL(native_write_cr0": "native_write_cr0(unsigned long val){unsigned long bits_missing = 0;set_register:asm volatile(\"mov %0,%%cr0\": \"+r\" (val) : : \"memory\");if (static_branch_likely(&cr_pinning)) {if (unlikely((val & X86_CR0_WP) != X86_CR0_WP)) {bits_missing = X86_CR0_WP;val |= bits_missing;goto set_register;}  Warn after we've set the missing bits. ", "if (counter > NMI_MAX_COUNTER_BITS)return 1;if (!test_and_set_bit(counter, perfctr_nmi_owner))return 1;return 0;}EXPORT_SYMBOL(reserve_perfctr_nmi": "reserve_perfctr_nmi(unsigned int msr){unsigned int counter;counter = nmi_perfctr_msr_to_bit(msr);  register not managed by the allocator? ", "if (counter > NMI_MAX_COUNTER_BITS)return;clear_bit(counter, perfctr_nmi_owner);}EXPORT_SYMBOL(release_perfctr_nmi": "release_perfctr_nmi(unsigned int msr){unsigned int counter;counter = nmi_perfctr_msr_to_bit(msr);  register not managed by the allocator? ", "if (counter > NMI_MAX_COUNTER_BITS)return 1;if (!test_and_set_bit(counter, evntsel_nmi_owner))return 1;return 0;}EXPORT_SYMBOL(reserve_evntsel_nmi": "reserve_evntsel_nmi(unsigned int msr){unsigned int counter;counter = nmi_evntsel_msr_to_bit(msr);  register not managed by the allocator? ", "if (counter > NMI_MAX_COUNTER_BITS)return;clear_bit(counter, evntsel_nmi_owner);}EXPORT_SYMBOL(release_evntsel_nmi": "release_evntsel_nmi(unsigned int msr){unsigned int counter;counter = nmi_evntsel_msr_to_bit(msr);  register not managed by the allocator? ", "const struct x86_cpu_id *x86_match_cpu(const struct x86_cpu_id *match)": "x86_match_cpu - match current CPU again an array of x86_cpu_ids   @match: Pointer to array of x86_cpu_ids. Last entry terminated with           {}.     Return the entry if the current CPU matches the entries in the   passed x86_cpu_id match table. Otherwise NULL.  The match table   contains vendor (X86_VENDOR_ ), family, model and feature bits or   respective wildcard entries.     A typical table entry would be to match a specific CPU     X86_MATCH_VENDOR_FAM_MODEL_FEATURE(INTEL, 6, INTEL_FAM6_BROADWELL,        X86_FEATURE_ANY, NULL);     Fields can be wildcarded with %X86_VENDOR_ANY, %X86_FAMILY_ANY,   %X86_MODEL_ANY, %X86_FEATURE_ANY (except for vendor)     asmcpu_device_id.h contains a set of useful macros which are shortcuts   for various common selections. The above can be shortened to:     X86_MATCH_INTEL_FAM6_MODEL(BROADWELL, NULL);     Arrays used to match for this should also be declared using   MODULE_DEVICE_TABLE(x86cpu, ...)     This always matches against the boot cpu, assuming models and features are   consistent over all CPUs. ", "#define MTRR_TO_PHYS_WC_OFFSET 1000u32 num_var_ranges;unsigned int mtrr_usage_table[MTRR_MAX_VAR_RANGES];DEFINE_MUTEX(mtrr_mutex);const struct mtrr_ops *mtrr_if;/*  Returns non-zero if we have the write-combining memory type  ": "arch_phys_wc_add returns an MTRR register index plus this offset. ", "void arch_phys_wc_del(int handle)": "arch_phys_wc_del - undoes arch_phys_wc_add   @handle: Return value from arch_phys_wc_add     This cleans up after mtrr_add_wc_if_needed.     The API guarantees that mtrr_del_wc_if_needed(error code) and   mtrr_del_wc_if_needed(0) do nothing. ", "if (this_cpu_read(in_kernel_fpu))return false;/* * When not in NMI or hard interrupt context, FPU can be used in: * * - Task context except from within fpregs_lock()'ed critical *   regions. * * - Soft interrupt processing context which cannot happen *   while in a fpregs_lock()'ed critical region. ": "irq_fpu_usable(void){if (WARN_ON_ONCE(in_nmi()))return false;  In kernel FPU usage already active? ", "flags->bm_check = 1;}/* * On all recent Intel platforms, ARB_DISABLE is a nop. * So, set bm_control to zero to indicate that ARB_DISABLE * is not required while entering C3 type state on * P4, Core and beyond CPUs ": "acpi_processor_power_init_bm_check(struct acpi_processor_flags  flags,unsigned int cpu){struct cpuinfo_x86  c = &cpu_data(cpu);flags->bm_check = 0;if (num_online_cpus() == 1)flags->bm_check = 1;else if (c->x86_vendor == X86_VENDOR_INTEL) {    Today all MP CPUs that support C3 share cache.   And caches should not be flushed by software while   entering C3 type state. ", "#include <linux/export.h>#include <linux/types.h>#include <linux/stddef.h>#include <linux/compiler.h>#include <linux/string.h>#ifdef CONFIG_OPT_LIB_FUNCTIONvoid *memcpy(void *v_dst, const void *v_src, __kernel_size_t c)": "memcpy on Microblaze   This is generic C code to do efficient, alignment-aware memcpy.     It is based on demo code originally Copyright 2001 by Intel Corp, taken from   http:www.embedded.comshowArticle.jhtml?articleID=19205567     Attempts were made, unsuccessfully, to contact the original   author of this code (Michael Morrow, Intel).  Below is the original   copyright notice.     This software has been developed by Intel Corporation.   Intel specifically disclaims all warranties, express or   implied, and all liability, including consequential and   other indirect damages, for the use of this program, including   liability for infringement of any proprietary rights,   and including the warranties of merchantability and fitness   for a particular purpose. Intel does not assume any   responsibility for and errors which may appear in this program   not any responsibility to update it. ", "#include <linux/export.h>#include <linux/types.h>#include <linux/stddef.h>#include <linux/compiler.h>#include <linux/string.h>#ifdef CONFIG_OPT_LIB_FUNCTIONvoid *memmove(void *v_dst, const void *v_src, __kernel_size_t c)": "memmove.     It is based on demo code originally Copyright 2001 by Intel Corp, taken from   http:www.embedded.comshowArticle.jhtml?articleID=19205567     Attempts were made, unsuccessfully, to contact the original   author of this code (Michael Morrow, Intel).  Below is the original   copyright notice.     This software has been developed by Intel Corporation.   Intel specifically disclaims all warranties, express or   implied, and all liability, including consequential and   other indirect damages, for the use of this program, including   liability for infringement of any proprietary rights,   and including the warranties of merchantability and fitness   for a particular purpose. Intel does not assume any   responsibility for and errors which may appear in this program   not any responsibility to update it. ", "#include <linux/export.h>#include <linux/types.h>#include <linux/stddef.h>#include <linux/compiler.h>#include <linux/string.h>#ifdef CONFIG_OPT_LIB_FUNCTIONvoid *memset(void *v_src, int c, __kernel_size_t n)": "memset on Microblaze   This is generic C code to do efficient, alignment-aware memcpy.     It is based on demo code originally Copyright 2001 by Intel Corp, taken from   http:www.embedded.comshowArticle.jhtml?articleID=19205567     Attempts were made, unsuccessfully, to contact the original   author of this code (Michael Morrow, Intel).  Below is the original   copyright notice.     This software has been developed by Intel Corporation.   Intel specifically disclaims all warranties, express or   implied, and all liability, including consequential and   other indirect damages, for the use of this program, including   liability for infringement of any proprietary rights,   and including the warranties of merchantability and fitness   for a particular purpose. Intel does not assume any   responsibility for and errors which may appear in this program   not any responsibility to update it. ", "if (mapping && !mapping_mapped(mapping)) ": "flush_dcache_page(struct page  page){struct address_space  mapping = page_mapping_file(page);    If we have a mapping but the page is not mapped to user-space   yet, we simply mark this page dirty and defer flushing the    caches until update_mmu(). ", "unsigned long phys = page_to_phys(pfn_to_page(pfn));unsigned long virt = TLBTEMP_BASE_1 + (address & DCACHE_ALIAS_MASK);preempt_disable();__flush_invalidate_dcache_page_alias(virt, phys);__invalidate_icache_page_alias(virt, phys);preempt_enable();}EXPORT_SYMBOL(local_flush_cache_page": "local_flush_cache_page(struct vm_area_struct  vma, unsigned long address,      unsigned long pfn){  Note that we have to use the 'alias' address to avoid multi-hit ", "EXPORT_SYMBOL(ccount_freq": "ccount_freq;  ccount Hz ", "/* Step 1.  ": "xtensa_backtrace_user(struct pt_regs  regs, unsigned int depth,   int ( ufn)(struct stackframe  frame, void  data),   void  data){unsigned long windowstart = regs->windowstart;unsigned long windowbase = regs->windowbase;unsigned long a0 = regs->areg[0];unsigned long a1 = regs->areg[1];unsigned long pc = regs->pc;struct stackframe frame;int index;if (!depth--)return;frame.pc = pc;frame.sp = a1;if (pc == 0 || pc >= TASK_SIZE || ufn(&frame, data))return;if (IS_ENABLED(CONFIG_USER_ABI_CALL0_ONLY) ||    (IS_ENABLED(CONFIG_USER_ABI_CALL0_PROBE) &&     !(regs->ps & PS_WOE_MASK)))return;  Two steps:     1. Look through the register window for the   previous PCs in the call trace.     2. Look on the stack. ", "spill_registers();/* Read the stack frames one by one and create the PC * from the a0 and a1 registers saved there. ": "xtensa_backtrace_kernel(struct pt_regs  regs, unsigned int depth,     int ( kfn)(struct stackframe  frame, void  data),     int ( ufn)(struct stackframe  frame, void  data),     void  data){unsigned long pc = regs->depc > VALID_DOUBLE_EXCEPTION_ADDRESS ?regs->depc : regs->pc;unsigned long sp_start, sp_end;unsigned long a0 = regs->areg[0];unsigned long a1 = regs->areg[1];sp_start = a1 & ~(THREAD_SIZE - 1);sp_end = sp_start + THREAD_SIZE;  Spill the register window to the stack first. ", "unsigned long return_address(unsigned level)": "return_address_cb(struct stackframe  frame, void  data){struct return_addr_data  r = data;if (r->skip) {--r->skip;return 0;}if (!kernel_text_address(frame->pc))return 0;r->addr = frame->pc;return 1;}    level == 0 is for the return address from the caller of this function,   not from this function itself. ", "for (; d.as_uptr & WORD_MASK; count--)*d.as_u8++ = *s.as_u8++;distance = s.as_uptr & WORD_MASK;if (distance) ": "memcpy(void  dest, const void  src, size_t count){union const_types s = { .as_u8 = src };union types d = { .as_u8 = dest };int distance = 0;if (count < MIN_THRESHOLD)goto copy_remainder;  Copy a byte at time until destination is aligned. ", "cu |= cu << 8;cu |= cu << 16;/* Suppress warning on 32 bit machines ": "memset(void  s, int c, size_t count){union types dest = { .as_u8 = s };if (count >= MIN_THRESHOLD) {unsigned long cu = (unsigned long)c;  Compose an ulong with 'c' repeated 48 times ", "\"       bt      3f              \\n\"\"       ldw     %3, (%2, 0)     \\n\"\"       ldw     %4, (%2, 4)     \\n\"\"       ldw     %5, (%2, 8)     \\n\"\"       ldw     %6, (%2, 12)    \\n\"\"2:     stw     %3, (%1, 0)     \\n\"\"9:     stw     %4, (%1, 4)     \\n\"\"10:    stw     %5, (%1, 8)     \\n\"\"11:    stw     %6, (%1, 12)    \\n\"\"       addi    %2, 16          \\n\"\"       addi    %1, 16          \\n\"\"       subi    %0, 16          \\n\"\"       br      1b              \\n\"\"3:     cmplti  %0, 4           \\n\" /* 1W ": "raw_copy_to_user(void  to, const void  from,unsigned long n){int w0, w1, w2, w3;__asm__ __volatile__(\"0:     cmpnei  %1, 0           \\n\"\"       bf      8f              \\n\"\"       mov     %3, %1          \\n\"\"       or      %3, %2          \\n\"\"       andi    %3, 3           \\n\"\"       cmpnei  %3, 0           \\n\"\"       bf      1f              \\n\"\"       br      5f              \\n\"\"1:     cmplti  %0, 16          \\n\"   4W ", "static inline void make_jbsr(unsigned long callee, unsigned long pc,     uint16_t *call, bool nolr)": "_mcount  nop32  nop32     If the (callee - current_pc) is less then 64MB, we'll use bsr:  pushlr  bsr_mcount  nop32  nop32   else we'll use (movih + ori + jsr):  pushlr  movihr26, ...  orir26, ...  jsrr26     (r26 is our reserved link-reg)   ", "result += (__force u32)sum;/* 32+c bits -> 32 bits ": "csum_partial(const void  buff, int len, __wsum sum){unsigned long result = do_csum(buff, len);  add in old sum, and carry.. ", "EXPORT_SYMBOL(zero_page_memmap_ptr": "zero_page_memmap_ptr;  map entry for zero page ", "}void flush_tlb_range(struct vm_area_struct *vma,unsigned long start, unsigned long end)": "flush_tlb_range (struct vm_area_struct  vma, unsigned long start, unsigned long end){struct mm_struct  mm = vma->vm_mm;unsigned long size = end - start;unsigned long nbits;#ifndef CONFIG_SMPif (mm != current->active_mm) {mm->context = 0;return;}#endifnbits = ia64_fls(size + 0xfff);while (unlikely (((1UL << nbits) & purge.mask) == 0) &&(nbits < purge.max_bits))++nbits;if (nbits > purge.max_bits)nbits = purge.max_bits;start &= ~((1UL << nbits) - 1);preempt_disable();#ifdef CONFIG_SMPif (mm != current->active_mm || cpumask_weight(mm_cpumask(mm)) != 1) {ia64_global_tlb_purge(mm, start, end, nbits);preempt_enable();return;}#endifdo {ia64_ptcl(start, (nbits<<2));start += (1UL << nbits);} while (start < end);preempt_enable();ia64_srlz_i();  srlz.i implies srlz.d ", "attr = kern_mem_attribute(phys_addr, size);if (attr & EFI_MEMORY_WB)return (void __iomem *) phys_to_virt(phys_addr);else if (attr & EFI_MEMORY_UC)return __ioremap_uc(phys_addr);/* * Some chipsets don't support UC access to memory.  If * WB is supported for the whole granule, we prefer that. ": "ioremap_uc(unsigned long phys_addr){return (void __iomem  ) (__IA64_UNCACHED_OFFSET | phys_addr);}void __iomem  early_ioremap (unsigned long phys_addr, unsigned long size){u64 attr;attr = kern_mem_attribute(phys_addr, size);if (attr & EFI_MEMORY_WB)return (void __iomem  ) phys_to_virt(phys_addr);return __ioremap_uc(phys_addr);}void __iomem  ioremap (unsigned long phys_addr, unsigned long size){void __iomem  addr;struct vm_struct  area;unsigned long offset;pgprot_t prot;u64 attr;unsigned long gran_base, gran_size;unsigned long page_base;    For things in kern_memmap, we must use the same attribute   as the rest of the kernel.  For more details, see   Documentationarchia64aliasing.rst. ", "0x2f, 0x20, 0x2e, 0x2d, 0x2c, 0x2b, 0x2a, 0x29,0x28, 0x27, 0x26, 0x25, 0x24, 0x23, 0x22, 0x21};EXPORT_SYMBOL(isa_irq_to_vector_map": "isa_irq_to_vector_map[16] = {  8259 IRQ translation, first 16 entries ", "node_cpuid[cpu].phys_id = physid;node_cpuid[cpu].nid = acpi_get_node(handle);#endifreturn 0;}int additional_cpus __initdata = -1;static __init int setup_additional_cpus(char *s)": "acpi_map_cpu2node(acpi_handle handle, int cpu, int physid){#ifdef CONFIG_ACPI_NUMA    We don't have cpu-only-node hotadd. But if the system equips   SRAT table, pxm is already found and node is ready.     So, just pxm_to_nid(pxm) is OK.   This code here is for the system which doesn't have full SRAT     table for possible cpus. ", "#endifreturn (0);}EXPORT_SYMBOL(acpi_unmap_cpu": "acpi_unmap_cpu(int cpu){ia64_cpu_to_sapicid[cpu] = -1;set_cpu_present(cpu, false);#ifdef CONFIG_ACPI_NUMA  NUMA specific cleanup's ", "return 0;}EXPORT_SYMBOL(acpi_register_ioapic": "acpi_register_ioapic(acpi_handle handle, u64 phys_addr, u32 gsi_base){int err;if ((err = iosapic_init(phys_addr, gsi_base)))return err;#ifdef CONFIG_ACPI_NUMAacpi_map_iosapic(handle, 0, NULL, NULL);#endif  CONFIG_ACPI_NUMA ", "EXPORT_SYMBOL(ia64_iobase": "ia64_iobase;  virtual address for IO accesses ", "static voidia64_itc_udelay (unsigned long usecs)": "udelay assumes that if preemption is allowed and the thread   migrates to another CPU, that the ITC values are synchronized across   all CPUs. ", "if (can_cpei_retarget() || !is_cpu_cpei_target(num))sysfs_cpus[num].cpu.hotpluggable = 1;map_cpu_to_node(num, node_cpuid[num].nid);return register_cpu(&sysfs_cpus[num].cpu, num);}EXPORT_SYMBOL(arch_register_cpu": "arch_register_cpu(int num){    If CPEI can be re-targeted or if this is not   CPEI target, then it is hotpluggable ", "#include <linux/module.h>#include <linux/acpi.h>#include <linux/memblock.h>#include <linux/cpu.h>#include <linux/delay.h>#include <linux/init.h>#include <linux/interrupt.h>#include <linux/irq.h>#include <linux/kernel.h>#include <linux/kernel_stat.h>#include <linux/mm.h>#include <linux/notifier.h>#include <linux/smp.h>#include <linux/spinlock.h>#include <linux/efi.h>#include <linux/percpu.h>#include <linux/bitops.h>#include <linux/atomic.h>#include <asm/cache.h>#include <asm/current.h>#include <asm/delay.h>#include <asm/efi.h>#include <asm/io.h>#include <asm/irq.h>#include <asm/mca.h>#include <asm/page.h>#include <asm/processor.h>#include <asm/ptrace.h>#include <asm/sal.h>#include <asm/tlbflush.h>#include <asm/unistd.h>#define SMP_DEBUG 0#if SMP_DEBUG#define Dprintk(x...)  printk(x)#else#define Dprintk(x...)#endif#ifdef CONFIG_HOTPLUG_CPU#ifdef CONFIG_PERMIT_BSP_REMOVE#define bsp_remove_ok1#else#define bsp_remove_ok0#endif/* * Global array allocated for NR_CPUS at boot time ": "cpu_core_map ", "static DEFINE_SPINLOCK(mlogbuf_rlock);/* normal context only ": "ia64_mca_printk(fmt)#define MLOGBUF_SIZE (512+256 NR_CPUS)#define MLOGBUF_MSGMAX 256static char mlogbuf[MLOGBUF_SIZE];static DEFINE_SPINLOCK(mlogbuf_wlock);  mca context only ", "while (mlogbuf_start != mlogbuf_end) ": "ia64_mlogbuf_dump(void){char temp_buf[MLOGBUF_MSGMAX];char  p;unsigned long index;unsigned long flags;unsigned int printed_len;  Get output from mlogbuf ", "*nat = 0;return 0;}UNW_DPRINT(0, \"unwind.%s: trying to access non-existent r%u\\n\",   __func__, regnum);return -1;}if (regnum < 32) ": "unw_access_gr (struct unw_frame_info  info, int regnum, unsigned long  val, char  nat, int write){unsigned long  addr,  nat_addr, nat_mask = 0, dummy_nat;struct unw_ireg  ireg;struct pt_regs  pt;if ((unsigned) regnum - 1 >= 127) {if (regnum == 0 && !write) { val = 0;  read r0 always returns 0 ", "      case 0: pt = get_scratch_regs(info); addr = &pt->b0; break;      case 6: pt = get_scratch_regs(info); addr = &pt->b6; break;      case 7: pt = get_scratch_regs(info); addr = &pt->b7; break;/* preserved: ": "unw_access_br (struct unw_frame_info  info, int regnum, unsigned long  val, int write){unsigned long  addr;struct pt_regs  pt;switch (regnum) {  scratch: ", "if (!unw_valid(info, info->rp_loc)) ": "unw_unwind (struct unw_frame_info  info){unsigned long prev_ip, prev_sp, prev_bsp;unsigned long ip, pr, num_regs;STAT(unsigned long start, flags;)int retval;STAT(local_irq_save(flags); ++unw.stat.api.unwinds; start = ia64_get_itc());prev_ip = info->ip;prev_sp = info->sp;prev_bsp = info->bsp;  validate the return IP pointer ", "unsigned long uncached_alloc_page(int starting_nid, int n_pages)": "uncached_alloc_page     @starting_nid: node id of node to start with, or -1   @n_pages: number of contiguous pages to allocate     Allocate the specified number of contiguous uncached pages on the   requested node. If not enough contiguous uncached pages are available   on the requested node, roundrobin starting with the next higher node. ", "void uncached_free_page(unsigned long uc_addr, int n_pages)": "uncached_free_page     @uc_addr: uncached address of first page to free   @n_pages: number of contiguous pages to free     Free the specified number of uncached pages. ", "/* * This is a reasonably optimized memcpy() routine. ": "memcpy.c      Copyright (C) 1995  Linus Torvalds ", "#include <asm/param.h>#include <asm/smp.h>#include <linux/delay.h>/* * Use only for very small delays (< 1 msec).  * * The active part of our cycle counter is only 32-bits wide, and * we're treating the difference between two marks as signed.  On * a 1GHz box, that's about 2 seconds. ": "udelay's use of smp_processor_id ", " FIXUP_MEMADDR_VGA(addr);/* * Find the hose. ": "marvel_ioremap(unsigned long addr, unsigned long size){struct pci_controller  hose;unsigned long baddr, last;struct vm_struct  area;unsigned long vaddr;unsigned long  ptes;unsigned long pfn;    Adjust the address. ", "return ~0;}voidmarvel_iowrite8(u8 b, void __iomem *xaddr)": "marvel_ioread8(const void __iomem  xaddr){unsigned long addr = (unsigned long) xaddr;if (__marvel_is_port_kbd(addr))return 0;else if (__marvel_is_port_rtc(addr))return __marvel_rtc_io(0, addr, 0);else if (marvel_is_ioaddr(addr))return __kernel_ldbu( (vucp)addr);else  this should catch other legacy addresses   that would normally fail on MARVEL,   because there really is nothing there...", "};#include <linux/uaccess.h>#include <asm/hwrpb.h>#include <asm/dma.h>#include <asm/mmu_context.h>#include <asm/console.h>#include \"proto.h\"#include \"pci_impl.h\"struct hwrpb_struct *hwrpb;EXPORT_SYMBOL(hwrpb);unsigned long srm_hae;int alpha_l1i_cacheshape;int alpha_l1d_cacheshape;int alpha_l2_cacheshape;int alpha_l3_cacheshape;#ifdef CONFIG_VERBOSE_MCHECK/* 0=minimum, 1=verbose, 2=all ": "screen_info.h>#include <linuxdelay.h>#include <linuxmc146818rtc.h>#include <linuxconsole.h>#include <linuxcpu.h>#include <linuxerrno.h>#include <linuxinit.h>#include <linuxstring.h>#include <linuxioport.h>#include <linuxpanic_notifier.h>#include <linuxplatform_device.h>#include <linuxmemblock.h>#include <linuxpci.h>#include <linuxseq_file.h>#include <linuxroot_dev.h>#include <linuxinitrd.h>#include <linuxeisa.h>#include <linuxpfn.h>#ifdef CONFIG_MAGIC_SYSRQ#include <linuxsysrq.h>#include <linuxreboot.h>#endif#include <linuxnotifier.h>#include <asmsetup.h>#include <asmio.h>#include <linuxlog2.h>#include <linuxexport.h>static int alpha_panic_event(struct notifier_block  , unsigned long, void  );static struct notifier_block alpha_panic_block = {alpha_panic_event,        NULL,        INT_MAX   try to do it first ", "if (addr)return vfree((void *)(PAGE_MASK & addr)); }EXPORT_SYMBOL(irongate_iounmap": "irongate_iounmap(volatile void __iomem  xaddr){unsigned long addr = (unsigned long) xaddr;if (((long)addr >> 41) == -2)return;  kseg map, nothing to do ", "struct switch_stack * sw = ((struct switch_stack *) pt) - 1;dest[ 0] = pt->r0;dest[ 1] = pt->r1;dest[ 2] = pt->r2;dest[ 3] = pt->r3;dest[ 4] = pt->r4;dest[ 5] = pt->r5;dest[ 6] = pt->r6;dest[ 7] = pt->r7;dest[ 8] = pt->r8;dest[ 9] = sw->r9;dest[10] = sw->r10;dest[11] = sw->r11;dest[12] = sw->r12;dest[13] = sw->r13;dest[14] = sw->r14;dest[15] = sw->r15;dest[16] = pt->r16;dest[17] = pt->r17;dest[18] = pt->r18;dest[19] = pt->r19;dest[20] = pt->r20;dest[21] = pt->r21;dest[22] = pt->r22;dest[23] = pt->r23;dest[24] = pt->r24;dest[25] = pt->r25;dest[26] = pt->r26;dest[27] = pt->r27;dest[28] = pt->r28;dest[29] = pt->gp;dest[30] = ti == current_thread_info() ? rdusp() : ti->pcb.usp;dest[31] = pt->pc;/* Once upon a time this was the PS value.  Which is stupid   since that is always 8 for usermode.  Usurped for the more   useful value of the thread's UNIQUE field.  ": "dump_elf_thread(elf_greg_t  dest, struct pt_regs  pt, struct thread_info  ti){  switch stack follows right below pt_regs: ", "EXPORT_SYMBOL(smp_num_cpus": "smp_num_cpus = 1;  Number that came online.  ", "on_each_cpu(ipi_imb, NULL, 1);}EXPORT_SYMBOL(smp_imb": "smp_imb(void){  Must wait other processors to flush their icache before continue. ", "flush_tlb_mm(vma->vm_mm);}EXPORT_SYMBOL(flush_tlb_range": "flush_tlb_range(struct vm_area_struct  vma, unsigned long start, unsigned long end){  On the Alpha we always flush the whole user tlb.  ", "if (count >= 8 && ((u64)to & 7) == ((u64)from & 7)) ": "memcpy_fromio(void  to, const volatile void __iomem  from, long count){  Optimize co-aligned transfers.  Everything else gets handled   a byte at a time. ", "/* FIXME -- align FROM.  ": "memcpy_toio(volatile void __iomem  to, const void  from, long count){  Optimize co-aligned transfers.  Everything else gets handled   a byte at a time. ", "if (count > 0 && ((u64)to & 1)) ": "_memset_c_io(volatile void __iomem  to, unsigned long c, long count){  Handle any initial odd byte ", "count /= 2;while (count--) ": "scr_memcpyw(u16  d, const u16  s, unsigned int count){const u16 __iomem  ios = (const u16 __iomem  ) s;u16 __iomem  iod = (u16 __iomem  ) d;int s_isio = __is_ioaddr(s);int d_isio = __is_ioaddr(d);if (s_isio) {if (d_isio) {  FIXME: Should handle unaligned ops and   operation widening.  ", "u8 readb_relaxed(const volatile void __iomem *addr)": "ioport_map(port, 1));}u16 inw(unsigned long port){return ioread16(ioport_map(port, 2));}u32 inl(unsigned long port){return ioread32(ioport_map(port, 4));}void outb(u8 b, unsigned long port){iowrite8(b, ioport_map(port, 1));}void outw(u16 b, unsigned long port){iowrite16(b, ioport_map(port, 2));}void outl(u32 b, unsigned long port){iowrite32(b, ioport_map(port, 4));}EXPORT_SYMBOL(inb);EXPORT_SYMBOL(inw);EXPORT_SYMBOL(inl);EXPORT_SYMBOL(outb);EXPORT_SYMBOL(outw);EXPORT_SYMBOL(outl);u8 __raw_readb(const volatile void __iomem  addr){return IO_CONCAT(__IO_PREFIX,readb)(addr);}u16 __raw_readw(const volatile void __iomem  addr){return IO_CONCAT(__IO_PREFIX,readw)(addr);}u32 __raw_readl(const volatile void __iomem  addr){return IO_CONCAT(__IO_PREFIX,readl)(addr);}u64 __raw_readq(const volatile void __iomem  addr){return IO_CONCAT(__IO_PREFIX,readq)(addr);}void __raw_writeb(u8 b, volatile void __iomem  addr){IO_CONCAT(__IO_PREFIX,writeb)(b, addr);}void __raw_writew(u16 b, volatile void __iomem  addr){IO_CONCAT(__IO_PREFIX,writew)(b, addr);}void __raw_writel(u32 b, volatile void __iomem  addr){IO_CONCAT(__IO_PREFIX,writel)(b, addr);}void __raw_writeq(u64 b, volatile void __iomem  addr){IO_CONCAT(__IO_PREFIX,writeq)(b, addr);}EXPORT_SYMBOL(__raw_readb); EXPORT_SYMBOL(__raw_readw); EXPORT_SYMBOL(__raw_readl); EXPORT_SYMBOL(__raw_readq); EXPORT_SYMBOL(__raw_writeb); EXPORT_SYMBOL(__raw_writew); EXPORT_SYMBOL(__raw_writel); EXPORT_SYMBOL(__raw_writeq); u8 readb(const volatile void __iomem  addr){u8 ret;mb();ret = __raw_readb(addr);mb();return ret;}u16 readw(const volatile void __iomem  addr){u16 ret;mb();ret = __raw_readw(addr);mb();return ret;}u32 readl(const volatile void __iomem  addr){u32 ret;mb();ret = __raw_readl(addr);mb();return ret;}u64 readq(const volatile void __iomem  addr){u64 ret;mb();ret = __raw_readq(addr);mb();return ret;}void writeb(u8 b, volatile void __iomem  addr){mb();__raw_writeb(b, addr);}void writew(u16 b, volatile void __iomem  addr){mb();__raw_writew(b, addr);}void writel(u32 b, volatile void __iomem  addr){mb();__raw_writel(b, addr);}void writeq(u64 b, volatile void __iomem  addr){mb();__raw_writeq(b, addr);}EXPORT_SYMBOL(readb);EXPORT_SYMBOL(readw);EXPORT_SYMBOL(readl);EXPORT_SYMBOL(readq);EXPORT_SYMBOL(writeb);EXPORT_SYMBOL(writew);EXPORT_SYMBOL(writel);EXPORT_SYMBOL(writeq);    The _relaxed functions must be ordered w.r.t. each other, but they don't   have to be ordered w.r.t. other memory accesses. ", " if (pci_vga_hose && __is_mem_vga(addr)) ": "titan_ioremap(unsigned long addr, unsigned long size){int h = (addr & TITAN_HOSE_MASK) >> TITAN_HOSE_SHIFT;unsigned long baddr = addr & ~TITAN_HOSE_MASK;unsigned long last = baddr + size - 1;struct pci_controller  hose;struct vm_struct  area;unsigned long vaddr;unsigned long  ptes;unsigned long pfn;#ifdef CONFIG_VGA_HOSE    Adjust the address and hose, if necessary. ", "/* The generic legacy mode IDE fixup in drivers/pci/probe.c   doesn't work correctly with the Cypress IDE controller as   it has non-standard register layout.  Fix that.  ": "isa_bridge(struct pci_dev  dev){dev->class = PCI_CLASS_BRIDGE_ISA << 8;}DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82378, quirk_isa_bridge);static void quirk_cypress(struct pci_dev  dev){  The Notorious Cy82C693 chip.  ", "min_high_pfn = PFN_DOWN(high_mem_start);max_high_pfn = PFN_DOWN(high_mem_start + high_mem_sz);/* * max_high_pfn should be ok here for both HIGHMEM and HIGHMEM+PAE. * For HIGHMEM without PAE max_high_pfn should be less than * min_low_pfn to guarantee that these two regions don't overlap. * For PAE case highmem is greater than lowmem, so it is natural * to use max_high_pfn. * * In both cases, holes should be handled by pfn_valid(). ": "pfn_valid()   handles the hole in the memory map. ", "mapping = page_mapping_file(page);if (!mapping)return;/* * pagecache page, file not yet mapped to userspace * Make a note that K-mapping is dirty ": "flush_dcache_page(struct page  page){struct address_space  mapping;if (!cache_is_vipt_aliasing()) {clear_bit(PG_dc_clean, &page->flags);return;}  don't handle anon pages here ", "static void read_decode_cache_bcr_arcv2(int cpu)": "dma_cache_wback_inv)(phys_addr_t start, unsigned long sz);void ( __dma_cache_inv)(phys_addr_t start, unsigned long sz);void ( __dma_cache_wback)(phys_addr_t start, unsigned long sz);char  arc_cache_mumbojumbo(int c, char  buf, int len){int n = 0;struct cpuinfo_arc_cache  p;#define PR_CACHE(p, cfg, str)\\if (!(p)->line_len)\\n += scnprintf(buf + n, len - n, str\"\\t\\t: NA\\n\");\\else\\n += scnprintf(buf + n, len - n,\\str\"\\t\\t: %uK, %dwayset, %uB Line, %s%s%s\\n\",\\(p)->sz_k, (p)->assoc, (p)->line_len,\\(p)->vipt ? \"VIPT\" : \"PIPT\",\\(p)->alias ? \" aliasing\" : \"\",\\IS_USED_CFG(cfg));PR_CACHE(&cpuinfo_arc700[c].icache, CONFIG_ARC_HAS_ICACHE, \"I-Cache\");PR_CACHE(&cpuinfo_arc700[c].dcache, CONFIG_ARC_HAS_DCACHE, \"D-Cache\");p = &cpuinfo_arc700[c].slc;if (p->line_len)n += scnprintf(buf + n, len - n,       \"SLC\\t\\t: %uK, %uB Line%s\\n\",       p->sz_k, p->line_len, IS_USED_RUN(slc_enable));n += scnprintf(buf + n, len - n, \"Peripherals\\t: %#lx%s%s\\n\",       perip_base,       IS_AVAIL3(ioc_exists, ioc_enable, \", IO-Coherency (per-device) \"));return buf;}    Read the Cache Build Confuration Registers, Decode them and save into   the cpuinfo structure for later use.   No Validation done here, simply readconvert the BCRs ", "tot_sz = kend - kstart;if (tot_sz > PAGE_SIZE) ": "flush_icache_range(unsigned long kstart, unsigned long kend){unsigned int tot_sz;WARN(kstart < TASK_SIZE, \"%s() can't handle user vaddr\", __func__);  Shortcut for bigger flush ranges.   Here we don't care if this was kernel virtual or phy addr ", "end = paddr + size - 1;if (!size || (end < paddr))return NULL;/* * If the region is h/w uncached, MMU mapping can be elided as optim * The cast to u32 is fine as this region can only be inside 4GB ": "ioremap(phys_addr_t paddr, unsigned long size){phys_addr_t end;  Don't allow wraparound or zero size ", "void __iomem *ioremap_prot(phys_addr_t paddr, unsigned long size,   unsigned long flags)": "ioremap_prot(paddr, size,    pgprot_val(pgprot_noncached(PAGE_KERNEL)));}EXPORT_SYMBOL(ioremap);    ioremap with access flags   Cache semantics wise it is same as ioremap - \"forced\" uncached.   However unlike vanilla ioremap which bypasses ARC MMU for addresses in   ARC hardware uncached region, this one still goes thru the MMU as caller   might need finer access control (RWX) ", "if (arc_uncached_addr_space((phys_addr_t)(u32)addr))return;vfree((void *)(PAGE_MASK & (unsigned long __force)addr));}EXPORT_SYMBOL(iounmap": "iounmap(const volatile void __iomem  addr){  weird double cast to handle phys_addr_t > 32 bits ", "#ifdef CONFIG_ARC_COMPACT_IRQ_LEVELS/* Complex version for 2 IRQ levels ": "arch_local_irq_enable - Enable interrupts.     1. Explicitly called to re-enable interrupts   2. Implicitly called from spin_unlock_irq, write_unlock_irq etc      which maybe in hard ISR itself     Semantics of this function change depending on where it is called from:     -If called from hard-ISR, it must not invert interrupt priorities    e.g. suppose TIMER is high priority (Level 2) IRQ      Time hard-ISR, timer_interrupt( ) calls spin_unlock_irq several times.      Here local_irq_enable( ) shd not re-enable lower priority interrupts   -If called from soft-ISR, it must re-enable all interrupts      soft ISR are low priority jobs which can be very slow, thus all IRQs      must be enabled while they run.      Now hardware context wise we may still be in L2 ISR (not done rtie)      still we must re-enable both L1 and L2 IRQs    Another twist is prev scenario with flow being       L1 ISR ==> interrupted by L2 ISR  ==> L2 soft ISR       here we must not re-enable Ll as prev Ll Interrupt's hw context will get       over-written (this is deficiency in ARC700 Interrupt mechanism) ", "init_unwind_hdr(&root_table, unw_hdr_alloc_early);}static const u32 bad_cie, not_fde;static const u32 *cie_for_fde(const u32 *fde, const struct unwind_table *);static const u32 *__cie_for_fde(const u32 *fde);static signed fde_pointer_type(const u32 *cie);struct eh_frame_hdr_table_entry ": "arc_unwind_init(void){init_unwind_table(&root_table, \"kernel\", _text, _end - _text, NULL, 0,  __start_unwind, __end_unwind - __start_unwind,  NULL, 0);   __start_unwind_hdr, __end_unwind_hdr - __start_unwind_hdr);", "void ioread8_rep(const void __iomem *addr, void *dst, unsigned long count)": "iowrite64be(u64 datum, void __iomem  addr){if (unlikely(INDIRECT_ADDR(addr))) {iomap_ops[ADDR_TO_REGION(addr)]->write64be(datum, addr);} else { ((u64  )addr) = datum;}}#endif  Repeating interfaces ", "void __iomem *ioport_map(unsigned long port, unsigned int nr)": "iowrite32_rep(void __iomem  addr, const void  src, unsigned long count){if (unlikely(INDIRECT_ADDR(addr))) {iomap_ops[ADDR_TO_REGION(addr)]->write32r(addr, src, count);} else {while (count--) { (u32  )addr =  (u32  )src;src += 4;}}}  Mapping interfaces ", "void insw (unsigned long port, void *dst, unsigned long count)": "insb (unsigned long port, void  dst, unsigned long count){unsigned char  p;p = (unsigned char  )dst;while (((unsigned long)p) & 0x3) {if (!count)return;count--; p = inb(port);p++;}while (count >= 4) {unsigned int w;count -= 4;w = inb(port) << 24;w |= inb(port) << 16;w |= inb(port) << 8;w |= inb(port); (unsigned int  ) p = w;p += 4;}while (count) {--count; p = inb(port);p++;}}    Read COUNT 16-bit words from port PORT into memory starting at   SRC.  SRC must be at least short aligned.  This is used by the   IDE driver to read disk sectors.  Performance is important, but   the interfaces seems to be slow: just using the inlined version   of the inw() breaks things. ", "while (count>=2) ": "outsw (unsigned long port, const void  src, unsigned long count){unsigned int l = 0, l2;const unsigned char  p;p = (const unsigned char  )src;if (!count)return;switch (((unsigned long)p) & 0x3){ case 0x00:  Buffer 32-bit aligned ", "while (count--)": "outsl (unsigned long port, const void  src, unsigned long count){unsigned int l = 0, l2;const unsigned char  p;p = (const unsigned char  )src;if (!count)return;switch (((unsigned long)p) & 0x3){ case 0x00:  Buffer 32-bit aligned ", "void outsw (unsigned long port, const void *src, unsigned long count)": "outsb(unsigned long port, const void   src, unsigned long count){const unsigned char  p;p = (const unsigned char  )src;while (count) {count--;outb( p, port);p++;}}    Like insw but in the opposite direction.  This is used by the IDE   driver to write disk sectors.  Performance is important, but the   interfaces seems to be slow: just using the inlined version of the   outw() breaks things. ", "#include <linux/vmalloc.h>#include <linux/errno.h>#include <linux/module.h>#include <linux/io.h>#include <linux/mm.h>/* * Generic mapping function (not visible outside): ": "ioremap.c     (C) Copyright 1995 1996 Linus Torvalds   (C) Copyright 2001-2019 Helge Deller <deller@gmx.de>   (C) Copyright 2005 Kyle McMartin <kyle@parisc-linux.org> ", "int register_parisc_driver(struct parisc_driver *driver)": "register_parisc_driver - Register this driver if it can handle a device   @driver: the PA-RISC driver to try ", "int unregister_parisc_driver(struct parisc_driver *driver)": "unregister_parisc_driver - Unregister this driver from the list of drivers   @driver: the PA-RISC driver to unregister ", "char *print_pa_hwpath(struct parisc_device *dev, char *output)": "print_pa_hwpath - Returns hardware path for PA devices   @dev: The device to return the path for   @output: Pointer to a previously-allocated array to place the path in.     This function fills in the output array with a human-readable path   to a PA device.  This string is compatible with that used by PDC, and   may be printed on the outside of the box. ", "void get_pci_node_path(struct pci_dev *pdev, struct hardware_path *path)": "get_pci_node_path - Determines the hardware path for a PCI device   @pdev: The device to return the path for   @path: Pointer to a previously-allocated array to place the path in.     This function fills in the hardware_path structure with the route to   the specified PCI device.  This structure is suitable for passing to   PDC calls. ", "char *print_pci_hwpath(struct pci_dev *dev, char *output)": "print_pci_hwpath - Returns hardware path for PCI devices   @dev: The device to return the path for   @output: Pointer to a previously-allocated array to place the path in.     This function fills in the output array with a human-readable path   to a PCI device.  This string is compatible with that used by PDC, and   may be printed on the outside of the box. ", "struct device *hwpath_to_device(struct hardware_path *modpath)": "hwpath_to_device - Finds the generic device corresponding to a given hardware path.   @modpath: the hardware path.   @return: The target device, NULL if not found. ", "void device_to_hwpath(struct device *dev, struct hardware_path *path)": "device_to_hwpath - Populates the hwpath corresponding to the given device.   @dev: the target device   @path: pointer to a previously allocated hwpath struct to be filled in ", "void pdc_emergency_unlock(void)": "pdc_add_valid) during kernel stack dump. ", "int pdc_iodc_read(unsigned long *actcnt, unsigned long hpa, unsigned int index,  void *iodc_data, unsigned int iodc_data_size)": "pdc_iodc_read - Read data from the modules IODC.   @actcnt: The actual number of bytes.   @hpa: The HPA of the module for the iodc read.   @index: The iodc entry point.   @iodc_data: A buffer memory for the iodc options.   @iodc_data_size: Size of the memory buffer.     This PDC call reads from the IODC of the module specified by the hpa   argument. ", "int pdc_lan_station_id(char *lan_addr, unsigned long hpa)": "pdc_lan_station_id - Get the LAN address.   @lan_addr: The return buffer.   @hpa: The network device HPA.     Get the LAN station address when it is not directly available from the LAN hardware. ", "int pdc_stable_read(unsigned long staddr, void *memaddr, unsigned long count)": "pdc_stable_read - Read data from Stable Storage.   @staddr: Stable Storage address to access.   @memaddr: The memory address where Stable Storage data shall be copied.   @count: number of bytes to transfer. count is multiple of 4.     This PDC call reads from the Stable Storage address supplied in staddr   and copies count bytes to the memory address memaddr.   The call will fail if staddr+count > PDC_STABLE size. ", "int pdc_stable_write(unsigned long staddr, void *memaddr, unsigned long count)": "pdc_stable_write - Write data to Stable Storage.   @staddr: Stable Storage address to access.   @memaddr: The memory address where Stable Storage data shall be read from.   @count: number of bytes to transfer. count is multiple of 4.     This PDC call reads count bytes from the supplied memaddr address,   and copies count bytes to the Stable Storage address staddr.   The call will fail if staddr+count > PDC_STABLE size. ", "int pdc_stable_get_size(unsigned long *size)": "pdc_stable_get_size - Get Stable Storage size in bytes.   @size: pointer where the size will be stored.     This PDC call returns the number of bytes in the processor's Stable   Storage, which is the number of contiguous bytes implemented in Stable   Storage starting from staddr=0. size in an unsigned 64-bit integer   which is a multiple of four. ", "int pdc_stable_verify_contents(void)": "pdc_stable_verify_contents - Checks that Stable Storage contents are valid.     This PDC call is meant to be used to check the integrity of the current   contents of Stable Storage. ", "int pdc_stable_initialize(void)": "pdc_stable_initialize - Sets Stable Storage contents to zero and initialize   the validity indicator.     This PDC call will erase all contents of Stable Storage. Use with care! ", "int pdc_get_initiator(struct hardware_path *hwpath, struct pdc_initiator *initiator)": "pdc_get_initiator - Get the SCSI Interface Card params (SCSI ID, SDTR, SE or LVD)   @hwpath: fully bc.mod style path to the device.   @initiator: the array to return the result into     Get the SCSI operational parameters from PDC.   Needed since HPUX never used BIOS or symbios card NVRAM.   Most ncrsym cards won't have an entry and just use whatever   capabilities of the card are (eg Ultra, LVD). But there are   several cases where it's useful:      o set SCSI id for Multi-initiator clusters,      o cable too long (ie SE scsi 10Mhz won't support 6m length),      o bus width exported is less than what the interface chip supports. ", "int pdc_tod_read(struct pdc_tod *tod)": "pdc_tod_read - Read the Time-Of-Day clock.   @tod: The return buffer:     Read the Time-Of-Day clock ", " int pdc_tod_set(unsigned long sec, unsigned long usec)": "pdc_tod_set - Set the Time-Of-Day clock.   @sec: The number of seconds since epoch.   @usec: The number of micro seconds.     Set the Time-Of-Day clock. ", "if (btlb_info.max_size==0) ": "flush_kernel_dcache_page_addr(pfn_va(pfn));clear_bit(PG_dcache_dirty, &page->flags);} else if (parisc_requires_coherency())flush_kernel_dcache_page_addr(pfn_va(pfn));}voidshow_cache_info(struct seq_file  m){char buf[32];seq_printf(m, \"I-cache\\t\\t: %ld KB\\n\", cache_info.ic_size1024 );if (cache_info.dc_loop != 1)snprintf(buf, 32, \"%lu-way associative\", cache_info.dc_loop);seq_printf(m, \"D-cache\\t\\t: %ld KB (%s%s, %s, alias=%d)\\n\",cache_info.dc_size1024,(cache_info.dc_conf.cc_wt ? \"WT\":\"WB\"),(cache_info.dc_conf.cc_sh ? \", shared ID\":\"\"),((cache_info.dc_loop == 1) ? \"direct mapped\" : buf),cache_info.dc_conf.cc_alias);seq_printf(m, \"ITLB entries\\t: %ld\\n\" \"DTLB entries\\t: %ld%s\\n\",cache_info.it_size,cache_info.dt_size,cache_info.dt_conf.tc_sh ? \" - shared with ITLB\":\"\");#ifndef CONFIG_PA20  BTLB - Block TLB ", "asm_syncdma();if ((!IS_ENABLED(CONFIG_SMP) || !arch_irqs_disabled()) &&    (unsigned long)size >= parisc_cache_flush_threshold) ": "invalidate_kernel_vmap_range(void  vaddr, int size){unsigned long start = (unsigned long)vaddr;unsigned long end = start + size;  Ensure DMA is complete ", "extern int $global$;EXPORT_SYMBOL($global$);#endif#include <asm/io.h>EXPORT_SYMBOL(memcpy_toio);EXPORT_SYMBOL(memcpy_fromio);EXPORT_SYMBOL(memset": "memset);#include <linuxatomic.h>EXPORT_SYMBOL(__xchg8);EXPORT_SYMBOL(__xchg32);EXPORT_SYMBOL(__cmpxchg_u32);EXPORT_SYMBOL(__cmpxchg_u64);#ifdef CONFIG_SMPEXPORT_SYMBOL(__atomic_hash);#endif#ifdef CONFIG_64BITEXPORT_SYMBOL(__xchg64);#endif#include <linuxuaccess.h>EXPORT_SYMBOL(lclear_user);#ifndef CONFIG_64BIT  Needed so insmod can set dp value ", "if (!sti)return true;/* return true if it's the default built-in framebuffer driver ": "fb_is_primary_device(struct fb_info  info){struct sti_struct  sti;sti = sti_get_rom(0);  if no built-in graphics card found, allow any fb driver as default ", "asm volatile(\"0: brct %0,0b\" : : \"d\" ((loops/2) + 1));}EXPORT_SYMBOL(__delay": "__delay(unsigned long loops){    Loop 'loops' times. Callers must not assume a specific   amount of time passes before this function returns. ", "if (!owner) ": "arch_spin_trylock_retry(arch_spinlock_t  lp){int cpu = SPINLOCK_LOCKVAL;int owner, count;for (count = spin_retry; count > 0; count--) {owner = READ_ONCE(lp->lock);  Try to get the lock if it is free. ", "__atomic_add_const(-1, &rw->cnts);/* Put the reader into the wait queue ": "arch_read_lock_wait(arch_rwlock_t  rw){if (unlikely(in_interrupt())) {while (READ_ONCE(rw->cnts) & 0x10000)barrier();return;}  Remove this reader again to allow recursive read locking ", "__atomic_add(0x20000, &rw->cnts);/* Put the writer into the wait queue ": "arch_write_lock_wait(arch_rwlock_t  rw){int old;  Add this CPU to the write waiters ", "return result + size;/* Nope. ": "find_next_bit_inv(const unsigned long  addr, unsigned long size,unsigned long offset){const unsigned long  p = addr + (offset  BITS_PER_LONG);unsigned long result = offset & ~(BITS_PER_LONG - 1);unsigned long tmp;if (offset >= size)return size;size -= result;offset %= BITS_PER_LONG;if (offset) {tmp =  (p++);tmp &= (~0UL >> offset);if (size < BITS_PER_LONG)goto found_first;if (tmp)goto found_middle;size -= BITS_PER_LONG;result += BITS_PER_LONG;}while (size & ~(BITS_PER_LONG-1)) {if ((tmp =  (p++)))goto found_middle;result += BITS_PER_LONG;size -= BITS_PER_LONG;}if (!size)return result;tmp =  p;found_first:tmp &= (~0UL << (BITS_PER_LONG - size));if (!tmp)  Are any bits set? ", "#ifdef __HAVE_ARCH_STRLENsize_t strlen(const char *s)": "strlen - Find the length of a string   @s: The string to be sized     returns the length of @s ", "#ifdef __HAVE_ARCH_STRNLENsize_t strnlen(const char *s, size_t n)": "strnlen - Find the length of a length-limited string   @s: The string to be sized   @n: The maximum number of bytes to search     returns the minimum of the length of @s and @n ", "#ifdef __HAVE_ARCH_STRCPYchar *strcpy(char *dest, const char *src)": "strcpy - Copy a %NUL terminated string   @dest: Where to copy the string to   @src: Where to copy the string from     returns a pointer to @dest ", "#ifdef __HAVE_ARCH_STRNCPYchar *strncpy(char *dest, const char *src, size_t n)": "strncpy - Copy a length-limited, %NUL-terminated string   @dest: Where to copy the string to   @src: Where to copy the string from   @n: The maximum number of bytes to copy     The result is not %NUL-terminated if the source exceeds   @n bytes. ", "#ifdef __HAVE_ARCH_STRCATchar *strcat(char *dest, const char *src)": "strcat - Append one %NUL-terminated string to another   @dest: The string to be appended to   @src: The string to append to it     returns a pointer to @dest ", "#ifdef __HAVE_ARCH_STRLCATsize_t strlcat(char *dest, const char *src, size_t n)": "strlcat - Append a length-limited, %NUL-terminated string to another   @dest: The string to be appended to   @src: The string to append to it   @n: The size of the destination buffer. ", "#ifdef __HAVE_ARCH_STRNCATchar *strncat(char *dest, const char *src, size_t n)": "strncat - Append a length-limited, %NUL-terminated string to another   @dest: The string to be appended to   @src: The string to append to it   @n: The maximum numbers of bytes to copy     returns a pointer to @dest     Note that in contrast to strncpy, strncat ensures the result is   terminated. ", "#ifdef __HAVE_ARCH_STRCMPint strcmp(const char *s1, const char *s2)": "strcmp - Compare two strings   @s1: One string   @s2: Another string     returns   0 if @s1 and @s2 are equal,     < 0 if @s1 is less than @s2     > 0 if @s1 is greater than @s2 ", "#ifdef __HAVE_ARCH_STRSTRchar *strstr(const char *s1, const char *s2)": "strstr - Find the first substring in a %NUL terminated string   @s1: The string to be searched   @s2: The string to search for ", "#ifdef __HAVE_ARCH_MEMCHRvoid *memchr(const void *s, int c, size_t n)": "memchr - Find a character in an area of memory.   @s: The memory area   @c: The byte to search for   @n: The size of the area.     returns the address of the first occurrence of @c, or %NULL   if @c is not found ", "#ifdef __HAVE_ARCH_MEMCMPint memcmp(const void *s1, const void *s2, size_t n)": "memcmp - Compare two areas of memory   @s1: One area of memory   @s2: Another area of memory   @n: The size of the area. ", "#ifdef __HAVE_ARCH_MEMSCANvoid *memscan(void *s, int c, size_t n)": "memscan - Find a character in an area of memory.   @s: The memory area   @c: The byte to search for   @n: The size of the area.     returns the address of the first occurrence of @c, or 1 byte past   the area if @c is not found ", "\"nr%[rem],%[val]\\n\"/* rem = (from + 4095) & -4096 ": "_copy_from_user_key(void  to, const void __user  from,    unsigned long size, unsigned long key){unsigned long rem;union oac spec = {.oac2.key = key,.oac2.as = PSW_BITS_AS_SECONDARY,.oac2.k = 1,.oac2.a = 1,};asm volatile(\"lr0,%[spec]\\n\"\"0:mvcos0(%[to]),0(%[from]),%[size]\\n\"\"1:jz5f\\n\"\"algr%[size],%[val]\\n\"\"slgr%[from],%[val]\\n\"\"slgr%[to],%[val]\\n\"\"j0b\\n\"\"2:la%[rem],4095(%[from])\\n\"  rem = from + 4095 ", "\"nr%[rem],%[val]\\n\"/* rem = (to + 4095) & -4096 ": "__clear_user(void __user  to, unsigned long size){unsigned long rem;union oac spec = {.oac1.as = PSW_BITS_AS_SECONDARY,.oac1.a = 1,};asm volatile(\"lr0,%[spec]\\n\"\"0:mvcos0(%[to]),0(%[zeropg]),%[size]\\n\"\"1:jz5f\\n\"\"algr%[size],%[val]\\n\"\"slgr%[to],%[val]\\n\"\"j0b\\n\"\"2:la%[rem],4095(%[to])\\n\"  rem = to + 4095 ", "hchacha_block_generic(state, stream, nrounds);}EXPORT_SYMBOL(hchacha_block_arch": "hchacha_block_arch(const u32  state, u32  stream, int nrounds){  TODO: implement hchacha_block_arch() in assembly ", "if (bytes <= CHACHA_BLOCK_SIZE || nrounds != 20 || !MACHINE_HAS_VX)chacha_crypt_generic(state, dst, src, bytes, nrounds);elsechacha20_crypt_s390(state, dst, src, bytes,    &state[4], &state[12]);}EXPORT_SYMBOL(chacha_crypt_arch": "chacha_crypt_arch(u32  state, u8  dst, const u8  src,       unsigned int bytes, int nrounds){  s390 chacha20 implementation has 20 rounds hard-coded,   it cannot handle a block of data or less, but otherwise   it can handle data of arbitrary size ", "set_pte(ptep, new);atomic_dec(&mm->context.flush_count);preempt_enable();}EXPORT_SYMBOL(ptep_reset_dat_prot": "ptep_reset_dat_prot(struct mm_struct  mm, unsigned long addr, pte_t  ptep, pte_t new){preempt_disable();atomic_inc(&mm->context.flush_count);if (cpumask_equal(mm_cpumask(mm), cpumask_of(smp_processor_id())))__ptep_rdp(addr, ptep, 0, 0, 1);else__ptep_rdp(addr, ptep, 0, 0, 0);    PTE is not invalidated by RDP, only _PAGE_PROTECT is cleared. That   means it is still valid and active, and must not be changed according   to the architecture. But writing a new value that only differs in SW   bits is allowed. ", "switch (pmd_lookup(mm, addr, &pmdp)) ": "reset_guest_reference_bit(struct mm_struct  mm, unsigned long addr){spinlock_t  ptl;unsigned long paddr;pgste_t old, new;pmd_t  pmdp;pte_t  ptep;int cc = 0;    If we don't have a PTE table and if there is no huge page mapped,   the storage key is 0 and there is nothing for us to do. ", "if (mr | mc) ": "cond_set_guest_storage_key(struct mm_struct  mm, unsigned long addr,       unsigned char key, unsigned char  oldkey,       bool nq, bool mr, bool mc){unsigned char tmp, mask = _PAGE_ACC_BITS | _PAGE_FP_BIT;int rc;  we can drop the pgste lock between getting and setting the key ", "int reset_guest_reference_bit(struct mm_struct *mm, unsigned long addr)": "get_guest_storage_key(current->mm, addr, &tmp);if (rc)return rc;if (oldkey) oldkey = tmp;if (!mr)mask |= _PAGE_REFERENCED;if (!mc)mask |= _PAGE_CHANGED;if (!((tmp ^ key) & mask))return 0;}rc = set_guest_storage_key(current->mm, addr, key, nq);return rc < 0 ? rc : 1;}EXPORT_SYMBOL(cond_set_guest_storage_key);    Reset a guest reference bit (rrbe), returning the reference and changed bit.     Returns < 0 in case of error, otherwise the cc to be reported to the guest. ", "int pgste_perform_essa(struct mm_struct *mm, unsigned long hva, int orc,unsigned long *oldpte, unsigned long *oldpgste)": "pgste_perform_essa - perform ESSA actions on the PGSTE.   @mm: the memory context. It must have PGSTEs, no check is performed here!   @hva: the host virtual address of the page whose PGSTE is to be processed   @orc: the specific action to perform, see the ESSA_SET_  macros.   @oldpte: the PTE will be saved there if the pointer is not NULL.   @oldpgste: the old PGSTE will be saved there if the pointer is not NULL.     Return: 1 if the page is to be added to the CBRL, otherwise 0,     or < 0 in case of error. -EINVAL is returned for invalid values     of orc, -EFAULT for invalid addresses. ", "int set_pgste_bits(struct mm_struct *mm, unsigned long hva,unsigned long bits, unsigned long value)": "set_pgste_bits - set specific PGSTE bits.   @mm: the memory context. It must have PGSTEs, no check is performed here!   @hva: the host virtual address of the page whose PGSTE is to be processed   @bits: a bitmask representing the bits that will be touched   @value: the values of the bits to be written. Only the bits in the mask     will be written.     Return: 0 on success, < 0 in case of error. ", "int get_pgste(struct mm_struct *mm, unsigned long hva, unsigned long *pgstep)": "get_pgste - get the current PGSTE for the given address.   @mm: the memory context. It must have PGSTEs, no check is performed here!   @hva: the host virtual address of the page whose PGSTE is to be processed   @pgstep: will be written with the current PGSTE for the given address.     Return: 0 on success, < 0 in case of error. ", "static int__segment_load (char *name, int do_nonshared, unsigned long *addr, unsigned long *end)": "segment_load   Must return either an error code < 0, or the segment type code >= 0 ", "voidsegment_save(char *name)": "segment_unload(char  name){unsigned long dummy;struct dcss_segment  seg;if (!MACHINE_IS_VM)return;mutex_lock(&dcss_lock);seg = segment_by_name (name);if (seg == NULL) {pr_err(\"Unloading unknown DCSS %s failed\\n\", name);goto out_unlock;}if (!refcount_dec_and_test(&seg->ref_count))goto out_unlock;release_resource(seg->res);kfree(seg->res);vmem_remove_mapping(seg->start_addr, seg->end - seg->start_addr + 1);list_del(&seg->list);dcss_diag(&purgeseg_scode, seg->dcss_name, &dummy, &dummy);kfree(seg);out_unlock:mutex_unlock(&dcss_lock);}    save segment content permanently ", "void segment_warning(int rc, char *seg_name)": "segment_save(char  name){struct dcss_segment  seg;char cmd1[160];char cmd2[80];int i, response;if (!MACHINE_IS_VM)return;mutex_lock(&dcss_lock);seg = segment_by_name (name);if (seg == NULL) {pr_err(\"Saving unknown DCSS %s failed\\n\", name);goto out;}sprintf(cmd1, \"DEFSEG %s\", name);for (i=0; i<seg->segcnt; i++) {sprintf(cmd1+strlen(cmd1), \" %lX-%lX %s\",seg->range[i].start >> PAGE_SHIFT,seg->range[i].end >> PAGE_SHIFT,segtype_string[seg->range[i].start & 0xff]);}sprintf(cmd2, \"SAVESEG %s\", name);response = 0;cpcmd(cmd1, NULL, 0, &response);if (response) {pr_err(\"Saving a DCSS failed with DEFSEG response code \"       \"%i\\n\", response);goto out;}cpcmd(cmd2, NULL, 0, &response);if (response) {pr_err(\"Saving a DCSS failed with SAVESEG response code \"       \"%i\\n\", response);goto out;}out:mutex_unlock(&dcss_lock);}    print appropriate error message for segment_load()segment_type()   return code ", "qin->qopcode = DCSS_FINDSEGA;qin->qoutptr = (unsigned long) qout;qin->qoutlen = sizeof(struct qout64);memcpy (qin->qname, seg->dcss_name, 8);diag_cc = dcss_diag(&segext_scode, qin, &dummy, &vmrc);if (diag_cc < 0) ": "segment_type (struct dcss_segment  seg){unsigned long dummy, vmrc;int diag_cc, rc, i;struct qout64  qout;struct qin64  qin;qin = kmalloc(sizeof( qin), GFP_KERNEL | GFP_DMA);qout = kmalloc(sizeof( qout), GFP_KERNEL | GFP_DMA);if ((qin == NULL) || (qout == NULL)) {rc = -ENOMEM;goto out_free;}  initialize diag input parameters ", "voidsegment_unload(char *name)": "segment_modify_shared (char  name, int do_nonshared){struct dcss_segment  seg;unsigned long start_addr, end_addr, dummy;int rc, diag_cc;start_addr = end_addr = 0;mutex_lock(&dcss_lock);seg = segment_by_name (name);if (seg == NULL) {rc = -EINVAL;goto out_unlock;}if (do_nonshared == seg->do_nonshared) {pr_info(\"DCSS %s is already in the requested access \"\"mode\\n\", name);rc = 0;goto out_unlock;}if (refcount_read(&seg->ref_count) != 1) {pr_warn(\"DCSS %s is in use and cannot be reloaded\\n\", name);rc = -EAGAIN;goto out_unlock;}release_resource(seg->res);if (do_nonshared)seg->res->flags &= ~IORESOURCE_READONLY;elseif (seg->vm_segtype == SEG_TYPE_SR ||    seg->vm_segtype == SEG_TYPE_ER)seg->res->flags |= IORESOURCE_READONLY;if (request_resource(&iomem_resource, seg->res)) {pr_warn(\"DCSS %s overlaps with used memory resources and cannot be reloaded\\n\",name);rc = -EBUSY;kfree(seg->res);goto out_del_mem;}dcss_diag(&purgeseg_scode, seg->dcss_name, &dummy, &dummy);if (do_nonshared)diag_cc = dcss_diag(&loadnsr_scode, seg->dcss_name,&start_addr, &end_addr);elsediag_cc = dcss_diag(&loadshr_scode, seg->dcss_name,&start_addr, &end_addr);if (diag_cc < 0) {rc = diag_cc;goto out_del_res;}if (diag_cc > 1) {pr_warn(\"Reloading DCSS %s failed with rc=%ld\\n\",name, end_addr);rc = dcss_diag_translate_rc(end_addr);goto out_del_res;}seg->start_addr = start_addr;seg->end = end_addr;seg->do_nonshared = do_nonshared;rc = 0;goto out_unlock; out_del_res:release_resource(seg->res);kfree(seg->res); out_del_mem:vmem_remove_mapping(seg->start_addr, seg->end - seg->start_addr + 1);list_del(&seg->list);dcss_diag(&purgeseg_scode, seg->dcss_name, &dummy, &dummy);kfree(seg); out_unlock:mutex_unlock(&dcss_lock);return rc;}    Decrease the use count of a DCSS segment and remove   it from the address space if nobody is using it   any longer. ", "WARN_ON(!++zpci_iomap_start[idx].count);zpci_iomap_start[idx].fh = zdev->fh;zpci_iomap_start[idx].bar = bar;spin_unlock(&zpci_iomap_lock);return (void __iomem *) ZPCI_ADDR(idx) + offset;}static void __iomem *pci_iomap_range_mio(struct pci_dev *pdev, int bar, unsigned long offset, unsigned long max)": "pci_iomap_range_fh(struct pci_dev  pdev, int bar,unsigned long offset, unsigned long max){struct zpci_dev  zdev =to_zpci(pdev);int idx;idx = zdev->bars[bar].map_idx;spin_lock(&zpci_iomap_lock);  Detect overrun ", "union zpci_sic_iib *zpci_aipb;EXPORT_SYMBOL_GPL(zpci_aipb);struct airq_iv *zpci_aif_sbv;EXPORT_SYMBOL_GPL(zpci_aif_sbv);struct zpci_dev *get_zdev_by_fid(u32 fid)": "pci_iomap_lock);static unsigned long  zpci_iomap_bitmap;struct zpci_iomap_entry  zpci_iomap_start;EXPORT_SYMBOL_GPL(zpci_iomap_start);DEFINE_STATIC_KEY_FALSE(have_mio);static struct kmem_cache  zdev_fmb_cache;  AEN structures that must be preserved over KVM module re-insertion ", "WARN_ON(!zpci_iomap_start[idx].count);if (!--zpci_iomap_start[idx].count) ": "pci_iounmap_fh(struct pci_dev  pdev, void __iomem  addr){unsigned int idx = ZPCI_IDX(addr);spin_lock(&zpci_iomap_lock);  Detect underrun ", "int stsi(void *sysinfo, int fc, int sel1, int sel2)": "stsi(void  sysinfo, int fc, int sel1, int sel2, int  lvl){int r0 = (fc << 28) | sel1;int rc = 0;asm volatile(\"lr0,%[r0]\\n\"\"lr1,%[r1]\\n\"\"stsi0(%[sysinfo])\\n\"\"0:jz2f\\n\"\"1:lhi%[rc],%[retval]\\n\"\"2:lr%[r0],0\\n\"EX_TABLE(0b, 1b): [r0] \"+d\" (r0), [rc] \"+d\" (rc): [r1] \"d\" (sel2),  [sysinfo] \"a\" (sysinfo),  [retval] \"K\" (-EOPNOTSUPP): \"cc\", \"0\", \"1\", \"memory\"); lvl = ((unsigned int) r0) >> 28;return rc;}    stsi - store system information     Returns the current configuration level if function code 0 was specified.   Otherwise returns 0 on success or a negative value on error. ", "int cpu_have_feature(unsigned int num)": "cpu_have_feature - Test CPU features on module initialization ", "void add_virt_timer(struct vtimer_list *timer)": "add_virt_timer - add a oneshot virtual CPU timer ", "debug_info_t *debug_register_mode(const char *name, int pages_per_area,  int nr_areas, int buf_size, umode_t mode,  uid_t uid, gid_t gid)": "debug_register_view(id, &debug_level_view);debug_register_view(id, &debug_flush_view);debug_register_view(id, &debug_pages_view);}     debug_register_mode() - creates and initializes debug area.     @name:Name of debug log (e.g. used for debugfs entry)   @pages_per_area:Number of pages, which will be allocated per area   @nr_areas:Number of debug areas   @buf_size:Size of data area in each debug entry   @mode:File mode for debugfs files. E.g. S_IRWXUGO   @uid:User ID for debugfs files. Currently only 0 is supported.   @gid:Group ID for debugfs files. Currently only 0 is supported.     Return:   - Handle for generated debug area   - %NULL if register failed     Allocates memory for a debug log.   Must not be called within an interrupt handler. ", "id->debugfs_root_entry = debugfs_create_dir(id->name,    debug_debugfs_root_entry);/* append new element to linked list ": "debug_register(debug_info_t  id){  create root directory ", "void debug_unregister(debug_info_t *id)": "debug_unregister(debug_info_t  id){int i;for (i = 0; i < DEBUG_MAX_VIEWS; i++) {if (!id->views[i])continue;debugfs_remove(id->debugfs_entries[i]);}debugfs_remove(id->debugfs_root_entry);if (id == debug_area_first)debug_area_first = id->next;if (id == debug_area_last)debug_area_last = id->prev;if (id->prev)id->prev->next = id->next;if (id->next)id->next->prev = id->prev;}     debug_unregister() - give back debug area.     @id:handle for debug log     Return:      none ", "void debug_set_level(debug_info_t *id, int new_level)": "debug_set_level() - Sets new actual debug level if new_level is valid.     @id:handle for debug log   @new_level:new debug level     Return:      none ", "void debug_stop_all(void)": "debug_stop_all() - stops the debug feature if stopping is allowed.     Return:   -   none     Currently used in case of a kernel oops. ", "debug_entry_t *debug_event_common(debug_info_t *id, int level, const void *buf,  int len)": "debug_event_common:   - write debug entry with given size ", "debug_entry_t *debug_exception_common(debug_info_t *id, int level,      const void *buf, int len)": "debug_exception_common:   - write debug entry with given size and switch to next debug area ", "int debug_unregister_view(debug_info_t *id, struct debug_view *view)": "debug_unregister_view() - unregisters debug view and removes debugfs       dir entry     @id:handle for debug log   @view:pointer to debug view struct     Return:   -   0  : ok   -   < 0: Error ", "static unsigned int __used debug_feature_version = __DEBUG_FEATURE_VERSION;/* static globals ": "debug_dflt_header_fn,&debug_hex_ascii_format_fn,NULL,NULL};EXPORT_SYMBOL(debug_hex_ascii_view);static struct debug_view debug_level_view = {\"level\",&debug_prolog_level_fn,NULL,NULL,&debug_input_level_fn,NULL};static struct debug_view debug_pages_view = {\"pages\",&debug_prolog_pages_fn,NULL,NULL,&debug_input_pages_fn,NULL};static struct debug_view debug_flush_view = {\"flush\",NULL,NULL,NULL,&debug_input_flush_fn,NULL};struct debug_view debug_sprintf_view = {\"sprintf\",NULL,&debug_dflt_header_fn,&debug_sprintf_format_fn,NULL,NULL};EXPORT_SYMBOL(debug_sprintf_view);  used by dump analysis tools to determine version of debug feature ", "return 0;if (!test_bit(CLOCK_SYNC_HAS_STP, &clock_sync_flags))return -EOPNOTSUPP;if (!test_bit(CLOCK_SYNC_STP, &clock_sync_flags))return -EACCES;return -EAGAIN;}EXPORT_SYMBOL(get_phys_clock": "get_phys_clock(unsigned long  clock){atomic_t  sw_ptr;unsigned int sw0, sw1;sw_ptr = &get_cpu_var(clock_sync_word);sw0 = atomic_read(sw_ptr); clock = get_tod_clock() - lpar_offset;sw1 = atomic_read(sw_ptr);put_cpu_var(clock_sync_word);if (sw0 == sw1 && (sw0 & 0x80000000U))  Success: time is in sync. ", "int  __cpcmd(const char *cmd, char *response, int rlen, int *response_code)": "cpcmd\"#define pr_fmt(fmt) KMSG_COMPONENT \": \" fmt#include <linuxkernel.h>#include <linuxexport.h>#include <linuxslab.h>#include <linuxspinlock.h>#include <linuxstddef.h>#include <linuxstring.h>#include <linuxmm.h>#include <linuxio.h>#include <asmdiag.h>#include <asmebcdic.h>#include <asmcpcmd.h>static DEFINE_SPINLOCK(cpcmd_lock);static char cpcmd_buf[241];static int diag8_noresponse(int cmdlen){asm volatile(\"diag%[rx],%[ry],0x8\\n\": [ry] \"+&d\" (cmdlen): [rx] \"d\" (__pa(cpcmd_buf)): \"cc\");return cmdlen;}static int diag8_response(int cmdlen, char  response, int  rlen){union register_pair rx, ry;int cc;rx.even = __pa(cpcmd_buf);rx.odd= __pa(response);ry.even = cmdlen | 0x40000000L;ry.odd=  rlen;asm volatile(\"diag%[rx],%[ry],0x8\\n\"\"ipm%[cc]\\n\"\"srl%[cc],28\\n\": [cc] \"=&d\" (cc), [ry] \"+&d\" (ry.pair): [rx] \"d\" (rx.pair): \"cc\");if (cc) rlen += ry.odd;else rlen = ry.odd;return ry.even;}    __cpcmd has some restrictions over cpcmd    - __cpcmd is unlocked and therefore not SMP-safe ", "flags &= state->mask;if (flags & KERNEL_FPC)/* Save floating point control ": "__kernel_fpu_begin(struct kernel_fpu  state, u32 flags){    Limit the save to the FPUvector registers already   in use by the previous context ", "flags &= state->mask;if (flags & KERNEL_FPC)/* Restore floating-point controls ": "__kernel_fpu_end(struct kernel_fpu  state, u32 flags){    Limit the restore to the FPUvector registers of the   previous context that have been overwritte by the   current context ", "     0x00, 0x01, 0x02, 0x03, 0x37, 0x2D, 0x2E, 0x2F, /*08  BS    HT    LF    VT    FF    CR    SO    SI ": "_ascebc[256] ={  00 NUL   SOH   STX   ETX   EOT   ENQ   ACK   BEL ", "          0x00, 0x01, 0x02, 0x03, 0x07, 0x09, 0x07, 0x7F, /* 0x08   -GE  -SPS  -RPT    VT    FF    CR    SO    SI ": "_ebcasc[256] ={   0x00   NUL   SOH   STX   ETX   SEL    HT   RNL   DEL ", "__u8 _ebc_toupper[256] =": "_ebc_tolower[256] ={0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17,0x18, 0x19, 0x1A, 0x1B, 0x1C, 0x1D, 0x1E, 0x1F,0x20, 0x21, 0x22, 0x23, 0x24, 0x25, 0x26, 0x27,0x28, 0x29, 0x2A, 0x2B, 0x2C, 0x2D, 0x2E, 0x2F,0x30, 0x31, 0x32, 0x33, 0x34, 0x35, 0x36, 0x37,0x38, 0x39, 0x3A, 0x3B, 0x3C, 0x3D, 0x3E, 0x3F,0x40, 0x41, 0x42, 0x43, 0x44, 0x45, 0x46, 0x47,0x48, 0x49, 0x4A, 0x4B, 0x4C, 0x4D, 0x4E, 0x4F,0x50, 0x51, 0x52, 0x53, 0x54, 0x55, 0x56, 0x57,0x58, 0x59, 0x5A, 0x5B, 0x5C, 0x5D, 0x5E, 0x5F,0x60, 0x61, 0x42, 0x43, 0x44, 0x45, 0x46, 0x47,0x48, 0x49, 0x6A, 0x6B, 0x6C, 0x6D, 0x6E, 0x6F,0x70, 0x51, 0x52, 0x53, 0x54, 0x55, 0x56, 0x57,0x58, 0x79, 0x7A, 0x7B, 0x7C, 0x7D, 0x7E, 0x7F,0x80, 0x81, 0x82, 0x83, 0x84, 0x85, 0x86, 0x87,0x88, 0x89, 0x8A, 0x8B, 0x8C, 0x8D, 0x8E, 0x8F,0x90, 0x91, 0x92, 0x93, 0x94, 0x95, 0x96, 0x97,0x98, 0x99, 0x9A, 0x9B, 0x9C, 0x9D, 0x9C, 0x9F,0xA0, 0xA1, 0xA2, 0xA3, 0xA4, 0xA5, 0xA6, 0xA7,0xA8, 0xA9, 0xAA, 0xAB, 0x8C, 0x8D, 0x8E, 0xAF,0xB0, 0xB1, 0xB2, 0xB3, 0xB4, 0xB5, 0xB6, 0xB7,0xB8, 0xB9, 0xBA, 0xBB, 0xBC, 0xBD, 0xBE, 0xBF,0xC0, 0x81, 0x82, 0x83, 0x84, 0x85, 0x86, 0x87,0x88, 0x89, 0xCA, 0xCB, 0xCC, 0xCD, 0xCE, 0xCF,0xD0, 0x91, 0x92, 0x93, 0x94, 0x95, 0x96, 0x97,0x98, 0x99, 0xDA, 0xDB, 0xDC, 0xDD, 0xDE, 0xDF,0xE0, 0xE1, 0xA2, 0xA3, 0xA4, 0xA5, 0xA6, 0xA7,0xA8, 0xA9, 0xEA, 0xCB, 0xCC, 0xCD, 0xCE, 0xCF,0xF0, 0xF1, 0xF2, 0xF3, 0xF4, 0xF5, 0xF6, 0xF7,0xF8, 0xF9, 0xFA, 0xDB, 0xDC, 0xDD, 0xDE, 0xFF};    EBCDIC 037500 conversion table:   from lower to upper case ", "int diag14(unsigned long rx, unsigned long ry1, unsigned long subcode)": "diag26c = _diag26c_amode31,.diag14 = _diag14_amode31,.diag0c = _diag0c_amode31,.diag8c = _diag8c_amode31,.diag308_reset = _diag308_reset_amode31};static struct diag210 _diag210_tmp_amode31 __section(\".amode31.data\");struct diag210 __amode31_ref  __diag210_tmp_amode31 = &_diag210_tmp_amode31;static struct diag8c _diag8c_tmp_amode31 __section(\".amode31.data\");static struct diag8c __amode31_ref  __diag8c_tmp_amode31 = &_diag8c_tmp_amode31;static int show_diag_stat(struct seq_file  m, void  v){struct diag_stat  stat;unsigned long n = (unsigned long) v - 1;int cpu, prec, tmp;cpus_read_lock();if (n == 0) {seq_puts(m, \"         \");for_each_online_cpu(cpu) {prec = 10;for (tmp = 10; cpu >= tmp; tmp  = 10)prec--;seq_printf(m, \"% s%d\", prec, \"CPU\", cpu);}seq_putc(m, '\\n');} else if (n <= NR_DIAG_STAT) {seq_printf(m, \"diag %03x:\", diag_map[n-1].code);for_each_online_cpu(cpu) {stat = &per_cpu(diag_stat, cpu);seq_printf(m, \" %10u\", stat->counter[n-1]);}seq_printf(m, \"    %s\\n\", diag_map[n-1].name);}cpus_read_unlock();return 0;}static void  show_diag_stat_start(struct seq_file  m, loff_t  pos){return  pos <= NR_DIAG_STAT ? (void  )((unsigned long)  pos + 1) : NULL;}static void  show_diag_stat_next(struct seq_file  m, void  v, loff_t  pos){++ pos;return show_diag_stat_start(m, pos);}static void show_diag_stat_stop(struct seq_file  m, void  v){}static const struct seq_operations show_diag_stat_sops = {.start= show_diag_stat_start,.next= show_diag_stat_next,.stop= show_diag_stat_stop,.show= show_diag_stat,};DEFINE_SEQ_ATTRIBUTE(show_diag_stat);static int __init show_diag_stat_init(void){debugfs_create_file(\"diag_stat\", 0400, NULL, NULL,    &show_diag_stat_fops);return 0;}device_initcall(show_diag_stat_init);void diag_stat_inc(enum diag_stat_enum nr){this_cpu_inc(diag_stat.counter[nr]);trace_s390_diagnose(diag_map[nr].code);}EXPORT_SYMBOL(diag_stat_inc);void notrace diag_stat_inc_norecursion(enum diag_stat_enum nr){this_cpu_inc(diag_stat.counter[nr]);trace_s390_diagnose_norecursion(diag_map[nr].code);}EXPORT_SYMBOL(diag_stat_inc_norecursion);    Diagnose 14: Input spool file manipulation ", "struct uv_info __bootdata_preserved(uv_info);EXPORT_SYMBOL(uv_info": "uv_info contains both host and guest information but it's currently only   expected to be used within modules if it's the KVM module or for   any PV guest module.     The kernel itself will write these values once in uv_query_info()   and then make some of them readable via a sysfs interface. ", "      w.s.high = uu.s.high >> (sizeof (SItype) * BITS_PER_UNIT - 1);      w.s.low = uu.s.high >> -bm;    }  else    ": "__ashrdi3 (DItype u, word_type b){  DIunion w;  word_type bm;  DIunion uu;  if (b == 0)    return u;  uu.ll = u;  bm = (sizeof (SItype)   BITS_PER_UNIT) - b;  if (bm <= 0)    {        w.s.high = 1..1 or 0..0 ", "#include <linux/module.h>#include <net/checksum.h>/* * computes a partial checksum, e.g. for TCP/UDP fragments ": "csum_partial_copy_from_user. ", "\"jeq 2f\\n\\t\"\"subql #2,%1\\n\\t\"/* buff%4==2: treat first word ": "csum_partial_copy_nocheck(const void  src, void  dst, int len){unsigned long tmp1, tmp2;__wsum sum = 0;__asm__(\"movel %2,%4\\n\\t\"\"btst #1,%4\\n\\t\"  Check alignment ", "void cache_clear (unsigned long paddr, int len)": "cache_clear() semantics: Clear any cache entries for the area in question,   without writing back dirty entries first. This is useful if the data will   be overwritten anyway, e.g. by DMA to memory. The range is defined by a   _physical_ address. ", "void cache_push (unsigned long paddr, int len)": "cache_push() semantics: Write back any dirty cache data in the given area,   and invalidate the range in the instruction cache. It needs not (but may)   invalidate those entries also in the data cache. The range is defined by a   _physical_ address. ", "unsigned long mm_cachebits;EXPORT_SYMBOL(mm_cachebits": "mm_cachebits    Bits to add to page descriptors for \"normal\" caching mode.   For 68020030 this is 0.   For 68040, this is _PAGE_CACHE040 (cachable, copyback) ", "if (!size || physaddr > (unsigned long)(-size))return NULL;#ifdef CONFIG_AMIGAif (MACH_IS_AMIGA) ": "__ioremap(unsigned long physaddr, unsigned long size, int cacheflag){struct vm_struct  area;unsigned long virtaddr, retaddr;long offset;pgd_t  pgd_dir;p4d_t  p4d_dir;pud_t  pud_dir;pmd_t  pmd_dir;pte_t  pte_dir;    Don't allow mappings that wrap.. ", "__free_io_area(tmp->addr, tmp->size - IO_SIZE);kfree(tmp);return;}}}#endif/* * Map some physical address range into the kernel address space. ": "iounmap: bad pud(%08lx)\\n\", pud_val( pud_dir));pud_clear(pud_dir);return;}pmd_dir = pmd_offset(pud_dir, virtaddr);#if CONFIG_PGTABLE_LEVELS == 3if (CPU_IS_020_OR_030) {int pmd_type = pmd_val( pmd_dir) & _DESCTYPE_MASK;if (pmd_type == _PAGE_PRESENT) {pmd_clear(pmd_dir);virtaddr += PMD_SIZE;size -= PMD_SIZE;} else if (pmd_type == 0)continue;}#endifif (pmd_bad( pmd_dir)) {printk(\"iounmap: bad pmd (%08lx)\\n\", pmd_val( pmd_dir));pmd_clear(pmd_dir);return;}pte_dir = pte_offset_kernel(pmd_dir, virtaddr);pte_val( pte_dir) = 0;virtaddr += PAGE_SIZE;size -= PAGE_SIZE;}flush_tlb_all();}static struct vm_struct  get_io_area(unsigned long size){unsigned long addr;struct vm_struct   p,  tmp,  area;area = kmalloc(sizeof( area), GFP_KERNEL);if (!area)return NULL;addr = KMAP_START;for (p = &iolist; (tmp =  p) ; p = &tmp->next) {if (size + addr < (unsigned long)tmp->addr)break;if (addr > KMAP_END-size) {kfree(area);return NULL;}addr = tmp->size + (unsigned long)tmp->addr;}area->addr = (void  )addr;area->size = size + IO_SIZE;area->next =  p; p = area;return area;}static inline void free_io_area(void  addr){struct vm_struct   p,  tmp;if (!addr)return;addr = (void  )((unsigned long)addr & -IO_SIZE);for (p = &iolist ; (tmp =  p) ; p = &tmp->next) {if (tmp->addr == addr) { p = tmp->next;  remove gap added in get_io_area() ", "offset = phys & (PAGE_SIZE-1);phys &= ~(PAGE_SIZE-1);size += offset;size = PAGE_ALIGN(size);if((area = get_vm_area(size, VM_IOREMAP)) == NULL)return NULL;#ifdef SUN3_KMAP_DEBUGpr_info(\"ioremap: got virt %p size %lx(%lx)\\n\", area->addr, size,area->size);#endifpages = size / PAGE_SIZE;virt = (unsigned long)area->addr;ret = virt + offset;while(pages) ": "sun3_ioremap(unsigned long phys, unsigned long size,   unsigned long type){struct vm_struct  area;unsigned long offset, virt, ret;int pages;if(!size)return NULL;  page align ", "int sun3_map_test(unsigned long addr, char *val)": "sun3_map_test(addr, val) -- Reads a byte from addr, storing to val,   trapping the potential read fault.  Returns 0 if the access faulted,   1 on success.     This function is primarily used to check addresses on the VME bus.     Mucking with the page fault handler seems a little hackish to me, but   SunOS, NetBSD, and Mach all implemented this check in such a manner,   so I figure we're allowed. ", "\"moveq #1,%0\\n\"\"Lberr2:\\n\\t\"\"movel %1,%/sp\\n\\t\"\"movec %2,%/vbr\": \"=&d\" (ret), \"=&r\" (save_sp), \"=&r\" (save_vbr): \"a\" (regp), \"a\" (tmp_vectors), \"g\" (val));local_irq_restore(flags);return ret;}EXPORT_SYMBOL(hwreg_write": "hwreg_write(volatile void  regp, unsigned short val){int ret;unsigned long flags;long save_sp, save_vbr;long tmp_vectors[3];local_irq_save(flags);__asm__ __volatile__ (\"movec %vbr,%2\\n\\t\"\"movel #Lberr2,%4@(8)\\n\\t\"\"movec %4,%vbr\\n\\t\"\"movel %sp,%1\\n\\t\"\"moveq #0,%0\\n\\t\"\"movew %5,%3@\\n\\t\"\"nop\\n\\t\"    If this nop isn't present, 'ret' may already be loaded   with 1 at the time the bus error happens! ", "EXPORT_SYMBOL(amiga_audio_min_period": "amiga_audio_min_period = 124;   Default for pre-OCS ", "int stdma_try_lock(irq_handler_t handler, void *data)": "stdma_try_lock - attempt to acquire ST DMA interrupt \"lock\"   @handler: interrupt handler to use after acquisition     Returns !0 if lock was acquired; otherwise 0. ", "/* put onto a queue and waked up later if the owner calls              ": "stdma_lock() that cannot granted access are ", "/* service routine to stdma_lock().                                    ": "stdma_release(). Additionally, the caller gives his interrupt       ", "int stdma_is_locked_by(irq_handler_t handler)": "stdma_is_locked_by - allow lock holder to check whether it needs to release.   @handler: interrupt handler previously used to acquire lock.     Returns !0 if locked for the given handler; 0 otherwise. ", "int stdma_islocked(void)": "stdma_islocked( void )     Purpose: Check if the ST-DMA is currently locked.   Note: Returned status is only valid if ints are disabled while calling and         as long as they remain disabled.         If called with ints enabled, status can change only from locked to         unlocked, because ints may not lock the ST-DMA.     Inputs: none     Returns: != 0 if locked, 0 otherwise   ", "#include <linux/types.h>#include <linux/kernel.h>#include <linux/kernel_stat.h>#include <linux/init.h>#include <linux/seq_file.h>#include <linux/module.h>#include <linux/irq.h>#include <asm/traps.h>#include <asm/atarihw.h>#include <asm/atariints.h>#include <asm/atari_stdma.h>#include <asm/irq.h>#include <asm/entry.h>#include <asm/io.h>/* * Atari interrupt handling scheme: * -------------------------------- * * All interrupt source have an internal number (defined in * <asm/atariints.h>): Autovector interrupts are 1..7, then follow ST-MFP, * TT-MFP, SCC, and finally VME interrupts. Vector numbers for the latter can * be allocated by atari_register_vme_int(). ": "atari_unregister_vme_int(), and    modified atari_register_vme_int() as well as IS_VALID_INTNO()    to work with it.     This file is subject to the terms and conditions of the GNU General Public   License.  See the file COPYING in the main directory of this archive   for more details.   ", "#include <linux/sched.h>#include <linux/timer.h>#include <linux/major.h>#include <linux/fcntl.h>#include <linux/errno.h>#include <linux/mm.h>#include <linux/module.h>#include <asm/atarihw.h>#include <asm/irq.h>#include <asm/atariints.h>/* * stuff from the old atasound.c ": "atari_microwire_cmd is   unknown. (++roman: That's me... :-)     This file is subject to the terms and conditions of the GNU General Public   License.  See the file COPYING in the main directory of this archive   for more details.     1998-05-31 ++andreas: atari_mksound rewritten to always use the envelope,   no timer, atari_nosound removed.   ", "void (*atari_input_mouse_interrupt_hook) (char *);EXPORT_SYMBOL(atari_input_keyboard_interrupt_hook": "atari_input_keyboard_interrupt_hook) (unsigned char, char);  Hook for mouse inputdev  driver ", "size = PAGE_ALIGN(size);res = kzalloc(sizeof(struct resource), GFP_KERNEL);if (!res)return NULL;res->name = owner;error = allocate_resource(&stram_pool, res, size, 0, UINT_MAX,  PAGE_SIZE, NULL, NULL);if (error < 0) ": "atari_stram_alloc(unsigned long size, const char  owner){struct resource  res;int error;pr_debug(\"atari_stram_alloc: allocate %lu bytes\\n\", size);  round up ", "static inline int refill(void)": "dvma_frees;static unsigned long long dvma_alloc_bytes;static unsigned long long dvma_free_bytes;static void print_use(void){int i;int j = 0;pr_info(\"dvma entry usage:\\n\");for(i = 0; i < IOMMU_TOTAL_ENTRIES; i++) {if(!iommu_use[i])continue;j++;pr_info(\"dvma entry: %08x len %08lx\\n\",(i << DVMA_PAGE_SHIFT) + DVMA_START, iommu_use[i]);}pr_info(\"%d entries in use total\\n\", j);pr_info(\"allocationfree calls: %lu%lu\\n\", dvma_allocs, dvma_frees);pr_info(\"allocationfree bytes: %Lx%Lx\\n\", dvma_alloc_bytes,dvma_free_bytes);}static void print_holes(struct list_head  holes){struct list_head  cur;struct hole  hole;pr_info(\"listing dvma holes\\n\");list_for_each(cur, holes) {hole = list_entry(cur, struct hole, list);if((hole->start == 0) && (hole->end == 0) && (hole->size == 0))continue;pr_info(\"hole: start %08lx end %08lx size %08lx\\n\",hole->start, hole->end, hole->size);}pr_info(\"end of hole listing...\\n\");}#endif   DVMA_DEBUG ", "#include <linux/module.h>#include <linux/kernel.h>#include <linux/types.h>#include <linux/init.h>#include <linux/string.h>#include <asm/oplib.h>#include <asm/idprom.h>#include <asm/machines.h>  /* Fun with Sun released architectures. ": "idprom.c: Routines to load the idprom into kernel addresses and             interpret the data contained within.     Copyright (C) 1995 David S. Miller (davem@caip.rutgers.edu)   Sun33x models added by David Monro (davidm@psrg.cs.usyd.edu.au) ", "if (gpio < MCFGPIO_SCR_START) ": "__mcfgpio_direction_output(unsigned gpio, int value){unsigned long flags;MCFGPIO_PORTTYPE data;local_irq_save(flags);data = mcfgpio_read(__mcfgpio_pddr(gpio));data |= mcfgpio_bit(gpio);mcfgpio_write(data, __mcfgpio_pddr(gpio));  now set the data to output ", "#endif /* MCFPM_PPMCR0 ": "clk_disable0(struct clk  clk){__raw_writeb(clk->slot, MCFPM_PPMSR0);}struct clk_ops clk_ops0 = {.enable= __clk_enable0,.disable= __clk_disable0,};#ifdef MCFPM_PPMCR1static void __clk_enable1(struct clk  clk){__raw_writeb(clk->slot, MCFPM_PPMCR1);}static void __clk_disable1(struct clk  clk){__raw_writeb(clk->slot, MCFPM_PPMSR1);}struct clk_ops clk_ops1 = {.enable= __clk_enable1,.disable= __clk_disable1,};#endif   MCFPM_PPMCR1 ", "struct mac_model *macintosh_config;EXPORT_SYMBOL(macintosh_config": "macintosh_config->ident == MAC_MODEL_IICI)mach_l2_flush = via_l2_flush;register_platform_power_off(mac_poweroff);}    Macintosh Table: hardcoded model configuration data.     Much of this was defined by Alan, based on who knows what docs.   I've added a lot more, and some of that was pure guesswork based   on hardware pages present on the Mac web site. Possibly wildly   inaccurate, so look here if a new Mac model won't run. Example: if   a Mac crashes immediately after the VIA1 registers have been dumped   to the screen, it probably died attempting to read DirB on a RBV.   Meaning it should have MAC_VIA_IICI here :-) ", "if (IS_ENABLED(CONFIG_PARAVIRT_SPINLOCKS) && is_shared_processor()) ": "queued_spin_lock_slowpath(struct qspinlock  lock){    This looks funny, but it induces the compiler to inline both   sides of the branch rather than share code as when the condition   is passed as the paravirt argument to the functions. ", "crash_shutdown_handles[i] = handler;rc = 0;break;}if (i == CRASH_HANDLER_MAX) ": "crash_shutdown_register(crash_shutdown_t handler){unsigned int i, rc;spin_lock(&crash_handlers_lock);for (i = 0 ; i < CRASH_HANDLER_MAX; i++)if (!crash_shutdown_handles[i]) {  Insert handle at first empty entry ", "for (; i < (CRASH_HANDLER_MAX - 1); i++)crash_shutdown_handles[i] =crash_shutdown_handles[i+1];/* * Reset last entry to NULL now that it has been shifted down, * this will allow new handles to be added here. ": "crash_shutdown_unregister(crash_shutdown_t handler){unsigned int i, rc;spin_lock(&crash_handlers_lock);for (i = 0 ; i < CRASH_HANDLER_MAX; i++)if (crash_shutdown_handles[i] == handler)break;if (i == CRASH_HANDLER_MAX) {printk(KERN_ERR \"Crash shutdown handle not found\\n\");rc = 1;} else {  Shift handles down ", "void flush_icache_range(unsigned long start, unsigned long stop)": "flush_icache_range: Write any modified data cache blocks out to memory   and invalidate the corresponding blocks in the instruction cache     Generic code will call this after writing memory, before executing from it.     @start: the start address   @stop: the stop address (exclusive) ", "flush_dcache_page(pg);}EXPORT_SYMBOL(clear_user_page": "clear_user_page(void  page, unsigned long vaddr, struct page  pg){clear_page(page);    We shouldn't have to do this, but some versions of glibc   require it (ld.so assumes zero filled pages are icache clean)   - Anton ", "distance *= 2;}return distance;}EXPORT_SYMBOL(__node_distance": "__node_distance(int a, int b){int i;int distance = LOCAL_DISTANCE;if (affinity_form == FORM2_AFFINITY)return numa_distance_table[a][b];else if (affinity_form == FORM0_AFFINITY)return ((a == b) ? LOCAL_DISTANCE : REMOTE_DISTANCE);for (i = 0; i < distance_ref_points_depth; i++) {if (distance_lookup_table[a][i] == distance_lookup_table[b][i])break;  Double the distance for each NUMA level ", "int of_node_to_nid(struct device_node *device)": "of_node_to_nid_single(struct device_node  device){int nid = NUMA_NO_NODE;const __be32  tmp;tmp = of_get_associativity(device);if (tmp)nid = associativity_to_nid(tmp);return nid;}  Walk the device tree upwards, looking for an associativity id ", "if (v_block_mapped((unsigned long)addr))return;if (addr > high_memory && (unsigned long)addr < ioremap_bot)vunmap((void *)(PAGE_MASK & (unsigned long)addr));}EXPORT_SYMBOL(iounmap": "iounmap(volatile void __iomem  addr){    If mapped by BATs then there is nothing to do.   Calling vfree() generates a benign warning. ", "if (pte_write(pte))pte = pte_mkdirty(pte);/* we don't want to let _PAGE_USER and _PAGE_EXEC leak out ": "ioremap_prot(phys_addr_t addr, unsigned long size, unsigned long flags){pte_t pte = __pte(flags);void  caller = __builtin_return_address(0);  writeable implies dirty for kernel addresses ", "#ifndef CONFIG_PPC_8xx/* * These are the base non-SMP variants of page and mm flushing ": "flush_tlb_range(vma, start, end) flushes a range of pages    - flush_tlb_kernel_range(start, end) flushes kernel pages      - local_  variants of page and mm only apply to the current      processor ", "if (radix_enabled() && ((vm_flags & VM_ACCESS_FLAGS) == VM_EXEC)) ": "vm_get_page_prot(unsigned long vm_flags){unsigned long prot;  Radix supports execute-only, but protection_map maps X -> RX ", "if (is_vm_hugetlb_page(vma))return radix__local_flush_hugetlb_page(vma, vmaddr);#endifradix__local_flush_tlb_page_psize(vma->vm_mm, vmaddr, mmu_virtual_psize);}EXPORT_SYMBOL(radix__local_flush_tlb_page": "radix__local_flush_tlb_page_psize(struct mm_struct  mm, unsigned long vmaddr,       int psize){unsigned long pid = mm->context.id;if (WARN_ON_ONCE(pid == MMU_NO_CONTEXT))return;preempt_disable();_tlbiel_va(vmaddr, pid, psize, RIC_FLUSH_TLB);preempt_enable();}void radix__local_flush_tlb_page(struct vm_area_struct  vma, unsigned long vmaddr){#ifdef CONFIG_HUGETLB_PAGE  need the return fix for nohash.c ", "smp_mb();type = flush_type_needed(mm, false);if (type == FLUSH_TYPE_LOCAL) ": "radix__flush_tlb_mm(struct mm_struct  mm){unsigned long pid;enum tlb_flush_type type;pid = mm->context.id;if (WARN_ON_ONCE(pid == MMU_NO_CONTEXT))return;preempt_disable();    Order loads of mm_cpumask (in flush_type_needed) vs previous   stores to clear ptes before the invalidate. See barrier in   switch_mm_irqs_off ", "type = flush_type_needed(mm, false);if (type == FLUSH_TYPE_LOCAL) ": "radix__flush_tlb_page_psize(struct mm_struct  mm, unsigned long vmaddr, int psize){unsigned long pid;enum tlb_flush_type type;pid = mm->context.id;if (WARN_ON_ONCE(pid == MMU_NO_CONTEXT))return;preempt_disable();smp_mb();   see radix__flush_tlb_mm ", "type = flush_type_needed(mm, false);if (type == FLUSH_TYPE_NONE)goto out;if (type == FLUSH_TYPE_GLOBAL)flush_pid = nr_pages > tlb_single_page_flush_ceiling;elseflush_pid = nr_pages > tlb_local_single_page_flush_ceiling;/* * full pid flush already does the PWC flush. if it is not full pid * flush check the range is more than PMD and force a pwc flush * mremap() depends on this behaviour. ": "radix__flush_tlb_range(struct mm_struct  mm,    unsigned long start, unsigned long end){unsigned long pid;unsigned int page_shift = mmu_psize_defs[mmu_virtual_psize].shift;unsigned long page_size = 1UL << page_shift;unsigned long nr_pages = (end - start) >> page_shift;bool flush_pid, flush_pwc = false;enum tlb_flush_type type;pid = mm->context.id;if (WARN_ON_ONCE(pid == MMU_NO_CONTEXT))return;WARN_ON_ONCE(end == TLB_FLUSH_ALL);preempt_disable();smp_mb();   see radix__flush_tlb_mm ", "isync();}EXPORT_SYMBOL(switch_mmu_context": "switch_mmu_context(struct mm_struct  prev, struct mm_struct  next, struct task_struct  tsk){long id = next->context.id;if (id < 0)panic(\"mm_struct %p has no context ID\", next);isync();update_user_segments(next->context.sr0);if (IS_ENABLED(CONFIG_BDI_SWITCH))abatron_pteptrs[1] = next->pgd;if (!mmu_has_feature(MMU_FTR_HPTE_TABLE))mtspr(SPRN_SDR1, rol32(__pa(next->pgd), 4) & 0xffff01ff);mb();  sync ", "for_each_vma(vmi, mp)hash__flush_range(mp->vm_mm, mp->vm_start, mp->vm_end);}EXPORT_SYMBOL(hash__flush_tlb_mm": "hash__flush_tlb_mm(struct mm_struct  mm){struct vm_area_struct  mp;VMA_ITERATOR(vmi, mm, 0);    It is safe to iterate the vmas when called from dup_mmap,   holding mmap_lock.  It would also be safe from unmap_region   or exit_mmap, but not from vmtruncate on SMP - but it seems   dup_mmap is the only SMP case which gets here. ", "bp = &cpmp->cp_brgc1;bp += brg;/* * The BRG has a 12-bit counter.  For really slow baud rates (or * really fast processors), we may have to further divide by 16. ": "cpm_setbrg(uint brg, uint rate){u32 __iomem  bp;  This is good enough to get SMCs running..... ", "int ps3_irq_plug_destroy(unsigned int virq)": "ps3_sb_event_receive_port_setup(). ", "int result;DBG(\" -> %s:%d: interrupt_id %u, virq %u\\n\", __func__, __LINE__,dev->interrupt_id, virq);result = lv1_disconnect_interrupt_event_receive_port(dev->bus_id,dev->dev_id, virq_to_hw(virq), dev->interrupt_id);if (result)FAIL(\"%s:%d: lv1_disconnect_interrupt_event_receive_port\"\" failed: %s\\n\", __func__, __LINE__,ps3_result(result));result = ps3_event_receive_port_destroy(virq);BUG_ON(result);/* * ps3_event_receive_port_destroy() destroys the IRQ plug, * so don't call ps3_irq_plug_destroy() here. ": "ps3_sb_event_receive_port_destroy(struct ps3_system_bus_device  dev,unsigned int virq){  this should go in system-bus.c ", "unsigned long ps3_mm_phys_to_lpar(unsigned long phys_addr)": "ps3_mm_phys_to_lpar - translate a linux physical address to lpar address   @phys_addr: linux physical address ", "int vio_cmo_entitlement_update(size_t new_entitlement)": "vio_cmo_entitlement_update - Manage system entitlement changes     @new_entitlement: new system entitlement to attempt to accommodate     Increases in entitlement will be used to fulfill the spare entitlement   and the rest is given to the excess pool.  Decreases, if they are   possible, come from the excess pool and from unused device entitlement     Returns: 0 on success, -ENOMEM when change can not be made ", "void vio_cmo_set_dev_desired(struct vio_dev *viodev, size_t desired)": "vio_cmo_set_dev_desired - Set desired entitlement for a device     @viodev: struct vio_dev for device to alter   @desired: new desired entitlement level in bytes     For use by devices to request a change to their entitlement at runtime or   through sysfs.  The desired entitlement level is changed and a balancing   of system resources is scheduled to run in the future. ", "int vio_h_cop_sync(struct vio_dev *vdev, struct vio_pfo_op *op)": "vio_h_cop_sync - Perform a synchronous PFO co-processor operation     @vdev - Pointer to a struct vio_dev for device   @op - Pointer to a struct vio_pfo_op for the operation parameters     Calls the hypervisor to synchronously perform the PFO operation   described in @op.  In the case of a busy response from the hypervisor,   the operation will be re-submitted indefinitely unless a non-zero timeout   is specified or an error occurs. The timeout places a limit on when to   stop re-submitting a operation, the total time can be exceeded if an   operation is in progress.     If op->hcall_ret is not NULL, this will be set to the return from the   last h_cop_op call or it will be 0 if an error not involving the h_call   was encountered.     Returns:  0 on success,  -EINVAL if the h_call fails due to an invalid parameter,  -E2BIG if the h_call can not be performed synchronously,  -EBUSY if a timeout is specified and has elapsed,  -EACCES if the memory area for datastatus has been rescinded, or  -EPERM if a hardware fault has been indicated ", "viodrv->driver.name = viodrv->name;viodrv->driver.pm = viodrv->pm;viodrv->driver.bus = &vio_bus_type;viodrv->driver.owner = owner;viodrv->driver.mod_name = mod_name;return driver_register(&viodrv->driver);}EXPORT_SYMBOL(__vio_register_driver": "__vio_register_driver(struct vio_driver  viodrv, struct module  owner,  const char  mod_name){ vio_bus_type is only initialised for pseriesif (!machine_is(pseries))return -ENODEV;pr_debug(\"%s: driver %s registering\\n\", __func__, viodrv->name);  fill in 'struct driver' fields ", "void vio_unregister_driver(struct vio_driver *viodrv)": "vio_unregister_driver - Remove registration of vio driver.   @viodrv:The vio_driver struct to be removed form registration ", "struct vio_dev *vio_register_device_node(struct device_node *of_node)": "vio_register_device_node: - Register a new vio device.   @of_node:The OF node for this device.     Creates and initializes a vio_dev structure from the data in   of_node and adds it to the list of virtual devices.   Returns a pointer to the created vio_dev or NULL if node has   NULL device_type or compatible fields. ", "const void *vio_get_attribute(struct vio_dev *vdev, char *which, int *length)": "vio_get_attribute: - get attribute for virtual device   @vdev:The vio device to get property.   @which:The propertyattribute to be extracted.   @length:Pointer to length of returned data size (unused if NULL).     Calls prom.c's of_get_property() to return the value of the   attribute specified by @which", "struct vio_dev *vio_find_node(struct device_node *vnode)": "vio_find_node - find an already-registered vio_dev   @vnode: device_node of the virtual device we're looking for     Takes a reference to the embedded struct device which needs to be dropped   after use. ", "ibmebus_create_devices(drv->driver.of_match_table);drv->driver.bus = &ibmebus_bus_type;return driver_register(&drv->driver);}EXPORT_SYMBOL(ibmebus_register_driver": "ibmebus_register_driver(struct platform_driver  drv){  If the driver uses devices that ibmebus doesn't know, add them ", "static const struct of_device_id ibmebus_matches[] __initconst = ": "ibmebus_bus_type;  These devices will automatically be added to the bus during init ", "int hvc_get_chars(uint32_t vtermno, char *buf, int count)": "hvc_get_chars - retrieve characters from firmware for denoted vterm adapter   @vtermno: The vtermno or unit_address of the adapter from which to fetch the  data.   @buf: The character buffer into which to put the character data fetched from  firmware.   @count: not used? ", "int hvc_put_chars(uint32_t vtermno, const char *buf, int count)": "hvc_put_chars: send characters to firmware for denoted vterm adapter   @vtermno: The vtermno or unit_address of the adapter from which the data  originated.   @buf: The character buffer that contains the character data to send to  firmware. Must be at least 16 bytes, even if count is less than 16.   @count: Send this number of characters. ", "int h_get_mpp(struct hvcall_mpp_data *mpp_data)": "h_get_mpp   H_GET_MPP hcall returns info in 7 parms ", "data->drc_type = (char *)p;p = of_prop_next_string(*prop, p);if (!p)return -EINVAL;/* Get drc-name-prefix:encode-string ": "of_read_drc_info_cell(struct property   prop, const __be32   curval,struct of_drc_info  data){const char  p = (char  )( curval);const __be32  p2;if (!data)return -EINVAL;  Get drc-type:encode-string ", "int hvcs_free_partner_info(struct list_head *head)": "hvcs_get_partner_info   @head: list_head pointer for an allocated list of partner info structs to  free.     This function is used to free the partner info list that was returned by   calling hvcs_get_partner_info(). ", "int hvcs_register_connection( uint32_t unit_address,uint32_t p_partition_ID, uint32_t p_unit_address)": "hvcs_register_connection - establish a connection between this vty-server and  a vty.   @unit_address: The unit address of the vty-server adapter that is to be  establish a connection.   @p_partition_ID: The partition ID of the vty adapter that is to be connected.   @p_unit_address: The unit address of the vty adapter to which the vty-server  is to be connected.     If this function is called once and -EINVAL is returned it may   indicate that the partner info needs to be refreshed for the   target unit address at which point the caller must invoke   hvcs_get_partner_info() and then call this function again.  If,   for a second time, -EINVAL is returned then it indicates that   there is probably already a partner connection registered to a   different vty-server adapter.  It is also possible that a second   -EINVAL may indicate that one of the parms is not valid, for   instance if the link was removed between the vty-server adapter   and the vty adapter that you are trying to open.  Don't shoot the   messenger.  Firmware implemented it this way. ", "int hvcs_free_connection(uint32_t unit_address)": "hvcs_free_connection - free the connection between a vty-server and vty   @unit_address: The unit address of the vty-server that is to have its  connection severed.     This function is used to free the partner connection between a vty-server   adapter and a vty adapter.     If -EBUSY is returned continue to call this function until 0 is returned. ", "for (i = 1; i < CXL_IRQ_RANGES && num; i++) ": "pnv_cxl_alloc_hwirq_ranges(struct cxl_irq_ranges  irqs,       struct pci_dev  dev, int num){struct pci_controller  hose = pci_bus_to_host(dev->bus);struct pnv_phb  phb = hose->private_data;int i, hwirq, try;memset(irqs, 0, sizeof(struct cxl_irq_ranges));  0 is reserved for the multiplexed PSL DSI interrupt ", "rc = opal_pci_set_xive_pe(phb->opal_id, pe->pe_number, xive_num);if (rc) ": "pnv_cxl_ioda_msi_setup(struct pci_dev  dev, unsigned int hwirq,   unsigned int virq){struct pci_controller  hose = pci_bus_to_host(dev->bus);struct pnv_phb  phb = hose->private_data;unsigned int xive_num = hwirq - phb->msi_base;struct pnv_ioda_pe  pe;int rc;if (!(pe = pnv_ioda_get_pe(dev)))return -ENODEV;  Assign XIVE to PE ", "dn = of_find_compatible_node(NULL, NULL, \"ibm,opal-event\");opal_event_irqchip.domain = irq_domain_add_linear(dn, MAX_NUM_EVENTS,&opal_event_domain_ops, &opal_event_irqchip);of_node_put(dn);if (!opal_event_irqchip.domain) ": "opal_event_request(...))   anyway. ", "struct mpc52xx_gpt_priv *mpc52xx_gpt_from_irq(int irq)": "mpc52xx_gpt_from_irq - Return the GPT device associated with an IRQ number   @irq: irq of timer. ", "int mpc52xx_gpt_start_timer(struct mpc52xx_gpt_priv *gpt, u64 period,                            int continuous)": "mpc52xx_gpt_start_timer - Set and enable the GPT timer   @gpt: Pointer to gpt private data structure   @period: period of timer in ns; max. ~130s @ 33MHz IPB clock   @continuous: set to 1 to make timer continuous free running     An interrupt will be generated every time the timer fires ", "int mpc52xx_gpt_stop_timer(struct mpc52xx_gpt_priv *gpt)": "mpc52xx_gpt_stop_timer - Stop a gpt   @gpt: Pointer to gpt private data structure     Returns an error if attempting to stop a wdt ", "u64 mpc52xx_gpt_timer_period(struct mpc52xx_gpt_priv *gpt)": "mpc52xx_gpt_timer_period - Read the timer period   @gpt: Pointer to gpt private data structure     Returns the timer period in ns ", "int mpc52xx_set_psc_clkdiv(int psc_id, int clkdiv)": "mpc52xx_set_psc_clkdiv: Set clock divider in the CDM for PSC ports     @psc_id: id of psc port; must be 1,2,3 or 6   @clkdiv: clock divider value to put into CDM PSC register. ", "static DEFINE_SPINLOCK(gpio_lock);struct mpc52xx_gpio __iomem *simple_gpio;struct mpc52xx_gpio_wkup __iomem *wkup_gpio;/** * mpc52xx_declare_of_platform_devices: register internal devices and children *of the localplus bus to the of_platform *bus. ": "mpc5200_psc_ac97_gpio_reset(). ", "unsigned int pasemi_read_iob_reg(unsigned int reg)": "pasemi_read_iob_reg - read IOB register   @reg: Register to read (offset into PCI CFG space) ", "void pasemi_write_iob_reg(unsigned int reg, unsigned int val)": "pasemi_write_iob_reg - write IOB register   @reg: Register to write to (offset into PCI CFG space)   @val: Value to write ", "unsigned int pasemi_read_mac_reg(int intf, unsigned int reg)": "pasemi_read_mac_reg - read MAC register   @intf: MAC interface   @reg: Register to read (offset into PCI CFG space) ", "void pasemi_write_mac_reg(int intf, unsigned int reg, unsigned int val)": "pasemi_write_mac_reg - write MAC register   @intf: MAC interface   @reg: Register to write to (offset into PCI CFG space)   @val: Value to write ", "unsigned int pasemi_read_dma_reg(unsigned int reg)": "pasemi_read_dma_reg - read DMA register   @reg: Register to read (offset into PCI CFG space) ", "void pasemi_write_dma_reg(unsigned int reg, unsigned int val)": "pasemi_write_dma_reg - write DMA register   @reg: Register to write to (offset into PCI CFG space)   @val: Value to write ", "void *pasemi_dma_alloc_chan(enum pasemi_dmachan_type type,    int total_size, int offset)": "pasemi_dma_alloc_chan - Allocate a DMA channel   @type: Type of channel to allocate   @total_size: Total size of structure to allocate (to allow for more  room behind the structure to be used by the client)   @offset: Offset in bytes from start of the total structure to the beginning      of struct pasemi_dmachan. Needed when struct pasemi_dmachan is      not the first member of the client structure.     pasemi_dma_alloc_chan allocates a DMA channel for use by a client. The   type argument specifies whether it's a RX or TX channel, and in the case   of TX channels which group it needs to belong to (if any).     Returns a pointer to the total structure allocated on success, NULL   on failure. ", "void pasemi_dma_free_chan(struct pasemi_dmachan *chan)": "pasemi_dma_free_chan - Free a previously allocated channel   @chan: Channel to free     Frees a previously allocated channel. It will also deallocate any   descriptor ring associated with the channel, if allocated. ", "int pasemi_dma_alloc_ring(struct pasemi_dmachan *chan, int ring_size)": "pasemi_dma_free_ring(chan);switch (chan->chan_type & (RXCHAN|TXCHAN)) {case RXCHAN:pasemi_free_rx_chan(chan->chno);break;case TXCHAN:pasemi_free_tx_chan(chan->chno);break;}kfree(chan->priv);}EXPORT_SYMBOL(pasemi_dma_free_chan);  pasemi_dma_alloc_ring - Allocate descriptor ring for a channel   @chan: Channel for which to allocate   @ring_size: Ring size in 64-bit (8-byte) words     Allocate a descriptor ring for a channel. Returns 0 on success, errno   on failure. The passed in struct pasemi_dmachan is updated with the   virtual and DMA addresses of the ring. ", "void pasemi_dma_start_chan(const struct pasemi_dmachan *chan, const u32 cmdsta)": "pasemi_dma_start_chan - Start a DMA channel   @chan: Channel to start   @cmdsta: Additional CCMDSTATCMDSTA bits to write     Enables (starts) a DMA channel with optional additional arguments. ", "#define MAX_RETRIES 5000int pasemi_dma_stop_chan(const struct pasemi_dmachan *chan)": "pasemi_dma_stop_chan - Stop a DMA channel   @chan: Channel to stop     Stops (disables) a DMA channel. This is done by setting the ST bit in the   CMDSTA register and waiting on the ACT (active) bit to clear, then   finally disabling the whole channel.     This function will only try for a short while for the channel to stop, if   it doesn't it will return failure.     Returns 1 on success, 0 on failure. ", "void *pasemi_dma_alloc_buf(struct pasemi_dmachan *chan, int size,   dma_addr_t *handle)": "pasemi_dma_alloc_buf - Allocate a buffer to use for DMA   @chan: Channel to allocate for   @size: Size of buffer in bytes   @handle: DMA handle     Allocate a buffer to be used by the DMA engine for readwrite,   similar to dma_alloc_coherent().     Returns the virtual address of the buffer, or NULL in case of failure. ", "void pasemi_dma_free_buf(struct pasemi_dmachan *chan, int size, dma_addr_t *handle)": "pasemi_dma_free_buf - Free a buffer used for DMA   @chan: Channel the buffer was allocated for   @size: Size of buffer in bytes   @handle: DMA handle     Frees a previously allocated buffer. ", "int pasemi_dma_alloc_flag(void)": "pasemi_dma_alloc_flag - Allocate a flag (event) for channel synchronization     Allocates a flag for use with channel synchronization (event descriptors).   Returns allocated flag (0-63), < 0 on error. ", "void pasemi_dma_free_flag(int flag)": "pasemi_dma_free_flag - Deallocates a flag (event)   @flag: Flag number to deallocate     Frees up a flag so it can be reused for other purposes. ", "void pasemi_dma_set_flag(int flag)": "pasemi_dma_set_flag - Sets a flag (event) to 1   @flag: Flag number to set active     Sets the flag provided to 1. ", "void pasemi_dma_clear_flag(int flag)": "pasemi_dma_clear_flag - Sets a flag (event) to 0   @flag: Flag number to set inactive     Sets the flag provided to 0. ", "int pasemi_dma_alloc_fun(void)": "pasemi_dma_alloc_fun - Allocate a function engine     Allocates a function engine to use for cryptochecksum offload   Returns allocated engine (0-8), < 0 on error. ", "void pasemi_dma_free_fun(int fun)": "pasemi_dma_free_fun - Deallocates a function engine   @flag: Engine number to deallocate     Frees up a function engine so it can be used for other purposes. ", "int pasemi_dma_init(void)": "pasemi_dma_init - Initialize the PA Semi DMA library     This function initializes the DMA library. It must be called before   any other function in the library.     Returns 0 on success, errno on failure. ", "},[WARP_RED_LED] = ": "pika_dtm_unregister_shutdown(void ( func)(void  arg), void  arg){struct dtm_shutdown  shutdown;list_for_each_entry(shutdown, &dtm_shutdown_list, list)if (shutdown->func == func && shutdown->arg == arg) {list_del(&shutdown->list);kfree(shutdown);return 0;}return -EINVAL;}#define WARP_GREEN_LED0#define WARP_RED_LED1static struct gpio_led warp_gpio_led_pins[] = {[WARP_GREEN_LED] = {.name= \"green\",.default_state= LEDS_DEFSTATE_KEEP,.gpiod= NULL,   to be filled by pika_setup_leds() ", "int mpc512x_cs_config(unsigned int cs, u32 val)": "mpc512x_cs_config - Setup chip select configuration   @cs: chip select number   @val: chip select configuration value     Perform chip select configuration for devices on LocalPlus Bus.   Intended to dynamically reconfigure the chip select parameters   for configurable devices on the bus. ", "if (lpbfifo.req)return -EBUSY;lpbfifo.wait_lpbfifo_irq = true;lpbfifo.wait_lpbfifo_callback = true;lpbfifo.req = req;ret = mpc512x_lpbfifo_kick();if (ret != 0)lpbfifo.req = NULL; /* Set the FIFO as idle ": "mpc512x_lpbfifo_submit_locked(struct mpc512x_lpbfifo_request  req){int ret = 0;if (!lpbfifo.regs)return -ENODEV;  Check whether a transfer is in progress ", "void mpic_start_timer(struct mpic_timer *handle)": "mpic_start_timer - start hardware timer   @handle: the timer to be started.     It will do ->fn(->dev) callback from the hardware interrupt at   the 'time64_t' point in the future. ", "void mpic_stop_timer(struct mpic_timer *handle)": "mpic_stop_timer - stop hardware timer   @handle: the timer to be stopped     The timer periodically generates an interrupt. Unless user stops the timer. ", "void mpic_get_remain_time(struct mpic_timer *handle, time64_t *time)": "mpic_get_remain_time - get timer time   @handle: the timer to be selected.   @time: time for timer     Query timer remaining time. ", "void mpic_free_timer(struct mpic_timer *handle)": "mpic_free_timer - free hardware timer   @handle: the timer to be removed.     Free the timer.     Note: can not be used in interrupt context. ", "struct mpic_timer *mpic_request_timer(irq_handler_t fn, void *dev,      time64_t time)": "mpic_request_timer - get a hardware timer   @fn: interrupt handler function   @dev: callback function of the data   @time: time for timer     This executes the \"request_irq\", returning NULL   else \"handle\" on success. ", "unsigned long mpc5xxx_fwnode_get_bus_frequency(struct fwnode_handle *fwnode)": "mpc5xxx_fwnode_get_bus_frequency - Find the bus frequency for a firmware node   @fwnode:firmware node     Returns bus frequency (IPS on MPC512x, IPB on MPC52xx),   or 0 if the bus frequency cannot be found. ", "void gtm_put_timer16(struct gtm_timer *tmr)": "gtm_put_timer16 - release 16 bits GTM timer   @tmr:pointer to the gtm_timer structure obtained from gtm_get_timer   Context:any     This function releases GTM timer so others may request it. ", "int gtm_set_timer16(struct gtm_timer *tmr, unsigned long usec, bool reload)": "gtm_set_timer16 - (re)set 16 bit timer with arbitrary precision   @tmr:pointer to the gtm_timer structure obtained from gtm_get_timer   @usec:timer interval in microseconds   @reload:if set, the timer will reset upon expiry rather than           continue running free.   Context:any     This function (re)sets the GTM timer so that it counts up to the requested   interval value, and fires the interrupt when the value is reached. This   function will reduce the precision of the timer as needed in order for the   requested timeout to fit in a 16-bit register. ", "const int freq = 1000000;/* * We can lower the frequency (and probably power consumption) by * dividing both frequency and usec by 2 until there is no remainder. * But we won't bother with this unless savings are measured, so just * run the timer as is. ": "gtm_set_exact_timer16(struct gtm_timer  tmr, u16 usec, bool reload){  quite obvious, frequency which is enough for \u00b5Sec precision ", "static int gtm_set_ref_timer16(struct gtm_timer *tmr, int frequency,       int reference_value, bool free_run)": "gtm_stop_timer16(tmr);spin_lock_irq(&tmr->gtm->lock);tmr->requested = false;spin_unlock_irq(&tmr->gtm->lock);}EXPORT_SYMBOL(gtm_put_timer16);    This is back-end for the exported functions, it's used to reset single   timer in reference mode. ", "void gtm_ack_timer16(struct gtm_timer *tmr, u16 events)": "gtm_ack_timer16 - acknowledge timer event (free-run timers only)   @tmr:pointer to the gtm_timer structure obtained from gtm_get_timer   @events:events mask to ack   Context:any     Thus function used to acknowledge timer interrupt event, use it inside the   interrupt handler. ", "if (brg < 4) ": "__cpm2_setbrg(uint brg, uint rate, uint clk, int div16, int src){u32 __iomem  bp;u32 val;  This is good enough to get SMCs running.....", "u32 fsl_lbc_addr(phys_addr_t addr_base)": "fsl_lbc_addr - convert the base address   @addr_base:base address of the memory bank     This function converts a base address of lbc into the right format for the   BR register. If the SOC has eLBC then it returns 32bit physical address   else it converts a 34bit local bus physical address to correct format of   32bit address for BR register (Example: MPC8641). ", "int fsl_lbc_find(phys_addr_t addr_base)": "fsl_lbc_find - find Localbus bank   @addr_base:base address of the memory bank     This function walks LBC banks comparing \"Base address\" field of the BR   registers with the supplied addr_base argument. When bases match this   function returns bank number (starting with 0), otherwise it returns   appropriate errno value. ", "int fsl_upm_find(phys_addr_t addr_base, struct fsl_upm *upm)": "fsl_upm_find - find pre-programmed UPM via base address   @addr_base:base address of the memory bank controlled by the UPM   @upm:pointer to the allocated fsl_upm structure     This function fills fsl_upm structure so you can use it with the rest of   UPM API. On success this function returns 0, otherwise it returns   appropriate errno value. ", "int fsl_upm_run_pattern(struct fsl_upm *upm, void __iomem *io_base, u32 mar)": "fsl_upm_run_pattern - actually run an UPM pattern   @upm:pointer to the fsl_upm structure obtained via fsl_upm_find   @io_base:remapped pointer to where memory access should happen   @mar:MAR register content during pattern execution     This function triggers dummy write to the memory specified by the io_base,   thus UPM pattern actually executed. Note that mar usage depends on the   pre-programmed AMX bits in the UPM RAM. ", "node = of_find_node_by_type(NULL, \"cpm\");if (!node)node = of_find_compatible_node(NULL, NULL, \"fsl,qe\");if (!node)node = of_find_node_by_type(NULL, \"qe\");if (node) ": "get_brgfreq(void){static u32 brgfreq = -1;struct device_node  node;if (brgfreq != -1)return brgfreq;node = of_find_compatible_node(NULL, NULL, \"fsl,cpm-brg\");if (node) {of_property_read_u32(node, \"clock-frequency\", &brgfreq);of_node_put(node);return brgfreq;}  Legacy device binding -- will go away when no users are left. ", "unsigned long pci_io_base;EXPORT_SYMBOL(pci_io_base": "pci_io_base -- the base address from which io bars are offsets.   This is the lowest IO base address (so bar values are always positive),   and it  must  be the start of ISA space if an ISA bus exists because   ISA drivers use hard coded offsets.  If no ISA bus exists nothing   is mapped on the first 64K of IO space ", "EXPORT_SYMBOL(isa_io_base": "isa_io_base;  NULL if no ISA bus ", "/* If a device driver keeps reading an MMIO register in an interrupt * handler after a slot isolation event, it might be broken. * This sets the threshold for how many read attempts we allow * before printing an error message. ": "eeh_check_failure() routine embedded in the MMIO macros.  If    the slot is found to be isolated, an \"EEH Event\" is synthesized    and sent out for processing. ", "void eeh_dev_release(struct pci_dev *pdev)": "eeh_dev_release - Decrease count of pass through devices for PE   @pdev: PCI device     Decrease count of pass through devices for the indicated PE. If   there is no passed through device in PE, the EEH errors detected   on the PE will be reported and handled as usual. ", "struct machdep_calls ppc_md;EXPORT_SYMBOL(ppc_md);struct machdep_calls *machine_id;EXPORT_SYMBOL(machine_id);int boot_cpuid = -1;EXPORT_SYMBOL_GPL(boot_cpuid);#ifdef CONFIG_PPC64int boot_cpu_hwid = -1;#endif/* * These are used in binfmt_elf.c to put aux entries on the stack * for each elf executable being started. ": "screen_info.h>#include <linuxroot_dev.h>#include <linuxcpu.h>#include <linuxunistd.h>#include <linuxseq_buf.h>#include <linuxserial.h>#include <linuxserial_8250.h>#include <linuxpercpu.h>#include <linuxmemblock.h>#include <linuxof_irq.h>#include <linuxof_fdt.h>#include <linuxof_platform.h>#include <linuxhugetlb.h>#include <linuxpgtable.h>#include <asmio.h>#include <asmpaca.h>#include <asmprocessor.h>#include <asmvdso_datapage.h>#include <asmsmp.h>#include <asmelf.h>#include <asmmachdep.h>#include <asmtime.h>#include <asmcputable.h>#include <asmsections.h>#include <asmfirmware.h>#include <asmbtext.h>#include <asmnvram.h>#include <asmsetup.h>#include <asmrtas.h>#include <asmiommu.h>#include <asmserial.h>#include <asmcache.h>#include <asmpage.h>#include <asmmmu.h>#include <asmxmon.h>#include <asmcputhreads.h>#include <mmmmu_decl.h>#include <asmarchrandom.h>#include <asmfadump.h>#include <asmudbg.h>#include <asmhugetlb.h>#include <asmlivepatch.h>#include <asmmmu_context.h>#include <asmcpu_has_feature.h>#include <asmkasan.h>#include <asmmce.h>#include \"setup.h\"#ifdef DEBUG#define DBG(fmt...) udbg_printf(fmt)#else#define DBG(fmt...)#endif  The main machine-dep calls structure ", "if (!np)np = of_find_node_by_name(NULL, \"8042\");if (np) ": "check_legacy_ioport(unsigned long base_port){struct device_node  parent,  np = NULL;int ret = -ENODEV;switch(base_port) {case I8042_DATA_REG:if (!(np = of_find_compatible_node(NULL, NULL, \"pnpPNP,303\")))np = of_find_compatible_node(NULL, NULL, \"pnpPNP,f03\");if (np) {parent = of_get_parent(np);of_i8042_kbd_irq = irq_of_parse_and_map(parent, 0);if (!of_i8042_kbd_irq)of_i8042_kbd_irq = 1;of_i8042_aux_irq = irq_of_parse_and_map(parent, 1);if (!of_i8042_aux_irq)of_i8042_aux_irq = 12;of_node_put(np);np = parent;break;}np = of_find_node_by_type(NULL, \"8042\");  Pegasos has no device_type on its 8042 node, look for the   name instead ", "if ((phb_id >= 0) && !test_and_set_bit(phb_id, phb_bitmap))goto out_unlock;/* If everything fails then fallback to dynamic PHB numbering. ": "pci_domain_nr(dn);if (ret >= 0) {prop = ret;ret = 0;}if (ret)ret = of_property_read_u64(dn, \"ibm,opal-phbid\", &prop);if (ret) {ret = of_alias_get_id(dn, \"pci\");if (ret >= 0) {prop = ret;ret = 0;}}if (ret) {u32 prop_32;ret = of_property_read_u32_index(dn, \"reg\", 1, &prop_32);prop = prop_32;}if (!ret)phb_id = (int)(prop & (MAX_PHBS - 1));spin_lock(&hose_spinlock);  We need to be sure to not use the same PHB number twice. ", "phb = pci_bus_to_host(bus);if (phb->controller_ops.dma_bus_setup)phb->controller_ops.dma_bus_setup(bus);}void pcibios_bus_add_device(struct pci_dev *dev)": "pcibios_fixup_bus)ppc_md.pcibios_fixup_bus(bus);  Setup bus DMA mappings ", "static unsigned long dscr_default;/** * read_dscr() - Fetch the cpu specific DSCR default * @val:Returned cpu specific DSCR default value * * This function returns the per cpu DSCR default value * for any cpu which is contained in it's PACA structure. ": "ppc_enable_pmcs()) \\__SYSFS_SPRSETUP_SHOW_STORE(NAME)#define SYSFS_SPRSETUP(NAME, ADDRESS) \\__SYSFS_SPRSETUP_READ_WRITE(NAME, ADDRESS, ) \\__SYSFS_SPRSETUP_SHOW_STORE(NAME)#define SYSFS_SPRSETUP_SHOW_STORE(NAME) \\__SYSFS_SPRSETUP_SHOW_STORE(NAME)#ifdef CONFIG_PPC64    This is the system wide DSCR register default value. Any   change to this default value through the sysfs interface   will update all per cpu DSCR default values across the   system stored in their respective PACA structures. ", "if ((phys ^ (unsigned long)dispDeviceBase) & 0xf0000000)return;dispDeviceBase = (__u8 *) phys;dispDeviceRect[0] = 0;dispDeviceRect[1] = 0;dispDeviceRect[2] = width;dispDeviceRect[3] = height;dispDeviceDepth = depth;dispDeviceRowBytes = pitch;if (boot_text_mapped) ": "btext_update_display(unsigned long phys, int width, int height,  int depth, int pitch){if (!dispDeviceBase)return;  check it's the same frame buffer (within 256MB) ", "EXPORT_SYMBOL(tb_ticks_per_usec": "tb_ticks_per_usec = 100;   sane default ", "spin_cpu_relax();} else ": "__delay(unsigned long loops){unsigned long start;spin_begin();if (tb_invalid) {    TB is in error state and isn't ticking anymore.   HMI handler was unable to recover from TB error.   Return immediately, so that kernel won't get stuck here. ", "DEFINE_INTERRUPT_HANDLER_ASYNC(timer_interrupt)": "timer_interrupt - gets called when the decrementer overflows,   with interrupts disabled. ", "if (!MSR_TM_ACTIVE(cpumsr) &&     MSR_TM_ACTIVE(current->thread.regs->msr))return;__giveup_fpu(current);}}EXPORT_SYMBOL(enable_kernel_fp": "enable_kernel_fp(void){unsigned long cpumsr;WARN_ON(preemptible());cpumsr = msr_check_and_set(MSR_FP);if (current->thread.regs && (current->thread.regs->msr & MSR_FP)) {check_if_tm_restore_required(current);    If a thread has already been reclaimed then the   checkpointed registers are on the CPU but have definitely   been saved by the reclaim code. Don't need to and  cannot    giveup as this would save  to the 'live' structure not the   checkpointed structure. ", "if (!MSR_TM_ACTIVE(cpumsr) &&     MSR_TM_ACTIVE(current->thread.regs->msr))return;__giveup_altivec(current);}}EXPORT_SYMBOL(enable_kernel_altivec": "enable_kernel_altivec(void){unsigned long cpumsr;WARN_ON(preemptible());cpumsr = msr_check_and_set(MSR_VEC);if (current->thread.regs && (current->thread.regs->msr & MSR_VEC)) {check_if_tm_restore_required(current);    If a thread has already been reclaimed then the   checkpointed registers are on the CPU but have definitely   been saved by the reclaim code. Don't need to and  cannot    giveup as this would save  to the 'live' structure not the   checkpointed structure. ", "if (!MSR_TM_ACTIVE(cpumsr) &&     MSR_TM_ACTIVE(current->thread.regs->msr))return;__giveup_vsx(current);}}EXPORT_SYMBOL(enable_kernel_vsx": "enable_kernel_vsx(void){unsigned long cpumsr;WARN_ON(preemptible());cpumsr = msr_check_and_set(MSR_FP|MSR_VEC|MSR_VSX);if (current->thread.regs &&    (current->thread.regs->msr & (MSR_VSX|MSR_VEC|MSR_FP))) {check_if_tm_restore_required(current);    If a thread has already been reclaimed then the   checkpointed registers are on the CPU but have definitely   been saved by the reclaim code. Don't need to and  cannot    giveup as this would save  to the 'live' structure not the   checkpointed structure. ", "if (IS_ENABLED(CONFIG_PPC_BOOK3S_64) && !radix_enabled())preload_new_slb_context(start, sp);#endif#ifdef CONFIG_PPC_TRANSACTIONAL_MEM/* * Clear any transactional state, we're exec()ing. The cause is * not important as there will never be a recheckpoint so it's not * user visible. ": "start_thread(struct pt_regs  regs, unsigned long start, unsigned long sp){#ifdef CONFIG_PPC64unsigned long load_addr = regs->gpr[2];  saved by ELF_PLAT_INIT ", "int of_get_ibm_chip_id(struct device_node *np)": "of_get_ibm_chip_id - Returns the IBM \"chip-id\" of a device   @np: device node of the device     This looks for a property \"ibm,chip-id\" in the node or any   of its parents and returns its content, or -1 if it cannot   be found. ", "int cpu_to_chip_id(int cpu)": "cpu_to_chip_id - Return the cpus chip-id   @cpu: The logical cpu number.     Return the value of the ibm,chip-id property corresponding to the given   logical cpu number. If the chip-id can not be found, returns -1. ", "if (!pci_find_hose_for_OF_device(node))return -ENODEV;reg = of_get_property(node, \"reg\", &size);if (!reg || size < 5 * sizeof(u32))return -ENODEV;*bus = (be32_to_cpup(&reg[0]) >> 16) & 0xff;*devfn = (be32_to_cpup(&reg[0]) >> 8) & 0xff;#ifndef CONFIG_PPC_PCI_OF_BUS_MAPreturn 0;#else/* Ok, here we need some tweak. If we have already renumbered * all busses, we can't rely on the OF bus number any more. * the pci_to_OF_bus_map is not enough as several PCI busses * may match the same OF bus number. ": "pci_device_from_OF_node(struct device_node  node, u8  bus, u8  devfn){#ifdef CONFIG_PPC_PCI_OF_BUS_MAPstruct pci_dev  dev = NULL;#endifconst __be32  reg;int size;  Check if it might have a chance to be a PCI device ", "__replay_soft_interrupts();irq_exit();}#if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_PPC_KUAP)static inline __no_kcsan void replay_soft_interrupts_irqrestore(void)": "arch_local_irq_restore ", "struct pci_dev *of_create_pci_dev(struct device_node *node, struct pci_bus *bus, int devfn)": "of_create_pci_dev - Given a device tree node on a pci bus, create a pci_dev   @node: device tree node pointer   @bus: bus the device is sitting on   @devfn: PCI function number, extracted from device tree by caller. ", "void of_scan_pci_bridge(struct pci_dev *dev)": "of_scan_pci_bridge - Set up a PCI bridge and scan for child nodes   @dev: pci_dev structure for the bridge     of_scan_bus() calls this routine for each PCI bridge that it finds, and   this routine in turn call of_scan_bus() recursively to scan for more child   devices. ", "static unsigned long in_xmon __read_mostly = 0;static int xmon_on = IS_ENABLED(CONFIG_XMON_DEFAULT);static bool xmon_is_ro = IS_ENABLED(CONFIG_XMON_DEFAULT_RO_MODE);static unsigned long adrs;static int size = 1;#define MAX_DUMP (64 * 1024)static unsigned long ndump = 64;#define MAX_IDUMP (MAX_DUMP >> 2)static unsigned long nidump = 16;static unsigned long ncsum = 4096;static int termch;static char tmpstr[KSYM_NAME_LEN];static int tracing_enabled;static long bus_error_jmp[JMP_BUF_LEN];static int catch_memory_errors;static int catch_spr_faults;static long *xmon_fault_jmp[NR_CPUS];/* Breakpoint stuff ": "xmon.h>#include <asmprocessor.h>#include <asmmmu.h>#include <asmmmu_context.h>#include <asmplpar_wrappers.h>#include <asmcputable.h>#include <asmrtas.h>#include <asmsstep.h>#include <asmirq_regs.h>#include <asmspu.h>#include <asmspu_priv1.h>#include <asmsetjmp.h>#include <asmreg.h>#include <asmdebug.h>#include <asmhw_breakpoint.h>#include <asmxive.h>#include <asmopal.h>#include <asmfirmware.h>#include <asmcode-patching.h>#include <asmsections.h>#include <asminst.h>#include <asminterrupt.h>#ifdef CONFIG_PPC64#include <asmhvcall.h>#include <asmpaca.h>#endif#include \"nonstdio.h\"#include \"dis-asm.h\"#include \"xmon_bpts.h\"#ifdef CONFIG_SMPstatic cpumask_t cpus_in_xmon = CPU_MASK_NONE;static unsigned long xmon_taken = 1;static int xmon_owner;static int xmon_gate;static int xmon_batch;static unsigned long xmon_batch_start_cpu;static cpumask_t xmon_batch_cpus = CPU_MASK_NONE;#else#define xmon_owner 0#endif   CONFIG_SMP ", "mask = SLB_VSID_B | HPTE_V_AVPN | HPTE_V_SECONDARY;val = 0;pshift = 12;if (slb_v & SLB_VSID_L) ": "kvmppc_hv_find_lock_hpte(struct kvm  kvm, gva_t eaddr, unsigned long slb_v,      unsigned long valid){unsigned int i;unsigned int pshift;unsigned long somask;unsigned long vsid, hash;unsigned long avpn;__be64  hpte;unsigned long mask, val;unsigned long v, r, orig_v;  Get page shift, work out hash and AVPN etc. ", "#include <linux/export.h>#include <linux/string.h>#ifdef CONFIG_OR1K_1200/* * Do memcpy with word copies and loop unrolling. This gives the * best performance on the OR1200 and MOR1KX archirectures ": "memcpy.c     Optimized memory copy routines for openrisc.  These are mostly copied   from ohter sources but slightly entended based on ideas discuassed in   #openrisc.     The word unroll implementation is an extension to the arm byte   unrolled implementation, but using word copies (if things are   properly aligned)     The great arm loop unroll algorithm can be found at:    archarmbootcompressedstring.c ", "#include <linux/vmalloc.h>#include <linux/io.h>#include <linux/pgtable.h>#include <asm/pgalloc.h>#include <asm/fixmap.h>#include <asm/bug.h>#include <linux/sched.h>#include <asm/tlbflush.h>extern int mem_init_done;static unsigned int fixmaps_used __initdata;/* * Remap an arbitrary physical address space into the kernel virtual * address space. Needed when the kernel wants to access high addresses * directly. * * NOTE! We need to allow non-page-aligned mappings too: we will obviously * have to convert them into an offset in a page-aligned mapping, but the * caller shouldn't need to know that small detail. ": "ioremap.c     Linux architectural port borrowing liberally from similar works of   others.  All original copyrights apply as per the original source   declaration.     Modifications for the OpenRISC architecture:   Copyright (C) 2003 Matjaz Breskvar <phoenix@bsemi.com>   Copyright (C) 2010-2011 Jonas Bonn <jonas@southpole.se> ", "if (unlikely((unsigned long)addr > FIXADDR_START)) ": "iounmap(volatile void __iomem  addr){  If the page is from the fixmap pool then we just clear out   the fixmap mapping. ", "static void default_power_off(void)": "pm_power_off has not been set by a power management   driver, in this case we can assume we are on a simulator.  On   OpenRISC simulators l.nop 1 will trigger the simulator exit. ", "static void __pollwait(struct file *filp, wait_queue_head_t *wait_address,       poll_table *p);void poll_initwait(struct poll_wqueues *pwq)": "poll_freewait() make all the   work.  poll_wait() is an inline-function defined in <linuxpoll.h>,   as all selectpoll functions have to call it to add an entry to the   poll table. ", "int sysctl_vfs_cache_pressure __read_mostly = 100;EXPORT_SYMBOL_GPL(sysctl_vfs_cache_pressure);__cacheline_aligned_in_smp DEFINE_SEQLOCK(rename_lock);EXPORT_SYMBOL(rename_lock": "rename_lock ", "int sysctl_vfs_cache_pressure __read_mostly = 100;EXPORT_SYMBOL_GPL(sysctl_vfs_cache_pressure);__cacheline_aligned_in_smp DEFINE_SEQLOCK(rename_lock);EXPORT_SYMBOL(rename_lock);static struct kmem_cache *dentry_cache __read_mostly;const struct qstr empty_name = QSTR_INIT(\"\", 0);EXPORT_SYMBOL(empty_name);const struct qstr slash_name = QSTR_INIT(\"/\", 1);EXPORT_SYMBOL(slash_name);const struct qstr dotdot_name = QSTR_INIT(\"..\", 2);EXPORT_SYMBOL(dotdot_name);/* * This is the single most critical data structure when it comes * to the dcache: the hashtable for lookups. Somebody should try * to make this good - I've just made it work. * * This hash-function tries to avoid losing too many bits of hash * information, yet avoid using a prime hash-size or similar. ": "d_drop)   dentry->d_sb->s_dentry_lru_lock protects:     - the dcache lru lists and counters   d_lock protects:     - d_flags     - d_name     - d_lru     - d_count     - d_unhashed()     - d_parent and d_subdirs     - childrens' d_child and d_parent     - d_u.d_alias, d_inode     Ordering:   dentry->d_inode->i_lock     dentry->d_lock       dentry->d_sb->s_dentry_lru_lock       dcache_hash_bucket lock       s_roots lock     If there is an ancestor relationship:   dentry->d_parent->...->d_parent->d_lock     ...       dentry->d_parent->d_lock         dentry->d_lock     If no ancestor relationship:   arbitrary, since it's serialized on rename_lock ", "static inline bool fast_dput(struct dentry *dentry)": "dput(), and return whether that was successful.     If unsuccessful, we return false, having already taken the dentry lock.     The caller needs to hold the RCU read lock, so that the dentry is   guaranteed to stay around even if the refcount goes down to zero! ", "rcu_read_lock();seq = raw_seqcount_begin(&dentry->d_seq);ret = READ_ONCE(dentry->d_parent);gotref = lockref_get_not_zero(&ret->d_lockref);rcu_read_unlock();if (likely(gotref)) ": "dget_parent(struct dentry  dentry){int gotref;struct dentry  ret;unsigned seq;    Do optimistic parent lookup without any   locking. ", "struct dentry *d_find_any_alias(struct inode *inode)": "d_find_any_alias(struct inode  inode){struct dentry  alias;if (hlist_empty(&inode->i_dentry))return NULL;alias = hlist_entry(inode->i_dentry.first, struct dentry, d_u.d_alias);__dget(alias);return alias;}     d_find_any_alias - find any alias for a given inode   @inode: inode to find an alias for     If any aliases exist for the given inode, take and return a   reference for one of them.  If no aliases exist, return %NULL. ", "struct dentry *d_find_alias(struct inode *inode)": "d_find_alias(struct inode  inode){struct dentry  alias;if (S_ISDIR(inode->i_mode))return __d_find_any_alias(inode);hlist_for_each_entry(alias, &inode->i_dentry, d_u.d_alias) {spin_lock(&alias->d_lock); if (!d_unhashed(alias)) {__dget_dlock(alias);spin_unlock(&alias->d_lock);return alias;}spin_unlock(&alias->d_lock);}return NULL;}     d_find_alias - grab a hashed alias of inode   @inode: inode in question     If inode has a hashed alias, or is a directory and has any alias,   acquire the reference to alias and return it. Otherwise return NULL.   Notice that if inode is a directory there can be only one alias and   it can be unhashed only if it has no children, or if it is the root   of a filesystem, or if the directory was renamed and d_revalidate   was the first vfs operation to notice.     If the inode has an IS_ROOT, DCACHE_DISCONNECTED alias, then prefer   any other hashed alias over that one. ", "void shrink_dcache_sb(struct super_block *sb)": "shrink_dcache_sb - shrink dcache for a superblock   @sb: superblock     Shrink the dcache for the specified super block. This is used to free   the dcache before unmounting a file system. ", "int path_has_submounts(const struct path *parent)": "path_has_submounts - check for mounts over a dentry in the                        current namespace.   @parent: path to check.     Return true if the parent or its subdirectories contain   a mount point in the current namespace. ", "void shrink_dcache_parent(struct dentry *parent)": "shrink_dcache_parent - prune dcache   @parent: parent of entries to prune     Prune the dcache to remove unused children of the parent dentry. ", "int d_set_mounted(struct dentry *dentry)": "d_invalidate() and d_set_mounted() must succeed.  For   this reason take rename_lock and d_lock on dentry and ancestors. ", " static struct dentry *__d_alloc(struct super_block *sb, const struct qstr *name)": "d_alloc-allocate a dcache entry   @sb: filesystem it will belong to   @name: qstr of the name     Allocates a dentry. It returns %NULL if there is insufficient memory   available. On a success the dentry is returned. The name passed in is   copied and the copy passed in may be reused after this call. ", "struct dentry *d_alloc(struct dentry * parent, const struct qstr *name)": "d_set_d_op(dentry, dentry->d_sb->s_d_op);if (dentry->d_op && dentry->d_op->d_init) {err = dentry->d_op->d_init(dentry);if (err) {if (dname_external(dentry))kfree(external_name(dentry));kmem_cache_free(dentry_cache, dentry);return NULL;}}this_cpu_inc(nr_dentry);return dentry;}     d_alloc-allocate a dcache entry   @parent: parent of entry to allocate   @name: qstr of the name     Allocates a dentry. It returns %NULL if there is insufficient memory   available. On a success the dentry is returned. The name passed in is   copied and the copy passed in may be reused after this call. ", "void d_set_fallthru(struct dentry *dentry)": "d_set_fallthru - Mark a dentry as falling through to a lower layer   @dentry - The dentry to mark     Mark a dentry as falling through to the lower layer (as set with   d_pin_lower()).  This flag may be recorded on the medium. ", "if (dentry->d_flags & DCACHE_LRU_LIST)this_cpu_dec(nr_dentry_negative);hlist_add_head(&dentry->d_u.d_alias, &inode->i_dentry);raw_write_seqcount_begin(&dentry->d_seq);__d_set_inode_and_type(dentry, inode, add_flags);raw_write_seqcount_end(&dentry->d_seq);fsnotify_update_flags(dentry);spin_unlock(&dentry->d_lock);}/** * d_instantiate - fill in inode information for a dentry * @entry: dentry to complete * @inode: inode to attach to this dentry * * Fill in inode information in the entry. * * This turns negative dentries into productive full members * of society. * * NOTE! This assumes that the inode count has been incremented * (or otherwise set) by the caller to indicate that it is now * in use by the dcache. ": "d_instantiate(struct dentry  dentry, struct inode  inode){unsigned add_flags = d_flags_for_inode(inode);WARN_ON(d_in_lookup(dentry));spin_lock(&dentry->d_lock);    Decrement negative dentry count if it was in the LRU list. ", "add_flags = d_flags_for_inode(inode);if (disconnected)add_flags |= DCACHE_DISCONNECTED;spin_lock(&dentry->d_lock);__d_set_inode_and_type(dentry, inode, add_flags);hlist_add_head(&dentry->d_u.d_alias, &inode->i_dentry);if (!disconnected) ": "d_instantiate_anon(struct dentry  dentry,   struct inode  inode,   bool disconnected){struct dentry  res;unsigned add_flags;security_d_instantiate(dentry, inode);spin_lock(&inode->i_lock);res = __d_find_any_alias(inode);if (res) {spin_unlock(&inode->i_lock);dput(dentry);goto out_iput;}  attach a disconnected dentry ", "struct dentry *d_obtain_alias(struct inode *inode)": "d_obtain_alias(struct inode  inode, bool disconnected){struct dentry  tmp;struct dentry  res;if (!inode)return ERR_PTR(-ESTALE);if (IS_ERR(inode))return ERR_CAST(inode);res = d_find_any_alias(inode);if (res)goto out_iput;tmp = d_alloc_anon(inode->i_sb);if (!tmp) {res = ERR_PTR(-ENOMEM);goto out_iput;}return __d_instantiate_anon(tmp, inode, disconnected);out_iput:iput(inode);return res;}     d_obtain_alias - find or allocate a DISCONNECTED dentry for a given inode   @inode: inode to allocate the dentry for     Obtain a dentry for an inode resulting from NFS filehandle conversion or   similar open by handle operations.  The returned dentry may be anonymous,   or may have a full name (if the inode was already in the cache).     When called on a directory inode, we must ensure that the inode only ever   has one dentry.  If a dentry is found, that is returned instead of   allocating a new one.     On successful return, the reference to the inode has been transferred   to the dentry.  In case of an error the reference on the inode is released.   To make it easier to use in export operations a %NULL or IS_ERR inode may   be passed in and the error will be propagated to the return value,   with a %NULL @inode replaced by ERR_PTR(-ESTALE). ", "if (unlikely(IS_ROOT(dentry)))b = &dentry->d_sb->s_roots;elseb = d_hash(dentry->d_name.hash);hlist_bl_lock(b);__hlist_bl_del(&dentry->d_hash);hlist_bl_unlock(b);}void __d_drop(struct dentry *dentry)": "d_obtain_root, which are always IS_ROOT: ", "struct dentry *d_add_ci(struct dentry *dentry, struct inode *inode,struct qstr *name)": "d_add_ci - lookup or allocate new dentry with case-exact name   @inode:  the inode case-insensitive lookup has found   @dentry: the negative dentry that was passed to the parent's lookup func   @name:   the case-exact name to be associated with the returned dentry     This is to avoid filling the dcache with case-insensitive names to the   same inode, only the actual correct case is stored in the dcache for   case-insensitive filesystems.     For a case-insensitive lookup match and if the case-exact dentry   already exists in the dcache, use it and return it.     If no entry exists with the exact case name, allocate new dentry with   the exact case, and return the spliced entry. ", "bool d_same_name(const struct dentry *dentry, const struct dentry *parent, const struct qstr *name)": "d_splice_alias(inode, found);if (res) {d_lookup_done(found);dput(found);return res;}return found;}EXPORT_SYMBOL(d_add_ci);     d_same_name - compare dentry name with case-exact name   @parent: parent dentry   @dentry: the negative dentry that was passed to the parent's lookup func   @name:   the case-exact name to be associated with the returned dentry     Return: true if names are same, or false ", "void d_drop(struct dentry *dentry)": "d_delete will try to mark the dentry negative if   possible, giving a successful _negative_ lookup, while d_drop will   just make the cache lookup fail.     d_drop() is used mainly for stuff that wants to invalidate a dentry for some   reason (NFS timeouts or autofs deletes).     __d_drop requires dentry->d_lock     ___d_drop doesn't mark dentry as \"unhashed\"   (dentry->d_hash.pprev will be LIST_POISON2, not NULL). ", " void d_rehash(struct dentry * entry)": "d_rehash(struct dentry  entry){struct hlist_bl_head  b = d_hash(entry->d_name.hash);hlist_bl_lock(b);hlist_bl_add_head_rcu(&entry->d_hash, b);hlist_bl_unlock(b);}     d_rehash- add an entry back to the hash   @entry: dentry to add to the hash     Adds a dentry to the hash according to its name. ", "struct dentry *d_exact_alias(struct dentry *entry, struct inode *inode)": "d_exact_alias - find and hash an exact unhashed alias   @entry: dentry to add   @inode: The inode to go with this dentry     If an unhashed dentry with the same nameparent and desired   inode already exists, hash and return it.  Otherwise, return   NULL.     Parent directory should be locked. ", "hlist_bl_for_each_entry_rcu(dentry, node, b, d_hash) ": "d_move().     It is possible that concurrent renames can mess up our list   walk here and result in missing our dentry, resulting in the   false-negative result. d_lookup() protects against concurrent   renames using rename_lock seqlock.     See Documentationfilesystemspath-lookup.txt for more details. ", "  bool is_subdir(struct dentry *new_dentry, struct dentry *old_dentry)": "is_subdir - is new dentry a subdirectory of old_dentry   @new_dentry: new dentry   @old_dentry: old dentry     Returns true if new_dentry is a subdirectory of the parent (at any depth).   Returns false otherwise.   Caller must ensure that \"new_dentry\" is pinned before calling is_subdir() ", "mnt_ns = READ_ONCE(mnt->mnt_ns);/* open-coded is_mounted() to use local mnt_ns ": "d_path(const struct dentry  dentry, const struct mount  mnt,  const struct path  root, struct prepend_buffer  p){while (dentry != root->dentry || &mnt->mnt != root->mnt) {const struct dentry  parent = READ_ONCE(dentry->d_parent);if (dentry == mnt->mnt.mnt_root) {struct mount  m = READ_ONCE(mnt->mnt_parent);struct mnt_namespace  mnt_ns;if (likely(mnt != m)) {dentry = READ_ONCE(mnt->mnt_mountpoint);mnt = m;continue;}  Global root ", "int simple_setattr(struct mnt_idmap *idmap, struct dentry *dentry,   struct iattr *iattr)": "simple_setattr - setattr for simple filesystem   @idmap: idmap of the target mount   @dentry: dentry   @iattr: iattr structure     Returns 0 on success, -error on failure.     simple_setattr is a simple ->setattr implementation without a proper   implementation of size changes.     It can either be used for in-memory filesystems or special files   on simple regular filesystems.  Anything that needs to change on-disk   or wire state on size changes needs its own setattr method. ", "inode->i_ino = 1;inode->i_mode = S_IFDIR | 0755;inode->i_atime = inode->i_mtime = inode->i_ctime = current_time(inode);inode->i_op = &simple_dir_inode_operations;inode->i_fop = &simple_dir_operations;set_nlink(inode, 2);root = d_make_root(inode);if (!root)return -ENOMEM;for (i = 0; !files->name || files->name[0]; i++, files++) ": "simple_fill_super(struct super_block  s, unsigned long magic,      const struct tree_descr  files){struct inode  inode;struct dentry  root;struct dentry  dentry;int i;s->s_blocksize = PAGE_SIZE;s->s_blocksize_bits = PAGE_SHIFT;s->s_magic = magic;s->s_op = &simple_super_operations;s->s_time_gran = 1;inode = new_inode(s);if (!inode)return -ENOMEM;    because the root inode is 1, the files array must not contain an   entry at index 1 ", "ssize_t simple_read_from_buffer(void __user *to, size_t count, loff_t *ppos,const void *from, size_t available)": "simple_read_from_buffer - copy data from the buffer to user space   @to: the user space buffer to read to   @count: the maximum number of bytes to read   @ppos: the current position in the buffer   @from: the buffer to read from   @available: the size of the buffer     The simple_read_from_buffer() function reads up to @count bytes from the   buffer @from at offset @ppos into the user space address starting at @to.     On success, the number of bytes read is returned and the offset @ppos is   advanced by this number, or negative value is returned on error.  ", "ssize_t simple_write_to_buffer(void *to, size_t available, loff_t *ppos,const void __user *from, size_t count)": "simple_write_to_buffer - copy data from user space to the buffer   @to: the buffer to write to   @available: the size of the buffer   @ppos: the current position in the buffer   @from: the user space buffer to read from   @count: the maximum number of bytes to read     The simple_write_to_buffer() function reads up to @count bytes from the user   space address starting at @from into the buffer @to at offset @ppos.     On success, the number of bytes written is returned and the offset @ppos is   advanced by this number, or negative value is returned on error.  ", "ssize_t memory_read_from_buffer(void *to, size_t count, loff_t *ppos,const void *from, size_t available)": "memory_read_from_buffer - copy data from the buffer   @to: the kernel space buffer to read to   @count: the maximum number of bytes to read   @ppos: the current position in the buffer   @from: the buffer to read from   @available: the size of the buffer     The memory_read_from_buffer() function reads up to @count bytes from the   buffer @from at offset @ppos into the kernel space address starting at @to.     On success, the number of bytes read is returned and the offset @ppos is   advanced by this number, or negative value is returned on error.  ", "smp_mb();ar->size = n;}EXPORT_SYMBOL(simple_transaction_set": "simple_transaction_set(struct file  file, size_t n){struct simple_transaction_argresp  ar = file->private_data;BUG_ON(n > SIMPLE_TRANSACTION_LIMIT);    The barrier ensures that ar->size will really remain zero until   ar->data is ready for reading. ", "if (file->private_data) ": "simple_transaction_get(struct file  file, const char __user  buf, size_t size){struct simple_transaction_argresp  ar;static DEFINE_SPINLOCK(simple_transaction_lock);if (size > SIMPLE_TRANSACTION_LIMIT - 1)return ERR_PTR(-EFBIG);ar = (struct simple_transaction_argresp  )get_zeroed_page(GFP_KERNEL);if (!ar)return ERR_PTR(-ENOMEM);spin_lock(&simple_transaction_lock);  only one write allowed per open ", "int __generic_file_fsync(struct file *file, loff_t start, loff_t end, int datasync)": "generic_file_fsync - generic fsync implementation for simple filesystems     @file:file to synchronize   @start:start offset in bytes   @end:end offset in bytes (inclusive)   @datasync:only synchronize essential metadata if true     This is a generic implementation of the fsync method for simple   filesystems which track all non-inode metadata in the buffers list   hanging off the address_space structure. ", "int generic_check_addressable(unsigned blocksize_bits, u64 num_blocks)": "generic_check_addressable - Check addressability of file system   @blocksize_bits:log of file system block size   @num_blocks:number of blocks in file system     Determine whether a file system with @num_blocks blocks (and a   block size of 2  @blocksize_bits) is addressable by the sector_t   and page cache of the system.  Return 0 if so and -EFBIG otherwise. ", "root->i_ino = 1;root->i_mode = S_IFDIR | S_IRUSR | S_IWUSR;root->i_atime = root->i_mtime = root->i_ctime = current_time(root);s->s_root = d_make_root(root);if (!s->s_root)return -ENOMEM;s->s_d_op = ctx->dops;return 0;}static int pseudo_fs_get_tree(struct fs_context *fc)": "noop_fsync,};EXPORT_SYMBOL(simple_dir_operations);const struct inode_operations simple_dir_inode_operations = {.lookup= simple_lookup,};EXPORT_SYMBOL(simple_dir_inode_operations);static struct dentry  find_next_child(struct dentry  parent, struct dentry  prev){struct dentry  child = NULL;struct list_head  p = prev ? &prev->d_child : &parent->d_subdirs;spin_lock(&parent->d_lock);while ((p = p->next) != &parent->d_subdirs) {struct dentry  d = container_of(p, struct dentry, d_child);if (simple_positive(d)) {spin_lock_nested(&d->d_lock, DENTRY_D_LOCK_NESTED);if (simple_positive(d))child = dget_dlock(d);spin_unlock(&d->d_lock);if (likely(child))break;}}spin_unlock(&parent->d_lock);dput(prev);return child;}void simple_recursive_removal(struct dentry  dentry,                              void ( callback)(struct dentry  )){struct dentry  this = dget(dentry);while (true) {struct dentry  victim = NULL,  child;struct inode  inode = this->d_inode;inode_lock(inode);if (d_is_dir(this))inode->i_flags |= S_DEAD;while ((child = find_next_child(this, victim)) == NULL) { kill and ascend update metadata while it's still lockedinode->i_ctime = current_time(inode);clear_nlink(inode);inode_unlock(inode);victim = this;this = this->d_parent;inode = this->d_inode;inode_lock(inode);if (simple_positive(victim)) {d_invalidate(victim); avoid lost mountsif (d_is_dir(victim))fsnotify_rmdir(inode, victim);elsefsnotify_unlink(inode, victim);if (callback)callback(victim);dput(victim); unpin it}if (victim == dentry) {inode->i_ctime = inode->i_mtime =current_time(inode);if (d_is_dir(dentry))drop_nlink(inode);inode_unlock(inode);dput(dentry);return;}}inode_unlock(inode);this = child;}}EXPORT_SYMBOL(simple_recursive_removal);static const struct super_operations simple_super_operations = {.statfs= simple_statfs,};static int pseudo_fs_fill_super(struct super_block  s, struct fs_context  fc){struct pseudo_fs_context  ctx = fc->fs_private;struct inode  root;s->s_maxbytes = MAX_LFS_FILESIZE;s->s_blocksize = PAGE_SIZE;s->s_blocksize_bits = PAGE_SHIFT;s->s_magic = ctx->magic;s->s_op = ctx->ops ?: &simple_super_operations;s->s_xattr = ctx->xattr;s->s_time_gran = 1;root = new_inode(s);if (!root)return -ENOMEM;    since this is the first inode, make it number 1. New inodes created   after this must take care not to collide with it (by passing   max_reserved of 1 to iunique). ", "inode->i_state = I_DIRTY;inode->i_mode = S_IRUSR | S_IWUSR;inode->i_uid = current_fsuid();inode->i_gid = current_fsgid();inode->i_flags |= S_PRIVATE;inode->i_atime = inode->i_mtime = inode->i_ctime = current_time(inode);return inode;}EXPORT_SYMBOL(alloc_anon_inode": "alloc_anon_inode(struct super_block  s){static const struct address_space_operations anon_aops = {.dirty_folio= noop_dirty_folio,};struct inode  inode = new_inode_pseudo(s);if (!inode)return ERR_PTR(-ENOMEM);inode->i_ino = get_next_ino();inode->i_mapping->a_ops = &anon_aops;    Mark the inode dirty from the very beginning,   that way it will never be moved to the dirty   list because mark_inode_dirty() will think   that it already _is_ on the dirty list. ", "intsimple_nosetlease(struct file *filp, long arg, struct file_lock **flp,  void **priv)": "simple_nosetlease - generic helper for prohibiting leases   @filp: file pointer   @arg: type of lease to obtain   @flp: new lease supplied for insertion   @priv: private data for lm_setup operation     Generic helper for filesystems that do not wish to allow leases to be set.   All arguments are ignored and it just returns -EINVAL. ", "const char *simple_get_link(struct dentry *dentry, struct inode *inode,    struct delayed_call *done)": "simple_get_link - generic helper to get the target of \"fast\" symlinks   @dentry: not used here   @inode: the symlink inode   @done: not used here     Generic helper for filesystems to use for symlink inodes where a pointer to   the symlink target is stored in ->i_link.  NOTE: this isn't normally called,   since as an optimization the path lookup code uses any non-NULL ->i_link   directly, without calling ->get_link().  But ->get_link() still must be set,   to mark the inode_operations as being for a symlink.     Return: the symlink target ", "void generic_set_encrypted_ci_d_ops(struct dentry *dentry)": "generic_set_encrypted_ci_d_ops - helper for setting d_ops for given dentry   @dentry:dentry to set ops on     Casefolded directories need d_hash and d_compare set, so that the dentries   contained in them are handled case-insensitively.  Note that these operations   are needed on the parent directory rather than on the dentries in it, and   while the casefolding flag can be toggled on and off on an empty directory,   dentry_operations can't be changed later.  As a result, if the filesystem has   casefolding support enabled at all, we have to give all dentries the   casefolding operations even if their inode doesn't have the casefolding flag   currently (and thus the casefolding ops would be no-ops for now).     Encryption works differently in that the only dentry operation it needs is   d_revalidate, which it only needs on dentries that have the no-key name flag.   The no-key flag can't be set \"later\", so we don't have to worry about that.     Finally, to maximize compatibility with overlayfs (which isn't compatible   with certain dentry operations) and to avoid taking an unnecessary   performance hit, we use custom dentry_operations for each possible   combination rather than always installing all operations. ", "bool inode_maybe_inc_iversion(struct inode *inode, bool force)": "inode_maybe_inc_iversion - increments i_version   @inode: inode with the i_version that should be updated   @force: increment the counter even if it's not necessary?     Every time the inode is modified, the i_version field must be seen to have   changed by any observer.     If \"force\" is set or the QUERIED flag is set, then ensure that we increment   the value, and clear the queried flag.     In the common case where neither is set, then we can return \"false\" without   updating i_version.     If this function returns false, and no other metadata has changed, then we   can avoid logging the metadata. ", "smp_mb();cur = inode_peek_iversion_raw(inode);do ": "inode_query_iversion() ", "int lookup_constant(const struct constant_table *tbl, const char *name, int not_found)": "lookup_constant(const struct constant_table  tbl, const char  name){for ( ; tbl->name; tbl++)if (strcmp(name, tbl->name) == 0)return tbl;return NULL;}     lookup_constant - Look up a constant by name in an ordered table   @tbl: The table of constants to search.   @name: The name to look up.   @not_found: The value to return if the name is not found. ", "if (is_flag(p)) ": "__fs_parse(struct p_log  log,     const struct fs_parameter_spec  desc,     struct fs_parameter  param,     struct fs_parse_result  result){const struct fs_parameter_spec  p;result->uint_64 = 0;p = fs_lookup_key(desc, param, &result->negated);if (!p)return -ENOPARAM;if (p->flags & fs_param_deprecated)warn_plog(log, \"Deprecated parameter '%s'\", param->key);  Try to turn the type we were given into the type desired by the   parameter and give an error if we can't. ", "int fs_lookup_param(struct fs_context *fc,    struct fs_parameter *param,    bool want_bdev,    unsigned int flags,    struct path *_path)": "fs_lookup_param - Look up a path referred to by a parameter   @fc: The filesystem context to log errors through.   @param: The parameter.   @want_bdev: T if want a blockdev   @flags: Pathwalk flags passed to filename_lookup()   @_path: The result of the lookup ", "return handler->set(handler, idmap, dentry, inode, name, value,    size, flags);}EXPORT_SYMBOL(__vfs_setxattr": "__vfs_setxattr(struct mnt_idmap  idmap, struct dentry  dentry,       struct inode  inode, const char  name, const void  value,       size_t size, int flags){const struct xattr_handler  handler;if (is_posix_acl_xattr(name))return -EOPNOTSUPP;handler = xattr_resolve_name(inode, &name);if (IS_ERR(handler))return PTR_ERR(handler);if (!handler->set)return -EOPNOTSUPP;if (size == 0)value = \"\";    empty EA, do not remove ", "ssize_tgeneric_listxattr(struct dentry *dentry, char *buffer, size_t buffer_size)": "generic_listxattr - run through a dentry's xattr list() operations   @dentry: dentry to list the xattrs   @buffer: result buffer   @buffer_size: size of @buffer     Combine the results of the list() operation from every xattr_handler in the   xattr_handler stack.     Note that this will not include the entries for POSIX ACLs. ", "const char *xattr_full_name(const struct xattr_handler *handler,    const char *name)": "xattr_full_name  -  Compute full attribute name from suffix     @handler:handler of the xattr_handler operation   @name:name passed to the xattr_handler operation     The get and set xattr handler operations are called with the remainder of   the attribute name after skipping the handler's prefix: for example, \"foo\"   is passed to the get operation of a handler with prefix \"user.\" to get   attribute \"user.foo\".  The full name is still \"there\" in the name though.     Note: the list xattr handler operation when called from the vfs is passed a   NULL name; some file systems use this operation internally, with varying   semantics. ", "static struct file_system_type *file_systems;static DEFINE_RWLOCK(file_systems_lock);/* WARNING: This can be used only if we _already_ own a reference ": "unregister_filesystem().  We can access the fields of list element if:  1) spinlock is held or  2) we hold the reference to the module.  The latter can be guaranteed by call of try_module_get(); if it  returned 0 we must skip the element, otherwise we got the reference.  Once the reference is obtained we can drop the spinlock. ", "loff_t vfs_setpos(struct file *file, loff_t offset, loff_t maxsize)": "generic_file_llseek,.read_iter= generic_file_read_iter,.mmap= generic_file_readonly_mmap,.splice_read= filemap_splice_read,};EXPORT_SYMBOL(generic_ro_fops);static inline bool unsigned_offsets(struct file  file){return file->f_mode & FMODE_UNSIGNED_OFFSET;}     vfs_setpos - update the file offset for lseek   @file:file structure in question   @offset:file offset to seek to   @maxsize:maximum file size     This is a low-level filesystem helper for updating the file offset to   the value specified by @offset if the given offset is valid and it is   not equal to the current file offset.     Return the specified offset on success and -EINVAL on invalid offset. ", "loff_tgeneric_file_llseek_size(struct file *file, loff_t offset, int whence,loff_t maxsize, loff_t eof)": "generic_file_llseek_size - generic llseek implementation for regular files   @file:file structure to seek on   @offset:file offset to seek to   @whence:type of seek   @size:max size of this file in file system   @eof:offset used for SEEK_END position     This is a variant of generic_file_llseek that allows passing in a custom   maximum file size and a custom EOF position, for e.g. hashed directories     Synchronization:   SEEK_SET and SEEK_END are unsynchronized (but atomic on 64bit platforms)   SEEK_CUR is synchronized against other SEEK_CURs, but not readwrites.   readwrites behave like SEEK_SET against seeks. ", "loff_t fixed_size_llseek(struct file *file, loff_t offset, int whence, loff_t size)": "fixed_size_llseek - llseek implementation for fixed-sized devices   @file:file structure to seek on   @offset:file offset to seek to   @whence:type of seek   @size:size of the file   ", "loff_t no_seek_end_llseek(struct file *file, loff_t offset, int whence)": "no_seek_end_llseek - llseek implementation for fixed-sized devices   @file:file structure to seek on   @offset:file offset to seek to   @whence:type of seek   ", "loff_t no_seek_end_llseek_size(struct file *file, loff_t offset, int whence, loff_t size)": "no_seek_end_llseek_size - llseek implementation for fixed-sized devices   @file:file structure to seek on   @offset:file offset to seek to   @whence:type of seek   @size:maximal offset allowed   ", "loff_t noop_llseek(struct file *file, loff_t offset, int whence)": "noop_llseek - No Operation Performed llseek implementation   @file:file structure to seek on   @offset:file offset to seek to   @whence:type of seek     This is an implementation of ->llseek useable for the rare special case when   userspace expects the seek to succeed but the (device) file is actually not   able to perform the seek. In this case you use noop_llseek() instead of   falling back to the default implementation of ->llseek. ", "if (offset >= inode->i_size) ": "default_llseek(struct file  file, loff_t offset, int whence){struct inode  inode = file_inode(file);loff_t retval;inode_lock(inode);switch (whence) {case SEEK_END:offset += i_size_read(inode);break;case SEEK_CUR:if (offset == 0) {retval = file->f_pos;goto out;}offset += file->f_pos;break;case SEEK_DATA:    In the generic case the entire file is data, so as   long as offset isn't at the end of the file then the   offset is data. ", "return -EOVERFLOW;} else if (unlikely((loff_t) (pos + count) < 0)) ": "rw_verify_area(int read_write, struct file  file, const loff_t  ppos, size_t count){if (unlikely((ssize_t) count < 0))return -EINVAL;if (ppos) {loff_t pos =  ppos;if (unlikely(pos < 0)) {if (!unsigned_offsets(file))return -EINVAL;if (count >= -pos)   both values are in 0..LLONG_MAX ", "if (unlikely(!file->f_op->read_iter || file->f_op->read))return warn_unsupported(file, \"read\");init_sync_kiocb(&kiocb, file);kiocb.ki_pos = pos ? *pos : 0;iov_iter_kvec(&iter, ITER_DEST, &iov, 1, iov.iov_len);ret = file->f_op->read_iter(&kiocb, &iter);if (ret > 0) ": "kernel_read(struct file  file, void  buf, size_t count, loff_t  pos){struct kvec iov = {.iov_base= buf,.iov_len= min_t(size_t, count, MAX_RW_COUNT),};struct kiocb kiocb;struct iov_iter iter;ssize_t ret;if (WARN_ON_ONCE(!(file->f_mode & FMODE_READ)))return -EINVAL;if (!(file->f_mode & FMODE_CAN_READ))return -EINVAL;    Also fail if ->read_iter and ->read are both wired up as that   implies very convoluted semantics. ", "if (unlikely(!file->f_op->write_iter || file->f_op->write))return warn_unsupported(file, \"write\");init_sync_kiocb(&kiocb, file);kiocb.ki_pos = pos ? *pos : 0;ret = file->f_op->write_iter(&kiocb, from);if (ret > 0) ": "kernel_write_iter(struct file  file, struct iov_iter  from, loff_t  pos){struct kiocb kiocb;ssize_t ret;if (WARN_ON_ONCE(!(file->f_mode & FMODE_WRITE)))return -EBADF;if (!(file->f_mode & FMODE_CAN_WRITE))return -EINVAL;    Also fail if ->write_iter and ->write are both wired up as that   implies very convoluted semantics. ", "ssize_t generic_copy_file_range(struct file *file_in, loff_t pos_in,struct file *file_out, loff_t pos_out,size_t len, unsigned int flags)": "generic_copy_file_range - copy data between two files   @file_in:file structure to read from   @pos_in:file offset to read from   @file_out:file structure to write data to   @pos_out:file offset to write data to   @len:amount of data to copy   @flags:copy flags     This is a generic filesystem helper to copy data from one file to another.   It has no constraints on the source or destination file owners - the files   can belong to different superblocks and different filesystem types. Short   copies are allowed.     This should be called from the @file_out filesystem, as per the   ->copy_file_range() method.     Returns the number of bytes copied or a negative error indicating the   failure. ", "if (!splice && file_out->f_op->copy_file_range) ": "vfs_copy_file_range(struct file  file_in, loff_t pos_in,    struct file  file_out, loff_t pos_out,    size_t len, unsigned int flags){ssize_t ret;bool splice = flags & COPY_FILE_SPLICE;if (flags & ~COPY_FILE_SPLICE)return -EINVAL;ret = generic_copy_file_checks(file_in, pos_in, file_out, pos_out, &len,       flags);if (unlikely(ret))return ret;ret = rw_verify_area(READ, file_in, &pos_in, len);if (unlikely(ret))return ret;ret = rw_verify_area(WRITE, file_out, &pos_out, len);if (unlikely(ret))return ret;if (len == 0)return 0;file_start_write(file_out);    Cloning is supported by more file systems, so we implement copy on   same sb using clone, but for filesystems where both clone and copy   are supported (e.g. nfs,cifs), we only call the copy method. ", "int generic_write_checks_count(struct kiocb *iocb, loff_t *count)": "generic_write_checks(), but takes size of write instead of iter. ", "int sync_mapping_buffers(struct address_space *mapping)": "sync_mapping_buffers - write out & wait upon a mapping's \"associated\" buffers   @mapping: the mapping which wants those buffers written     Starts IO against the buffers at mapping->private_list, and waits upon   that IO.     Basically, this is a convenience function for fsync().   @mapping is a file or directory which needs those buffers to be written for   a successful fsync(). ", "int generic_buffers_fsync_noflush(struct file *file, loff_t start, loff_t end,  bool datasync)": "generic_buffers_fsync_noflush - generic buffer fsync implementation   for simple filesystems with no inode lock     @file:file to synchronize   @start:start offset in bytes   @end:end offset in bytes (inclusive)   @datasync:only synchronize essential metadata if true     This is a generic implementation of the fsync method for simple   filesystems which track all non-inode metadata in the buffers list   hanging off the address_space structure. ", "/* * The buffer's backing address_space's private_lock must be held ": "mark_buffer_dirty_inode() is a data-plane operation.  It should   take an address_space, not an inode.  And it should be called   mark_buffer_dirty_fsync() to clearly define why those buffers are being   queued up.     FIXME: mark_buffer_dirty_inode() doesn't need to add the buffer to the   list if it is already on a list.  Because if the buffer is on a list,   it  must  already be on the right one.  If not, the filesystem is being   silly.  This will save a ton of locking.  But first we have to ensure   that buffers are taken  off  the old inode's list when they are freed   (presumably in truncate).  That requires careful auditing of all   filesystems (do it inside bforget()).  It could also be done by bringing   b_inode back. ", "folio_memcg_lock(folio);newly_dirty = !folio_test_set_dirty(folio);spin_unlock(&mapping->private_lock);if (newly_dirty)__folio_mark_dirty(folio, mapping, 1);folio_memcg_unlock(folio);if (newly_dirty)__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);return newly_dirty;}EXPORT_SYMBOL(block_dirty_folio": "block_dirty_folio(struct address_space  mapping, struct folio  folio){struct buffer_head  head;bool newly_dirty;spin_lock(&mapping->private_lock);head = folio_buffers(folio);if (head) {struct buffer_head  bh = head;do {set_buffer_dirty(bh);bh = bh->b_this_page;} while (bh != head);}    Lock out page's memcg migration to keep PageDirty   synchronized with per-memcg dirty page counters. ", "static struct buffer_head *__find_get_block_slow(struct block_device *bdev, sector_t block)": "try_to_free_buffers with the blockdev mapping's   private_lock.     Hack idea: for the blockdev mapping, private_lock contention   may be quite high.  This code could TryLock the page, and if that   succeeds, there is no need to take private_lock. ", "struct buffer_head *__getblk_gfp(struct block_device *bdev, sector_t block,     unsigned size, gfp_t gfp)": "__getblk_gfp() will locate (and, if necessary, create) the buffer_head   which corresponds to the passed block_device, block and size. The   returned buffer has its reference count incremented.     __getblk_gfp() will lock up the machine if grow_dev_page's   try_to_free_buffers() attempt is failing.  FIXME, perhaps? ", "struct buffer_head *__bread_gfp(struct block_device *bdev, sector_t block,   unsigned size, gfp_t gfp)": "__bread_gfp() - reads a specified block and returns the bh    @bdev: the block_device to read from    @block: number of block    @size: size (in bytes) to read    @gfp: page allocation flag      Reads a specified block, and returns buffer head that contains it.    The page cache can be allocated from non-movable area    not to prevent page migration if you set gfp to zero.    It returns NULL if the block was unreadable. ", "bh->b_data = (char *)(0 + offset);elsebh->b_data = page_address(page) + offset;}EXPORT_SYMBOL(set_bh_page": "set_bh_page(struct buffer_head  bh,struct page  page, unsigned long offset){bh->b_page = page;BUG_ON(offset >= PAGE_SIZE);if (PageHighMem(page))    This catches illegal uses and preserves the offset: ", "no_grow:if (head) ": "folio_set_bh(bh, folio, offset);}out:set_active_memcg(old_memcg);return head;    In case anything failed, we just free everything we got. ", "void block_invalidate_folio(struct folio *folio, size_t offset, size_t length)": "block_invalidate_folio - Invalidate part or all of a buffer-backed folio.   @folio: The folio which is affected.   @offset: start of the range to invalidate   @length: length of the range to invalidate     block_invalidate_folio() is called when all or part of the folio has been   invalidated by a truncate operation.     block_invalidate_folio() does not have to release all buffers, but it must   ensure that no dirty buffer is left outside @offset and that no IO   is underway against any of the blocks which are outside the truncation   point.  Because the caller is about to free (and possibly reuse) those   blocks on-disk. ", "void clean_bdev_aliases(struct block_device *bdev, sector_t block, sector_t len)": "clean_bdev_aliases: clean a range of buffers in block device   @bdev: Block device to clean buffers in   @block: Start of a range of blocks to clean   @len: Number of blocks to clean     We are taking a range of blocks for data and we don't want writeback of any   buffer-cache aliases starting from return from this function and until the   moment when something will explicitly mark the buffer dirty (hopefully that   will not happen until we will free that block ;-) We don't even need to mark   it not-uptodate - nobody can expect anything from a newly allocated buffer   anyway. We used to use unmap_buffer() for such invalidation, but that was   wrong. We definitely don't want to mark the alias unmapped, for example - it   would confuse anyone who might pick it with bread() afterwards...     Also..  Note that bforget() doesn't lock the buffer.  So there can be   writeout IO going on against recently-freed buffers.  We don't wait on that   IO in bforget() - it's more efficient to wait on the IO only if we really   need to.  That happens here. ", "bh = head;blocksize = bh->b_size;bbits = block_size_bits(blocksize);block = (sector_t)folio->index << (PAGE_SHIFT - bbits);last_block = (i_size_read(inode) - 1) >> bbits;/* * Get all the dirty buffers mapped to disk addresses and * handle any aliases from the underlying blockdev's mapping. ": "__block_write_full_folio(struct inode  inode, struct folio  folio,get_block_t  get_block, struct writeback_control  wbc,bh_end_io_t  handler){int err;sector_t block;sector_t last_block;struct buffer_head  bh,  head;unsigned int blocksize, bbits;int nr_underway = 0;blk_opf_t write_flags = wbc_to_write_flags(wbc);head = folio_create_buffers(folio, inode,    (1 << BH_Dirty) | (1 << BH_Uptodate));    Be very careful.  We have no exclusion from block_dirty_folio   here, and the (potentially unmapped) buffers may become dirty at   any time.  If a buffer becomes dirty here after we've inspected it   then we just miss that fact, and the folio stays dirty.     Buffers outside i_size may be dirtied by block_dirty_folio;   handle that here by just cleaning them. ", "while(wait_bh > wait) ": "block_write_begin_int(struct folio  folio, loff_t pos, unsigned len,get_block_t  get_block, const struct iomap  iomap){unsigned from = pos & (PAGE_SIZE - 1);unsigned to = from + len;struct inode  inode = folio->mapping->host;unsigned block_start, block_end;sector_t block;int err = 0;unsigned blocksize, bbits;struct buffer_head  bh,  head,  wait[2],   wait_bh=wait;BUG_ON(!folio_test_locked(folio));BUG_ON(from > PAGE_SIZE);BUG_ON(to > PAGE_SIZE);BUG_ON(from > to);head = folio_create_buffers(folio, inode, 0);blocksize = head->b_size;bbits = block_size_bits(blocksize);block = (sector_t)folio->index << (PAGE_SHIFT - bbits);for(bh = head, block_start = 0; bh != head || !block_start;    block++, block_start=block_end, bh = bh->b_this_page) {block_end = block_start + blocksize;if (block_end <= from || block_start >= to) {if (folio_test_uptodate(folio)) {if (!buffer_uptodate(bh))set_buffer_uptodate(bh);}continue;}if (buffer_new(bh))clear_buffer_new(bh);if (!buffer_mapped(bh)) {WARN_ON(bh->b_size != blocksize);if (get_block) {err = get_block(inode, block, bh, 1);if (err)break;} else {iomap_to_bh(inode, block, bh, iomap);}if (buffer_new(bh)) {clean_bdev_bh_alias(bh);if (folio_test_uptodate(folio)) {clear_buffer_new(bh);set_buffer_uptodate(bh);mark_buffer_dirty(bh);continue;}if (block_end > to || block_start < from)folio_zero_segments(folio,to, block_end,block_start, from);continue;}}if (folio_test_uptodate(folio)) {if (!buffer_uptodate(bh))set_buffer_uptodate(bh);continue; }if (!buffer_uptodate(bh) && !buffer_delay(bh) &&    !buffer_unwritten(bh) &&     (block_start < from || block_end > to)) {bh_read_nowait(bh, 0); wait_bh++=bh;}}    If we issued read requests - let them complete. ", "if (!folio_test_uptodate(folio))copied = 0;folio_zero_new_buffers(folio, start+copied, start+len);}flush_dcache_folio(folio);/* This could be a short (even 0-length) commit ": "block_write_end(struct file  file, struct address_space  mapping,loff_t pos, unsigned len, unsigned copied,struct page  page, void  fsdata){struct folio  folio = page_folio(page);struct inode  inode = mapping->host;size_t start = pos - folio_pos(folio);if (unlikely(copied < len)) {    The buffers that were written will now be uptodate, so   we don't have to worry about a read_folio reading them   and overwriting a partial write. However if we have   encountered a short write and only partially written   into a buffer, it will not be marked uptodate, so a   read_folio might come in and destroy our partial write.     Do the simplest thing, and just treat any short write to a   non uptodate folio as a zero-length write, and force the   caller to redo the whole thing. ", "if (pos + copied > inode->i_size) ": "generic_write_end(struct file  file, struct address_space  mapping,loff_t pos, unsigned len, unsigned copied,struct page  page, void  fsdata){struct inode  inode = mapping->host;loff_t old_size = inode->i_size;bool i_size_changed = false;copied = block_write_end(file, mapping, pos, len, copied, page, fsdata);    No need to use i_size_read() here, the i_size cannot change under us   because we hold i_rwsem.     But it's important to update i_size while still holding page lock:   page writeout could otherwise come in and zero beyond i_size. ", "bool block_is_partially_uptodate(struct folio *folio, size_t from, size_t count)": "block_is_partially_uptodate checks whether buffers within a folio are   uptodate or not.     Returns true if all buffers which correspond to the specified part   of the folio are uptodate. ", "static void end_buffer_async_read_io(struct buffer_head *bh, int uptodate)": "block_read_full_folio() - pages   which come unlocked at the end of IO. ", "if (!partial)folio_mark_uptodate(folio);return 0;}/* * block_write_begin takes care of the basic task of block allocation and * bringing partial write blocks uptodate first. * * The filesystem needs to handle block truncation upon failure. ": "block_commit_write(struct inode  inode, struct folio  folio,size_t from, size_t to){size_t block_start, block_end;bool partial = false;unsigned blocksize;struct buffer_head  bh,  head;bh = head = folio_buffers(folio);blocksize = bh->b_size;block_start = 0;do {block_end = block_start + blocksize;if (block_end <= from || block_start >= to) {if (!buffer_uptodate(bh))partial = true;} else {set_buffer_uptodate(bh);mark_buffer_dirty(bh);}if (buffer_new(bh))clear_buffer_new(bh);block_start = block_end;bh = bh->b_this_page;} while (bh != head);    If this is a partial write which happened to make all buffers   uptodate then we can optimize away a bogus read_folio() for   the next read(). Here we 'discover' whether the folio went   uptodate as a result of this (potentially partial) write. ", "int block_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf, get_block_t get_block)": "block_page_mkwrite() is not allowed to change the file size as it gets   called from a page fault handler when a page is first dirtied. Hence we must   be careful to check for EOF conditions here. We set the page up correctly   for a written page which means we get ENOSPC checking when writing into   holes and correct delalloc and unwritten extent mapping on filesystems that   support these features.     We are not allowed to take the i_mutex here so we have to play games to   protect against truncate races as the page could now be beyond EOF.  Because   truncate writes the inode size before removing pages, once we have the   page lock we can determine safely if the page is beyond EOF. If it is not   beyond EOF, then the page is guaranteed safe against truncation until we   unlock the page.     Direct callers of this function should protect against filesystem freezing   using sb_start_pagefault() - sb_end_pagefault() functions. ", "if (!length)return 0;length = blocksize - length;iblock = (sector_t)index << (PAGE_SHIFT - inode->i_blkbits);folio = filemap_grab_folio(mapping, index);if (IS_ERR(folio))return PTR_ERR(folio);bh = folio_buffers(folio);if (!bh) ": "block_truncate_page(struct address_space  mapping,loff_t from, get_block_t  get_block){pgoff_t index = from >> PAGE_SHIFT;unsigned blocksize;sector_t iblock;size_t offset, length, pos;struct inode  inode = mapping->host;struct folio  folio;struct buffer_head  bh;int err = 0;blocksize = i_blocksize(inode);length = from & (blocksize - 1);  Block boundary? Nothing to do ", "void end_buffer_async_write(struct buffer_head *bh, int uptodate)": "block_write_full_page() - pages which are unlocked   during IO, and which have PageWriteback cleared upon IO completion. ", "void buffer_check_dirty_writeback(struct folio *folio,     bool *dirty, bool *writeback)": "submit_bh_wbc(blk_opf_t opf, struct buffer_head  bh,  struct writeback_control  wbc);#define BH_ENTRY(list) list_entry((list), struct buffer_head, b_assoc_buffers)inline void touch_buffer(struct buffer_head  bh){trace_block_touch_buffer(bh);folio_mark_accessed(bh->b_folio);}EXPORT_SYMBOL(touch_buffer);void __lock_buffer(struct buffer_head  bh){wait_on_bit_lock_io(&bh->b_state, BH_Lock, TASK_UNINTERRUPTIBLE);}EXPORT_SYMBOL(__lock_buffer);void unlock_buffer(struct buffer_head  bh){clear_bit_unlock(BH_Lock, &bh->b_state);smp_mb__after_atomic();wake_up_bit(&bh->b_state, BH_Lock);}EXPORT_SYMBOL(unlock_buffer);    Returns if the folio has dirty or writeback buffers. If all the buffers   are unlocked and clean then the folio_test_dirty information is stale. If   any of the buffers are locked, it is assumed they are locked for IO. ", "static int osync_buffers_list(spinlock_t *lock, struct list_head *list)": "write_dirty_buffer   as you dirty the buffers, and then use osync_inode_buffers to wait for   completion.  Any other dirty buffers which are not yet queued for   write will not be flushed to disk by the osync. ", "if (!buffer_mapped(bh)) ": "sync_dirty_buffer(struct buffer_head  bh, blk_opf_t op_flags){WARN_ON(atomic_read(&bh->b_count) < 1);lock_buffer(bh);if (test_clear_buffer_dirty(bh)) {    The bh should be mapped, but it might not be if the   device was hot-removed. Not much we can do but fail the IO. ", "folio_set_bh(bh, folio, offset);}out:set_active_memcg(old_memcg);return head;/* * In case anything failed, we just free everything we got. ": "alloc_buffer_head(gfp);if (!bh)goto no_grow;bh->b_this_page = head;bh->b_blocknr = -1;head = bh;bh->b_size = size;  Link the buffer to its folio ", " static sector_t folio_init_buffers(struct folio *folio,struct block_device *bdev, sector_t block, int size)": "free_buffer_head(bh);} while (head);}goto out;}EXPORT_SYMBOL_GPL(folio_alloc_buffers);struct buffer_head  alloc_page_buffers(struct page  page, unsigned long size,       bool retry){return folio_alloc_buffers(page_folio(page), size, retry);}EXPORT_SYMBOL_GPL(alloc_page_buffers);static inline void link_dev_buffers(struct folio  folio,struct buffer_head  head){struct buffer_head  bh,  tail;bh = head;do {tail = bh;bh = bh->b_this_page;} while (bh);tail->b_this_page = head;folio_attach_private(folio, head);}static sector_t blkdev_max_block(struct block_device  bdev, unsigned int size){sector_t retval = ~((sector_t)0);loff_t sz = bdev_nr_bytes(bdev);if (sz) {unsigned int sizebits = blksize_bits(size);retval = (sz >> sizebits);}return retval;}    Initialise the state of a blockdev folio's buffers. ", "int bh_uptodate_or_lock(struct buffer_head *bh)": "bh_uptodate_or_lock - Test whether the buffer is uptodate   @bh: struct buffer_head     Return true if the buffer is up-to-date and false,   with the buffer locked, if not. ", "int __bh_read(struct buffer_head *bh, blk_opf_t op_flags, bool wait)": "__bh_read - Submit read for a locked buffer   @bh: struct buffer_head   @op_flags: appending REQ_OP_  flags besides REQ_OP_READ   @wait: wait until reading finish     Returns zero on success or don't wait, and -EIO on error. ", "void __bh_read_batch(int nr, struct buffer_head *bhs[],     blk_opf_t op_flags, bool force_lock)": "__bh_read_batch - Submit read for a batch of unlocked buffers   @nr: entry number of the buffer batch   @bhs: a batch of struct buffer_head   @op_flags: appending REQ_OP_  flags besides REQ_OP_READ   @force_lock: force to get a lock on the buffer if set, otherwise drops any                buffer that cannot lock.     Returns zero on success or don't wait, and -EIO on error. ", "pipe_lock_nested(pipe, I_MUTEX_PARENT);}EXPORT_SYMBOL(pipe_lock": "pipe_lock_nested(struct pipe_inode_info  pipe, int subclass){if (pipe->files)mutex_lock_nested(&pipe->mutex, subclass);}void pipe_lock(struct pipe_inode_info  pipe){    pipe_lock() nests non-pipe inode locks (for writing to a file) ", "bool generic_pipe_buf_try_steal(struct pipe_inode_info *pipe,struct pipe_buffer *buf)": "generic_pipe_buf_try_steal - attempt to take ownership of a &pipe_buffer   @pipe:the pipe that the buffer belongs to   @buf:the buffer to attempt to steal     Description:  This function attempts to steal the &struct page attached to  @buf. If successful, this function returns 0 and returns with  the page locked. The caller may then reuse the page for whatever  he wishes; the typical use is insertion into a different file  page cache. ", "bool generic_pipe_buf_get(struct pipe_inode_info *pipe, struct pipe_buffer *buf)": "generic_pipe_buf_get - get a reference to a &struct pipe_buffer   @pipe:the pipe that the buffer belongs to   @buf:the buffer to get a reference to     Description:  This function grabs an extra reference to @buf. It's used in  the tee() system call, when we duplicate the buffers in one  pipe into another. ", "void generic_pipe_buf_release(struct pipe_inode_info *pipe,      struct pipe_buffer *buf)": "generic_pipe_buf_release - put a reference to a &struct pipe_buffer   @pipe:the pipe that the buffer belongs to   @buf:the buffer to put a reference to     Description:  This function releases a reference to @buf. ", "WARN_ON(!rwsem_is_locked(&sb->s_umount));/* * No point in syncing out anything if the filesystem is read-only. ": "sync_filesystem(struct super_block  sb){int ret = 0;    We need to be protected against the filesystem going from   ro to rw or vice versa. ", "int vfs_fsync_range(struct file *file, loff_t start, loff_t end, int datasync)": "vfs_fsync_range - helper to sync a range of data & metadata to disk   @file:file to sync   @start:offset in bytes of the beginning of data range to sync   @end:offset in bytes of the end of data range (inclusive)   @datasync:perform only datasync     Write back data in range @start..@end and metadata for @file to disk.  If   @datasync is set only metadata needed to access modified file data is   written. ", "smp_mb();if (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))inode->i_state |= I_DIRTY_PAGES;else if (unlikely(inode->i_state & I_PINNING_FSCACHE_WB)) ": "__mark_inode_dirty().  This allows   __mark_inode_dirty() to test i_state without grabbing i_lock -   either they see the I_DIRTY bits cleared or we see the dirtied   inode.     I_DIRTY_PAGES is always cleared together above even if @mapping   still has dirty pages.  The flag is reinstated after smp_mb() if   necessary.  This guarantees that either __mark_inode_dirty()   sees clear I_DIRTY_PAGES or we see PAGECACHE_TAG_DIRTY. ", "void writeback_inodes_sb_nr(struct super_block *sb,    unsigned long nr,    enum wb_reason reason)": "writeback_inodes_sb_nr(struct super_block  sb, unsigned long nr,     enum wb_reason reason, bool skip_if_busy){struct backing_dev_info  bdi = sb->s_bdi;DEFINE_WB_COMPLETION(done, bdi);struct wb_writeback_work work = {.sb= sb,.sync_mode= WB_SYNC_NONE,.tagged_writepages= 1,.done= &done,.nr_pages= nr,.reason= reason,};if (!bdi_has_dirty_io(bdi) || bdi == &noop_backing_dev_info)return;WARN_ON(!rwsem_is_locked(&sb->s_umount));bdi_split_work_to_wbs(sb->s_bdi, &work, skip_if_busy);wb_wait_for_completion(&done);}     writeback_inodes_sb_nr -writeback dirty inodes from given super_block   @sb: the superblock   @nr: the number of pages to write   @reason: reason why some writeback work initiated     Start writeback on some inodes on this super_block. No guarantees are made   on how many (if any) will be written, and this function does not wait   for IO completion of submitted IO. ", "void try_to_writeback_inodes_sb(struct super_block *sb, enum wb_reason reason)": "try_to_writeback_inodes_sb - try to start writeback if none underway   @sb: the superblock   @reason: reason why some writeback work was initiated     Invoke __writeback_inodes_sb_nr if no writeback is currently underway. ", "down_read(&bdi->wb_switch_rwsem);/* * By the time control reaches here, RCU grace period has passed * since I_WB_SWITCH assertion and all wb stat update transactions * between unlocked_inode_to_wb_begin/end() are guaranteed to be * synchronizing against the i_pages lock. * * Grabbing old_wb->list_lock, inode->i_lock and the i_pages lock * gives us exclusion against all wb related operations on @inode * including IO list manipulations and stat updates. ": "sync_inodes_sb() is   being issued, sync_inodes_sb() might miss it.  Synchronize. ", "int write_inode_now(struct inode *inode, int sync)": "write_inode_now-write an inode to disk   @inode: inode to write to disk   @sync: whether the write should be synchronous or not     This function commits an inode to disk immediately if it is dirty. This is   primarily needed by knfsd.     The caller must either have a ref on the inode or must have set I_WILL_FREE. ", "int sync_inode_metadata(struct inode *inode, int wait)": "sync_inode_metadata - write an inode to disk   @inode: the inode to sync   @wait: wait for IO to complete.     Write an inode to disk and adjust its dirty state after completion.     Note: only writes the actual inode, no associated data or other metadata. ", "if (atomic_read(&files->count) > 1)synchronize_rcu();spin_lock(&files->file_lock);if (!new_fdt)return -ENOMEM;/* * extremely unlikely race - sysctl_nr_open decreased between the check in * caller and alloc_fdtable().  Cheaper to catch it here... ": "fd_install() have seen resize_in_progress   or have finished their rcu_read_lock_sched() section. ", "if (unlikely(!get_file_rcu(file)))continue;/* *  (b) the file table entry has changed under us. *       Note that we don't need to re-check the 'fdt->fd' *       pointer having changed, because it always goes *       hand-in-hand with 'fdt'. * * If so, we need to put our ref and try again. ": "fget_files_rcu(struct files_struct  files,unsigned int fd, fmode_t mask){for (;;) {struct file  file;struct fdtable  fdt = rcu_dereference_raw(files->fdt);struct file __rcu   fdentry;if (unlikely(fd >= fdt->max_fds))return NULL;fdentry = fdt->fd + array_index_nospec(fd, fdt->max_fds);file = rcu_dereference_raw( fdentry);if (unlikely(!file))return NULL;if (unlikely(file->f_mode & mask))return NULL;    Ok, we have a file pointer. However, because we do   this all locklessly under RCU, we may be racing with   that file being closed.     Such a race can take two forms:      (a) the file ref already went down to zero,        and get_file_rcu() fails. Just try again: ", "struct files_struct *files;unsigned int fd = *ret_fd;struct file *file = NULL;task_lock(task);files = task->files;if (files) ": "task_lookup_next_fd_rcu(struct task_struct  task, unsigned int  ret_fd){  Must be called with rcu_read_lock held ", "int vfs_parse_fs_param_source(struct fs_context *fc, struct fs_parameter *param)": "vfs_parse_fs_param_source - Handle setting \"source\" via parameter   @fc: The filesystem context to modify   @param: The parameter     This is a simple helper for filesystems to verify that the \"source\" they   accept is sane.     Returns 0 on success, -ENOPARAM if this is not  \"source\" parameter, and   -EINVAL otherwise. In the event of failure, supplementary error information    is logged. ", "int vfs_parse_fs_string(struct fs_context *fc, const char *key,const char *value, size_t v_size)": "vfs_parse_fs_string - Convenience function to just parse a string. ", "int generic_parse_monolithic(struct fs_context *fc, void *data)": "generic_parse_monolithic - Parse key[=val][,key[=val]]  mount data   @ctx: The superblock configuration to fill in.   @data: The data to parse     Parse a blob of data that's in key[=val][,key[=val]]  form.  This can be   called from the ->monolithic_mount_data() fs_context operation.     Returns 0 on success or the error returned by the ->parse_option() fs_context   operation on failure. ", "ret = fc->ops->dup(fc, src_fc);if (ret < 0)goto err_fc;ret = security_fs_context_dup(fc, src_fc);if (ret < 0)goto err_fc;return fc;err_fc:put_fs_context(fc);return ERR_PTR(ret);}EXPORT_SYMBOL(vfs_dup_fs_context": "vfs_dup_fs_context(struct fs_context  src_fc){struct fs_context  fc;int ret;if (!src_fc->ops->dup)return ERR_PTR(-EOPNOTSUPP);fc = kmemdup(src_fc, sizeof(struct fs_context), GFP_KERNEL);if (!fc)return ERR_PTR(-ENOMEM);mutex_init(&fc->uapi_mutex);fc->fs_private= NULL;fc->s_fs_info= NULL;fc->source= NULL;fc->security= NULL;get_filesystem(fc->fs_type);get_net(fc->net_ns);get_user_ns(fc->user_ns);get_cred(fc->cred);if (fc->log.log)refcount_inc(&fc->log.log->usage);  Can't call put until we've called ->dup ", "void logfc(struct fc_log *log, const char *prefix, char level, const char *fmt, ...)": "logfc - Log a message to a filesystem context   @fc: The filesystem context to log to.   @fmt: The format of the buffer. ", "struct fs_context *vfs_dup_fs_context(struct fs_context *src_fc)": "put_fs_context(fc);return ERR_PTR(ret);}struct fs_context  fs_context_for_mount(struct file_system_type  fs_type,unsigned int sb_flags){return alloc_fs_context(fs_type, NULL, sb_flags, 0,FS_CONTEXT_FOR_MOUNT);}EXPORT_SYMBOL(fs_context_for_mount);struct fs_context  fs_context_for_reconfigure(struct dentry  dentry,unsigned int sb_flags,unsigned int sb_flags_mask){return alloc_fs_context(dentry->d_sb->s_type, dentry, sb_flags,sb_flags_mask, FS_CONTEXT_FOR_RECONFIGURE);}EXPORT_SYMBOL(fs_context_for_reconfigure);struct fs_context  fs_context_for_submount(struct file_system_type  type,   struct dentry  reference){return alloc_fs_context(type, reference, 0, 0, FS_CONTEXT_FOR_SUBMOUNT);}EXPORT_SYMBOL(fs_context_for_submount);void fc_drop_locked(struct fs_context  fc){struct super_block  sb = fc->root->d_sb;dput(fc->root);fc->root = NULL;deactivate_locked_super(sb);}static void legacy_fs_context_free(struct fs_context  fc);     vfs_dup_fc_config: Duplicate a filesystem context.   @src_fc: The context to copy. ", "void generic_fillattr(struct mnt_idmap *idmap, struct inode *inode,      struct kstat *stat)": "generic_fillattr - Fill in the basic attributes from the inode struct   @idmap:idmap of the mount the inode was found from   @inode:Inode to use as the source   @stat:Where to fill in the attributes     Fill in the basic attributes in the kstat structure from data that's to be   found on the VFS inode structure.  This is the default if no getattr inode   operation is supplied.     If the inode has been found through an idmapped mount the idmap of   the vfsmount must be passed through @idmap. This function will then   take care to map the inode according to @idmap before filling in the   uid and gid filds. On non-idmapped mounts or if permission checking is to be   performed on the raw inode simply passs @nop_mnt_idmap. ", "void generic_fill_statx_attr(struct inode *inode, struct kstat *stat)": "generic_fill_statx_attr - Fill in the statx attributes from the inode flags   @inode:Inode to use as the source   @stat:Where to fill in the attribute flags     Fill in the STATX_ATTR_  flags in the kstat structure for properties of the   inode that are published on i_flags and enforced by the VFS. ", "int vfs_getattr_nosec(const struct path *path, struct kstat *stat,      u32 request_mask, unsigned int query_flags)": "vfs_getattr_nosec - getattr without security checks   @path: file to get attributes from   @stat: structure to return attributes in   @request_mask: STATX_xxx flags indicating what the caller wants   @query_flags: Query mode (AT_STATX_SYNC_TYPE)     Get attributes without calling security_inode_getattr.     Currently the only caller other than vfs_getattr is internal to the   filehandle lookup code, which uses only the inode number and returns no   attributes to any user.  Any other code probably wants vfs_getattr. ", "inode->i_blocks = bytes >> 9;inode->i_bytes = bytes & 511;}EXPORT_SYMBOL(inode_set_bytes": "inode_set_bytes(struct inode  inode, loff_t bytes){  Caller is here responsible for sufficient locking   (ie. inode->i_lock) ", "struct file *alloc_empty_file(int flags, const struct cred *cred)": "fput() time. ", "int inode_init_always(struct super_block *sb, struct inode *inode)": "inode_init_always - perform inode structure initialisation   @sb: superblock inode belongs to   @inode: inode to initialise     These are initializations that need to be done on every inode   allocation as the fields are not initialised by slab allocation. ", "void drop_nlink(struct inode *inode)": "drop_nlink - directly drop an inode's link count   @inode: inode     This is a low-level filesystem helper to replace any   direct filesystem manipulation of i_nlink.  In cases   where we are attempting to track writes to the   filesystem, a decrement to zero means an imminent   write when the file is truncated and actually unlinked   on the filesystem. ", "void clear_nlink(struct inode *inode)": "clear_nlink - directly zero an inode's link count   @inode: inode     This is a low-level filesystem helper to replace any   direct filesystem manipulation of i_nlink.  See   drop_nlink() for why we care about i_nlink hitting zero. ", "void set_nlink(struct inode *inode, unsigned int nlink)": "set_nlink - directly set an inode's link count   @inode: inode   @nlink: new nlink (should be non-zero)     This is a low-level filesystem helper to replace any   direct filesystem manipulation of i_nlink. ", "void inc_nlink(struct inode *inode)": "inc_nlink - directly increment an inode's link count   @inode: inode     This is a low-level filesystem helper to replace any   direct filesystem manipulation of i_nlink.  Currently,   it is only here for parity with dec_nlink(). ", "void __insert_inode_hash(struct inode *inode, unsigned long hashval)": "__insert_inode_hash - hash an inode  @inode: unhashed inode  @hashval: unsigned long value used to locate this object in the  inode_hashtable.    Add an inode to the inode hash for this superblock. ", "void __remove_inode_hash(struct inode *inode)": "__remove_inode_hash - remove an inode from the hash  @inode: inode to unhash    Remove an inode from the superblock. ", "xa_lock_irq(&inode->i_data.i_pages);BUG_ON(inode->i_data.nrpages);/* * Almost always, mapping_empty(&inode->i_data) here; but there are * two known and long-standing ways in which nodes may get left behind * (when deep radix-tree node allocation failed partway; or when THP * collapse_file() failed). Until those two known cases are cleaned up, * or a cleanup function is called here, do not BUG_ON(!mapping_empty), * nor even WARN_ON(!mapping_empty). ": "clear_inode(struct inode  inode){    We have to cycle the i_pages lock here because reclaim can be in the   process of removing the last page (in __filemap_remove_folio())   and we must not free the mapping under it. ", "if (unlikely(!res))res++;*p = res;put_cpu_var(last_ino);return res;}EXPORT_SYMBOL(get_next_ino": "get_next_ino(void){unsigned int  p = &get_cpu_var(last_ino);unsigned int res =  p;#ifdef CONFIG_SMPif (unlikely((res & (LAST_INO_BATCH-1)) == 0)) {static atomic_t shared_last_ino;int next = atomic_add_return(LAST_INO_BATCH, &shared_last_ino);res = next - LAST_INO_BATCH;}#endifres++;  get_next_ino should not provide a 0 inode number ", "struct inode *new_inode_pseudo(struct super_block *sb)": "new_inode_pseudo - obtain an inode  @sb: superblock    Allocates a new inode for given superblock.  Inode wont be chained in superblock s_inodes list  This means :  - fs can't be unmount  - quotas, fsnotify, writeback can't work ", "if (lockdep_match_class(&inode->i_rwsem, &type->i_mutex_key)) ": "lockdep_annotate_inode_mutex_key(struct inode  inode){if (S_ISDIR(inode->i_mode)) {struct file_system_type  type = inode->i_sb->s_type;  Set new key only if filesystem hasn't already changed it ", "void unlock_new_inode(struct inode *inode)": "unlock_new_inode - clear the I_NEW state and wake up any waiters   @inode:new inode to unlock     Called when the inode is fully initialised to clear the new state of the   inode and wake up anyone waiting for the inode to finish initialisation. ", "void lock_two_nondirectories(struct inode *inode1, struct inode *inode2)": "lock_two_nondirectories - take two i_mutexes on non-directory objects     Lock any non-NULL argument. Passed objects must not be directories.   Zero, one or two objects may be locked by this function.     @inode1: first inode to lock   @inode2: second inode to lock ", "void unlock_two_nondirectories(struct inode *inode1, struct inode *inode2)": "unlock_two_nondirectories - release locks from lock_two_nondirectories()   @inode1: first inode to unlock   @inode2: second inode to unlock ", "struct inode *inode_insert5(struct inode *inode, unsigned long hashval,    int (*test)(struct inode *, void *),    int (*set)(struct inode *, void *), void *data)": "iget5_locked() for callers that don't want to fail on memory   allocation of inode.     If the inode is not in cache, insert the pre-allocated inode to cache and   return it locked, hashed, and with the I_NEW flag set. The file system gets   to fill it in before unlocking it via unlock_new_inode().     Note both @test and @set are called with the inode_hash_lock held, so can't   sleep. ", "static struct inode *find_inode_fast(struct super_block *sb,struct hlist_head *head, unsigned long ino)": "iget_locked for details. ", "static unsigned int i_hash_mask __read_mostly;static unsigned int i_hash_shift __read_mostly;static struct hlist_head *inode_hashtable __read_mostly;static __cacheline_aligned_in_smp DEFINE_SPINLOCK(inode_hash_lock);/* * Empty aops. Can be used for the cases where the user does not * define any of the address_space operations. ": "iunique_lock     inode_hash_lock ", "inode = NULL;}return inode;}EXPORT_SYMBOL(igrab": "igrab(struct inode  inode){spin_lock(&inode->i_lock);if (!(inode->i_state & (I_FREEING|I_WILL_FREE))) {__iget(inode);spin_unlock(&inode->i_lock);} else {spin_unlock(&inode->i_lock);    Handle the case where s_op->clear_inode is not been   called yet, and somebody is calling igrab   while the inode is getting freed. ", "struct inode *ilookup5_nowait(struct super_block *sb, unsigned long hashval,int (*test)(struct inode *, void *), void *data)": "ilookup5_nowait - search for an inode in the inode cache   @sb:super block of file system to search   @hashval:hash value (usually inode number) to search for   @test:callback used for comparisons between inodes   @data:opaque data pointer to pass to @test     Search for the inode specified by @hashval and @data in the inode cache.   If the inode is in the cache, the inode is returned with an incremented   reference count.     Note: I_NEW is not waited upon so you have to be very careful what you do   with the returned inode.  You probably should be using ilookup5() instead.     Note2: @test is called with the inode_hash_lock held, so can't sleep. ", "struct inode *iget_locked(struct super_block *sb, unsigned long ino)": "ilookup5(sb, hashval, test, data);if (!inode) {struct inode  new = alloc_inode(sb);if (new) {new->i_state = 0;inode = inode_insert5(new, hashval, test, set, data);if (unlikely(inode != new))destroy_inode(new);}}return inode;}EXPORT_SYMBOL(iget5_locked);     iget_locked - obtain an inode from a mounted file system   @sb:super block of file system   @ino:inode number to get     Search for the inode specified by @ino in the inode cache and if present   return it with an increased reference count. This is for file systems   where the inode number is sufficient for unique identification of an inode.     If the inode is not in cache, allocate a new inode and return it locked,   hashed, and with the I_NEW flag set.  The file system gets to fill it in   before unlocking it via unlock_new_inode(). ", "struct inode *find_inode_nowait(struct super_block *sb,unsigned long hashval,int (*match)(struct inode *, unsigned long,     void *),void *data)": "find_inode_nowait - find an inode in the inode cache   @sb:super block of file system to search   @hashval:hash value (usually inode number) to search for   @match:callback used for comparisons between inodes   @data:opaque data pointer to pass to @match     Search for the inode specified by @hashval and @data in the inode   cache, where the helper function @match will return 0 if the inode   does not match, 1 if the inode does match, and -1 if the search   should be stopped.  The @match function must be responsible for   taking the i_lock spin_lock and checking i_state for an inode being   freed or being initialized, and incrementing the reference count   before returning 1.  It also must not sleep, since it is called with   the inode_hash_lock spinlock held.     This is a even more generalized version of ilookup5() when the   function must never block --- find_inode() can block in   __wait_on_freeing_inode() --- or when the caller can not increment   the reference count because the resulting iput() might cause an   inode eviction.  The tradeoff is that the @match funtion must be   very carefully implemented. ", "struct inode *find_inode_rcu(struct super_block *sb, unsigned long hashval,     int (*test)(struct inode *, void *), void *data)": "find_inode_rcu - find an inode in the inode cache   @sb:Super block of file system to search   @hashval:Key to hash   @test:Function to test match on an inode   @data:Data for test function     Search for the inode specified by @hashval and @data in the inode cache,   where the helper function @test will return 0 if the inode does not match   and 1 if it does.  The @test function must be responsible for taking the   i_lock spin_lock and checking i_state for an inode being freed or being   initialized.     If successful, this will return the inode for which the @test function   returned 1 and NULL otherwise.     The @test function is not permitted to take a ref on any inode presented.   It is also not permitted to sleep.     The caller must hold the RCU read lock. ", "struct inode *find_inode_by_ino_rcu(struct super_block *sb,    unsigned long ino)": "find_inode_by_ino_rcu - Find an inode in the inode cache   @sb:Super block of file system to search   @ino:The inode number to match     Search for the inode specified by @hashval and @data in the inode cache,   where the helper function @test will return 0 if the inode does not match   and 1 if it does.  The @test function must be responsible for taking the   i_lock spin_lock and checking i_state for an inode being freed or being   initialized.     If successful, this will return the inode for which the @test function   returned 1 and NULL otherwise.     The @test function is not permitted to take a ref on any inode presented.   It is also not permitted to sleep.     The caller must hold the RCU read lock. ", "static enum lru_status inode_lru_isolate(struct list_head *item,struct list_lru_one *lru, spinlock_t *lru_lock, void *arg)": "iput_final(). When we encounter such an   inode, clear the flag and move it to the back of the LRU so it gets another   pass through the LRU before it gets reclaimed. This is necessary because of   the fact we are doing lazy LRU updates to minimise lock contention so the   LRU does not have strict ordering. Hence we don't want to reclaim inodes   with this flag set because they are the inodes that are out of order. ", "int bmap(struct inode *inode, sector_t *block)": "bmap- find a block number in a file  @inode:  inode owning the block number being requested  @block: pointer containing the block to find    Replaces the value in `` block`` with the block number on the device holding  corresponding to the requested block number in the file.  That is, asked for block 4 of inode 1 the function will replace the  4 in `` block``, with disk block relative to the disk start that holds that  block of the file.    Returns -EINVAL in case of error, 0 otherwise. If mapping falls into a  hole, returns 0 and `` block`` is also set to 0. ", "now = current_time(inode);inode_update_time(inode, &now, S_ATIME);__mnt_drop_write(mnt);skip_update:sb_end_write(inode->i_sb);}EXPORT_SYMBOL(touch_atime": "touch_atime(const struct path  path){struct vfsmount  mnt = path->mnt;struct inode  inode = d_inode(path->dentry);struct timespec64 now;if (!atime_needs_update(path, inode))return;if (!sb_start_write_trylock(inode->i_sb))return;if (__mnt_want_write(mnt) != 0)goto skip_update;    File systems can error out when updating inodes if they need to   allocate new space to modify an inode (such is the case for   Btrfs), but since we touch atime while walking down the path we   really don't care if we failed to update the atime of the file,   so just ignore the return value.   We may also fail on filesystems that have the ability to make parts   of the fs read only, e.g. subvolumes in Btrfs. ", "int file_remove_privs(struct file *file)": "file_remove_privs(struct file  file, unsigned int flags){struct dentry  dentry = file_dentry(file);struct inode  inode = file_inode(file);int error = 0;int kill;if (IS_NOSEC(inode) || !S_ISREG(inode->i_mode))return 0;kill = dentry_needs_remove_privs(file_mnt_idmap(file), dentry);if (kill < 0)return kill;if (kill) {if (flags & IOCB_NOWAIT)return -EAGAIN;error = __remove_privs(file_mnt_idmap(file), dentry, kill);}if (!error)inode_has_no_xattr(inode);return error;}     file_remove_privs - remove special file privileges (suid, capabilities)   @file: file to remove privileges from     When file is modified by a write or truncation ensure that special   file privileges are removed.     Return: 0 on success, negative errno on failure. ", "if (!__mnt_want_write_file(file)) ": "file_update_time(struct file  file, struct timespec64  now,int sync_mode){int ret = 0;struct inode  inode = file_inode(file);  try to update time settings ", "static int file_modified_flags(struct file *file, int flags)": "file_modified_flags - handle mandated vfs changes when modifying a file   @file: file that was modified   @flags: kiocb flags     When file has been modified ensure that special   file privileges are removed and time settings are updated.     If IOCB_NOWAIT is set, special file privileges will not be removed and   time settings will not be updated. It will return -EAGAIN.     Context: Caller must hold the file's inode lock.     Return: 0 on success, negative errno on failure. ", "elseprintk(KERN_DEBUG \"init_special_inode: bogus i_mode (%o) for\"  \" inode %s:%lu\\n\", mode, inode->i_sb->s_id,  inode->i_ino);}EXPORT_SYMBOL(init_special_inode": "init_special_inode(struct inode  inode, umode_t mode, dev_t rdev){inode->i_mode = mode;if (S_ISCHR(mode)) {inode->i_fop = &def_chr_fops;inode->i_rdev = rdev;} else if (S_ISBLK(mode)) {if (IS_ENABLED(CONFIG_BLOCK))inode->i_fop = &def_blk_fops;inode->i_rdev = rdev;} else if (S_ISFIFO(mode))inode->i_fop = &pipefifo_fops;else if (S_ISSOCK(mode));  leave it no_open_fops ", "void inode_init_owner(struct mnt_idmap *idmap, struct inode *inode,      const struct inode *dir, umode_t mode)": "inode_init_owner - Init uid,gid,mode for new inode according to posix standards   @idmap: idmap of the mount the inode was created from   @inode: New inode   @dir: Directory inode   @mode: mode of the new inode     If the inode has been created through an idmapped mount the idmap of   the vfsmount must be passed through @idmap. This function will then take   care to map the inode according to @idmap before checking permissions   and initializing i_uid and i_gid. On non-idmapped mounts or if permission   checking is to be performed on the raw inode simply pass @nop_mnt_idmap. ", "bool inode_owner_or_capable(struct mnt_idmap *idmap,    const struct inode *inode)": "inode_owner_or_capable - check current task permissions to inode   @idmap: idmap of the mount the inode was found from   @inode: inode being checked     Return true if current either has CAP_FOWNER in a namespace with the   inode owner uid mapped, or owns the file.     If the inode has been found through an idmapped mount the idmap of   the vfsmount must be passed through @idmap. This function will then take   care to map the inode according to @idmap before checking permissions.   On non-idmapped mounts or if permission checking is to be performed on the   raw inode simply passs @nop_mnt_idmap. ", "void inode_dio_wait(struct inode *inode)": "inode_dio_wait(struct inode  inode){wait_queue_head_t  wq = bit_waitqueue(&inode->i_state, __I_DIO_WAKEUP);DEFINE_WAIT_BIT(q, &inode->i_state, __I_DIO_WAKEUP);do {prepare_to_wait(wq, &q.wq_entry, TASK_UNINTERRUPTIBLE);if (atomic_read(&inode->i_dio_count))schedule();} while (atomic_read(&inode->i_dio_count));finish_wait(wq, &q.wq_entry);}     inode_dio_wait - wait for outstanding DIO requests to finish   @inode: inode to wait for     Waits for all pending direct IO requests to finish so that we can   proceed with a truncate or equivalent operation.     Must be called under a lock that serializes taking new references   to i_dio_count, usually by inode->i_mutex. ", "void inode_set_flags(struct inode *inode, unsigned int flags,     unsigned int mask)": "inode_set_flags - atomically set some inode flags     Note: the caller should be holding i_mutex, or else be sure that   they have exclusive access to the inode structure (i.e., while the   inode is being instantiated).  The reason for the cmpxchg() loop   --- which wouldn't be necessary if all code paths which modify   i_flags actually followed this rule, is that there is at least one   code path which doesn't today so we use cmpxchg() out of an abundance   of caution.     In the long run, i_mutex is overkill, and we should probably look   at using the i_lock spinlock to protect i_flags, and then make sure   it is so documented in includelinuxfs.h and that all code follows   the locking convention!! ", "struct timespec64 timestamp_truncate(struct timespec64 t, struct inode *inode)": "timestamp_truncate - Truncate timespec to a granularity   @t: Timespec   @inode: inode being updated     Truncate a timespec to the granularity supported by the fs   containing the inode. Always rounds down. gran must   not be 0 nor greater than a second (NSEC_PER_SEC, or 10^9 ns). ", "now = current_time(inode);inode_update_time(inode, &now, S_ATIME);__mnt_drop_write(mnt);skip_update:sb_end_write(inode->i_sb);}EXPORT_SYMBOL(touch_atime);/* * Return mask of changes for notify_change() that need to be done as a * response to write or truncate. Return 0 if nothing has to be changed. * Negative value on error (change should be denied). ": "current_time(inode);if (!relatime_need_update(mnt, inode, now))return false;if (timespec64_equal(&inode->i_atime, &now))return false;return true;}void touch_atime(const struct path  path){struct vfsmount  mnt = path->mnt;struct inode  inode = d_inode(path->dentry);struct timespec64 now;if (!atime_needs_update(path, inode))return;if (!sb_start_write_trylock(inode->i_sb))return;if (__mnt_want_write(mnt) != 0)goto skip_update;    File systems can error out when updating inodes if they need to   allocate new space to modify an inode (such is the case for   Btrfs), but since we touch atime while walking down the path we   really don't care if we failed to update the atime of the file,   so just ignore the return value.   We may also fail on filesystems that have the ability to make parts   of the fs read only, e.g. subvolumes in Btrfs. ", "umode_t mode_strip_sgid(struct mnt_idmap *idmap,const struct inode *dir, umode_t mode)": "mode_strip_sgid - handle the sgid bit for non-directories   @idmap: idmap of the mount the inode was created from   @dir: parent directory inode   @mode: mode of the file to be created in @dir     If the @mode of the new file has both the S_ISGID and S_IXGRP bit   raised and @dir has the S_ISGID bit raised ensure that the caller is   either in the group of the parent directory or they have CAP_FSETID   in their user namespace and are privileged over the parent directory.   In all other cases, strip the S_ISGID bit from @mode.     Return: the new mode to use for the file ", "static int __dump_emit(struct coredump_params *cprm, const void *addr, int nr)": "dump_emit(&cprm, \"\", 1);}file_end_write(cprm.file);free_vma_snapshot(&cprm);}if (ispipe && core_pipe_limit)wait_for_dump_helpers(cprm.file);close_fail:if (cprm.file)filp_close(cprm.file, NULL);fail_dropcount:if (ispipe)atomic_dec(&core_dump_count);fail_unlock:kfree(argv);kfree(cn.corename);coredump_finish(core_dumped);revert_creds(old_cred);fail_creds:put_cred(cred);fail:return;}    Core dumping helper functions.  These are the only things you should   do on a core-file: use only these functions to write out all the   necessary info. ", "if (cprm.to_skip) ": "dump_skip. ", "long vfs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)": "vfs_ioctl - call filesystem specific ioctl methods   @filp:open file to invoke ioctl method on   @cmd:ioctl command to execute   @arg:command-specific argument for ioctl     Invokes filesystem specific ->unlocked_ioctl, if one exists; otherwise   returns -ENOTTY.     Returns 0 on success, -errno on error. ", "#define SET_UNKNOWN_FLAGS(FIEMAP_EXTENT_DELALLOC)#define SET_NO_UNMOUNTED_IO_FLAGS(FIEMAP_EXTENT_DATA_ENCRYPTED)#define SET_NOT_ALIGNED_FLAGS(FIEMAP_EXTENT_DATA_TAIL|FIEMAP_EXTENT_DATA_INLINE)int fiemap_fill_next_extent(struct fiemap_extent_info *fieinfo, u64 logical,    u64 phys, u64 len, u32 flags)": "fiemap_fill_next_extent - Fiemap helper function   @fieinfo:Fiemap context passed into ->fiemap   @logical:Extent logical start offset, in bytes   @phys:Extent physical start offset, in bytes   @len:Extent length, in bytes   @flags:FIEMAP_EXTENT flags that describe this extent     Called from file system ->fiemap callback. Will populate extent   info as passed in via arguments and copy to user memory. On   success, extent count on fieinfo is incremented.     Returns 0 on success, -errno on error, 1 if this was the last   extent that will fit in user array. ", "int fiemap_prep(struct inode *inode, struct fiemap_extent_info *fieinfo,u64 start, u64 *len, u32 supported_flags)": "fiemap_prep - check validity of requested flags for fiemap   @inode:Inode to operate on   @fieinfo:Fiemap context passed into ->fiemap   @start:Start of the mapped range   @len:Length of the mapped range, can be truncated by this function.   @supported_flags:Set of fiemap flags that the file system understands     This function must be called from each ->fiemap instance to validate the   fiemap request against the file system parameters.     Returns 0 on success, or a negative error on failure. ", "void fileattr_fill_xflags(struct fileattr *fa, u32 xflags)": "fileattr_fill_xflags - initialize fileattr with xflags   @fa:fileattr pointer   @xflags:FS_XFLAG_  flags     Set ->fsx_xflags, ->fsx_valid and ->flags (translated xflags).  All   other fields are zeroed. ", "void fileattr_fill_flags(struct fileattr *fa, u32 flags)": "fileattr_fill_flags - initialize fileattr with flags   @fa:fileattr pointer   @flags:FS_ _FL flags     Set ->flags, ->flags_valid and ->fsx_xflags (translated flags).   All other fields are zeroed. ", "int vfs_fileattr_get(struct dentry *dentry, struct fileattr *fa)": "vfs_fileattr_get - retrieve miscellaneous file attributes   @dentry:the object to retrieve from   @fa:fileattr pointer     Call i_op->fileattr_get() callback, if exists.     Return: 0 on success, or a negative error on failure. ", "int copy_fsxattr_to_user(const struct fileattr *fa, struct fsxattr __user *ufa)": "copy_fsxattr_to_user - copy fsxattr to userspace.   @fa:fileattr pointer   @ufa:fsxattr user pointer     Return: 0 on success, or -EFAULT on failure. ", "int vfs_fileattr_set(struct mnt_idmap *idmap, struct dentry *dentry,     struct fileattr *fa)": "vfs_fileattr_set - change miscellaneous file attributes   @idmap:idmap of the mount   @dentry:the object to change   @fa:fileattr pointer     After verifying permissions, call i_op->fileattr_set() callback, if   exists.     Verifying attributes involves retrieving current attributes with   i_op->fileattr_get(), this also allows initializing attributes that have   not been set by the caller to current values.  Inode lock is held   thoughout to prevent racing with another instance.     Return: 0 on success, or a negative error on failure. ", "long compat_ptr_ioctl(struct file *file, unsigned int cmd, unsigned long arg)": "compat_ptr_ioctl - generic implementation of .compat_ioctl file operation     This is not normally called as a function, but instead set in struct   file_operations as         .compat_ioctl = compat_ptr_ioctl,     On most architectures, the compat_ptr_ioctl() just passes all arguments   to the corresponding ->ioctl handler. The exception is archs390, where   compat_ptr() clears the top bit of a 32-bit pointer value, so user space   pointers to the second 2GB alias the first 2GB, as is the case for   native 32-bit s390 user space.     The compat_ptr_ioctl() function must therefore be used only with ioctl   functions that either ignore the argument or pass a pointer to a   compatible data type.     If any ioctl command handled by fops->unlocked_ioctl passes a plain   integer instead of a pointer, or any of the passed data types   is incompatible between 32-bit and 64-bit architectures, a proper   handler is required instead of compat_ptr_ioctl. ", "if (IS_IMMUTABLE(inode_out))return -EPERM;if (IS_SWAPFILE(inode_in) || IS_SWAPFILE(inode_out))return -ETXTBSY;/* Don't reflink dirs, pipes, sockets... ": "generic_remap_file_range_prep(struct file  file_in, loff_t pos_in,struct file  file_out, loff_t pos_out,loff_t  len, unsigned int remap_flags,const struct iomap_ops  dax_read_ops){struct inode  inode_in = file_inode(file_in);struct inode  inode_out = file_inode(file_out);bool same_inode = (inode_in == inode_out);int ret;  Don't touch certain kinds of inodes ", "ret = remap_verify_area(src_file, src_pos, len, false);if (ret)goto out_drop_write;ret = remap_verify_area(dst_file, dst_pos, len, true);if (ret)goto out_drop_write;ret = -EPERM;if (!allow_file_dedupe(dst_file))goto out_drop_write;ret = -EXDEV;if (file_inode(src_file)->i_sb != file_inode(dst_file)->i_sb)goto out_drop_write;ret = -EISDIR;if (S_ISDIR(file_inode(dst_file)->i_mode))goto out_drop_write;ret = -EINVAL;if (!dst_file->f_op->remap_file_range)goto out_drop_write;if (len == 0) ": "vfs_dedupe_file_range_one(struct file  src_file, loff_t src_pos, struct file  dst_file, loff_t dst_pos, loff_t len, unsigned int remap_flags){loff_t ret;WARN_ON_ONCE(remap_flags & ~(REMAP_FILE_DEDUP |     REMAP_FILE_CAN_SHORTEN));ret = mnt_want_write_file(dst_file);if (ret)return ret;    This is redundant if called from vfs_dedupe_file_range(), but other   callers need it and it's not performance sesitive... ", "if (!folio_test_uptodate(src_folio) || !folio_test_uptodate(dst_folio) ||    src_folio->mapping != src->f_mapping ||    dst_folio->mapping != dest->f_mapping) ": "vfs_dedupe_file_range_compare(struct file  src, loff_t srcoff, struct file  dest, loff_t dstoff, loff_t len, bool  is_same){bool same = true;int error = -EINVAL;while (len) {struct folio  src_folio,  dst_folio;void  src_addr,  dst_addr;loff_t cmp_len = min(PAGE_SIZE - offset_in_page(srcoff),     PAGE_SIZE - offset_in_page(dstoff));cmp_len = min(cmp_len, len);if (cmp_len <= 0)goto out_error;src_folio = vfs_dedupe_get_folio(src, srcoff);if (IS_ERR(src_folio)) {error = PTR_ERR(src_folio);goto out_error;}dst_folio = vfs_dedupe_get_folio(dest, dstoff);if (IS_ERR(dst_folio)) {error = PTR_ERR(dst_folio);folio_put(src_folio);goto out_error;}vfs_lock_two_folios(src_folio, dst_folio);    Now that we've locked both folios, make sure they're still   mapped to the file data we're interested in.  If not,   someone is invalidating pages on us and we lose. ", "struct file_lock *locks_alloc_lock(void)": "locks_init_lock_heads(struct file_lock  fl){INIT_HLIST_NODE(&fl->fl_link);INIT_LIST_HEAD(&fl->fl_list);INIT_LIST_HEAD(&fl->fl_blocked_requests);INIT_LIST_HEAD(&fl->fl_blocked_member);init_waitqueue_head(&fl->fl_wait);}  Allocate an empty lock structure. ", "WARN_ON_ONCE(new->fl_ops);locks_copy_conflock(new, fl);new->fl_file = fl->fl_file;new->fl_ops = fl->fl_ops;if (fl->fl_ops) ": "locks_copy_lock(struct file_lock  new, struct file_lock  fl){  \"new\" must be a freshly-initialized lock ", "smp_store_release(&waiter->fl_blocker, NULL);}}/** *locks_delete_block - stop waiting for a file lock *@waiter: the lock which was waiting * *lockd/nfsd need to disconnect the lock while working on it. ": "locks_delete_block(struct file_lock  waiter){locks_delete_global_blocked(waiter);list_del_init(&waiter->fl_blocked_member);}static void __locks_wake_up_blocks(struct file_lock  blocker){while (!list_empty(&blocker->fl_blocked_requests)) {struct file_lock  waiter;waiter = list_first_entry(&blocker->fl_blocked_requests,  struct file_lock, fl_blocked_member);__locks_delete_block(waiter);if (waiter->fl_lmops && waiter->fl_lmops->lm_notify)waiter->fl_lmops->lm_notify(waiter);elsewake_up(&waiter->fl_wait);    The setting of fl_blocker to NULL marks the \"done\"   point in deleting a block. Paired with acquire at the top   of locks_delete_block(). ", "int posix_lock_file(struct file *filp, struct file_lock *fl,struct file_lock *conflock)": "posix_lock_file - Apply a POSIX-style lock to a file   @filp: The file to apply the lock to   @fl: The lock to be applied   @conflock: Place to return a copy of the conflicting lock, if found.     Add a POSIX style lock to a file.   We merge adjacent & overlapping locks whenever possible.   POSIX locks are sorted by owner task, then by starting address     Note that if called with an FL_EXISTS argument, the caller may determine   whether or not a lock was successfully freed by testing the return   value for -ENOENT. ", "static int lease_init(struct file *filp, long type, struct file_lock *fl)": "lease_modify,.lm_setup = lease_setup,};    Initialize a lease, use the default lock manager operations ", "int __break_lease(struct inode *inode, unsigned int mode, unsigned int type)": "__break_lease-revoke all outstanding leases on file  @inode: the inode of the file to return  @mode: O_RDONLY: break only write leases; O_WRONLY or O_RDWR:      break all leases  @type: FL_LEASE: break leases and delegations; FL_DELEG: break      only delegations    break_lease (inlined for speed) has checked there already is at least  some kind of lock (maybe a lease) on this file.  Leases are broken on  a call to open() or truncate().  This function can sleep unless you  specified %O_NONBLOCK to your open(). ", "void lease_get_mtime(struct inode *inode, struct timespec64 *time)": "lease_get_mtime - update modified time of an inode with exclusive lease  @inode: the inode        @time:  pointer to a timespec which contains the last modified time     This is to force NFS clients to flush their caches for files with   exclusive leases.  The justification is that if someone has an   exclusive lease, then they could be modifying it. ", "int generic_setlease(struct file *filp, long arg, struct file_lock **flp,void **priv)": "generic_setlease-sets a lease on an open file  @filp:file pointer  @arg:type of lease to obtain  @flp:input - file_lock to use, output - file_lock inserted  @priv:private data for lm_setup (may be NULL if lm_setup  doesn't require it)    The (input) flp->fl_lmops->lm_break function is required  by break_lease(). ", "int locks_lock_inode_wait(struct inode *inode, struct file_lock *fl)": "locks_lock_inode_wait - Apply a lock to an inode   @inode: inode of the file to apply to   @fl: The lock to be applied     Apply a POSIX or FLOCK style lock request to an inode. ", "ctx = locks_inode_context(inode);if (!ctx || list_empty(&ctx->flc_posix))return;locks_init_lock(&lock);lock.fl_type = F_UNLCK;lock.fl_flags = FL_POSIX | FL_CLOSE;lock.fl_start = 0;lock.fl_end = OFFSET_MAX;lock.fl_owner = owner;lock.fl_pid = current->tgid;lock.fl_file = filp;lock.fl_ops = NULL;lock.fl_lmops = NULL;error = vfs_lock_file(filp, F_SETLK, &lock, NULL);if (lock.fl_ops && lock.fl_ops->fl_release_private)lock.fl_ops->fl_release_private(&lock);trace_locks_remove_posix(inode, &lock, error);}EXPORT_SYMBOL(locks_remove_posix": "locks_remove_posix(struct file  filp, fl_owner_t owner){int error;struct inode  inode = file_inode(filp);struct file_lock lock;struct file_lock_context  ctx;    If there are no locks held on this file, we don't need to call   posix_lock_file().  Another process could be setting a lock on this   file at the same time, but we wouldn't remove that lock anyway. ", "int finish_open(struct file *file, struct dentry *dentry,int (*open)(struct inode *, struct file *))": "finish_open - finish opening a file   @file: file pointer   @dentry: pointer to dentry   @open: open callback     This can be used to finish opening a file passed to i_op->atomic_open().     If the open callback is set to NULL, then the standard f_op->open()   filesystem callback is substituted.     NB: the dentry reference is _not_ consumed.  If, for example, the dentry is   the return value of d_splice_alias(), then the caller needs to perform dput()   on it after finish_open().     Returns zero on success or -errno if the open failed. ", "int finish_no_open(struct file *file, struct dentry *dentry)": "finish_no_open - finish ->atomic_open() without opening the file     @file: file pointer   @dentry: dentry or NULL (as returned from ->lookup())     This can be used to set the result of a successful lookup in ->atomic_open().     NB: unlike finish_open() this function does consume the dentry reference and   the caller need not dput() it.     Returns \"0\" which must be the return value of ->atomic_open() after having   called this function. ", "if (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode))f->f_mode |= FMODE_ATOMIC_POS;f->f_op = fops_get(inode->i_fop);if (WARN_ON(!f->f_op)) ": "dentry_open(struct file  f,  struct inode  inode,  int ( open)(struct inode  , struct file  )){static const struct file_operations empty_fops = {};int error;path_get(&f->f_path);f->f_inode = inode;f->f_mapping = inode->i_mapping;f->f_wb_err = filemap_sample_wb_err(f->f_mapping);f->f_sb_err = file_sample_sb_err(f);if (unlikely(f->f_flags & O_PATH)) {f->f_mode = FMODE_PATH | FMODE_OPENED;f->f_op = &empty_fops;return 0;}if ((f->f_mode & (FMODE_READ | FMODE_WRITE)) == FMODE_READ) {i_readcount_inc(inode);} else if (f->f_mode & FMODE_WRITE && !special_file(inode->i_mode)) {error = get_write_access(inode);if (unlikely(error))goto cleanup_file;error = __mnt_want_write(f->f_path.mnt);if (unlikely(error)) {put_write_access(inode);goto cleanup_file;}f->f_mode |= FMODE_WRITER;}  POSIX.1-2008SUSv4 Section XSI 2.9.7 ", "struct file *dentry_create(const struct path *path, int flags, umode_t mode,   const struct cred *cred)": "dentry_create - Create and open a file   @path: path to create   @flags: O_ flags   @mode: mode bits for new file   @cred: credentials to use     Caller must hold the parent directory's lock, and have prepared   a negative dentry, placed in @path->dentry, for the new file.     Caller sets @path->mnt to the vfsmount of the filesystem where   the new file is to be created. The parent directory and the   negative dentry must reside on the same filesystem instance.     On success, returns a \"struct file  \". Otherwise a ERR_PTR   is returned. ", "struct file *filp_open(const char *filename, int flags, umode_t mode)": "filp_open(AT_FDCWD, name, &op);}     filp_open - open file and return file pointer     @filename:path to open   @flags:open flags as per the open(2) second argument   @mode:mode for the new file if O_CREAT is set, else ignored     This is the helper to open a file from kernelspace if you really   have to.  But in generally you should not do this, so please move   along, nothing to see here.. ", "int stream_open(struct inode *inode, struct file *filp)": "stream_open is used by subsystems that want stream-like file descriptors.   Such file descriptors are not seekable and don't have notion of position   (file.f_pos is always 0 and ppos passed to .read().write() is always NULL).   Contrary to file descriptors of other regular files, .read() and .write()   can run simultaneously.     stream_open never fails and is marked to return int so that it could be   directly used as file_operations.open . ", "acl = get_cached_acl(inode, type);if (!is_uncached_acl(acl))return acl;if (!IS_POSIXACL(inode))return NULL;sentinel = uncached_acl_sentinel(current);p = acl_by_type(inode, type);/* * If the ACL isn't being read yet, set our sentinel.  Otherwise, the * current value of the ACL will not be ACL_NOT_CACHED and so our own * sentinel will not be set; another task will update the cache.  We * could wait for that other task to complete its job, but it's easier * to just call ->get_inode_acl to fetch the ACL ourself.  (This is * going to be an unlikely race.) ": "get_inode_acl(inode, type, LOOKUP_RCU);if (!IS_ERR(ret))acl = ret;}return acl;}EXPORT_SYMBOL(get_cached_acl_rcu);void set_cached_acl(struct inode  inode, int type, struct posix_acl  acl){struct posix_acl   p = acl_by_type(inode, type);struct posix_acl  old;old = xchg(p, posix_acl_dup(acl));if (!is_uncached_acl(old))posix_acl_release(old);}EXPORT_SYMBOL(set_cached_acl);static void __forget_cached_acl(struct posix_acl   p){struct posix_acl  old;old = xchg(p, ACL_NOT_CACHED);if (!is_uncached_acl(old))posix_acl_release(old);}void forget_cached_acl(struct inode  inode, int type){__forget_cached_acl(acl_by_type(inode, type));}EXPORT_SYMBOL(forget_cached_acl);void forget_all_cached_acls(struct inode  inode){__forget_cached_acl(&inode->i_acl);__forget_cached_acl(&inode->i_default_acl);}EXPORT_SYMBOL(forget_all_cached_acls);static struct posix_acl  __get_acl(struct mnt_idmap  idmap,   struct dentry  dentry, struct inode  inode,   int type){struct posix_acl  sentinel;struct posix_acl   p;struct posix_acl  acl;    The sentinel is used to detect when another operation like   set_cached_acl() or forget_cached_acl() races with get_inode_acl().   It is guaranteed that is_uncached_acl(sentinel) is true. ", "if (!acl)return 0;FOREACH_ACL_ENTRY(pa, acl, pe) ": "posix_acl_equiv_mode(const struct posix_acl  acl, umode_t  mode_p){const struct posix_acl_entry  pa,  pe;umode_t mode = 0;int not_equiv = 0;    A null ACL can always be presented as mode bits. ", "FOREACH_ACL_ENTRY(pa, acl, pe) ": "posix_acl_chmod_masq(struct posix_acl  acl, umode_t mode){struct posix_acl_entry  group_obj = NULL,  mask_obj = NULL;struct posix_acl_entry  pa,  pe;  assert(atomic_read(acl->a_refcount) == 1); ", "int posix_acl_update_mode(struct mnt_idmap *idmap,  struct inode *inode, umode_t *mode_p,  struct posix_acl **acl)": "posix_acl_update_mode  -  update mode in set_acl   @idmap:idmap of the mount @inode was found from   @inode:target inode   @mode_p:mode (pointer) for update   @acl:acl pointer     Update the file mode when setting an ACL: compute the new file permission   bits based on the ACL.  In addition, if the ACL is equivalent to the new   file mode, set  @acl to NULL to indicate that no ACL should be set.     As with chmod, clear the setgid bit if the caller is not in the owning group   or capable of CAP_FSETID (see inode_change_ok).     If the inode has been found through an idmapped mount the idmap of   the vfsmount must be passed through @idmap. This function will then   take care to map the inode according to @idmap before checking   permissions. On non-idmapped mounts or if permission checking is to be   performed on the raw inode simply passs @nop_mnt_idmap.     Called from set_acl inode operations. ", "ssize_t copy_splice_read(struct file *in, loff_t *ppos, struct pipe_inode_info *pipe, size_t len, unsigned int flags)": "copy_splice_read -  Copy data from a file and splice the copy into a pipe   @in: The file to read from   @ppos: Pointer to the file position to read from   @pipe: The pipe to splice into   @len: The amount to splice   @flags: The SPLICE_F_  flags     This function allocates a bunch of pages sufficient to hold the requested   amount of data (but limited by the remaining pipe capacity), passes it to   the file's ->read_iter() to read into and then splices the used pages into   the pipe.     Return: On success, the number of bytes read will be returned and  @ppos   will be updated if appropriate; 0 will be returned if there is no more data   to be read; -EAGAIN will be returned if the pipe had no space, and some   other negative error code will be returned on error.  A short read may occur   if the pipe has insufficient space, we reach the end of the data or we hit a   hole. ", "static int splice_from_pipe_feed(struct pipe_inode_info *pipe, struct splice_desc *sd,  splice_actor *actor)": "__splice_from_pipe() when      locking is required around copying the pipe buffers to the      destination. ", "ssize_titer_file_splice_write(struct pipe_inode_info *pipe, struct file *out,  loff_t *ppos, size_t len, unsigned int flags)": "iter_file_splice_write - splice data from a pipe to a file   @pipe:pipe info   @out:file to write to   @ppos:position in @out   @len:number of bytes to splice   @flags:splice modifier flags     Description:      Will either move or copy pages (determined by @flags options) from      the given pipe inode to the given file.      This one is ->write_iter-based.   ", "ssize_t splice_direct_to_actor(struct file *in, struct splice_desc *sd,       splice_direct_actor *actor)": "splice_direct_to_actor - splices data directly between two non-pipes   @in:file to splice from   @sd:actor information on where to splice to   @actor:handles the data splicing     Description:      This is a special case helper to splice directly between two      points, without requiring an explicit pipe. Internally an allocated      pipe is cached in the process, and reused during the lifetime of      that process.   ", "long do_splice_direct(struct file *in, loff_t *ppos, struct file *out,      loff_t *opos, size_t len, unsigned int flags)": "do_splice_direct - splices data directly between two files   @in:file to splice from   @ppos:input file offset   @out:file to splice to   @opos:output file offset   @len:number of bytes to splice   @flags:splice modifier flags     Description:      For use by do_sendfile(). splice can easily emulate sendfile, but      doing it in the application would incur an extra system call      (splice in + splice out, as compared to just sendfile()). So this helper      can splice directly through a process-private pipe.   ", "/* watch out for a 0 len io from a tricksy fs ": "__blockdev_direct_IO(struct kiocb  iocb, struct inode  inode,struct block_device  bdev, struct iov_iter  iter,get_block_t get_block, dio_iodone_t end_io,int flags){unsigned i_blkbits = READ_ONCE(inode->i_blkbits);unsigned blkbits = i_blkbits;unsigned blocksize_mask = (1 << blkbits) - 1;ssize_t retval = -EINVAL;const size_t count = iov_iter_count(iter);loff_t offset = iocb->ki_pos;const loff_t end = offset + count;struct dio  dio;struct dio_submit sdio = { 0, };struct buffer_head map_bh = { 0, };struct blk_plug plug;unsigned long align = offset | iov_iter_alignment(iter);    Avoid references to bdev if not absolutely needed to give   the early prefetch in the caller enough time. ", ";unsigned long pos = bprm->p;if (len == 0)return -EFAULT;if (!valid_arg_len(bprm, len))return -E2BIG;/* We're going to work our way backwards. ": "copy_string_kernel(const char  arg, struct linux_binprm  bprm){int len = strnlen(arg, MAX_ARG_STRLEN) + 1   terminating NUL ", "static int bprm_mm_init(struct linux_binprm *bprm)": "setup_arg_pages(). ", "err = -EACCES;if (WARN_ON_ONCE(!S_ISREG(file_inode(file)->i_mode) || path_noexec(&file->f_path)))goto exit;err = deny_write_access(file);if (err)goto exit;out:return file;exit:fput(file);return ERR_PTR(err);}struct file *open_exec(const char *name)": "open_execat(int fd, struct filename  name, int flags){struct file  file;int err;struct open_flags open_exec_flags = {.open_flag = O_LARGEFILE | O_RDONLY | __FMODE_EXEC,.acc_mode = MAY_EXEC,.intent = LOOKUP_OPEN,.lookup_flags = LOOKUP_FOLLOW,};if ((flags & ~(AT_SYMLINK_NOFOLLOW | AT_EMPTY_PATH)) != 0)return ERR_PTR(-EINVAL);if (flags & AT_SYMLINK_NOFOLLOW)open_exec_flags.lookup_flags &= ~LOOKUP_FOLLOW;if (flags & AT_EMPTY_PATH)open_exec_flags.lookup_flags |= LOOKUP_EMPTY;file = do_filp_open(fd, name, &open_exec_flags);if (IS_ERR(file))goto out;    may_open() has already checked for this, so it should be   impossible to trip now. But we need to be extra cautious   and check again at the very end too. ", "retval = bprm_creds_from_file(bprm);if (retval)return retval;/* * Ensure all future errors are fatal. ": "begin_new_exec(struct linux_binprm   bprm){struct task_struct  me = current;int retval;  Once we are committed compute the creds ", "acct_arg_size(bprm, 0);retval = exec_mmap(bprm->mm);if (retval)goto out;bprm->mm = NULL;retval = exec_task_namespaces();if (retval)goto out_unlock;#ifdef CONFIG_POSIX_TIMERSspin_lock_irq(&me->sighand->siglock);posix_cpu_timers_exit(me);spin_unlock_irq(&me->sighand->siglock);exit_itimers(me);flush_itimer_signals();#endif/* * Make the signal table private. ": "would_dump(bprm, bprm->file);if (bprm->have_execfd)would_dump(bprm, bprm->executable);    Release all of the old mmap stuff ", "do_close_on_exec(me->files);if (bprm->secureexec) ": "setup_new_exec) to avoid a race with a process in userspace   trying to access the should-be-closed file descriptors of a process   undergoing exec(2). ", "task_lock(current->group_leader);current->signal->rlim[RLIMIT_STACK] = bprm->rlim_stack;task_unlock(current->group_leader);}EXPORT_SYMBOL(finalize_exec": "finalize_exec(struct linux_binprm  bprm){  Store any stack rlimit changes before starting thread. ", "if (bprm->interp != bprm->filename)kfree(bprm->interp);bprm->interp = kstrdup(interp, GFP_KERNEL);if (!bprm->interp)return -ENOMEM;return 0;}EXPORT_SYMBOL(bprm_change_interp": "bprm_change_interp(const char  interp, struct linux_binprm  bprm){  If a binfmt changed the interp, free it first. ", "int register_chrdev_region(dev_t from, unsigned count, const char *name)": "__unregister_chrdev_region(unsigned major, unsigned baseminor, int minorct){struct char_device_struct  cd = NULL,   cp;int i = major_to_index(major);mutex_lock(&chrdevs_lock);for (cp = &chrdevs[i];  cp; cp = &( cp)->next)if (( cp)->major == major &&    ( cp)->baseminor == baseminor &&    ( cp)->minorct == minorct)break;if ( cp) {cd =  cp; cp = cd->next;}mutex_unlock(&chrdevs_lock);return cd;}     register_chrdev_region() - register a range of device numbers   @from: the first in the desired range of device numbers; must include          the major number.   @count: the number of consecutive device numbers required   @name: the name of the device or driver.     Return value is zero on success, a negative error code on failure. ", "int alloc_chrdev_region(dev_t *dev, unsigned baseminor, unsigned count,const char *name)": "alloc_chrdev_region() - register a range of char device numbers   @dev: output parameter for first assigned number   @baseminor: first of the requested range of minor numbers   @count: the number of minor numbers required   @name: the name of the associated device or driver     Allocates a range of char device numbers.  The major number will be   chosen dynamically, and returned (along with the first minor number)   in @dev.  Returns zero or a negative error code. ", "void cdev_init(struct cdev *cdev, const struct file_operations *fops)": "cdev_init() - initialize a cdev structure   @cdev: the structure to initialize   @fops: the file_operations for this device     Initializes @cdev, remembering @fops, making it ready to add to the   system with cdev_add(). ", "void unregister_chrdev_region(dev_t from, unsigned count)": "cdev_add(cdev, MKDEV(cd->major, baseminor), count);if (err)goto out;cd->cdev = cdev;return major ? 0 : cd->major;out:kobject_put(&cdev->kobj);out2:kfree(__unregister_chrdev_region(cd->major, baseminor, count));return err;}     unregister_chrdev_region() - unregister a range of device numbers   @from: the first in the range of numbers to unregister   @count: the number of device numbers to unregister     This function will unregister a range of @count device numbers,   starting with @from.  The caller should normally be the one who   allocated those numbers in the first place... ", "static int chrdev_open(struct inode *inode, struct file *filp)": "cdev_del(cd->cdev);kfree(cd);}static DEFINE_SPINLOCK(cdev_lock);static struct kobject  cdev_get(struct cdev  p){struct module  owner = p->owner;struct kobject  kobj;if (owner && !try_module_get(owner))return NULL;kobj = kobject_get_unless_zero(&p->kobj);if (!kobj)module_put(owner);return kobj;}void cdev_put(struct cdev  p){if (p) {struct module  owner = p->owner;kobject_put(&p->kobj);module_put(owner);}}    Called every time a character special file is opened ", "void cdev_set_parent(struct cdev *p, struct kobject *kobj)": "cdev_set_parent() - set the parent kobject for a char device   @p: the cdev structure   @kobj: the kobject to take a reference to     cdev_set_parent() sets a parent kobject which will be referenced   appropriately so the parent is not freed before the cdev. This   should be called before cdev_add. ", "int cdev_device_add(struct cdev *cdev, struct device *dev)": "cdev_device_add() - add a char device and it's corresponding  struct device, linkink   @dev: the device structure   @cdev: the cdev structure     cdev_device_add() adds the char device represented by @cdev to the system,   just as cdev_add does. It then adds @dev to the system using device_add   The dev_t for the char device will be taken from the struct device which   needs to be initialized first. This helper function correctly takes a   reference to the parent device so the parent will not get released until   all references to the cdev are released.     This helper uses dev->devt for the device number. If it is not set   it will not add the cdev and it will be equivalent to device_add.     This function should be used whenever the struct cdev and the   struct device are members of the same structure whose lifetime is   managed by the struct device.     NOTE: Callers must assume that userspace was able to open the cdev and   can call cdev fops callbacks at any time, even if this function fails. ", "void cdev_device_del(struct cdev *cdev, struct device *dev)": "cdev_device_del() - inverse of cdev_device_add   @dev: the device structure   @cdev: the cdev structure     cdev_device_del() is a helper function to call cdev_del and device_del.   It should be used whenever cdev_device_add is used.     If dev->devt is not set it will not remove the cdev and will be equivalent   to device_del.     NOTE: This guarantees that associated sysfs callbacks are not running   or runnable, however any cdevs already open will remain and their fops   will still be callable even after this function returns. ", " /** *make_bad_inode - mark an inode bad due to an I/O error *@inode: Inode to mark bad * *When an inode cannot be read due to a media or remote network *failure this function makes the inode \"bad\" and causes I/O operations *on it to fail from this point on. ": "make_bad_inode() to return a   set of stubs which will return EIO errors as required.      We only need to do limited initialisation: all other fields are   preinitialised to zero automatically. ", " bool is_bad_inode(struct inode *inode)": "is_bad_inode - is an inode errored  @inode: inode to test    Returns true if the inode in question has been marked as bad. ", "void iget_failed(struct inode *inode)": "iget_failed - Mark an under-construction inode as dead and release it   @inode: The inode to discard     Mark an under-construction inode as dead and release it. ", "static void map_buffer_to_folio(struct folio *folio, struct buffer_head *bh,int page_block)": "mpage_readahead.  The fs supplied get_block might   return an up to date buffer.  This is used to map that buffer into   the page, which allows read_folio to avoid triggering a duplicate call   to get_block.     The idea is to avoid adding buffers to pages that don't already have   them.  So when the buffer is up to date and the page size == block size,   this marks the page up to date instead of adding new buffers. ", "intmpage_writepages(struct address_space *mapping,struct writeback_control *wbc, get_block_t get_block)": "mpage_writepages - walk the list of dirty pages of the given address space & writepage() all of them   @mapping: address space structure to write   @wbc: subtract the number of written pages from  @wbc->nr_to_write   @get_block: the filesystem's block mapper function.     This is a library function, which implements the writepages()   address_space_operation. ", "int setattr_should_drop_sgid(struct mnt_idmap *idmap,     const struct inode *inode)": "setattr_copy().     Return: ATTR_KILL_SGID if setgid bit needs to be removed, 0 otherwise. ", "int setattr_should_drop_suidgid(struct mnt_idmap *idmap,struct inode *inode)": "setattr_should_drop_suidgid - determine whether the set{g,u}id bit needs to                                 be dropped   @idmap:idmap of the mount @inode was found from   @inode:inode to check     This function determines whether the set{g,u}id bits need to be removed.   If the setuid bit needs to be removed ATTR_KILL_SUID is returned. If the   setgid bit needs to be removed ATTR_KILL_SGID is returned. If both   set{g,u}id bits need to be removed the corresponding mask of both flags is   returned.     Return: A mask of ATTR_KILL_S{G,U}ID indicating which - if any - setid bits   to remove, 0 otherwise. ", "if (ia_valid & ATTR_FORCE)goto kill_priv;/* Make sure a caller can chown. ": "inode_newsize_ok(inode, attr->ia_size);if (error)return error;}  If force is set do it anyway. ", "if (ia_valid & ATTR_TOUCH) ": "may_setattr(struct mnt_idmap  idmap, struct inode  inode,unsigned int ia_valid){int error;if (ia_valid & (ATTR_MODE | ATTR_UID | ATTR_GID | ATTR_TIMES_SET)) {if (IS_IMMUTABLE(inode) || IS_APPEND(inode))return -EPERM;}    If utimes(2) and friends are called with times == NULL (or both   times are UTIME_NOW), then we need to check for write permission ", "int notify_change(struct mnt_idmap *idmap, struct dentry *dentry,  struct iattr *attr, struct inode **delegated_inode)": "notify_change - modify attributes of a filesytem object   @idmap:idmap of the mount the inode was found from   @dentry:object affected   @attr:new attributes   @delegated_inode: returns inode, if the inode is delegated     The caller must hold the i_mutex on the affected object.     If notify_change discovers a delegation in need of breaking,   it will return -EWOULDBLOCK and return a reference to the inode in   delegated_inode.  The caller should then break the delegation and   retry.  Because breaking a delegation may take a long time, the   caller should drop the i_mutex before doing so.     Alternatively, a caller may pass NULL for delegated_inode.  This may   be appropriate for callers that expect the underlying filesystem not   to be NFS exported.  Also, passing NULL is fine for callers holding   the file open for write, as there can be no conflicting delegation in   that case.     If the inode has been found through an idmapped mount the idmap of   the vfsmount must be passed through @idmap. This function will then   take care to map the inode according to @idmap before checking   permissions. On non-idmapped mounts or if permission checking is to be   performed on the raw inode simply pass @nop_mnt_idmap. ", "if (unlikely(len == EMBEDDED_NAME_MAX)) ": "putname(result);return ERR_PTR(len);}    Uh-oh. We have a name that's approaching PATH_MAX. Allocate a   separate struct filename so we can dedicate the entire   names_cache allocation for the pathname, and re-do the copy from   userland. ", "int generic_permission(struct mnt_idmap *idmap, struct inode *inode,       int mask)": "generic_permission -  check for access rights on a Posix-like filesystem   @idmap:idmap of the mount the inode was found from   @inode:inode to check access rights for   @mask:right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC,  %MAY_NOT_BLOCK ...)     Used to check for readwriteexecute permissions on a file.   We use \"fsuid\" for this, letting us set arbitrary permissions   for filesystem access without changing the \"normal\" uids which   are used for other things.     generic_permission is rcu-walk aware. It returns -ECHILD in case an rcu-walk   request cannot be satisfied (eg. requires blocking or too much complexity).   It would then be called again in ref-walk mode.     If the inode has been found through an idmapped mount the idmap of   the vfsmount must be passed through @idmap. This function will then take   care to map the inode according to @idmap before checking permissions.   On non-idmapped mounts or if permission checking is to be performed on the   raw inode simply passs @nop_mnt_idmap. ", "static inline int do_inode_permission(struct mnt_idmap *idmap,      struct inode *inode, int mask)": "inode_permission - UNIX permission checking   @idmap:idmap of the mount the inode was found from   @inode:inode to check permissions on   @mask:right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC ...)     We _really_ want to just do \"generic_permission()\" without   even looking at the inode->i_op values. So we keep a cache   flag in inode->i_opflags, that says \"this has not special   permission function, use the fast case\". ", "void path_get(const struct path *path)": "path_get - get a reference to a path   @path: path to get the reference to     Given a path increment the reference count to the dentry and the vfsmount. ", "void path_put(const struct path *path)": "path_put - put a reference to a path   @path: path to put the reference to     Given a path decrement the reference count to the dentry and the vfsmount. ", "int follow_up(struct path *path)": "follow_up - Find the mountpoint of path's vfsmount     Given a path, find the mountpoint of its source file system.   Replace @path with the path of the mountpoint in the parent mount.   Up is towards .     Return 1 if we went up a level and 0 if we were already at the   root. ", "if (unlikely(IS_DEADDIR(dir)))return ERR_PTR(-ENOENT);dentry = d_alloc(base, name);if (unlikely(!dentry))return ERR_PTR(-ENOMEM);old = dir->i_op->lookup(dir, dentry, flags);if (unlikely(old)) ": "lookup_one_qstr_excl(const struct qstr  name,    struct dentry  base,    unsigned int flags){struct dentry  dentry = lookup_dcache(name, base, flags);struct dentry  old;struct inode  dir = base->d_inode;if (dentry)return dentry;  Don't create child dentry for a dead directory. ", "int vfs_path_parent_lookup(struct filename *filename, unsigned int flags,   struct path *parent, struct qstr *last, int *type,   const struct path *root)": "vfs_path_parent_lookup - lookup a parent path relative to a dentry-vfsmount pair   @filename: filename structure   @flags: lookup flags   @parent: pointer to struct path to fill   @last: last component   @type: type of the last component   @root: pointer to struct path of the base directory ", "int vfs_path_lookup(struct dentry *dentry, struct vfsmount *mnt,    const char *name, unsigned int flags,    struct path *path)": "vfs_path_lookup - lookup a file path relative to a dentry-vfsmount pair   @dentry:  pointer to dentry of the base directory   @mnt: pointer to vfs mount of the base directory   @name: pointer to file name   @flags: lookup flags   @path: pointer to struct path to fill ", "struct dentry *try_lookup_one_len(const char *name, struct dentry *base, int len)": "lookup_one_len - filesystem helper to lookup single pathname component   @name:pathname component to lookup   @base:base directory to lookup from   @len:maximum length @len should be interpreted to     Look up a dentry by name in the dcache, returning NULL if it does not   currently exist.  The function does not try to create a dentry.     Note that this routine is purely a helper for filesystem usage and should   not be called by generic code.     The caller must hold base->i_mutex. ", "struct dentry *lookup_one_unlocked(struct mnt_idmap *idmap,   const char *name, struct dentry *base,   int len)": "lookup_one_unlocked - filesystem helper to lookup single pathname component   @idmap:idmap of the mount the lookup is performed from   @name:pathname component to lookup   @base:base directory to lookup from   @len:maximum length @len should be interpreted to     Note that this routine is purely a helper for filesystem usage and should   not be called by generic code.     Unlike lookup_one_len, it should be called without the parent   i_mutex held, and will take the i_mutex itself if necessary. ", "struct dentry *lookup_one_positive_unlocked(struct mnt_idmap *idmap,    const char *name,    struct dentry *base, int len)": "lookup_one_positive_unlocked - filesystem helper to lookup single    pathname component   @idmap:idmap of the mount the lookup is performed from   @name:pathname component to lookup   @base:base directory to lookup from   @len:maximum length @len should be interpreted to     This helper will yield ERR_PTR(-ENOENT) on negatives. The helper returns   known positive or ERR_PTR(). This is what most of the users want.     Note that pinned negative with unlocked parent _can_ become positive at any   time, so callers of lookup_one_unlocked() need to be very careful; pinned   positives have >d_inode stable, so this one avoids such problems.     Note that this routine is purely a helper for filesystem usage and should   not be called by generic code.     The helper should be called without i_mutex held. ", "struct dentry *lookup_one_len_unlocked(const char *name,       struct dentry *base, int len)": "lookup_one_len_unlocked - filesystem helper to lookup single pathname component   @name:pathname component to lookup   @base:base directory to lookup from   @len:maximum length @len should be interpreted to     Note that this routine is purely a helper for filesystem usage and should   not be called by generic code.     Unlike lookup_one_len, it should be called without the parent   i_mutex held, and will take the i_mutex itself if necessary. ", "inode_lock_nested(p2->d_inode, I_MUTEX_PARENT);/* * now that p2 is locked, nobody can move in or out of it, * so the test below is safe. ": "lock_rename_child(struct dentry  c1, struct dentry  p2){if (READ_ONCE(c1->d_parent) == p2) {    hopefully won't need to touch ->s_vfs_rename_mutex at all. ", "int vfs_create(struct mnt_idmap *idmap, struct inode *dir,       struct dentry *dentry, umode_t mode, bool want_excl)": "vfs_create - create new file   @idmap:idmap of the mount the inode was found from   @dir:inode of @dentry   @dentry:pointer to dentry of the base directory   @mode:mode of the new file   @want_excl:whether the file must not yet exist     Create a new file.     If the inode has been found through an idmapped mount the idmap of   the vfsmount must be passed through @idmap. This function will then take   care to map the inode according to @idmap before checking permissions.   On non-idmapped mounts or if permission checking is to be performed on the   raw inode simply passs @nop_mnt_idmap. ", "struct file *kernel_tmpfile_open(struct mnt_idmap *idmap, const struct path *parentpath, umode_t mode, int open_flag, const struct cred *cred)": "kernel_tmpfile_open - open a tmpfile for kernel internal use   @idmap:idmap of the mount the inode was found from   @parentpath:path of the base directory   @mode:mode of the new tmpfile   @open_flag:flags   @cred:credentials for open     Create and open a temporary file.  The file is not accounted in nr_files,   hence this is only for kernel internal use, and must not be installed into   file tables or such. ", "if (unlikely(!create_flags)) ": "vfs_mknod() POV we just have a negative dentry -   all is fine. Let's be bastards - you had  on the end, you've   been asking for (non-existent) directory. -ENOENT for you. ", "int vfs_mkdir(struct mnt_idmap *idmap, struct inode *dir,      struct dentry *dentry, umode_t mode)": "vfs_mkdir - create directory   @idmap:idmap of the mount the inode was found from   @dir:inode of @dentry   @dentry:pointer to dentry of the base directory   @mode:mode of the new directory     Create a directory.     If the inode has been found through an idmapped mount the idmap of   the vfsmount must be passed through @idmap. This function will then take   care to map the inode according to @idmap before checking permissions.   On non-idmapped mounts or if permission checking is to be performed on the   raw inode simply passs @nop_mnt_idmap. ", "int vfs_rmdir(struct mnt_idmap *idmap, struct inode *dir,     struct dentry *dentry)": "vfs_rmdir - remove directory   @idmap:idmap of the mount the inode was found from   @dir:inode of @dentry   @dentry:pointer to dentry of the base directory     Remove a directory.     If the inode has been found through an idmapped mount the idmap of   the vfsmount must be passed through @idmap. This function will then take   care to map the inode according to @idmap before checking permissions.   On non-idmapped mounts or if permission checking is to be performed on the   raw inode simply passs @nop_mnt_idmap. ", "int vfs_unlink(struct mnt_idmap *idmap, struct inode *dir,       struct dentry *dentry, struct inode **delegated_inode)": "vfs_unlink - unlink a filesystem object   @idmap:idmap of the mount the inode was found from   @dir:parent directory   @dentry:victim   @delegated_inode: returns victim inode, if the inode is delegated.     The caller must hold dir->i_mutex.     If vfs_unlink discovers a delegation, it will return -EWOULDBLOCK and   return a reference to the inode in delegated_inode.  The caller   should then break the delegation on that inode and retry.  Because   breaking a delegation may take a long time, the caller should drop   dir->i_mutex before doing so.     Alternatively, a caller may pass NULL for delegated_inode.  This may   be appropriate for callers that expect the underlying filesystem not   to be NFS exported.     If the inode has been found through an idmapped mount the idmap of   the vfsmount must be passed through @idmap. This function will then take   care to map the inode according to @idmap before checking permissions.   On non-idmapped mounts or if permission checking is to be performed on the   raw inode simply passs @nop_mnt_idmap. ", "int vfs_symlink(struct mnt_idmap *idmap, struct inode *dir,struct dentry *dentry, const char *oldname)": "vfs_symlink - create symlink   @idmap:idmap of the mount the inode was found from   @dir:inode of @dentry   @dentry:pointer to dentry of the base directory   @oldname:name of the file to link to     Create a symlink.     If the inode has been found through an idmapped mount the idmap of   the vfsmount must be passed through @idmap. This function will then take   care to map the inode according to @idmap before checking permissions.   On non-idmapped mounts or if permission checking is to be performed on the   raw inode simply passs @nop_mnt_idmap. ", "int vfs_link(struct dentry *old_dentry, struct mnt_idmap *idmap,     struct inode *dir, struct dentry *new_dentry,     struct inode **delegated_inode)": "vfs_link - create a new link   @old_dentry:object to be linked   @idmap:idmap of the mount   @dir:new parent   @new_dentry:where to create the new link   @delegated_inode: returns inode needing a delegation break     The caller must hold dir->i_mutex     If vfs_link discovers a delegation on the to-be-linked file in need   of breaking, it will return -EWOULDBLOCK and return a reference to the   inode in delegated_inode.  The caller should then break the delegation   and retry.  Because breaking a delegation may take a long time, the   caller should drop the i_mutex before doing so.     Alternatively, a caller may pass NULL for delegated_inode.  This may   be appropriate for callers that expect the underlying filesystem not   to be NFS exported.     If the inode has been found through an idmapped mount the idmap of   the vfsmount must be passed through @idmap. This function will then take   care to map the inode according to @idmap before checking permissions.   On non-idmapped mounts or if permission checking is to be performed on the   raw inode simply passs @nop_mnt_idmap. ", "/* In order to reduce some races, while at the same time doing additional * checking and hopefully speeding things up, we copy filenames to the * kernel data space before using them.. * * POSIX.1 2.4: an empty pathname is invalid (ENOENT). * PATH_MAX includes the nul terminator --RR. ": "vfs_rename_mutex gives   any extra contention... ", "int vfs_readlink(struct dentry *dentry, char __user *buffer, int buflen)": "vfs_readlink - copy symlink body into userspace buffer   @dentry: dentry on which to get symbolic link   @buffer: user memory pointer   @buflen: size of buffer     Does not touch atime.  That's up to the caller if necessary     Does not call security hook. ", "const char *vfs_get_link(struct dentry *dentry, struct delayed_call *done)": "vfs_get_link - get symlink body   @dentry: dentry on which to get symbolic link   @done: caller needs to free returned data with this     Calls security hook and i_op->get_link() on the supplied inode.     It does not touch atime.  That's up to the caller if necessary.     Does not work on \"special\" symlinks like proc$$fdN ", "int mb_cache_entry_create(struct mb_cache *cache, gfp_t mask, u32 key,  u64 value, bool reusable)": "mb_cache_entry_create - create entry in cache   @cache - cache where the entry should be created   @mask - gfp mask with which the entry should be allocated   @key - key of the entry   @value - value of the entry   @reusable - is the entry reusable by others?     Creates entry in @cache with key @key and value @value. The function returns   -EBUSY if entry with the same key and value already exists in cache.   Otherwise 0 is returned. ", "void mb_cache_entry_wait_unused(struct mb_cache_entry *entry)": "mb_cache_entry_wait_unused - wait to be the last user of the entry     @entry - entry to work on     Wait to be the last user of the entry. ", "struct mb_cache_entry *mb_cache_entry_find_first(struct mb_cache *cache, u32 key)": "mb_cache_entry_find_first - find the first reusable entry with the given key   @cache: cache where we should search   @key: key to look for     Search in @cache for a reusable entry with key @key. Grabs reference to the   first reusable entry found and returns the entry. ", "struct mb_cache_entry *mb_cache_entry_find_next(struct mb_cache *cache,struct mb_cache_entry *entry)": "mb_cache_entry_find_next - find next reusable entry with the same key   @cache: cache where we should search   @entry: entry to start search from     Finds next reusable entry in the hash chain which has the same key as @entry.   If @entry is unhashed (which can happen when deletion of entry races with the   search), finds the first reusable entry in the hash chain. The function drops   reference to @entry and returns with a reference to the found entry. ", "struct mb_cache_entry *mb_cache_entry_get(struct mb_cache *cache, u32 key,  u64 value)": "mb_cache_entry_get - get a cache entry by value (and key)   @cache - cache we work with   @key - key   @value - value ", "struct mb_cache ": "mb_cache_entry_delete_or_get()).     Ext2 and ext4 use this cache for deduplication of extended attribute blocks.   Ext4 also uses it for deduplication of xattr values stored in inodes.   They use hash of data as a key and provide a value that may represent a   block or inode number. That's why keys need not be unique (hash of different   data may be the same). However user provided value always uniquely   identifies a cache entry.     We provide functions for creation and removal of entries, search by key,   and a special \"delete entry with given key-value pair\" operation. Fixed   size hash table is used for fast key lookups. ", "void mb_cache_entry_touch(struct mb_cache *cache,  struct mb_cache_entry *entry)": "mb_cache_entry_touch - cache entry got used   @cache - cache the entry belongs to   @entry - entry that got used     Marks entry as used to give hit higher chances of surviving in cache. ", "struct mb_cache *mb_cache_create(int bucket_bits)": "mb_cache_create - create cache   @bucket_bits: log2 of the hash table size     Create cache for keys with 2^bucket_bits hash entries. ", "void mb_cache_destroy(struct mb_cache *cache)": "mb_cache_destroy - destroy cache   @cache: the cache to destroy     Free all entries in cache and cache itself. Caller must make sure nobody   (except shrinker) can reach @cache when calling this. ", "int seq_open(struct file *file, const struct seq_operations *op)": "seq_open -initialize sequential file  @file: file we initialize  @op: method table describing the sequence    seq_open() sets @file, associating it with a sequence described  by @op.  @op->start() sets the iterator up and returns the first  element of sequence. @op->stop() shuts it down.  @op->next()  returns the next element of sequence.  @op->show() prints element  into the buffer.  In case of error ->start() and ->next() return  ERR_PTR(error).  In the end of sequence they return %NULL. ->show()  returns 0 in case of success and negative number in case of error.  Returning SEQ_SKIP means \"discard this element and move on\".  Note: seq_open() will allocate a struct seq_file and store its  pointer in @file->private_data. This pointer should not be modified. ", "ssize_t seq_read(struct file *file, char __user *buf, size_t size, loff_t *ppos)": "seq_read -->read() method for sequential files.  @file: the file to read from  @buf: the buffer to read to  @size: the maximum number of bytes to read  @ppos: the current position in the file    Ready-made ->f_op->read() ", "ssize_t seq_read_iter(struct kiocb *iocb, struct iov_iter *iter)": "seq_read_iter(&kiocb, &iter); ppos = kiocb.ki_pos;return ret;}EXPORT_SYMBOL(seq_read);    Ready-made ->f_op->read_iter() ", "loff_t seq_lseek(struct file *file, loff_t offset, int whence)": "seq_lseek -->llseek() method for sequential files.  @file: the file in question  @offset: new position  @whence: 0 for absolute, 1 for relative position    Ready-made ->f_op->llseek() ", "int seq_release(struct inode *inode, struct file *file)": "seq_release -free the structures associated with sequential file.  @file: file in question  @inode: its inode    Frees the structures associated with sequential file; can be used  as ->f_op->release() if you don't have private data to destroy. ", "void seq_escape_mem(struct seq_file *m, const char *src, size_t len,    unsigned int flags, const char *esc)": "seq_escape_mem - print data into buffer, escaping some characters   @m: target buffer   @src: source buffer   @len: size of source buffer   @flags: flags to pass to string_escape_mem()   @esc: set of characters that need escaping     Puts data into buffer, replacing each occurrence of character from   given class (defined by @flags and @esc) with printable escaped sequence.     Use seq_has_overflowed() to check for errors. ", "char *mangle_path(char *s, const char *p, const char *esc)": "mangle_path -mangle and copy path to buffer beginning  @s: buffer start  @p: beginning of path in above buffer  @esc: set of characters that need escaping          Copy the path from @p to @s, replacing each occurrence of character from        @esc with usual octal escape.        Returns pointer past last written character in @s, or NULL in case of        failure. ", "int seq_path(struct seq_file *m, const struct path *path, const char *esc)": "seq_path - seq_file interface to print a pathname   @m: the seq_file handle   @path: the struct path to print   @esc: set of characters to escape in the output     return the absolute path of 'path', as represented by the   dentry  mnt pair in the path parameter. ", "int seq_file_path(struct seq_file *m, struct file *file, const char *esc)": "seq_file_path - seq_file interface to print a pathname of a file   @m: the seq_file handle   @file: the struct file to print   @esc: set of characters to escape in the output     return the absolute path to the file. ", "void seq_put_decimal_ull_width(struct seq_file *m, const char *delimiter, unsigned long long num, unsigned int width)": "seq_put_decimal_ull_width - A helper routine for putting decimal numbers          without rich format of printf().   only 'unsigned long long' is supported.   @m: seq_file identifying the buffer to which data should be written   @delimiter: a string which is printed before the number   @num: the number   @width: a minimum field width     This routine will put strlen(delimiter) + number into seq_filed.   This routine is very quick when you show lots of numbers.   In usual cases, it will be better to use seq_printf(). It's easier to read. ", "goto overflow;if (delimiter && delimiter[0]) ": "seq_put_decimal_ll(struct seq_file  m, const char  delimiter, long long num){int len;if (m->count + 3 >= m->size)   we'll write 2 bytes at least ", "int seq_write(struct seq_file *seq, const void *data, size_t len)": "seq_write - write arbitrary data to buffer   @seq: seq_file identifying the buffer to which data should be written   @data: data address   @len: number of bytes     Return 0 on success, non-zero otherwise. ", "void seq_pad(struct seq_file *m, char c)": "seq_pad - write padding spaces to buffer   @m: seq_file identifying the buffer to which data should be written   @c: the byte to append after padding if non-zero ", "struct hlist_node *seq_hlist_start(struct hlist_head *head, loff_t pos)": "seq_hlist_start - start an iteration of a hlist   @head: the head of the hlist   @pos:  the start position of the sequence     Called at seq_file->op->start(). ", "struct hlist_node *seq_hlist_start_head(struct hlist_head *head, loff_t pos)": "seq_hlist_start_head - start an iteration of a hlist   @head: the head of the hlist   @pos:  the start position of the sequence     Called at seq_file->op->start(). Call this function if you want to   print a header at the top of the output. ", "struct hlist_node *seq_hlist_next(void *v, struct hlist_head *head,  loff_t *ppos)": "seq_hlist_next - move to the next position of the hlist   @v:    the current iterator   @head: the head of the hlist   @ppos: the current position     Called at seq_file->op->next(). ", "struct hlist_node *seq_hlist_start_rcu(struct hlist_head *head,       loff_t pos)": "seq_hlist_start_rcu - start an iteration of a hlist protected by RCU   @head: the head of the hlist   @pos:  the start position of the sequence     Called at seq_file->op->start().     This list-traversal primitive may safely run concurrently with   the _rcu list-mutation primitives such as hlist_add_head_rcu()   as long as the traversal is guarded by rcu_read_lock(). ", "struct hlist_node *seq_hlist_start_head_rcu(struct hlist_head *head,    loff_t pos)": "seq_hlist_start_head_rcu - start an iteration of a hlist protected by RCU   @head: the head of the hlist   @pos:  the start position of the sequence     Called at seq_file->op->start(). Call this function if you want to   print a header at the top of the output.     This list-traversal primitive may safely run concurrently with   the _rcu list-mutation primitives such as hlist_add_head_rcu()   as long as the traversal is guarded by rcu_read_lock(). ", "struct hlist_node *seq_hlist_next_rcu(void *v,      struct hlist_head *head,      loff_t *ppos)": "seq_hlist_next_rcu - move to the next position of the hlist protected by RCU   @v:    the current iterator   @head: the head of the hlist   @ppos: the current position     Called at seq_file->op->next().     This list-traversal primitive may safely run concurrently with   the _rcu list-mutation primitives such as hlist_add_head_rcu()   as long as the traversal is guarded by rcu_read_lock(). ", "struct hlist_node *seq_hlist_start_percpu(struct hlist_head __percpu *head, int *cpu, loff_t pos)": "seq_hlist_start_percpu - start an iteration of a percpu hlist array   @head: pointer to percpu array of struct hlist_heads   @cpu:  pointer to cpu \"cursor\"   @pos:  start position of sequence     Called at seq_file->op->start(). ", "struct hlist_node *seq_hlist_next_percpu(void *v, struct hlist_head __percpu *head,int *cpu, loff_t *pos)": "seq_hlist_next_percpu - move to the next position of the percpu hlist array   @v:    pointer to current hlist_node   @head: pointer to percpu array of struct hlist_heads   @cpu:  pointer to cpu \"cursor\"   @pos:  start position of sequence     Called at seq_file->op->next(). ", "if (who == INT_MIN)return -EINVAL;type = PIDTYPE_PGID;who = -who;}rcu_read_lock();if (who) ": "f_setown(struct file  filp, struct pid  pid, enum pid_type type,int force){security_file_set_fowner(filp);f_modown(filp, pid, type, force);}EXPORT_SYMBOL(__f_setown);int f_setown(struct file  filp, unsigned long arg, int force){enum pid_type type;struct pid  pid = NULL;int who = arg, ret = 0;type = PIDTYPE_TGID;if (who < 0) {  avoid overflow below ", "int fasync_helper(int fd, struct file * filp, int on, struct fasync_struct **fapp)": "fasync_helper() is used by almost all character device drivers   to set up the fasync queue, and for regular files by the file   lease code. It returns negative on error, 0 if it did no changes   and positive if it addeddeleted the entry. ", "if (!(sig == SIGURG && fown->signum == 0))send_sigio(fown, fa->fa_fd, band);}read_unlock_irqrestore(&fa->fa_lock, flags);fa = rcu_dereference(fa->fa_next);}}void kill_fasync(struct fasync_struct **fp, int sig, int band)": "kill_fasync_rcu(struct fasync_struct  fa, int sig, int band){while (fa) {struct fown_struct  fown;unsigned long flags;if (fa->magic != FASYNC_MAGIC) {printk(KERN_ERR \"kill_fasync: bad magic number in \"       \"fasync_struct!\\n\");return;}read_lock_irqsave(&fa->fa_lock, flags);if (fa->fa_file) {fown = &fa->fa_file->f_owner;  Don't send SIGURG to processes which have not set a   queued signum: SIGURG has its own default signalling   mechanism. ", "void deactivate_locked_super(struct super_block *s)": "deactivate_locked_super-drop an active reference to superblock  @s: superblock to deactivate    Drops an active reference to superblock, converting it into a temporary  one if there is no other active references left.  In that case we  tell fs driver to shut it down and drop the temporary reference we  had just acquired.    Caller holds exclusive lock on superblock; that lock is released. ", "void deactivate_super(struct super_block *s)": "deactivate_super-drop an active reference to superblock  @s: superblock to deactivate    Variant of deactivate_locked_super(), except that superblock is  not   locked by caller.  If we are going to drop the final active reference,  lock will be acquired prior to that. ", "void retire_super(struct super_block *sb)": "generic_shutdown_super().  The function can not be called  concurrently with generic_shutdown_super().  It is safe to call the  function multiple times, subsequent calls have no effect.    The marker will affect the re-use only for block-device-based  superblocks.  Other superblocks will still get marked if this function  is used, but that will not affect their reusability. ", "struct super_block *sget_fc(struct fs_context *fc,    int (*test)(struct super_block *, struct fs_context *),    int (*set)(struct super_block *, struct fs_context *))": "sget_fc - Find or create a superblock   @fc:Filesystem context.   @test: Comparison callback   @set: Setup callback     Find or create a superblock using the parameters stored in the filesystem   context and the two callback functions.     If an extant superblock is matched, then that will be returned with an   elevated reference count that the caller must transfer or discard.     If no match is made, a new superblock will be allocated and basic   initialisation will be performed (s_type, s_fs_info and s_id will be set and   the set() callback will be invoked), the superblock will be published and it   will be returned in a partially constructed state with SB_BORN and SB_ACTIVE   as yet unset. ", "down_write_nested(&s->s_umount, SINGLE_DEPTH_NESTING);if (security_sb_alloc(s))goto fail;for (i = 0; i < SB_FREEZE_LEVELS; i++) ": "sget() can have s_umount recursion.     When it cannot find a suitable sb, it allocates a new   one (this one), and tries again to find a suitable old   one.     In case that succeeds, it will acquire the s_umount   lock of the old one. Since these are clearly distrinct   locks, and this object isn't exposed yet, there's no   risk of deadlocks.     Annotate this by putting this lock in a different   subclass. ", "void iterate_supers_type(struct file_system_type *type,void (*f)(struct super_block *, void *), void *arg)": "iterate_supers_type - call function for superblocks of given type  @type: fs type  @f: function to call  @arg: argument to pass to it    Scans the superblock list and calls given function, passing it  locked superblock and given argument. ", "int get_anon_bdev(dev_t *p)": "get_anon_bdev - Allocate a block device for filesystems which don't have one.   @p: Pointer to a dev_t.     Filesystems which don't use real block devices can call this function   to allocate a virtual block device.     Context: Any context.  Frequently called while holding sb_lock.   Return: 0 on success, -EMFILE if there are no anonymous bdevs left   or -ENOMEM if memory allocation failed. ", "int get_tree_bdev(struct fs_context *fc,int (*fill_super)(struct super_block *,  struct fs_context *))": "get_tree_bdev - Get a superblock based on a single block device   @fc: The filesystem context holding the parameters   @fill_super: Helper to initialise a new superblock ", "mutex_lock(&bdev->bd_fsfreeze_mutex);if (bdev->bd_fsfreeze_count > 0) ": "mount_bdev(struct file_system_type  fs_type,int flags, const char  dev_name, void  data,int ( fill_super)(struct super_block  , void  , int)){struct block_device  bdev;struct super_block  s;int error = 0;bdev = blkdev_get_by_path(dev_name, sb_open_mode(flags), fs_type,  &fs_holder_ops);if (IS_ERR(bdev))return ERR_CAST(bdev);    once the super is inserted into the list by sget, s_umount   will protect the lockfs code from trying to start a snapshot   while we are mounting ", "fc = fs_context_for_reconfigure(s->s_root, flags, MS_RMT_MASK);if (IS_ERR(fc))return PTR_ERR(fc);ret = parse_monolithic_mount_data(fc, data);if (ret < 0)goto out;ret = reconfigure_super(fc);out:put_fs_context(fc);return ret;}static int compare_single(struct super_block *s, void *p)": "mount_single(),   then a chunk of this can be removed.  [Bollocks -- AV]   Better yet, reconfiguration shouldn't happen, but rather the second   mount should be rejected if the parameters are not compatible. ", "int vfs_get_tree(struct fs_context *fc)": "vfs_get_tree - Get the mountable root   @fc: The superblock configuration context.     The filesystem is invoked to get or create a superblock which can then later   be used for mounting.  The filesystem places a pointer to the root to be   used for mounting in @fc->root. ", "int freeze_super(struct super_block *sb)": "freeze_super - lock the filesystem and force it into a consistent state   @sb: the super to lock     Syncs the super to make sure the filesystem is consistent and calls the fs's   freeze_fs.  Subsequent calls to this without first thawing the fs will return   -EBUSY.     During this function, sb->s_writers.frozen goes through these values:     SB_UNFROZEN: File system is normal, all writes progress as usual.     SB_FREEZE_WRITE: The file system is in the process of being frozen.  New   writes should be blocked, though page faults are still allowed. We wait for   all writes to complete and then proceed to the next stage.     SB_FREEZE_PAGEFAULT: Freezing continues. Now also page faults are blocked   but internal fs threads can still modify the filesystem (although they   should not dirty new pages or inodes), writeback can run etc. After waiting   for all running page faults we sync the filesystem which will clean all   dirty pages and inodes (no new dirty pages or inodes can be created when   sync is running).     SB_FREEZE_FS: The file system is frozen. Now all internal sources of fs   modification are blocked (e.g. XFS preallocation truncation on inode   reclaim). This is usually implemented by blocking new transactions for   filesystems that have them and need this additional guard. After all   internal writers are finished we call ->freeze_fs() to finish filesystem   freezing. Then we transition to SB_FREEZE_COMPLETE state. This state is   mostly auxiliary for filesystems to verify they do not modify frozen fs.     sb->s_writers.frozen is protected by sb->s_umount. ", "static unsigned long super_cache_scan(struct shrinker *shrink,      struct shrink_control *sc)": "thaw_super_locked(struct super_block  sb);static LIST_HEAD(super_blocks);static DEFINE_SPINLOCK(sb_lock);static char  sb_writers_name[SB_FREEZE_LEVELS] = {\"sb_writers\",\"sb_pagefaults\",\"sb_internal\",};    One thing we have to be careful of with a per-sb shrinker is that we don't   drop the last active reference to the superblock from within the shrinker.   If that happens we could trigger unregistering the shrinker from within the   shrinker path and that leads to deadlock on the shrinker_rwsem. Hence we   take a passive reference to the superblock to avoid this from occurring. ", "int __mnt_want_write_file(struct file *file)": "mnt_drop_write_file. ", "struct vfsmount *vfs_create_mount(struct fs_context *fc)": "vfs_create_mount - Create a mount for a configured superblock   @fc: The configuration context with the superblock attached     Create a mount to an already configured superblock.  If necessary, the   caller should invoke vfs_get_tree() before calling this.     Note that this does not attach the mount to anything. ", "return -1;}/* call under rcu_read_lock ": "mntput_no_expire()if (likely(!read_seqretry(&mount_lock, seq)))return 0;if (bastard->mnt_flags & MNT_SYNC_UMOUNT) {mnt_add_count(mnt, -1);return 1;}lock_mount_hash();if (unlikely(bastard->mnt_flags & MNT_DOOMED)) {mnt_add_count(mnt, -1);unlock_mount_hash();return 1;}unlock_mount_hash();  caller will mntput() ", "child_mnt->mnt_mountpoint = mp->m_dentry;child_mnt->mnt_parent = mnt;child_mnt->mnt_mp = mp;hlist_add_head(&child_mnt->mnt_mp_list, &mp->m_list);}/** * mnt_set_mountpoint_beneath - mount a mount beneath another one * * @new_parent: the source mount * @top_mnt:    the mount beneath which @new_parent is mounted * @new_mp:     the new mountpoint of @top_mnt on @new_parent * * Remove @top_mnt from its current mountpoint @top_mnt->mnt_mp and * parent @top_mnt->mnt_parent and mount it on top of @new_parent at * @new_mp. And mount @new_parent on the old parent and old * mountpoint of @top_mnt. * * Context: This function expects namespace_lock() and lock_mount_hash() *          to have been acquired in that order. ": "mntget ", "bool path_is_mountpoint(const struct path *path)": "path_is_mountpoint() - Check if path is a mount in the current namespace.   @path: path to check      d_mountpoint() can only be used reliably to establish if a dentry is    not mounted in any namespace and that common case is handled inline.    d_mountpoint() isn't aware of the possibility there may be multiple    mounts using a given dentry in a different namespace. This function    checks if the passed in path is a mountpoint rather than the dentry    alone. ", "int may_umount_tree(struct vfsmount *m)": "may_umount_tree - check if a mount tree is busy   @m: root of mount tree     This is called to check if a tree of mounts has any   open files, pwds, chroots or sub mounts that are   busy. ", "void mnt_set_expiry(struct vfsmount *mnt, struct list_head *expiry_list)": "mnt_set_expiry - Put a mount on an expiration list   @mnt: The mount to list.   @expiry_list: The list to add the mount to. ", "s = path.mnt->mnt_sb;atomic_inc(&s->s_active);mntput(path.mnt);/* lock the sucker ": "mount_subtree(struct vfsmount  m, const char  name){struct mount  mnt = real_mount(m);struct mnt_namespace  ns;struct super_block  s;struct path path;int err;ns = alloc_mnt_ns(&init_user_ns, true);if (IS_ERR(ns)) {mntput(m);return ERR_CAST(ns);}mnt->mnt_ns = ns;ns->root = mnt;ns->mounts++;list_add(&mnt->mnt_list, &ns->list);err = vfs_path_lookup(m->mnt_root, m,name, LOOKUP_FOLLOW|LOOKUP_AUTOMOUNT, &path);put_mnt_ns(ns);if (err)return ERR_PTR(err);  trade a vfsmount reference for active sb one ", "new_mnt->mnt_ns = MNT_NS_INTERNAL;return &new_mnt->mnt;invalid:up_read(&namespace_sem);return ERR_PTR(-EINVAL);}EXPORT_SYMBOL_GPL(clone_private_mount);int iterate_mounts(int (*f)(struct vfsmount *, void *), void *arg,   struct vfsmount *root)": "kern_unmount () ", "void fscache_end_volume_access(struct fscache_volume *volume,       struct fscache_cookie *cookie,       enum fscache_access_trace why)": "fscache_end_volume_access(volume, cookie, fscache_access_unlive);return false;}return true;}     fscache_end_volume_access - Unpin a cache at the end of an access.   @volume: The volume cookie   @cookie: A datafile cookie for a tracing reference (or NULL)   @why: An indication of the circumstances of the access for tracing     Unpin a cache volume after we've accessed it.  The datafile @cookie and the   @why indicator are merely provided for tracing purposes. ", "void fscache_withdraw_volume(struct fscache_volume *volume)": "fscache_withdraw_volume - Withdraw a volume from being cached   @volume: Volume cookie     Withdraw a cache volume from service, waiting for all accesses to complete   before returning. ", "void fscache_end_cookie_access(struct fscache_cookie *cookie,       enum fscache_access_trace why)": "fscache_end_cookie_access - Unpin a cache at the end of an access.   @cookie: A data file cookie   @why: An indication of the circumstances of the access for tracing     Unpin a cache cookie after we've accessed it and bring a deferred   relinquishment or withdrawal state into effect.     The @why indicator is provided for tracing purposes. ", "void fscache_cookie_lookup_negative(struct fscache_cookie *cookie)": "fscache_cookie_lookup_negative - Note negative lookup   @cookie: The cookie that was being looked up     Note that some part of the metadata path in the cache doesn't exist and so   we can release any waiting readers in the certain knowledge that there's   nothing for them to actually read.     This function uses no locking and must only be called from the state machine. ", "void fscache_resume_after_invalidation(struct fscache_cookie *cookie)": "fscache_resume_after_invalidation - Allow IO to resume after invalidation   @cookie: The cookie that was invalidated     Tell fscache that invalidation is sufficiently complete that IO can be   allowed again. ", "void fscache_caching_failed(struct fscache_cookie *cookie)": "fscache_caching_failed - Report that a failure stopped caching on a cookie   @cookie: The cookie that was affected     Tell fscache that caching on a cookie needs to be stopped due to some sort   of failure.     This function uses no locking and must only be called from the state machine. ", "if (test_and_clear_bit(FSCACHE_COOKIE_DO_LRU_DISCARD, &cookie->flags))fscache_see_cookie(cookie, fscache_cookie_see_lru_discard_clear);break;case FSCACHE_COOKIE_STATE_FAILED:case FSCACHE_COOKIE_STATE_WITHDRAWING:break;case FSCACHE_COOKIE_STATE_LRU_DISCARDING:spin_unlock(&cookie->lock);wait_var_event(&cookie->state,       fscache_cookie_state(cookie) !=       FSCACHE_COOKIE_STATE_LRU_DISCARDING);spin_lock(&cookie->lock);goto again;case FSCACHE_COOKIE_STATE_DROPPED:case FSCACHE_COOKIE_STATE_RELINQUISHING:WARN(1, \"Can't use cookie in state %u\\n\", state);break;}spin_unlock(&cookie->lock);if (queue)fscache_queue_cookie(cookie, fscache_cookie_get_use_work);_leave(\"\");}EXPORT_SYMBOL(__fscache_use_cookie": "__fscache_use_cookie(struct fscache_cookie  cookie, bool will_modify){enum fscache_cookie_state state;bool queue = false;int n_active;_enter(\"c=%08x\", cookie->debug_id);if (WARN(test_bit(FSCACHE_COOKIE_RELINQUISHED, &cookie->flags), \"Trying to use relinquished cookie\\n\"))return;spin_lock(&cookie->lock);n_active = atomic_inc_return(&cookie->n_active);trace_fscache_active(cookie->debug_id, refcount_read(&cookie->ref),     n_active, atomic_read(&cookie->n_accesses),     will_modify ?     fscache_active_use_modify : fscache_active_use);again:state = fscache_cookie_state(cookie);switch (state) {case FSCACHE_COOKIE_STATE_QUIESCENT:queue = fscache_begin_lookup(cookie, will_modify);break;case FSCACHE_COOKIE_STATE_LOOKING_UP:case FSCACHE_COOKIE_STATE_CREATING:if (will_modify)set_bit(FSCACHE_COOKIE_LOCAL_WRITE, &cookie->flags);break;case FSCACHE_COOKIE_STATE_ACTIVE:case FSCACHE_COOKIE_STATE_INVALIDATING:if (will_modify &&    !test_and_set_bit(FSCACHE_COOKIE_LOCAL_WRITE, &cookie->flags)) {set_bit(FSCACHE_COOKIE_DO_PREP_TO_WRITE, &cookie->flags);queue = true;}    We could race with cookie_lru which may set LRU_DISCARD bit   but has yet to run the cookie state machine.  If this happens   and another thread tries to use the cookie, clear LRU_DISCARD   so we don't end up withdrawing the cookie while in use. ", "c = atomic_fetch_add_unless(&cookie->n_active, -1, 1);if (c != 1) ": "__fscache_unuse_cookie(struct fscache_cookie  cookie,    const void  aux_data, const loff_t  object_size){unsigned int debug_id = cookie->debug_id;unsigned int r = refcount_read(&cookie->ref);unsigned int a = atomic_read(&cookie->n_accesses);unsigned int c;if (aux_data || object_size)__fscache_update_cookie(cookie, aux_data, object_size);  Subtract 1 from counter unless that drops it to 0 (ie. it was 1) ", "static bool fscache_begin_lookup(struct fscache_cookie *cookie, bool will_modify)": "fscache_withdraw_cookie(cookie);fscache_end_volume_access(cookie->volume, cookie, trace);}    Begin the process of looking up a cookie.  We offload the actual process to   a worker thread. ", "static void fscache_init_access_gate(struct fscache_cookie *cookie)": "fscache_get_cookie(cookie, where);__fscache_queue_cookie(cookie);}    Initialise the access gate on a cookie by setting a flag to prevent the   state machine from being queued when the access counter transitions to 0.   We're only interested in this when we withdraw caching services from the   cookie. ", "default:spin_unlock(&cookie->lock);_leave(\" [no %u]\", cookie->state);return;case FSCACHE_COOKIE_STATE_LOOKING_UP:if (!test_and_set_bit(FSCACHE_COOKIE_DO_INVALIDATE, &cookie->flags))__fscache_begin_cookie_access(cookie, fscache_access_invalidate_cookie);fallthrough;case FSCACHE_COOKIE_STATE_CREATING:spin_unlock(&cookie->lock);_leave(\" [look %x]\", cookie->inval_counter);return;case FSCACHE_COOKIE_STATE_ACTIVE:is_caching = fscache_begin_cookie_access(cookie, fscache_access_invalidate_cookie);if (is_caching)__fscache_set_cookie_state(cookie, FSCACHE_COOKIE_STATE_INVALIDATING);spin_unlock(&cookie->lock);wake_up_cookie_state(cookie);if (is_caching)fscache_queue_cookie(cookie, fscache_cookie_get_inval_work);_leave(\" [inv]\");return;}}EXPORT_SYMBOL(__fscache_invalidate": "__fscache_invalidate(struct fscache_cookie  cookie,  const void  aux_data, loff_t new_size,  unsigned int flags){bool is_caching;_enter(\"c=%x\", cookie->debug_id);fscache_stat(&fscache_n_invalidates);if (WARN(test_bit(FSCACHE_COOKIE_RELINQUISHED, &cookie->flags), \"Trying to invalidate relinquished cookie\\n\"))return;if ((flags & FSCACHE_INVAL_DIO_WRITE) &&    test_and_set_bit(FSCACHE_COOKIE_DISABLED, &cookie->flags))return;spin_lock(&cookie->lock);set_bit(FSCACHE_COOKIE_NO_DATA_TO_READ, &cookie->flags);fscache_update_aux(cookie, aux_data, &new_size);cookie->inval_counter++;trace_fscache_invalidate(cookie, new_size);switch (cookie->state) {case FSCACHE_COOKIE_STATE_INVALIDATING:   is_still_valid will catch it ", "bool fscache_wait_for_operation(struct netfs_cache_resources *cres,enum fscache_want_state want_state)": "fscache_wait_for_operation - Wait for an object become accessible   @cres: The cache resources for the operation being performed   @want_state: The minimum state the object must be at     See if the target cache object is at the specified minimum state of   accessibility yet, and if not, wait for it. ", "bool fscache_dirty_folio(struct address_space *mapping, struct folio *folio,struct fscache_cookie *cookie)": "fscache_dirty_folio - Mark folio dirty and pin a cache object for writeback   @mapping: The mapping the folio belongs to.   @folio: The folio being dirtied.   @cookie: The cookie referring to the cache object     Set the dirty flag on a folio and pin an in-use cache object in memory   so that writeback can later write to it.  This is intended   to be called from the filesystem's ->dirty_folio() method.     Return: true if the dirty flag was set on the folio, false otherwise. ", "iov_iter_xarray(&iter, ITER_SOURCE, &mapping->i_pages, start, len);fscache_write(cres, start, &iter, fscache_wreq_done, wreq);return;abandon_end:return fscache_wreq_done(wreq, ret, false);abandon_free:kfree(wreq);abandon:fscache_clear_page_bits(mapping, start, len, cond);if (term_func)term_func(term_func_priv, ret, false);}EXPORT_SYMBOL(__fscache_write_to_cache": "__fscache_write_to_cache(struct fscache_cookie  cookie,      struct address_space  mapping,      loff_t start, size_t len, loff_t i_size,      netfs_io_terminated_t term_func,      void  term_func_priv,      bool cond){struct fscache_write_request  wreq;struct netfs_cache_resources  cres;struct iov_iter iter;int ret = -ENOBUFS;if (len == 0)goto abandon;_enter(\"%llx,%zx\", start, len);wreq = kzalloc(sizeof(struct fscache_write_request), GFP_NOFS);if (!wreq)goto abandon;wreq->mapping= mapping;wreq->start= start;wreq->len= len;wreq->set_bits= cond;wreq->term_func= term_func;wreq->term_func_priv= term_func_priv;cres = &wreq->cache_resources;if (fscache_begin_operation(cres, cookie, FSCACHE_WANT_WRITE,    fscache_access_io_write) < 0)goto abandon_free;ret = cres->ops->prepare_write(cres, &start, &len, i_size, false);if (ret < 0)goto abandon_end;  TODO: Consider clearing page bits now for space the write isn't   covering.  This is more complicated than it appears when THPs are   taken into account. ", "cookie->volume->cache->ops->resize_cookie(&cres, new_size);fscache_end_operation(&cres);} else ": "__fscache_resize_cookie(struct fscache_cookie  cookie, loff_t new_size){struct netfs_cache_resources cres;trace_fscache_resize(cookie, new_size);if (fscache_begin_operation(&cres, cookie, FSCACHE_WANT_WRITE,    fscache_access_io_resize) == 0) {fscache_stat(&fscache_n_resizes);set_bit(FSCACHE_COOKIE_NEEDS_UPDATE, &cookie->flags);  We cannot defer a resize as we need to do it inside the   netfs's inode lock so that we're serialised with respect to   writes. ", "struct fscache_cache *fscache_acquire_cache(const char *name)": "fscache_acquire_cache - Acquire a cache-level cookie.   @name: The name of the cache.     Get a cookie to represent an actual cache.  If a name is given and there is   a nameless cache record available, this will acquire that and set its name,   directing all the volumes using it to this cache.     The cache will be switched over to the preparing state if not currently in   use, otherwise -EBUSY will be returned. ", "void fscache_relinquish_cache(struct fscache_cache *cache)": "fscache_relinquish_cache - Reset cache state and release cookie   @cache: The cache cookie to be released     Reset the state of a cache and release the caller's reference on a cache   cookie. ", "int fscache_add_cache(struct fscache_cache *cache,      const struct fscache_cache_ops *ops,      void *cache_priv)": "fscache_add_cache - Declare a cache as being open for business   @cache: The cache-level cookie representing the cache   @ops: Table of cache operations to use   @cache_priv: Private data for the cache record     Add a cache to the system, making it available for netfs's to use.     See Documentationfilesystemscachingbackend-api.rst for a complete   description. ", "void fscache_io_error(struct fscache_cache *cache)": "fscache_io_error - Note a cache IO error   @cache: The record describing the cache     Note that an IO error occurred in a cache and that it should no longer be   used for anything.  This also reports the error into the kernel log.     See Documentationfilesystemscachingbackend-api.rst for a complete   description. ", "void fscache_withdraw_cache(struct fscache_cache *cache)": "fscache_withdraw_cache - Withdraw a cache from the active service   @cache: The cache cookie     Begin the process of withdrawing a cache from service.  This stops new   cache-level and volume-level accesses from taking place and waits for   currently ongoing cache-level accesses to end. ", "bool fscrypt_decrypt_bio(struct bio *bio)": "fscrypt_decrypt_bio() - decrypt the contents of a bio   @bio: the bio to decrypt     Decrypt the contents of a \"read\" bio following successful completion of the   underlying disk read.  The bio must be reading a whole number of blocks of an   encrypted file directly into the page cache.  If the bio is reading the   ciphertext into bounce pages instead of the page cache (for example, because   the file is also compressed, so decompression is required after decryption),   then this function isn't applicable.  This function may sleep, so it must be   called from a workqueue rather than from the bio's bi_end_io callback.     Return: %true on success; %false on failure.  On failure, bio->bi_status is     also set to an error status. ", "bio = bio_alloc(inode->i_sb->s_bdev, BIO_MAX_VECS, REQ_OP_WRITE,GFP_NOFS);while (len) ": "fscrypt_zeroout_range_inline_crypt(const struct inode  inode,      pgoff_t lblk, sector_t pblk,      unsigned int len){const unsigned int blockbits = inode->i_blkbits;const unsigned int blocks_per_page = 1 << (PAGE_SHIFT - blockbits);struct bio  bio;int ret, err = 0;int num_pages = 0;  This always succeeds since __GFP_DIRECT_RECLAIM is set. ", "void fscrypt_put_encryption_info(struct inode *inode)": "fscrypt_put_encryption_info() - free most of an inode's fscrypt data   @inode: an inode being evicted     Free the inode's fscrypt_info.  Filesystems must call this when the inode is   being evicted.  An RCU grace period need not have elapsed yet. ", "void fscrypt_free_inode(struct inode *inode)": "fscrypt_free_inode() - free an inode's fscrypt data requiring RCU delay   @inode: an inode being freed     Free the inode's cached decrypted symlink target, if any.  Filesystems must   call this after an RCU grace period, just before they free the inode. ", "version = policy.version;if (copy_from_user(&policy, arg, size))return -EFAULT;policy.version = version;if (!inode_owner_or_capable(&nop_mnt_idmap, inode))return -EACCES;ret = mnt_want_write_file(filp);if (ret)return ret;inode_lock(inode);ret = fscrypt_get_policy(inode, &existing_policy);if (ret == -ENODATA) ": "fscrypt_ioctl_set_policy(struct file  filp, const void __user  arg){union fscrypt_policy policy;union fscrypt_policy existing_policy;struct inode  inode = file_inode(filp);u8 version;int size;int ret;if (get_user(policy.version, (const u8 __user  )arg))return -EFAULT;size = fscrypt_policy_size(&policy);if (size <= 0)return -EINVAL;    We should just copy the remaining 'size - 1' bytes here, but a   bizarre bug in gcc 7 and earlier (fixed by gcc r255731) causes gcc to   think that size can be 0 here (despite the check above!)  and  that   it's a compile-time constant.  Thus it would think copy_from_user()   is passed compile-time constant ULONG_MAX, causing the compile-time   buffer overflow check to fail, breaking the build. This only occurred   when building an i386 kernel with -Os and branch profiling enabled.     Work around it by just copying the first byte again... ", "int fscrypt_has_permitted_context(struct inode *parent, struct inode *child)": "fscrypt_has_permitted_context() - is a file's encryption policy permitted       within its directory?     @parent: inode for parent directory   @child: inode for file being looked up, opened, or linked into @parent     Filesystems must call this before permitting access to an inode in a   situation where the parent directory is encrypted (either before allowing   ->lookup() to succeed, or for a regular file before allowing it to be opened)   and before any operation that involves linking an inode into an encrypted   directory, including link, rename, and cross rename.  It enforces the   constraint that within a given encrypted directory tree, all files use the   same encryption policy.  The pre-access check is needed to detect potentially   malicious offline violations of this constraint, while the link and rename   checks are needed to prevent online violations of this constraint.     Return: 1 if permitted, 0 if forbidden. ", "static int fname_decrypt(const struct inode *inode, const struct fscrypt_str *iname, struct fscrypt_str *oname)": "fscrypt_fname_alloc_buffer().     Return: 0 on success, -errno on failure ", "void fscrypt_fname_free_buffer(struct fscrypt_str *crypto_str)": "fscrypt_fname_free_buffer() - free a buffer for presented filenames   @crypto_str: the buffer to free     Free a buffer that was allocated by fscrypt_fname_alloc_buffer(). ", "int fscrypt_fname_disk_to_usr(const struct inode *inode,      u32 hash, u32 minor_hash,      const struct fscrypt_str *iname,      struct fscrypt_str *oname)": "fscrypt_fname_disk_to_usr() - convert an encrypted filename to   user-presentable form   @inode: inode of the parent directory (for regular filenames)     or of the symlink (for symlink targets)   @hash: first part of the name's dirhash, if applicable.  This only needs to    be provided if the filename is located in an indexed directory whose    encryption key may be unavailable.  Not needed for symlink targets.   @minor_hash: second part of the name's dirhash, if applicable   @iname: encrypted filename to convert.  May also be \".\" or \"..\", which     aren't actually encrypted.   @oname: output buffer for the user-presentable filename.  The caller must     have allocated enough space for this, e.g. using     fscrypt_fname_alloc_buffer().     If the key is available, we'll decrypt the disk name.  Otherwise, we'll   encode it for presentation in fscrypt_nokey_name format.   See struct fscrypt_nokey_name for details.     Return: 0 on success, -errno on failure ", "int fscrypt_setup_filename(struct inode *dir, const struct qstr *iname,      int lookup, struct fscrypt_name *fname)": "fscrypt_setup_filename() - prepare to search a possibly encrypted directory   @dir: the directory that will be searched   @iname: the user-provided filename being searched for   @lookup: 1 if we're allowed to proceed without the key because it's  ->lookup() or we're finding the dir_entry for deletion; 0 if we cannot  proceed without the key because we're going to create the dir_entry.   @fname: the filename information to be filled in     Given a user-provided filename @iname, this function sets @fname->disk_name   to the name that would be stored in the on-disk directory entry, if possible.   If the directory is unencrypted this is simply @iname.  Else, if we have the   directory's encryption key, then @iname is the plaintext, so we encrypt it to   get the disk_name.     Else, for keyless @lookup operations, @iname should be a no-key name, so we   decode it to get the struct fscrypt_nokey_name.  Non-@lookup operations will   be impossible in this case, so we fail them with ENOKEY.     If successful, fscrypt_free_filename() must be called later to clean up.     Return: 0 on success, -errno on failure ", "void fscrypt_free_bounce_page(struct page *bounce_page)": "fscrypt_encrypt_pagecache_blocks(),   or by fscrypt_alloc_bounce_page() directly. ", "int fscrypt_encrypt_block_inplace(const struct inode *inode, struct page *page,  unsigned int len, unsigned int offs,  u64 lblk_num, gfp_t gfp_flags)": "fscrypt_encrypt_block_inplace() - Encrypt a filesystem block in-place   @inode:     The inode to which this block belongs   @page:      The page containing the block to encrypt   @len:       Size of block to encrypt.  This must be a multiple of  FSCRYPT_CONTENTS_ALIGNMENT.   @offs:      Byte offset within @page at which the block to encrypt begins   @lblk_num:  Filesystem logical block number of the block, i.e. the 0-based  number of the block within the file   @gfp_flags: Memory allocation flags     Encrypt a possibly-compressed filesystem block that is located in an   arbitrary page, not necessarily in the original pagecache page.  The @inode   and @lblk_num must be specified, as they can't be determined from @page.     Return: 0 on success; -errno on failure ", "int fscrypt_decrypt_pagecache_blocks(struct folio *folio, size_t len,     size_t offs)": "fscrypt_decrypt_pagecache_blocks() - Decrypt filesystem blocks in a  pagecache folio   @folio:     The locked pagecache folio containing the block(s) to decrypt   @len:       Total size of the block(s) to decrypt.  Must be a nonzero  multiple of the filesystem's block size.   @offs:      Byte offset within @folio of the first block to decrypt.  Must be  a multiple of the filesystem's block size.     The specified block(s) are decrypted in-place within the pagecache folio,   which must still be locked and not uptodate.     This is for use by the filesystem's ->readahead() method.     Return: 0 on success; -errno on failure ", "int fscrypt_decrypt_block_inplace(const struct inode *inode, struct page *page,  unsigned int len, unsigned int offs,  u64 lblk_num)": "fscrypt_decrypt_block_inplace() - Decrypt a filesystem block in-place   @inode:     The inode to which this block belongs   @page:      The page containing the block to decrypt   @len:       Size of block to decrypt.  This must be a multiple of  FSCRYPT_CONTENTS_ALIGNMENT.   @offs:      Byte offset within @page at which the block to decrypt begins   @lblk_num:  Filesystem logical block number of the block, i.e. the 0-based  number of the block within the file     Decrypt a possibly-compressed filesystem block that is located in an   arbitrary page, not necessarily in the original pagecache page.  The @inode   and @lblk_num must be specified, as they can't be determined from @page.     Return: 0 on success; -errno on failure ", "err = fuse_fill_super_submount(sb, mp_fi);if (err) ": "fuse_mount_destroy(fm);if (IS_ERR(sb))return PTR_ERR(sb);  Initialize superblock, making @mp_fi its root ", "if ((failed << 1) < quorum)return;}o2quo_disk_timeout();}static void o2hb_arm_timeout(struct o2hb_region *reg)": "o2hb_global_heartbeat_active()) {spin_lock(&o2hb_live_lock);if (test_bit(reg->hr_region_num, o2hb_quorum_region_bitmap))set_bit(reg->hr_region_num, o2hb_failed_region_bitmap);failed = bitmap_weight(o2hb_failed_region_bitmap,O2NM_MAX_REGIONS);quorum = bitmap_weight(o2hb_quorum_region_bitmap,O2NM_MAX_REGIONS);spin_unlock(&o2hb_live_lock);mlog(ML_HEARTBEAT, \"Number of regions %d, failed regions %d\\n\",     quorum, failed);    Fence if the number of failed regions >= half the number   of  quorum regions ", "/* * configfs_depend_prep() * * Only subdirectories count here.  Files (CONFIGFS_NOT_PINNED) are * attributes.  This is similar but not the same to configfs_detach_prep(). * Note that configfs_detach_prep() expects the parent to be locked when it * is called, but we lock the parent *inside* configfs_depend_prep().  We * do that so we can unlock it if we find nothing. * * Here we do a depth-first search of the dentry hierarchy looking for * our object. * We deliberately ignore items tagged as dropping since they are virtually * dead, as well as items in the middle of attachment since they virtually * do not exist yet. This completes the locking out of racing mkdir() and * rmdir(). * Note: subdirectories in the middle of attachment start with s_type = * CONFIGFS_DIR|CONFIGFS_USET_CREATING set by create_dir().  When * CONFIGFS_USET_CREATING is set, we ignore the item.  The actual set of * s_type is in configfs_new_dirent(), which has configfs_dirent_lock. * * If the target is not found, -ENOENT is bubbled up. * * This adds a requirement that all config_items be unique! * * This is recursive.  There isn't * much on the stack, though, so folks that need this function - be careful * about your stack!  Patches will be accepted to make it iterative. ": "configfs_register_subsystem().  So we take the same   precautions.  We pin the filesystem.  We lock configfs_dirent_lock.   If we can find the target item in the   configfs tree, it must be part of the subsystem tree as well, so we   do not need the subsystem semaphore.  Holding configfs_dirent_lock helps   locking out mkdir() and rmdir(), who might be racing us. ", "if (configfs_is_root(target))return -EINVAL;parent = target->ci_group;/* * This may happen when someone is trying to depend root * directory of some subsystem ": "configfs_depend_item_unlocked(struct configfs_subsystem  caller_subsys,  struct config_item  target){struct configfs_subsystem  target_subsys;struct config_group  root,  parent;struct configfs_dirent  subsys_sd;int ret = -ENOENT;  Disallow this function for configfs root ", "int configfs_register_group(struct config_group *parent_group,    struct config_group *group)": "configfs_register_group - creates a parent-child relation between two groups   @parent_group:parent group   @group:child group     link groups, creates dentry for the child and attaches it to the   parent dentry.     Return: 0 on success, negative errno code on error ", "void configfs_unregister_group(struct config_group *group)": "configfs_unregister_group() - unregisters a child group from its parent   @group: parent group to be unregistered     Undoes configfs_register_group() ", "struct config_group *configfs_register_default_group(struct config_group *parent_group,const char *name,const struct config_item_type *item_type)": "configfs_register_default_group() - allocates and registers a child group   @parent_group:parent group   @name:child group name   @item_type:child item type description     boilerplate to allocate and register a child group with its parent. We need   kzalloc'ed memory because child's default_group is initially empty.     Return: allocated config group or ERR_PTR() on error ", "void configfs_unregister_default_group(struct config_group *group)": "configfs_unregister_default_group() - unregisters and frees a child group   @group:the group to act on ", "int config_item_set_name(struct config_item *item, const char *fmt, ...)": "config_item_set_name - Set the name of an item  @item:item.  @fmt:  The vsnprintf()'s format string.    If strlen(name) >= CONFIGFS_ITEM_NAME_LEN, then use a  dynamically allocated string that @item->ci_name points to.  Otherwise, use the static @item->ci_namebuf array. ", "void config_item_put(struct config_item *item)": "config_item_put(parent);}static void config_item_release(struct kref  kref){config_item_cleanup(container_of(kref, struct config_item, ci_kref));}    config_item_put - decrement refcount for item.  @item:item.    Decrement the refcount, and if 0, call config_item_cleanup(). ", "struct config_item *config_group_find_item(struct config_group *group,   const char *name)": "config_group_find_item - search for item in group.  @group:group we're looking in.  @name:item's name.    Iterate over @group->cg_list, looking for a matching config_item.  If matching item is found take a reference and return the item.  Caller must have locked group via @group->cg_subsys->su_mtx. ", "void netfs_readahead(struct readahead_control *ractl)": "netfs_readahead - Helper to manage a read request   @ractl: The description of the readahead request     Fulfil a readahead request by drawing data from the cache if possible, or   the netfs if not.  Space beyond the EOF is zero-filled.  Multiple IO   requests from different sources will get munged together.  If necessary, the   readahead window can be expanded in either direction to a more convenient   alighment for RPC efficiency or to make storage in the cache feasible.     The calling netfs must initialise a netfs context contiguous to the vfs   inode before calling this.     This is usable whether or not caching is enabled. ", "int netfs_read_folio(struct file *file, struct folio *folio)": "netfs_read_folio - Helper to manage a read_folio request   @file: The file to read from   @folio: The folio to read     Fulfil a read_folio request by drawing data from the cache if   possible, or the netfs if not.  Space beyond the EOF is zero-filled.   Multiple IO requests from different sources will get munged together.     The calling netfs must initialise a netfs context contiguous to the vfs   inode before calling this.     This is usable whether or not caching is enabled. ", "int netfs_write_begin(struct netfs_inode *ctx,      struct file *file, struct address_space *mapping,      loff_t pos, unsigned int len, struct folio **_folio,      void **_fsdata)": "netfs_write_begin - Helper to prepare for writing   @ctx: The netfs context   @file: The file to read from   @mapping: The mapping to read from   @pos: File position at which the write will begin   @len: The length of the write (may extend beyond the end of the folio chosen)   @_folio: Where to put the resultant folio   @_fsdata: Place for the netfs to store a cookie     Pre-read data for a write-begin request by drawing data from the cache if   possible, or the netfs if not.  Space beyond the EOF is zero-filled.   Multiple IO requests from different sources will get munged together.  If   necessary, the readahead window can be expanded in either direction to a   more convenient alighment for RPC efficiency or to make storage in the cache   feasible.     The calling netfs must provide a table of operations, only one of which,   issue_op, is mandatory.     The check_write_begin() operation can be provided to check for and flush   conflicting writes once the folio is grabbed and locked.  It is passed a   pointer to the fsdata cookie that gets returned to the VM to be passed to   write_end.  It is permitted to sleep.  It should return 0 if the request   should go ahead or it may return an error.  It may also unlock and put the   folio, provided it sets `` foliop`` to NULL, in which case a return of 0   will cause the folio to be re-got and the process to be retried.     The calling netfs must initialise a netfs context contiguous to the vfs   inode before calling this.     This is usable whether or not caching is enabled. ", "static void netfs_read_from_cache(struct netfs_io_request *rreq,  struct netfs_io_subrequest *subreq,  enum netfs_read_from_hole read_hole)": "netfs_subreq_terminated(subreq, transferred_or_error, was_async);}    Issue a read against the cache.   - Eats the caller's ref on subreq. ", "handle_t *jbd2_journal_start(journal_t *journal, int nblocks)": "jbd2_journal_start_reserved() before   it can be used.     Return a pointer to a newly allocated handle, or an ERR_PTR() value   on failure. ", "read_lock(&journal->j_state_lock);__jbd2_journal_unreserve_handle(handle, journal->j_running_transaction);read_unlock(&journal->j_state_lock);jbd2_free_handle(handle);}EXPORT_SYMBOL(jbd2_journal_free_reserved": "jbd2_journal_free_reserved(handle_t  handle){journal_t  journal = handle->h_journal;  Get j_state_lock to pin running transaction if it exists ", "int jbd2__journal_restart(handle_t *handle, int nblocks, int revoke_records,  gfp_t gfp_mask)": "jbd2_journal_restart will commit the   handle's transaction so far and reattach the handle to a new   transaction capable of guaranteeing the requested number of   credits. We preserve reserved handle if there's any attached to the   passed in handle. ", "if (tid_geq(journal->j_commit_sequence, tid))goto out;commit_trans = journal->j_committing_transaction;if (!commit_trans || commit_trans->t_tid != tid) ": "jbd2_trans_will_send_data_barrier(journal_t  journal, tid_t tid){int ret = 0;transaction_t  commit_trans;if (!(journal->j_flags & JBD2_BARRIER))return 0;read_lock(&journal->j_state_lock);  Transaction already committed? ", "if (!journal->j_stats.ts_tid)return -EINVAL;write_lock(&journal->j_state_lock);if (tid <= journal->j_commit_sequence) ": "jbd2_fc_begin_commit(journal_t  journal, tid_t tid){if (unlikely(is_journal_aborted(journal)))return -EIO;    Fast commits only allowed if at least one full commit has   been processed. ", "int jbd2_transaction_committed(journal_t *journal, tid_t tid)": "jbd2_complete_transaction(journal, tid);return 0;}int jbd2_fc_end_commit(journal_t  journal){return __jbd2_fc_end_commit(journal, 0, false);}EXPORT_SYMBOL(jbd2_fc_end_commit);int jbd2_fc_end_commit_fallback(journal_t  journal){tid_t tid;read_lock(&journal->j_state_lock);tid = journal->j_running_transaction ?journal->j_running_transaction->t_tid : 0;read_unlock(&journal->j_state_lock);return __jbd2_fc_end_commit(journal, tid, true);}EXPORT_SYMBOL(jbd2_fc_end_commit_fallback);  Return 1 when transaction with given tid has already committed. ", "for (i = j_fc_off - 1; i >= j_fc_off - num_blks; i--) ": "jbd2_fc_wait_bufs(journal_t  journal, int num_blks){struct buffer_head  bh;int i, j_fc_off;j_fc_off = journal->j_fc_off;    Wait in reverse order to minimize chances of us being woken up before   all IOs have completed ", "if (unlikely(!buffer_uptodate(bh))) ": "jbd2_fc_release_bufs can release remain   buffer head. ", "void jbd2_journal_update_sb_errno(journal_t *journal)": "jbd2_journal_update_sb_errno() - Update error in the journal.   @journal: The journal to update.     Update a journal's errno.  Write updated superblock to disk waiting for IO   to complete. ", "void jbd2_journal_clear_features(journal_t *journal, unsigned long compat,unsigned long ro, unsigned long incompat)": "jbd2_journal_clear_features() - Clear a given journal feature in the       superblock   @journal: Journal to act on.   @compat: bitmask of compatible features   @ro: bitmask of features that force read-only mount   @incompat: bitmask of incompatible features     Clear a given journal feature as present on the   superblock. ", "/* * Give a buffer_head a journal_head. * * May sleep. ": "jbd2_journal_put_journal_head() to undo this.     So the typical usage would be:    (Attach a journal_head if needed.  Increments b_jcount)  struct journal_head  jh = jbd2_journal_add_journal_head(bh);  ...        (Get another reference for transaction)  jbd2_journal_grab_journal_head(bh);  jh->b_transaction = xxx;  (Put original reference)  jbd2_journal_put_journal_head(jh); ", "if ((*parent)->proc_dops == &proc_net_dentry_ops)pde_force_lookup(ent);out:return ent;}struct proc_dir_entry *proc_symlink(const char *name,struct proc_dir_entry *parent, const char *dest)": "proc_create(struct proc_dir_entry   parent,  const char  name,  umode_t mode,  nlink_t nlink){struct proc_dir_entry  ent = NULL;const char  fn;struct qstr qstr;if (xlate_proc_name(name, parent, &fn) != 0)goto out;qstr.name = fn;qstr.len = strlen(fn);if (qstr.len == 0 || qstr.len >= 256) {WARN(1, \"name len %u\\n\", qstr.len);return NULL;}if (qstr.len == 1 && fn[0] == '.') {WARN(1, \"name '.'\\n\");return NULL;}if (qstr.len == 2 && fn[0] == '.' && fn[1] == '.') {WARN(1, \"name '..'\\n\");return NULL;}if ( parent == &proc_root && name_to_int(&qstr) != ~0U) {WARN(1, \"create 'proc%s' by hand\\n\", qstr.name);return NULL;}if (is_empty_pde( parent)) {WARN(1, \"attempt to add to permanently empty directory\");return NULL;}ent = kmem_cache_zalloc(proc_dir_entry_cache, GFP_KERNEL);if (!ent)goto out;if (qstr.len + 1 <= SIZEOF_PDE_INLINE_NAME) {ent->name = ent->inline_name;} else {ent->name = kmalloc(qstr.len + 1, GFP_KERNEL);if (!ent->name) {pde_free(ent);return NULL;}}memcpy(ent->name, fn, qstr.len + 1);ent->namelen = qstr.len;ent->mode = mode;ent->nlink = nlink;ent->subdir = RB_ROOT;refcount_set(&ent->refcnt, 1);spin_lock_init(&ent->pde_unload_lock);INIT_LIST_HEAD(&ent->pde_openers);proc_set_user(ent, ( parent)->uid, ( parent)->gid);ent->proc_dops = &proc_misc_dentry_ops;  Revalidate everything under proc${pid}net ", "static int __xlate_proc_name(const char *name, struct proc_dir_entry **ret,     const char **residual)": "proc_set_user(de, inode->i_uid, inode->i_gid);de->mode = inode->i_mode;return 0;}static int proc_getattr(struct mnt_idmap  idmap,const struct path  path, struct kstat  stat,u32 request_mask, unsigned int query_flags){struct inode  inode = d_inode(path->dentry);struct proc_dir_entry  de = PDE(inode);if (de) {nlink_t nlink = READ_ONCE(de->nlink);if (nlink > 0) {set_nlink(inode, nlink);}}generic_fillattr(&nop_mnt_idmap, inode, stat);return 0;}static const struct inode_operations proc_file_inode_operations = {.setattr= proc_notify_change,};    This function parses a name such as \"ttydriverserial\", and   returns the struct proc_dir_entry for \"procttydriver\", and   returns \"serial\" in residual. ", "struct ctl_table_header *register_sysctl_mount_point(const char *path)": "register_sysctl_mount_point() - registers a sysctl mount point   @path: path for the mount point     Used to create a permanently empty directory to serve as mount point.   There are some subtle but important permission checks this allows in the   case of unprivileged mounts. ", "struct ctl_table_header *register_sysctl(const char *path, struct ctl_table *table)": "unregister_sysctl_table() is called with the given returned table   with this registration. If your code is non modular then you don't need   to call unregister_sysctl_table() and can instead use something like   register_sysctl_init() which does not care for the result of the syctl   registration.     Register a sysctl table. @table should be a filled in ctl_table   array. A completely 0 filled entry terminates the table.     See __register_sysctl_table for more details. ", "int vmcore_add_device_dump(struct vmcoredd_data *data)": "vmcore_add_device_dump - Add a buffer containing device dump to vmcore   @data: dump info.     Allocate a buffer and invoke the calling driver's dump collect routine.   Write ELF note at the beginning of the buffer to indicate vmcore device   dump and add the dump to global list. ", "struct dentry *debugfs_create_automount(const char *name,struct dentry *parent,debugfs_automount_t f,void *data)": "debugfs_create_automount - create automount point in the debugfs filesystem   @name: a pointer to a string containing the name of the file to create.   @parent: a pointer to the parent dentry for this file.  This should be a            directory dentry if set.  If this parameter is NULL, then the            file will be created in the root of the debugfs filesystem.   @f: function to be called when pathname resolution steps on that one.   @data: opaque argument to pass to f().     @f should return what ->d_automount() would. ", "continue;}if (inlen <= 0)break;v = get_utf16(*pwcs, endian);if ((v & SURROGATE_MASK) != SURROGATE_PAIR ||!(v & SURROGATE_LOW)) ": "utf16s_to_utf8s(const wchar_t  pwcs, int inlen, enum utf16_endian endian,u8  s, int maxout){u8  op;int size;unsigned long u, v;op = s;while (inlen > 0 && maxout > 0) {u = get_utf16( pwcs, endian);if (!u)break;pwcs++;inlen--;if (u > 0x7f) {if ((u & SURROGATE_MASK) == SURROGATE_PAIR) {if (u & SURROGATE_LOW) {  Ignore character and move on ", "0x0000, 0x0001, 0x0002, 0x0003,0x0004, 0x0005, 0x0006, 0x0007,0x0008, 0x0009, 0x000a, 0x000b,0x000c, 0x000d, 0x000e, 0x000f,/* 0x10": "load_nls(const char  charset){return try_then_request_module(find_nls(charset), \"nls_%s\", charset);}void unload_nls(struct nls_table  nls){if (nls)module_put(nls->owner);}static const wchar_t charset2uni[256] = {  0x00", "bool qid_eq(struct kqid left, struct kqid right)": "qid_eq - Test to see if to kquid values are the same  @left: A qid value  @right: Another quid value    Return true if the two qid values are equal and false otherwise. ", "bool qid_lt(struct kqid left, struct kqid right)": "qid_lt - Test to see if one qid value is less than another  @left: The possibly lesser qid value  @right: The possibly greater qid value    Return true if left is less than right and false otherwise. ", "qid_t from_kqid(struct user_namespace *targ, struct kqid kqid)": "from_kqid - Create a qid from a kqid user-namespace pair.  @targ: The user namespace we want a qid in.  @kqid: The kernel internal quota identifier to start with.    Map @kqid into the user-namespace specified by @targ and  return the resulting qid.    There is always a mapping into the initial user_namespace.    If @kqid has no mapping in @targ (qid_t)-1 is returned. ", "qid_t from_kqid_munged(struct user_namespace *targ, struct kqid kqid)": "from_kqid_munged - Create a qid from a kqid user-namespace pair.  @targ: The user namespace we want a qid in.  @kqid: The kernel internal quota identifier to start with.    Map @kqid into the user-namespace specified by @targ and  return the resulting qid.    There is always a mapping into the initial user_namespace.    Unlike from_kqid from_kqid_munged never fails and always  returns a valid projid.  This makes from_kqid_munged  appropriate for use in places where failing to provide  a qid_t is not a good option.    If @kqid has no mapping in @targ the kqid.type specific  overflow identifier is returned. ", "bool qid_valid(struct kqid qid)": "qid_valid - Report if a valid value is stored in a kqid.  @qid: The kernel internal quota identifier to test. ", "if (!dquot->dq_off) ": "qtree_write_dquot(struct qtree_mem_dqinfo  info, struct dquot  dquot){int type = dquot->dq_id.type;struct super_block  sb = dquot->dq_sb;ssize_t ret;char  ddquot = kmalloc(info->dqi_entry_size, GFP_NOFS);if (!ddquot)return -ENOMEM;  dq_off is guarded by dqio_sem ", "return 0;return remove_tree(info, dquot, &tmp, 0);}EXPORT_SYMBOL(qtree_delete_dquot": "qtree_delete_dquot(struct qtree_mem_dqinfo  info, struct dquot  dquot){uint tmp = QT_TREEOFF;if (!dquot->dq_off)  Even not allocated? ", "if (!sb_dqopt(dquot->dq_sb)->files[type]) ": "qtree_read_dquot(struct qtree_mem_dqinfo  info, struct dquot  dquot){int type = dquot->dq_id.type;struct super_block  sb = dquot->dq_sb;loff_t offset;char  ddquot;int ret = 0;#ifdef __QUOTA_QT_PARANOIA  Invalidated quota? ", "static __cacheline_aligned_in_smp DEFINE_SPINLOCK(dq_list_lock);static __cacheline_aligned_in_smp DEFINE_SPINLOCK(dq_state_lock);__cacheline_aligned_in_smp DEFINE_SPINLOCK(dq_data_lock);EXPORT_SYMBOL(dq_data_lock": "dq_data_lock protects mem_dqinfo structures and modifications of dquot     pointers in the inode     dq_state_lock protects modifications of quota state (on quotaon and     quotaoff) and readers who care about latest values take it as well.     The spinlock ordering is hence:     dq_data_lock > dq_list_lock > i_lock > dquot->dq_dqb_lock,     dq_list_lock > dq_state_lock     Note that some things (eg. sb pointer, type, id) doesn't change during   the life of the dquot structure and so needn't to be protected by a lock     Operation accessing dquots via inode pointers are protected by dquot_srcu.   Operation of reading pointer needs srcu_read_lock(&dquot_srcu), and   synchronize_srcu(&dquot_srcu) is called after clearing pointers from   inode and before dropping dquot references to avoid use of dquots after   they are freed. dq_data_lock is used to serialize the pointer setting and   clearing operations.   Special care needs to be taken about S_NOQUOTA inode flag (marking that   inode is a quota file). Functions adding pointers from inode to dquots have   to check this flag under dq_data_lock and then (if S_NOQUOTA is not set) they   have to do all pointer modifications before dropping dq_data_lock. This makes   sure they cannot race with quotaon which first sets S_NOQUOTA flag and   then drops all pointers to dquots from an inode.     Each dquot has its dq_lock mutex.  Dquot is locked when it is being read to   memory (or space for it is being allocated) on the first dqget(), when it is   being written out, and when it is being released on the last dqput(). The   allocation and release operations are serialized by the dq_lock and by   checking the use count in dquot_release().     Lock ordering (including related VFS locks) is the following:     s_umount > i_mutex > journal_lock > dquot->dq_lock > dqio_sem ", "static LIST_HEAD(inuse_list);static LIST_HEAD(free_dquots);static unsigned int dq_hash_bits, dq_hash_mask;static struct hlist_head *dquot_hash;struct dqstats dqstats;EXPORT_SYMBOL(dqstats": "dqstats.free_dquots gives the number of dquots on the list. When   dquot is invalidated it's completely released from memory.     Dirty dquots are added to the dqi_dirty_list of quota_info when mark   dirtied, and this list is searched when writing dirty dquots back to   quota file. Note that some filesystems do dirty dquot tracking on their   own (e.g. in a journal) and thus don't use dqi_dirty_list.     Dquots with a specific identity (device, type and id) are placed on   one of the dquot_hash[] hash chains. The provides an efficient search   mechanism to locate a specific dquot. ", "if (test_bit(DQ_MOD_B, &dquot->dq_flags))return 1;spin_lock(&dq_list_lock);if (!test_and_set_bit(DQ_MOD_B, &dquot->dq_flags)) ": "dquot_mark_dquot_dirty(struct dquot  dquot){int ret = 1;if (!test_bit(DQ_ACTIVE_B, &dquot->dq_flags))return 0;if (sb_dqopt(dquot->dq_sb)->flags & DQUOT_NOLIST_DIRTY)return test_and_set_bit(DQ_MOD_B, &dquot->dq_flags);  If quota is dirty already, we don't have to acquire dq_list_lock ", "smp_mb__before_atomic();set_bit(DQ_READ_B, &dquot->dq_flags);/* Instantiate dquot if needed ": "dquot_acquire(struct dquot  dquot){int ret = 0, ret2 = 0;unsigned int memalloc;struct quota_info  dqopt = sb_dqopt(dquot->dq_sb);mutex_lock(&dquot->dq_lock);memalloc = memalloc_nofs_save();if (!test_bit(DQ_READ_B, &dquot->dq_flags)) {ret = dqopt->ops[dquot->dq_id.type]->read_dqblk(dquot);if (ret < 0)goto out_iolock;}  Make sure flags update is visible after dquot has been filled ", "if (test_bit(DQ_ACTIVE_B, &dquot->dq_flags))ret = dqopt->ops[dquot->dq_id.type]->commit_dqblk(dquot);elseret = -EIO;out_lock:memalloc_nofs_restore(memalloc);mutex_unlock(&dquot->dq_lock);return ret;}EXPORT_SYMBOL(dquot_commit": "dquot_commit(struct dquot  dquot){int ret = 0;unsigned int memalloc;struct quota_info  dqopt = sb_dqopt(dquot->dq_sb);mutex_lock(&dquot->dq_lock);memalloc = memalloc_nofs_save();if (!clear_dquot_dirty(dquot))goto out_lock;  Inactive dquot can be only if there was error during readinit   => we have better not writing it ", "static __cacheline_aligned_in_smp DEFINE_SPINLOCK(dq_list_lock);static __cacheline_aligned_in_smp DEFINE_SPINLOCK(dq_state_lock);__cacheline_aligned_in_smp DEFINE_SPINLOCK(dq_data_lock);EXPORT_SYMBOL(dq_data_lock);DEFINE_STATIC_SRCU(dquot_srcu);static DECLARE_WAIT_QUEUE_HEAD(dquot_ref_wq);void __quota_error(struct super_block *sb, const char *func,   const char *fmt, ...)": "dqput(). The   allocation and release operations are serialized by the dq_lock and by   checking the use count in dquot_release().     Lock ordering (including related VFS locks) is the following:     s_umount > i_mutex > journal_lock > dquot->dq_lock > dqio_sem ", "atomic_inc(&dquot->dq_count);spin_unlock(&dq_list_lock);dqput(old_dquot);old_dquot = dquot;/* * ->release_dquot() can be racing with us. Our reference * protects us from new calls to it so just wait for any * outstanding call and recheck the DQ_ACTIVE_B after that. ": "dquot_scan_active(struct super_block  sb,      int ( fn)(struct dquot  dquot, unsigned long priv),      unsigned long priv){struct dquot  dquot,  old_dquot = NULL;int ret = 0;WARN_ON_ONCE(!rwsem_is_locked(&sb->s_umount));spin_lock(&dq_list_lock);list_for_each_entry(dquot, &inuse_list, dq_inuse) {if (!test_bit(DQ_ACTIVE_B, &dquot->dq_flags))continue;if (dquot->dq_sb != sb)continue;  Now we have active dquot so we can just increase use count ", "list_replace_init(&dqopt->info[cnt].dqi_dirty_list, &dirty);while (!list_empty(&dirty)) ": "dquot_writeback_dquots(struct super_block  sb, int type){struct list_head dirty;struct dquot  dquot;struct quota_info  dqopt = sb_dqopt(sb);int cnt;int err, ret = 0;WARN_ON_ONCE(!rwsem_is_locked(&sb->s_umount));for (cnt = 0; cnt < MAXQUOTAS; cnt++) {if (type != -1 && cnt != type)continue;if (!sb_has_quota_active(sb, cnt))continue;spin_lock(&dq_list_lock);  Move list away to avoid livelock. ", "if (sb->s_op->sync_fs) ": "dquot_quota_sync(struct super_block  sb, int type){struct quota_info  dqopt = sb_dqopt(sb);int cnt;int ret;ret = dquot_writeback_dquots(sb, type);if (ret)return ret;if (dqopt->flags & DQUOT_QUOTA_SYS_FILE)return 0;  This is not very clever (and fast) but currently I don't know about   any other simple way of getting quota data to disk and we must get   them there for userspace to be visible... ", "#include <linux/errno.h>#include <linux/kernel.h>#include <linux/fs.h>#include <linux/mount.h>#include <linux/mm.h>#include <linux/time.h>#include <linux/types.h>#include <linux/string.h>#include <linux/fcntl.h>#include <linux/stat.h>#include <linux/tty.h>#include <linux/file.h>#include <linux/slab.h>#include <linux/sysctl.h>#include <linux/init.h>#include <linux/module.h>#include <linux/proc_fs.h>#include <linux/security.h>#include <linux/sched.h>#include <linux/cred.h>#include <linux/kmod.h>#include <linux/namei.h>#include <linux/capability.h>#include <linux/quotaops.h>#include <linux/blkdev.h>#include <linux/sched/mm.h>#include \"../internal.h\" /* ugh ": "dquot_transfer(), dqget() and dquot_alloc_...().  As the consequence the locking was moved from dquot_decr_...(),  dquot_incr_...() to calling functions.  invalidate_dquots() now writes modified dquots.  Serialized quota_off() and quota_on() for mount point.  Fixed a few bugs in grow_dquots().  Fixed deadlock in write_dquot() - we no longer account quotas on  quota files  remove_dquot_ref() moved to inode.c - it now traverses through inodes  add_dquot_ref() restarts after blocking  Added check for bogus uid and fixed check for group in quotactl.  Jan Kara, <jack@suse.cz>, sponsored by SuSE CR, 10-1199    Used struct list_head instead of own list struct  Invalidation of referenced dquots is no longer possible  Improved free_dquots list management  Quota and i_blocks are now updated in one place to avoid races  Warnings are now delayed so we won't block in critical section  Write updated not to require dquot lock  Jan Kara, <jack@suse.cz>, 92000    Added dynamic quota structure allocation  Jan Kara <jack@suse.cz> 122000    Rewritten quota interface. Implemented new quota format and  formats registering.  Jan Kara, <jack@suse.cz>, 2001,2002    New SMP locking.  Jan Kara, <jack@suse.cz>, 102002    Added journalled quota support, fix lock inversion problems  Jan Kara, <jack@suse.cz>, 2003,2004     (C) Copyright 1994 - 1997 Marco van Wieringen ", "static inline void insert_dquot_hash(struct dquot *dquot)": "dquot_initialize(struct inode  inode, int type);static inline unsigned inthashfn(const struct super_block  sb, struct kqid qid){unsigned int id = from_kqid(&init_user_ns, qid);int type = qid.type;unsigned long tmp;tmp = (((unsigned long)sb>>L1_CACHE_SHIFT) ^ id)   (MAXQUOTAS - type);return (tmp + (tmp >> dq_hash_bits)) & dq_hash_mask;}    Following list functions expect dq_list_lock to be held ", "dquots = i_dquot(inode);for (cnt = 0; cnt < MAXQUOTAS; cnt++) ": "dquot_drop(struct inode  inode){int cnt;struct dquot   dquots = i_dquot(inode);struct dquot  put[MAXQUOTAS];spin_lock(&dq_data_lock);for (cnt = 0; cnt < MAXQUOTAS; cnt++) {put[cnt] = dquots[cnt];dquots[cnt] = NULL;}spin_unlock(&dq_data_lock);dqput_all(put);}void dquot_drop(struct inode  inode){struct dquot   const  dquots;int cnt;if (IS_NOQUOTA(inode))return;    Test before calling to rule out calls from proc and such   where we are not allowed to block. Note that this is   actually reliable test even without the lock - the caller   must assure that nobody can come after the DQUOT_DROP and   add quota pointers back anyway. ", "for (cnt--; cnt >= 0; cnt--) ": "__dquot_alloc_space(struct inode  inode, qsize_t number, int flags){int cnt, ret = 0, index;struct dquot_warn warn[MAXQUOTAS];int reserve = flags & DQUOT_SPACE_RESERVE;struct dquot   dquots;if (!dquot_active(inode)) {if (reserve) {spin_lock(&inode->i_lock); inode_reserved_space(inode) += number;spin_unlock(&inode->i_lock);} else {inode_add_bytes(inode, number);}goto out;}for (cnt = 0; cnt < MAXQUOTAS; cnt++)warn[cnt].w_type = QUOTA_NL_NOWARN;dquots = i_dquot(inode);index = srcu_read_lock(&dquot_srcu);spin_lock(&inode->i_lock);for (cnt = 0; cnt < MAXQUOTAS; cnt++) {if (!dquots[cnt])continue;if (reserve) {ret = dquot_add_space(dquots[cnt], 0, number, flags,      &warn[cnt]);} else {ret = dquot_add_space(dquots[cnt], number, 0, flags,      &warn[cnt]);}if (ret) {  Back out changes we already did ", "spin_lock(&dquots[cnt]->dq_dqb_lock);dquot_decr_inodes(dquots[cnt], 1);spin_unlock(&dquots[cnt]->dq_dqb_lock);}goto warn_put_all;}}warn_put_all:spin_unlock(&inode->i_lock);if (ret == 0)mark_all_dquot_dirty(dquots);srcu_read_unlock(&dquot_srcu, index);flush_warnings(warn);return ret;}EXPORT_SYMBOL(dquot_alloc_inode": "dquot_alloc_inode(struct inode  inode){int cnt, ret = 0, index;struct dquot_warn warn[MAXQUOTAS];struct dquot   const  dquots;if (!dquot_active(inode))return 0;for (cnt = 0; cnt < MAXQUOTAS; cnt++)warn[cnt].w_type = QUOTA_NL_NOWARN;dquots = i_dquot(inode);index = srcu_read_lock(&dquot_srcu);spin_lock(&inode->i_lock);for (cnt = 0; cnt < MAXQUOTAS; cnt++) {if (!dquots[cnt])continue;ret = dquot_add_inodes(dquots[cnt], 1, &warn[cnt]);if (ret) {for (cnt--; cnt >= 0; cnt--) {if (!dquots[cnt])continue;  Back out changes we already did ", "for (cnt = 0; cnt < MAXQUOTAS; cnt++) ": "__dquot_transfer(struct inode  inode, struct dquot   transfer_to){qsize_t cur_space;qsize_t rsv_space = 0;qsize_t inode_usage = 1;struct dquot  transfer_from[MAXQUOTAS] = {};int cnt, ret = 0;char is_valid[MAXQUOTAS] = {};struct dquot_warn warn_to[MAXQUOTAS];struct dquot_warn warn_from_inodes[MAXQUOTAS];struct dquot_warn warn_from_space[MAXQUOTAS];if (IS_NOQUOTA(inode))return 0;if (inode->i_sb->dq_op->get_inode_usage) {ret = inode->i_sb->dq_op->get_inode_usage(inode, &inode_usage);if (ret)return ret;}  Initialize the arrays ", "if (WARN_ON_ONCE(down_read_trylock(&sb->s_umount)))up_read(&sb->s_umount);/* Cannot turn off usage accounting without turning off limits, or * suspend quotas and simultaneously turn quotas off. ": "dquot_disable(struct super_block  sb, int type, unsigned int flags){int cnt;struct quota_info  dqopt = sb_dqopt(sb);  s_umount should be held in exclusive mode ", "BUG_ON(flags & DQUOT_SUSPENDED);/* s_umount should be held in exclusive mode ": "dquot_load_quota_sb(struct super_block  sb, int type, int format_id,unsigned int flags){struct quota_format_type  fmt = find_quota_format(format_id);struct quota_info  dqopt = sb_dqopt(sb);int error;  Just unsuspend quotas? ", "if (WARN_ON_ONCE(down_read_trylock(&sb->s_umount)))up_read(&sb->s_umount);for (cnt = 0; cnt < MAXQUOTAS; cnt++) ": "dquot_resume(struct super_block  sb, int type){struct quota_info  dqopt = sb_dqopt(sb);int ret = 0, cnt;unsigned int flags;  s_umount should be held in exclusive mode ", "if (path->dentry->d_sb != sb)error = -EXDEV;elseerror = dquot_load_quota_inode(d_inode(path->dentry), type,     format_id, DQUOT_USAGE_ENABLED |     DQUOT_LIMITS_ENABLED);return error;}EXPORT_SYMBOL(dquot_quota_on": "dquot_quota_on(struct super_block  sb, int type, int format_id,   const struct path  path){int error = security_quota_on(path->dentry);if (error)return error;  Quota file not on the same filesystem? ", "spin_unlock(&dq_data_lock);}return 0;}EXPORT_SYMBOL(dquot_get_state": "dquot_get_state(struct super_block  sb, struct qc_state  state){struct mem_dqinfo  mi;struct qc_type_state  tstate;struct quota_info  dqopt = sb_dqopt(sb);int type;memset(state, 0, sizeof( state));for (type = 0; type < MAXQUOTAS; type++) {if (!sb_has_quota_active(sb, type))continue;tstate = state->s_state + type;mi = sb_dqopt(sb)->info + type;tstate->flags = QCI_ACCT_ENABLED;spin_lock(&dq_data_lock);if (mi->dqi_flags & DQF_SYS_FILE)tstate->flags |= QCI_SYSFILE;if (mi->dqi_flags & DQF_ROOT_SQUASH)tstate->flags |= QCI_ROOT_SQUASH;if (sb_has_quota_limits_enabled(sb, type))tstate->flags |= QCI_LIMITS_ENFORCED;tstate->spc_timelimit = mi->dqi_bgrace;tstate->ino_timelimit = mi->dqi_igrace;if (dqopt->files[type]) {tstate->ino = dqopt->files[type]->i_ino;tstate->blocks = dqopt->files[type]->i_blocks;}tstate->nextents = 1;  We don't know... ", "return sb->dq_op->write_info(sb, type);}EXPORT_SYMBOL(dquot_set_dqinfo": "dquot_set_dqinfo(struct super_block  sb, int type, struct qc_info  ii){struct mem_dqinfo  mi;if ((ii->i_fieldmask & QC_WARNS_MASK) ||    (ii->i_fieldmask & QC_RT_SPC_TIMER))return -EINVAL;if (!sb_has_quota_active(sb, type))return -ESRCH;mi = sb_dqopt(sb)->info + type;if (ii->i_fieldmask & QC_FLAGS) {if ((ii->i_flags & QCI_ROOT_SQUASH &&     mi->dqi_format->qf_fmt_id != QFMT_VFS_OLD))return -EINVAL;}spin_lock(&dq_data_lock);if (ii->i_fieldmask & QC_SPC_TIMER)mi->dqi_bgrace = ii->i_spc_timelimit;if (ii->i_fieldmask & QC_INO_TIMER)mi->dqi_igrace = ii->i_ino_timelimit;if (ii->i_fieldmask & QC_FLAGS) {if (ii->i_flags & QCI_ROOT_SQUASH)mi->dqi_flags |= DQF_ROOT_SQUASH;elsemi->dqi_flags &= ~DQF_ROOT_SQUASH;}spin_unlock(&dq_data_lock);mark_info_dirty(sb, type);  Force write to disk ", "void quota_send_warning(struct kqid qid, dev_t dev,const char warntype)": "quota_send_warning - Send warning to userspace about exceeded quota   @qid: The kernel internal quota identifier.   @dev: The device on which the fs is mounted (sb->s_dev)   @warntype: The type of the warning: QUOTA_NL_...     This can be used by filesystems (including those which don't use   dquot) to send a message to userspace relating to quota limits.   ", "if (ikm || ikmlen)return -EINVAL;/* Check according to SP800-108 section 7.2 ": "crypto_kdf108_setkey(struct crypto_shash  kmd, const u8  key, size_t keylen, const u8  ikm, size_t ikmlen){unsigned int ds = crypto_shash_digestsize(kmd);  SP800-108 does not support IKM ", "if (vli_cmp(one, private_key, ndigits) != -1)return -EINVAL;vli_sub(res, curve->n, one, ndigits);vli_sub(res, res, one, ndigits);if (vli_cmp(res, private_key, ndigits) != 1)return -EINVAL;return 0;}int ecc_is_key_valid(unsigned int curve_id, unsigned int ndigits,     const u64 *private_key, unsigned int private_key_len)": "ecc_is_key_valid(const struct ecc_curve  curve,      const u64  private_key, unsigned int ndigits){u64 one[ECC_MAX_DIGITS] = { 1, };u64 res[ECC_MAX_DIGITS];if (!private_key)return -EINVAL;if (curve->g.ndigits != ndigits)return -EINVAL;  Make sure the private key is in the range [2, n-3]. ", "if (nbits < 160 || ndigits > ARRAY_SIZE(priv))return -EINVAL;/* * FIPS 186-4 recommends that the private key should be obtained from a * RBG with a security strength equal to or greater than the security * strength associated with N. * * The maximum security strength identified by NIST SP800-57pt1r4 for * ECC is 256 (N >= 512). * * This condition is met by the default RNG because it selects a favored * DRBG with a security strength of 256. ": "ecc_gen_privkey(unsigned int curve_id, unsigned int ndigits, u64  privkey){const struct ecc_curve  curve = ecc_get_curve(curve_id);u64 priv[ECC_MAX_DIGITS];unsigned int nbytes = ndigits << ECC_DIGITS_TO_BYTES_SHIFT;unsigned int nbits = vli_num_bits(curve->n, ndigits);int err;  Check that N is included in Table 1 of FIPS 186-4, section 6.1.1 ", "if (ecc_is_pubkey_valid_full(curve, pk)) ": "ecc_make_pub_key(unsigned int curve_id, unsigned int ndigits,     const u64  private_key, u64  public_key){int ret = 0;struct ecc_point  pk;u64 priv[ECC_MAX_DIGITS];const struct ecc_curve  curve = ecc_get_curve(curve_id);if (!private_key || !curve || ndigits > ARRAY_SIZE(priv)) {ret = -EINVAL;goto out;}ecc_swap_digits(private_key, priv, ndigits);pk = ecc_alloc_point(ndigits);if (!pk) {ret = -ENOMEM;goto out;}ecc_point_mult(pk, &curve->g, priv, NULL, curve, ndigits);  SP800-56A rev 3 5.6.2.1.3 key check ", "if (ecc_point_is_zero(pk))return -EINVAL;/* Check 2: Verify key is in the range [1, p-1]. ": "ecc_is_pubkey_valid_partial(const struct ecc_curve  curve,struct ecc_point  pk){u64 yy[ECC_MAX_DIGITS], xxx[ECC_MAX_DIGITS], w[ECC_MAX_DIGITS];if (WARN_ON(pk->ndigits != curve->g.ndigits))return -EINVAL;  Check 1: Verify key is not the zero point. ", "int ecc_is_pubkey_valid_partial(const struct ecc_curve *curve,struct ecc_point *pk)": "ecc_is_pubkey_valid_full(curve, pk)) {ret = -EAGAIN;goto err_free_point;}ecc_swap_digits(pk->x, public_key, ndigits);ecc_swap_digits(pk->y, &public_key[ndigits], ndigits);err_free_point:ecc_free_point(pk);out:return ret;}EXPORT_SYMBOL(ecc_make_pub_key);  SP800-56A section 5.6.2.3.4 partial verification: ephemeral keys only ", "struct dma_pool *dma_pool_create(const char *name, struct device *dev, size_t size, size_t align, size_t boundary)": "dma_pool_alloc()   may be used to allocate memory.  Such memory will all have \"consistent\"   DMA mappings, accessible by the device and its driver without using   cache flushing primitives.  The actual size of blocks allocated may be   larger than requested because of alignment.     If @boundary is nonzero, objects returned from dma_pool_alloc() won't   cross that size boundary.  This is useful for devices which have   addressing restrictions on individual DMA transfers, such as not crossing   boundaries of 4KBytes.     Return: a dma allocation pool with the requested characteristics, or   %NULL if one can't be created. ", "mutex_lock(&pools_reg_lock);mutex_lock(&pools_lock);empty = list_empty(&dev->dma_pools);list_add(&retval->pools, &dev->dma_pools);mutex_unlock(&pools_lock);if (empty) ": "dma_pool_destroy() or within dma_pool_create()   when the first invocation of dma_pool_create() failed on   device_create_file() and the second assumes that it has been done (I   know it is a short window). ", "void dma_pool_free(struct dma_pool *pool, void *vaddr, dma_addr_t dma)": "dma_pool_free - put block back into dma pool   @pool: the dma pool holding the block   @vaddr: virtual address of block   @dma: dma address of block     Caller promises neither device nor driver will again touch this block   unless it is first re-allocated. ", "struct dma_pool *dmam_pool_create(const char *name, struct device *dev,  size_t size, size_t align, size_t allocation)": "dmam_pool_create - Managed dma_pool_create()   @name: name of pool, for diagnostics   @dev: device that will be doing the DMA   @size: size of the blocks in this pool.   @align: alignment requirement for blocks; must be a power of two   @allocation: returned blocks won't cross this boundary (or zero)     Managed dma_pool_create().  DMA pool created with this function is   automatically destroyed on driver detach.     Return: a managed dma allocation pool with the requested   characteristics, or %NULL if one can't be created. ", "void dmam_pool_destroy(struct dma_pool *pool)": "dmam_pool_destroy - Managed dma_pool_destroy()   @pool: dma pool that will be destroyed     Managed dma_pool_destroy(). ", "};EXPORT_SYMBOL(node_states": "node_states[NR_NODE_STATES] __read_mostly = {[N_POSSIBLE] = NODE_MASK_ALL,[N_ONLINE] = { { [0] = 1UL } },#ifndef CONFIG_NUMA[N_NORMAL_MEMORY] = { { [0] = 1UL } },#ifdef CONFIG_HIGHMEM[N_HIGH_MEMORY] = { { [0] = 1UL } },#endif[N_MEMORY] = { { [0] = 1UL } },[N_CPU] = { { [0] = 1UL } },#endif  NUMA ", "int movable_zone;EXPORT_SYMBOL(movable_zone": "movable_zone is the \"real\" zone pages in ZONE_MOVABLE are taken from ", "if (!page)page = get_page_from_freelist(gfp_mask, order,alloc_flags, ac);return page;}static inline struct page *__alloc_pages_may_oom(gfp_t gfp_mask, unsigned int order,const struct alloc_context *ac, unsigned long *did_some_progress)": "__alloc_pages_cpuset_fallback(gfp_t gfp_mask, unsigned int order,      unsigned int alloc_flags,      const struct alloc_context  ac){struct page  page;page = get_page_from_freelist(gfp_mask, order,alloc_flags|ALLOC_CPUSET, ac);    fallback to ignore cpuset restriction if our nodes   are depleted ", "static int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES] = ": "__free_pages_ok(struct page  page, unsigned int order,    fpi_t fpi_flags);    results with 256, 32 in the lowmem_reserve sysctl:  1G machine -> (16M dma, 800M-16M normal, 1G-800M high)  1G machine -> (16M dma, 784M normal, 224M high)  NORMAL allocation will leave 784M256 of ram reserved in the ZONE_DMA  HIGHMEM allocation will leave 224M32 of ram reserved in ZONE_NORMAL  HIGHMEM allocation will leave (224M+784M)256 of ram reserved in ZONE_DMA     TBD: should special case ZONE_DMA32 machines here - in those we normally   don't need any ZONE_NORMAL reservation ", "typedef int __bitwise fpi_t;/* No special request ": "free_pages(). ", "size = nc->size;#endif/* Even if we own the page, we do not use atomic_set(). * This would break get_page_unless_zero() users. ": "page_frag_alloc_align(struct page_frag_cache  nc,      unsigned int fragsz, gfp_t gfp_mask,      unsigned int align_mask){unsigned int size = PAGE_SIZE;struct page  page;int offset;if (unlikely(!nc->va)) {refill:page = __page_frag_cache_refill(nc, gfp_mask);if (!page)return NULL;#if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)  if size can vary use size else just use PAGE_SIZE ", "void *alloc_pages_exact(size_t size, gfp_t gfp_mask)": "free_pages_exact().     Return: pointer to the allocated area or %NULL in case of error. ", "int alloc_contig_range(unsigned long start, unsigned long end,       unsigned migratetype, gfp_t gfp_mask)": "free_contig_range(). ", "void *high_memory;EXPORT_SYMBOL(high_memory": "high_memory defines the upper bound on direct map memory, then end   of ZONE_NORMAL.  Under CONFIG_DISCONTIG this means that max_low_pfn and   highstart_pfn must be the same; there must be no gap between ZONE_NORMAL   and ZONE_HIGHMEM. ", "int vm_insert_pages(struct vm_area_struct *vma, unsigned long addr,struct page **pages, unsigned long *num)": "vm_insert_pages - insert multiple pages into user vma, batching the pmd lock.   @vma: user vma to map to   @addr: target start user address of these pages   @pages: source kernel pages   @num: in: number of pages to map. out: number of pages that were  not    mapped. (0 means all pages were successfully mapped).     Preferred over vm_insert_page() when inserting multiple pages.     In case of error, we may have mapped a subset of the provided   pages. It is the caller's responsibility to account for this case.     The same restrictions apply as in vm_insert_page(). ", "static int __vm_map_pages(struct vm_area_struct *vma, struct page **pages,unsigned long num, unsigned long offset)": "vm_map_pages - maps range of kernel pages into user vma   @vma: user vma to map to   @pages: pointer to array of source kernel pages   @num: number of pages in page array   @offset: user's requested vm_pgoff     This allows drivers to map range of kernel pages into a user vma.     Return: 0 on success and error code otherwise. ", "int vm_map_pages_zero(struct vm_area_struct *vma, struct page **pages,unsigned long num)": "vm_map_pages_zero - map range of kernel pages starts with zero offset   @vma: user vma to map to   @pages: pointer to array of source kernel pages   @num: number of pages in page array     Similar to vm_map_pages(), except that it explicitly sets the offset   to 0. This function is intended for the drivers that did not consider   vm_pgoff.     Context: Process context. Called by mmap handlers.   Return: 0 on success and error code otherwise. ", "vm_fault_t vmf_insert_pfn_prot(struct vm_area_struct *vma, unsigned long addr,unsigned long pfn, pgprot_t pgprot)": "vmf_insert_pfn_prot - insert single pfn into user vma with specified pgprot   @vma: user vma to map to   @addr: target user address of this page   @pfn: source kernel pfn   @pgprot: pgprot flags for the inserted page     This is exactly like vmf_insert_pfn(), except that it allows drivers   to override pgprot on a per-page basis.     This only makes sense for IO mappings, and it makes no sense for   COW mappings.  In general, using multiple vmas is preferable;   vmf_insert_pfn_prot should only be used if using multiple VMAs is   impractical.     pgprot typically only differs from @vma->vm_page_prot when drivers set   caching- and encryption bits different than those of @vma->vm_page_prot,   because the caching- or encryption mode may not be known at mmap() time.     This is ok as long as @vma->vm_page_prot is not used by the core vm   to set caching and encryption bits for those vmas (except for COW pages).   This is ensured by core vm only modifying these page table entries using   functions that don't touch caching- or encryption bits, using pte_modify()   if needed. (See for example mprotect()).     Also when new page-table entries are created, this is only done using the   fault() callback, and never using the value of vma->vm_page_prot,   except for page-table entries that point to anonymous pages as the result   of COW.     Context: Process context.  May allocate using %GFP_KERNEL.   Return: vm_fault_t value. ", "struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,    pte_t pte)": "remap_pfn_range()\": the vma will have the VM_PFNMAP bit   set, and the vm_pgoff will point to the first PFN mapped: thus every special   mapping will always honor the rule    pfn_of_page == vma->vm_pgoff + ((addr - vma->vm_start) >> PAGE_SHIFT)     And for normal mappings this is false.     This restricts such mappings to be a linear translation from virtual address   to pfn. To get around this restriction, we allow arbitrary mappings so long   as the vma is not a COW mapping; in that case, we know that all ptes are   special (because none can have been COWed).       In order to support COW of arbitrary special mappings, we have VM_MIXEDMAP.     VM_MIXEDMAP mappings can likewise contain memory with or without \"struct   page\" backing, however the difference is that _all_ pages with a struct   page (that is, those where pfn_valid is true) are refcounted and considered   normal pages by the VM. The disadvantage is that pages are refcounted   (which can be slower and simply not an option for some PFNMAP users). The   advantage is that we don't have to follow the strict linearity rule of   PFNMAP mappings in order to support COWable mappings.   ", "int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)": "vm_iomap_memory - remap memory to userspace   @vma: user vma to map to   @start: start of the physical memory to be mapped   @len: size of area     This is a simplified io_remap_pfn_range() for common driver use. The   driver just needs to give us the physical memory range to be mapped,   we'll figure out the rest from the vma information.     NOTE! Some drivers might want to tweak vma->vm_page_prot first to get   whatever write-combining details or similar.     Return: %0 on success, negative error code otherwise. ", "void unmap_mapping_folio(struct folio *folio)": "unmap_mapping_range_vma(struct vm_area_struct  vma,unsigned long start_addr, unsigned long end_addr,struct zap_details  details){zap_page_range_single(vma, start_addr, end_addr - start_addr, details);}static inline void unmap_mapping_range_tree(struct rb_root_cached  root,    pgoff_t first_index,    pgoff_t last_index,    struct zap_details  details){struct vm_area_struct  vma;pgoff_t vba, vea, zba, zea;vma_interval_tree_foreach(vma, root, first_index, last_index) {vba = vma->vm_pgoff;vea = vba + vma_pages(vma) - 1;zba = max(first_index, vba);zea = min(last_index, vea);unmap_mapping_range_vma(vma,((zba - vba) << PAGE_SHIFT) + vma->vm_start,((zea - vba + 1) << PAGE_SHIFT) + vma->vm_start,details);}}     unmap_mapping_folio() - Unmap single folio from processes.   @folio: The locked folio to be unmapped.     Unmap this folio from any userspace process which still has it mmaped.   Typically, for efficiency, the range of nearby pages has already been   unmapped by unmap_mapping_pages() or unmap_mapping_range().  But once   truncation or invalidation holds the lock on a folio, it may find that   the page has been remapped again: and then uses unmap_mapping_folio()   to unmap it finally. ", "int follow_pte(struct mm_struct *mm, unsigned long address,       pte_t **ptepp, spinlock_t **ptlp)": "follow_pfn``,   it is not a good general-purpose API.     Return: zero on success, -ve otherwise. ", "VIRTUAL_BUG_ON(!is_vmalloc_or_module_addr(vmalloc_addr));if (pgd_none(*pgd))return NULL;if (WARN_ON_ONCE(pgd_leaf(*pgd)))return NULL; /* XXX: no allowance for huge pgd ": "vmalloc_to_page(const void  vmalloc_addr){unsigned long addr = (unsigned long) vmalloc_addr;struct page  page = NULL;pgd_t  pgd = pgd_offset_k(addr);p4d_t  p4d;pud_t  pud;pmd_t  pmd;pte_t  ptep, pte;    XXX we might need to change this if we add VIRTUAL_BUG_ON for   architectures that do not vmalloc module space ", "static struct xarray *addr_to_vb_xa(unsigned long addr)": "vm_unmap_ram(6), 6 belongs to CPU0 zone, thus     it access: CPU0INDEX0 -> vmap_blocks -> xa_lock;     - CPU_2 invokes vm_unmap_ram(11), 11 belongs to CPU1 zone, thus     it access: CPU1INDEX1 -> vmap_blocks -> xa_lock;     - CPU_0 invokes vm_unmap_ram(20), 20 belongs to CPU2 zone, thus     it access: CPU2INDEX2 -> vmap_blocks -> xa_lock.     This technique almost always avoids lock contention on insertremove,   however xarray spinlocks protect against any contention that remains. ", "#define VMAP_BLOCK0x2 /* mark out the vmap_block sub-type": "vm_map_ram area", "#include <linux/vmalloc.h>#include <linux/mm.h>#include <linux/module.h>#include <linux/highmem.h>#include <linux/sched/signal.h>#include <linux/slab.h>#include <linux/spinlock.h>#include <linux/interrupt.h>#include <linux/proc_fs.h>#include <linux/seq_file.h>#include <linux/set_memory.h>#include <linux/debugobjects.h>#include <linux/kallsyms.h>#include <linux/list.h>#include <linux/notifier.h>#include <linux/rbtree.h>#include <linux/xarray.h>#include <linux/io.h>#include <linux/rcupdate.h>#include <linux/pfn.h>#include <linux/kmemleak.h>#include <linux/atomic.h>#include <linux/compiler.h>#include <linux/memcontrol.h>#include <linux/llist.h>#include <linux/uio.h>#include <linux/bitops.h>#include <linux/rbtree_augmented.h>#include <linux/overflow.h>#include <linux/pgtable.h>#include <linux/hugetlb.h>#include <linux/sched/mm.h>#include <asm/tlbflush.h>#include <asm/shmparam.h>#define CREATE_TRACE_POINTS#include <trace/events/vmalloc.h>#include \"internal.h\"#include \"pgalloc-track.h\"#ifdef CONFIG_HAVE_ARCH_HUGE_VMAPstatic unsigned int __ro_after_init ioremap_max_page_shift = BITS_PER_LONG - 1;static int __init set_nohugeiomap(char *str)": "vmallocvfreeioremap, Tigran Aivazian <tigran@veritas.com>, May 2000    Major rework to support vmapvunmap, Christoph Hellwig, SGI, August 2002    Numa awareness, Christoph Lameter, SGI, June 2005    Improving global KVA allocator, Uladzislau Rezki, Sony, May 2019 ", "if (!(flags & VM_ALLOC))area->addr = kasan_unpoison_vmalloc(area->addr, requested_size,    KASAN_VMALLOC_PROT_NORMAL);return area;}struct vm_struct *__get_vm_area_caller(unsigned long size, unsigned long flags,       unsigned long start, unsigned long end,       const void *caller)": "vmalloc_node_range().   With hardware tag-based KASAN, marking is skipped for   non-VM_ALLOC mappings, see __kasan_unpoison_vmalloc(). ", "void *vzalloc(unsigned long size)": "vzalloc - allocate virtually contiguous memory with zero fill    @size:allocation size    Allocate enough pages to cover @size from the page level  allocator and map them into contiguous kernel virtual space.  The memory allocated is set to zero.    For tight control over page level allocator and protection flags  use __vmalloc() instead. ", "void *vmalloc_user(unsigned long size)": "vmalloc_user - allocate zeroed virtually contiguous memory for userspace   @size: allocation size     The resulting memory area is zeroed so it can be mapped to userspace   without leaking data.     Return: pointer to the allocated memory or %NULL on error ", "void *vzalloc_node(unsigned long size, int node)": "vzalloc_node - allocate memory on a specific node with zero fill   @size:allocation size   @node:numa node     Allocate enough pages to cover @size from the page level   allocator and map them into contiguous kernel virtual space.   The memory allocated is set to zero.     For tight control over page level allocator and protection flags   use __vmalloc() instead. ", "void *vmalloc_32(unsigned long size)": "vmalloc_32  -  allocate virtually contiguous memory (32bit addressable)  @size:allocation size    Allocate enough 32bit PA addressable pages to cover @size from the  page level allocator and map them into contiguous kernel virtual space. ", "void *vmalloc_32_user(unsigned long size)": "remap_vmalloc_range() are permissible. ", "int remap_vmalloc_range_partial(struct vm_area_struct *vma, unsigned long uaddr,void *kaddr, unsigned long pgoff,unsigned long size)": "remap_vmalloc_range_partial - map vmalloc pages to userspace   @vma:vma to cover   @uaddr:target user address to start at   @kaddr:virtual address of vmalloc kernel memory   @pgoff:offset from @kaddr to start at   @size:size of map area     Returns:0 for success, -Exxx on failure     This function checks that @kaddr is a valid vmalloc'ed area,   and that it is big enough to cover the range starting at   @uaddr in @vma. Will return failure if that criteria isn't   met.     Similar to remap_pfn_range() (see mmmemory.c) ", "if (current->brk_randomized)min_brk = mm->start_brk;elsemin_brk = mm->end_data;#elsemin_brk = mm->start_brk;#endifif (brk < min_brk)goto out;/* * Check against rlimit here. If this check is done later after the test * of oldbrk with newbrk then it can escape the test and let the data * segment grow beyond its set limit the in case where the limit is * not page aligned -Ram Gupta ": "get_unmapped_area(NULL, addr, len, 0, MAP_FIXED);if (IS_ERR_VALUE(mapped_addr))return mapped_addr;return mlock_future_ok(current->mm, current->mm->def_flags, len)? 0 : -EAGAIN;}static int do_brk_flags(struct vma_iterator  vmi, struct vm_area_struct  brkvma,unsigned long addr, unsigned long request, unsigned long flags);SYSCALL_DEFINE1(brk, unsigned long, brk){unsigned long newbrk, oldbrk, origbrk;struct mm_struct  mm = current->mm;struct vm_area_struct  brkvma,  next = NULL;unsigned long min_brk;bool populate = false;LIST_HEAD(uf);struct vma_iterator vmi;if (mmap_write_lock_killable(mm))return -EINTR;origbrk = mm->brk;#ifdef CONFIG_COMPAT_BRK    CONFIG_COMPAT_BRK can still be overridden by setting   randomize_va_space to 2, which will still cause mm->start_brk   to be arbitrarily shifted ", "    end == curr->vm_end)/* cases 6 - 8, adjacent VMA ": "find_vma_intersection(mm, prev ? prev->vm_end : 0, end);if (!curr ||  cases 1 - 4 ", "if ((flags & (~VM_EXEC)) != 0)return -EINVAL;ret = check_brk_limits(addr, len);if (ret)goto limits_failed;ret = do_vmi_munmap(&vmi, mm, addr, len, &uf, 0);if (ret)goto munmap_failed;vma = vma_prev(&vmi);ret = do_brk_flags(&vmi, vma, addr, len, flags);populate = ((mm->def_flags & VM_LOCKED) != 0);mmap_write_unlock(mm);userfaultfd_unmap_complete(mm, &uf);if (populate && !ret)mm_populate(addr, len);return ret;munmap_failed:limits_failed:mmap_write_unlock(mm);return ret;}EXPORT_SYMBOL(vm_brk_flags": "vm_brk_flags(unsigned long addr, unsigned long request, unsigned long flags){struct mm_struct  mm = current->mm;struct vm_area_struct  vma = NULL;unsigned long len;int ret;bool populate;LIST_HEAD(uf);VMA_ITERATOR(vmi, mm, addr);len = PAGE_ALIGN(request);if (len < request)return -ENOMEM;if (!len)return 0;if (mmap_write_lock_killable(mm))return -EINTR;  Until we need other flags, refuse anything except VM_EXEC. ", "if ((flags & (~VM_EXEC)) != 0)return -EINVAL;ret = check_brk_limits(addr, len);if (ret)goto limits_failed;ret = do_vmi_munmap(&vmi, mm, addr, len, &uf, 0);if (ret)goto munmap_failed;vma = vma_prev(&vmi);ret = do_brk_flags(&vmi, vma, addr, len, flags);populate = ((mm->def_flags & VM_LOCKED) != 0);mmap_write_unlock(mm);userfaultfd_unmap_complete(mm, &uf);if (populate && !ret)mm_populate(addr, len);return ret;munmap_failed:limits_failed:mmap_write_unlock(mm);return ret;}EXPORT_SYMBOL(vm_brk": "vm_brk_flags(unsigned long addr, unsigned long request, unsigned long flags){struct mm_struct  mm = current->mm;struct vm_area_struct  vma = NULL;unsigned long len;int ret;bool populate;LIST_HEAD(uf);VMA_ITERATOR(vmi, mm, addr);len = PAGE_ALIGN(request);if (len < request)return -ENOMEM;if (!len)return 0;if (mmap_write_lock_killable(mm))return -EINTR;  Until we need other flags, refuse anything except VM_EXEC. ", "void synchronize_shrinkers(void)": "synchronize_shrinkers - Wait for all running shrinkers to complete.     This is equivalent to calling unregister_shrink() and register_shrinker(),   but atomically and with less overhead. This is useful to guarantee that all   shrinker invocations have seen an update, before freeing memory, similar to   rcu. ", "int migrate_vma_setup(struct migrate_vma *args)": "migrate_vma_finalize() to update the CPU page table to point to new pages   for successfully migrated pages or otherwise restore the CPU page table to   point to the original source pages. ", "VM_BUG_ON(!migrate);addr = migrate->start + i*PAGE_SIZE;if (!notified) ": "migrate_device_pages(unsigned long  src_pfns,unsigned long  dst_pfns, unsigned long npages,struct migrate_vma  migrate){struct mmu_notifier_range range;unsigned long i;bool notified = false;for (i = 0; i < npages; i++) {struct page  newpage = migrate_pfn_to_page(dst_pfns[i]);struct page  page = migrate_pfn_to_page(src_pfns[i]);struct address_space  mapping;int r;if (!newpage) {src_pfns[i] &= ~MIGRATE_PFN_MIGRATE;continue;}if (!page) {unsigned long addr;if (!(src_pfns[i] & MIGRATE_PFN_MIGRATE))continue;    The only time there is no vma is when called from   migrate_device_coherent_page(). However this isn't   called if the page could not be unmapped. ", "void migrate_device_finalize(unsigned long *src_pfns,unsigned long *dst_pfns, unsigned long npages)": "migrate_device_finalize() - complete page migration   @src_pfns: src_pfns returned from migrate_device_range()   @dst_pfns: array of pfns allocated by the driver to migrate memory to   @npages: number of pages in the range     Completes migration of the page by removing special migration entries.   Drivers must ensure copying of page data is complete and visible to the CPU   before calling this. ", "void migrate_device_pages(unsigned long *src_pfns, unsigned long *dst_pfns,unsigned long npages)": "migrate_device_range()   @dst_pfns: array of pfns allocated by the driver to migrate memory to   @npages: number of pages in the range     Equivalent to migrate_vma_pages(). This is called to migrate struct page   meta-data from source struct page to destination. ", "static long ratelimit_pages = 32;/* The following parameters are exported via /proc/sys/vm ": "balance_dirty_pages_ratelimited   will look to see if it needs to force writeback or throttling. ", "void tag_pages_for_writeback(struct address_space *mapping,     pgoff_t start, pgoff_t end)": "write_cache_pages   @mapping: address space structure to write   @start: starting page index   @end: ending page index (inclusive)     This function scans the page range from @start to @end (inclusive) and tags   all pages that have DIRTY tag set with a special TOWRITE tag. The idea is   that write_cache_pages (or whoever calls this function) will then use   TOWRITE tag to identify pages eligible for writeback.  This mechanism is   used to avoid livelocking of writeback by a process steadily creating new   dirty pages in the file (thus it is important for this function to be quick   so that it can tag pages faster than a dirtying process can create them). ", "bool filemap_dirty_folio(struct address_space *mapping, struct folio *folio)": "filemap_dirty_folio - Mark a folio dirty for filesystems which do not use buffer_heads.   @mapping: Address space this folio belongs to.   @folio: Folio to be marked as dirty.     Filesystems which do not use buffer heads should call this function   from their set_page_dirty address space operation.  It ignores the   contents of folio_get_private(), so if the filesystem marks individual   blocks as dirty, the filesystem should handle that itself.     This is also sometimes used by filesystems which use buffer_heads when   a single buffer is being dirtied: we want to set the folio dirty in   that case, but not all the buffers.  This is a \"bottom-up\" dirtying,   whereas block_dirty_folio() is a \"top-down\" dirtying.     The caller must ensure this doesn't race with truncation.  Most will   simply hold the folio lock, but e.g. zap_pte_range() calls with the   folio mapped and the pte lock held, which also locks out truncation. ", "bw = written - min(written, wb->written_stamp);bw *= HZ;if (unlikely(elapsed > period)) ": "folio_account_redirty().   Avoid underflowing @bw calculation. ", "void folio_account_redirty(struct folio *folio)": "folio_redirty_for_writepage() instead   of this fuction.  If your filesystem is doing writeback outside the   context of a writeback_control(), it can call this when redirtying   a folio, to de-account the dirty counters (NR_DIRTIED, WB_DIRTIED,   tsk->nr_dirtied), so that they match the written counters (NR_WRITTEN,   WB_WRITTEN) in long term. The mismatches will lead to systematic errors   in balanced_dirty_ratelimit and the dirty pages position control. ", "WARN_ON_ONCE(warn && !folio_test_uptodate(folio));folio_account_dirtied(folio, mapping);__xa_set_mark(&mapping->i_pages, folio_index(folio),PAGECACHE_TAG_DIRTY);}xa_unlock_irqrestore(&mapping->i_pages, flags);}/** * filemap_dirty_folio - Mark a folio dirty for filesystems which do not use buffer_heads. * @mapping: Address space this folio belongs to. * @folio: Folio to be marked as dirty. * * Filesystems which do not use buffer heads should call this function * from their set_page_dirty address space operation.  It ignores the * contents of folio_get_private(), so if the filesystem marks individual * blocks as dirty, the filesystem should handle that itself. * * This is also sometimes used by filesystems which use buffer_heads when * a single buffer is being dirtied: we want to set the folio dirty in * that case, but not all the buffers.  This is a \"bottom-up\" dirtying, * whereas block_dirty_folio() is a \"top-down\" dirtying. * * The caller must ensure this doesn't race with truncation.  Most will * simply hold the folio lock, but e.g. zap_pte_range() calls with the * folio mapped and the pte lock held, which also locks out truncation. ": "folio_mark_dirty(struct folio  folio, struct address_space  mapping,     int warn){unsigned long flags;xa_lock_irqsave(&mapping->i_pages, flags);if (folio->mapping) {  Race with truncate? ", "if (error == AOP_WRITEPAGE_ACTIVATE) ": "folio_clear_dirty_for_io(folio))goto continue_unlock;trace_wbc_writepage(wbc, inode_to_bdi(mapping->host));error = writepage(folio, wbc, data);nr = folio_nr_pages(folio);if (unlikely(error)) {    Handle errors according to the type of   writeback. There's no need to continue for   background writeback. Just push done_index   past this page so media errors won't choke   writeout for the entire file. For integrity   writeback, we must process the entire dirty   set regardless of errors because the fs may   still have state to clear for each page. In   that case we continue processing and return   the first error. ", "if (mapping->host && !on_wblist)sb_mark_inode_writeback(mapping->host);}if (!folio_test_dirty(folio))xas_clear_mark(&xas, PAGECACHE_TAG_DIRTY);if (!keep_write)xas_clear_mark(&xas, PAGECACHE_TAG_TOWRITE);xas_unlock_irqrestore(&xas, flags);} else ": "__folio_start_writeback(struct folio  folio, bool keep_write){long nr = folio_nr_pages(folio);struct address_space  mapping = folio_mapping(folio);bool ret;int access_ret;folio_memcg_lock(folio);if (mapping && mapping_use_writeback_tags(mapping)) {XA_STATE(xas, &mapping->i_pages, folio_index(folio));struct inode  inode = mapping->host;struct backing_dev_info  bdi = inode_to_bdi(inode);unsigned long flags;xas_lock_irqsave(&xas, flags);xas_load(&xas);ret = folio_test_set_writeback(folio);if (!ret) {bool on_wblist;on_wblist = mapping_tagged(mapping,   PAGECACHE_TAG_WRITEBACK);xas_set_mark(&xas, PAGECACHE_TAG_WRITEBACK);if (bdi->capabilities & BDI_CAP_WRITEBACK_ACCT) {struct bdi_writeback  wb = inode_to_wb(inode);wb_stat_mod(wb, WB_WRITEBACK, nr);if (!on_wblist)wb_inode_writeback_start(wb);}    We can come through here when swapping   anonymous folios, so we don't necessarily   have an inode to track for sync. ", "void unpin_user_page(struct page *page)": "unpin_user_pages () routines. This is so   that such pages can be separately tracked and uniquely handled. In   particular, interactions with RDMA and filesystems need special handling. ", "void unpin_user_pages_dirty_lock(struct page **pages, unsigned long npages, bool make_dirty)": "unpin_user_pages_dirty_lock() - release and optionally dirty gup-pinned pages   @pages:  array of pages to be maybe marked dirty, and definitely released.   @npages: number of pages in the @pages array.   @make_dirty: whether to mark the pages dirty     \"gup-pinned page\" refers to a page that has had one of the get_user_pages()   variants called on that page.     For each page in the @pages array, make that page (or its head page, if a   compound page) dirty, if @make_dirty is true, and if the page was previously   listed as clean. In any case, releases all pages using unpin_user_page(),   possibly via unpin_user_pages(), for the non-dirty case.     Please see the unpin_user_page() documentation for details.     set_page_dirty_lock() is used internally. If instead, set_page_dirty() is   required, then the caller should a) verify that this is really correct,   because _lock() is usually required, and b) hand code it:   set_page_dirty_lock(), unpin_user_page().   ", "void unpin_user_page_range_dirty_lock(struct page *page, unsigned long npages,      bool make_dirty)": "unpin_user_page_range_dirty_lock() - release and optionally dirty   gup-pinned page range     @page:  the starting page of a range maybe marked dirty, and definitely released.   @npages: number of consecutive pages to release.   @make_dirty: whether to mark the pages dirty     \"gup-pinned page range\" refers to a range of pages that has had one of the   pin_user_pages() variants called on that page.     For the page ranges defined by [page .. page+npages], make that range (or   its head pages, if a compound page) dirty, if @make_dirty is true, and if the   page range was previously listed as clean.     set_page_dirty_lock() is used internally. If instead, set_page_dirty() is   required, then the caller should a) verify that this is really correct,   because _lock() is usually required, and b) hand code it:   set_page_dirty_lock(), unpin_user_page().   ", "size_t fault_in_writeable(char __user *uaddr, size_t size)": "fault_in_writeable - fault in userspace address range for writing   @uaddr: start of address range   @size: size of address range     Returns the number of bytes not faulted in (like copy_to_user() and   copy_from_user()). ", "size_t fault_in_subpage_writeable(char __user *uaddr, size_t size)": "fault_in_subpage_writeable - fault in an address range for writing   @uaddr: start of address range   @size: size of address range     Fault in a user address range for writing while checking for permissions at   sub-page granularity (e.g. arm64 MTE). This function should be used when   the caller cannot guarantee forward progress of a copy_to_user() loop.     Returns the number of bytes not faulted in (like copy_to_user() and   copy_from_user()). ", "size_t fault_in_safe_writeable(const char __user *uaddr, size_t size)": "fault_in_safe_writeable - fault in an address range for writing   @uaddr: start of address range   @size: length of address range     Faults in an address range for writing.  This is primarily useful when we   already know that some or all of the pages in the address range aren't in   memory.     Unlike fault_in_writeable(), this function is non-destructive.     Note that we don't pin or otherwise hold the pages referenced that we fault   in.  There's no guarantee that they'll stay in memory for any duration of   time.     Returns the number of bytes not faulted in, like copy_to_user() and   copy_from_user(). ", "size_t fault_in_readable(const char __user *uaddr, size_t size)": "fault_in_readable - fault in userspace address range for reading   @uaddr: start of user address range   @size: size of user address range     Returns the number of bytes not faulted in (like copy_to_user() and   copy_from_user()). ", "long get_user_pages_remote(struct mm_struct *mm,unsigned long start, unsigned long nr_pages,unsigned int gup_flags, struct page **pages,int *locked)": "get_user_pages_remote() - pin user pages in memory   @mm:mm_struct of target mm   @start:starting user address   @nr_pages:number of pages from start to pin   @gup_flags:flags modifying lookup behaviour   @pages:array that receives pointers to the pages pinned.  Should be at least nr_pages long. Or NULL, if caller  only intends to ensure the pages are faulted in.   @locked:pointer to lock flag indicating whether lock is held and  subsequently whether VM_FAULT_RETRY functionality can be  utilised. Lock must initially be held.     Returns either number of pages pinned (which may be less than the   number requested), or an error. Details about the return value:     -- If nr_pages is 0, returns 0.   -- If nr_pages is >0, but no pages were pinned, returns -errno.   -- If nr_pages is >0, and some pages were pinned, returns the number of      pages pinned. Again, this may be less than nr_pages.     The caller is responsible for releasing returned @pages, via put_page().     Must be called with mmap_lock held for read or write.     get_user_pages_remote walks a process's page tables and takes a reference   to each struct page that each user address corresponds to at a given   instant. That is, it takes the page that would be accessed if a user   thread accesses the given user virtual address at that instant.     This does not guarantee that the page exists in the user mappings when   get_user_pages_remote returns, and there may even be a completely different   page there in some cases (eg. if mmapped pagecache has been invalidated   and subsequently re-faulted). However it does guarantee that the page   won't be freed completely. And mostly callers simply care that the page   contains data that was valid  at some point in time . Typically, an IO   or similar operation cannot guarantee anything stronger anyway because   locks can't be held over the syscall boundary.     If gup_flags & FOLL_WRITE == 0, the page must not be written to. If the page   is written to, set_page_dirty (or set_page_dirty_lock, as appropriate) must   be called after the page is finished with, and before put_page is called.     get_user_pages_remote is typically used for fewer-copy IO operations,   to get a handle on the memory by some means other than accesses   via the user virtual addresses. The pages may be submitted for   DMA to devices or accessed via their kernel linear mapping (via the   kmap APIs). Care should be taken to use the correct cache flushing APIs.     See also get_user_pages_fast, for performance critical applications.     get_user_pages_remote should be phased out in favor of   get_user_pages_locked|unlocked or get_user_pages_fast. Nothing   should use get_user_pages_remote because it cannot pass   FAULT_FLAG_ALLOW_RETRY to handle_mm_fault. ", "struct folio *try_grab_folio(struct page *page, int refs, unsigned int flags)": "pin_user_pages () APIs.) Cases:        FOLL_GET: folio's refcount will be incremented by @refs.        FOLL_PIN on large folios: folio's refcount will be incremented by      @refs, and its pincount will be incremented by @refs.        FOLL_PIN on single-page folios: folio's refcount will be incremented by      @refs   GUP_PIN_COUNTING_BIAS.     Return: The folio containing @page (with refcount appropriately   incremented) for success, or NULL upon failure. If neither FOLL_GET   nor FOLL_PIN was set, that's considered failure, and furthermore,   a likely bug in the caller, so a warning is also emitted. ", "long get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,     struct page **pages, unsigned int gup_flags)": "get_user_pages_unlocked() is suitable to replace the form:          mmap_read_lock(mm);        get_user_pages(mm, ..., pages, NULL);        mmap_read_unlock(mm);      with:          get_user_pages_unlocked(mm, ..., pages);     It is functionally equivalent to get_user_pages_fast so   get_user_pages_fast should be used instead if specific gup_flags   (e.g. FOLL_FORCE) are not required. ", "long pin_user_pages_remote(struct mm_struct *mm,   unsigned long start, unsigned long nr_pages,   unsigned int gup_flags, struct page **pages,   int *locked)": "pin_user_pages_remote() - pin pages of a remote process     @mm:mm_struct of target mm   @start:starting user address   @nr_pages:number of pages from start to pin   @gup_flags:flags modifying lookup behaviour   @pages:array that receives pointers to the pages pinned.  Should be at least nr_pages long.   @locked:pointer to lock flag indicating whether lock is held and  subsequently whether VM_FAULT_RETRY functionality can be  utilised. Lock must initially be held.     Nearly the same as get_user_pages_remote(), except that FOLL_PIN is set. See   get_user_pages_remote() for documentation on the function arguments, because   the arguments here are identical.     FOLL_PIN means that the pages must be released via unpin_user_page(). Please   see Documentationcore-apipin_user_pages.rst for details.     Note that if a zero_page is amongst the returned pages, it will not have   pins in it and unpin_user_page () will not remove pins from it. ", "long pin_user_pages_unlocked(unsigned long start, unsigned long nr_pages,     struct page **pages, unsigned int gup_flags)": "pin_user_pages_unlocked() is the FOLL_PIN variant of   get_user_pages_unlocked(). Behavior is the same, except that this one sets   FOLL_PIN and rejects FOLL_GET.     Note that if a zero_page is amongst the returned pages, it will not have   pins in it and unpin_user_page () will not remove pins from it. ", "void try_offline_node(int nid)": "try_offline_node   @nid: the node ID     Offline a node if all memory sections and cpus of the node are removed.     NOTE: The caller must call lock_device_hotplug() to serialize hotplug   and onlineoffline operations before this call. ", "void mempool_exit(mempool_t *pool)": "mempool_init()   @pool:      pointer to the memory pool which was initialized with               mempool_init().     Free all reserved elements in @pool and @pool itself.  This function   only sleeps if the free_fn() function sleeps.     May be called on a zeroed but uninitialized mempool (i.e. allocated with   kzalloc()). ", "void mempool_destroy(mempool_t *pool)": "mempool_create().     Free all reserved elements in @pool and @pool itself.  This function   only sleeps if the free_fn() function sleeps. ", "while (pool->curr_nr < pool->min_nr) ": "mempool_init_node(mempool_t  pool, int min_nr, mempool_alloc_t  alloc_fn,      mempool_free_t  free_fn, void  pool_data,      gfp_t gfp_mask, int node_id){spin_lock_init(&pool->lock);pool->min_nr= min_nr;pool->pool_data = pool_data;pool->alloc= alloc_fn;pool->free= free_fn;init_waitqueue_head(&pool->wait);pool->elements = kmalloc_array_node(min_nr, sizeof(void  ),    gfp_mask, node_id);if (!pool->elements)return -ENOMEM;    First pre-allocate the guaranteed number of buffers. ", "int mempool_resize(mempool_t *pool, int new_min_nr)": "mempool_resize - resize an existing memory pool   @pool:       pointer to the memory pool which was allocated via                mempool_create().   @new_min_nr: the new minimum number of elements guaranteed to be                allocated for this pool.     This function shrinksgrows the pool. In the case of growing,   it cannot be guaranteed that the pool will be grown to the new   size immediately, but new mempool_free() calls will refill it.   This function may sleep.     Note, the caller must guarantee that no mempool_destroy is called   while this function is running. mempool_alloc() & mempool_free()   might be called (eg. from IRQ contexts) while this function executes.     Return: %0 on success, negative error code otherwise. ", "int order = (int)(long)pool->pool_data;void *addr = kmap_atomic((struct page *)element);__poison_element(addr, 1UL << (PAGE_SHIFT + order));kunmap_atomic(addr);}}#else /* CONFIG_DEBUG_SLAB || CONFIG_SLUB_DEBUG_ON ": "mempool_alloc_pages) {  Mempools backed by page allocator ", "int order = (int)(long)pool->pool_data;void *addr = kmap_atomic((struct page *)element);__check_element(pool, addr, 1UL << (PAGE_SHIFT + order));kunmap_atomic(addr);}}static void __poison_element(void *element, size_t size)": "mempool_free_pages) {  Mempools backed by page allocator ", "page->mapping = (void *)PAGE_MAPPING_MOVABLE;}EXPORT_SYMBOL(__ClearPageMovable": "__ClearPageMovable(struct page  page){VM_BUG_ON_PAGE(!PageMovable(page), page);    This page still has the type of a movable page, but it's   actually not movable any more. ", "if (page_ref_sub_return(page, refs) == 1)wake_up_var(&page->_refcount);return true;}EXPORT_SYMBOL(__put_devmap_managed_page_refs": "__put_devmap_managed_page_refs(struct page  page, int refs){if (page->pgmap->type != MEMORY_DEVICE_FS_DAX)return false;    fsdax page refcounts are 1-based, rather than 0-based: if   refcount is 1, then the page is free and the refcount is   stable because nobody holds a reference on the page. ", "#ifdef CONFIG_SPARSEMEM_EXTREMEstruct mem_section **mem_section;#elsestruct mem_section mem_section[NR_SECTION_ROOTS][SECTIONS_PER_ROOT]____cacheline_internodealigned_in_smp;#endifEXPORT_SYMBOL(mem_section": "mem_section- memory sections, mem_map's for valid memory ", "void readahead_expand(struct readahead_control *ractl,      loff_t new_start, size_t new_len)": "readahead_expand - Expand a readahead request   @ractl: The request to be expanded   @new_start: The revised start   @new_len: The revised size of the request     Attempt to expand a readahead request outwards from the current size to the   specified size by inserting locked pages before and after the current window   to increase the size to the new window.  This may involve the insertion of   THPs, in which case the window may get expanded even beyond what was   requested.     The algorithm will stop if it encounters a conflicting page already in the   pagecache and leave a smaller expansion than requested.     The caller must check for this by examining the revised @ractl object for a   different expansion than was requested. ", "if (hmm_range_need_fault(hmm_vma_walk, range->hmm_pfns + ((start - range->start) >> PAGE_SHIFT), (end - start) >> PAGE_SHIFT, 0))return -EFAULT;hmm_pfns_fill(start, end, range, HMM_PFN_ERROR);/* Skip this vma and continue processing the next vma. ": "hmm_range_fault().     If the vma does not allow read access, then assume that it does not   allow write access either. HMM does not support architectures that   allow write without read.     If a fault is requested for an unsupported range then it is a hard   failure. ", "if (folio_ref_count(folio) != expected_count)return -EAGAIN;/* No turning back from here ": "folio_migrate_mapping(struct address_space  mapping,struct folio  newfolio, struct folio  folio, int extra_count){XA_STATE(xas, &mapping->i_pages, folio_index(folio));struct zone  oldzone,  newzone;int dirty;int expected_count = folio_expected_refs(mapping, folio) + extra_count;long nr = folio_nr_pages(folio);if (!mapping) {  Anonymous page without mapping ", "if (folio_test_mappedtodisk(folio))folio_set_mappedtodisk(newfolio);/* Move dirty on pages not done by folio_migrate_mapping() ": "folio_migrate_flags(struct folio  newfolio, struct folio  folio){int cpupid;if (folio_test_error(folio))folio_set_error(newfolio);if (folio_test_referenced(folio))folio_set_referenced(newfolio);if (folio_test_uptodate(folio))folio_mark_uptodate(newfolio);if (folio_test_clear_active(folio)) {VM_BUG_ON_FOLIO(folio_test_unevictable(folio), folio);folio_set_active(newfolio);} else if (folio_test_clear_unevictable(folio))folio_set_unevictable(newfolio);if (folio_test_workingset(folio))folio_set_workingset(newfolio);if (folio_test_checked(folio))folio_set_checked(newfolio);    PG_anon_exclusive (-> PG_mappedtodisk) is always migrated via   migration entries. We can still have PG_anon_exclusive set on an   effectively unmapped and unreferenced first sub-pages of an   anonymous THP: we can simply copy it here via PG_mappedtodisk. ", "rc = folio_migrate_mapping(mapping, dst, src, extra_count);if (rc != MIGRATEPAGE_SUCCESS)return rc;if (mode != MIGRATE_SYNC_NO_COPY)folio_migrate_copy(dst, src);elsefolio_migrate_flags(dst, src);return MIGRATEPAGE_SUCCESS;}/** * migrate_folio() - Simple folio migration. * @mapping: The address_space containing the folio. * @dst: The folio to migrate the data to. * @src: The folio containing the current data. * @mode: How to migrate the page. * * Common logic to directly migrate a single LRU folio suitable for * folios that do not use PagePrivate/PagePrivate2. * * Folios are locked upon entry and exit. ": "migrate_folio_extra(struct address_space  mapping, struct folio  dst,struct folio  src, enum migrate_mode mode, int extra_count){int rc;BUG_ON(folio_test_writeback(src));  Writeback must be complete ", "expected_count = folio_expected_refs(mapping, src);if (folio_ref_count(src) != expected_count)return -EAGAIN;if (!buffer_migrate_lock_buffers(head, mode))return -EAGAIN;if (check_refs) ": "buffer_migrate_folio(struct address_space  mapping,struct folio  dst, struct folio  src, enum migrate_mode mode,bool check_refs){struct buffer_head  bh,  head;int rc;int expected_count;head = folio_buffers(src);if (!head)return migrate_folio(mapping, dst, src, mode);  Check whether page does not have extra refs before we do more work ", "cgwb_release_wq = alloc_workqueue(\"cgwb_release\", 0, 1);if (!cgwb_release_wq)return -ENOMEM;return 0;}subsys_initcall(cgwb_init);#else/* CONFIG_CGROUP_WRITEBACK ": "bdi_register(struct backing_dev_info  bdi){spin_lock_irq(&cgwb_lock);list_add_tail_rcu(&bdi->wb.bdi_node, &bdi->wb_list);spin_unlock_irq(&cgwb_lock);}static int __init cgwb_init(void){    There can be many concurrent release work items overwhelming   system_wq.  Put them in a separate wq and limit concurrency.   There's no point in executing many of these in parallel. ", "static void cleanup_offline_cgwbs_workfn(struct work_struct *work)": "bdi_unregister(struct backing_dev_info  bdi){struct radix_tree_iter iter;void   slot;struct bdi_writeback  wb;WARN_ON(test_bit(WB_registered, &bdi->wb.state));spin_lock_irq(&cgwb_lock);radix_tree_for_each_slot(slot, &bdi->cgwb_tree, &iter, 0)cgwb_kill( slot);spin_unlock_irq(&cgwb_lock);mutex_lock(&bdi->cgwb_release_mutex);spin_lock_irq(&cgwb_lock);while (!list_empty(&bdi->wb_list)) {wb = list_first_entry(&bdi->wb_list, struct bdi_writeback,      bdi_node);spin_unlock_irq(&cgwb_lock);wb_shutdown(wb);spin_lock_irq(&cgwb_lock);}spin_unlock_irq(&cgwb_lock);mutex_unlock(&bdi->cgwb_release_mutex);}    cleanup_offline_cgwbs_workfn - try to release dying cgwbs     Try to release dying cgwbs by switching attached inodes to the nearest   living ancestor's writeback. Processed wbs are placed at the end   of the list to guarantee the forward progress. ", "spin_lock_irqsave(&cgwb_lock, flags);wb = radix_tree_lookup(&bdi->cgwb_tree, memcg_css->id);if (wb && wb->blkcg_css != blkcg_css) ": "bdi_put(bdi);WARN_ON_ONCE(!list_empty(&wb->b_attached));call_rcu(&wb->rcu, cgwb_free_rcu);}static void cgwb_release(struct percpu_ref  refcnt){struct bdi_writeback  wb = container_of(refcnt, struct bdi_writeback,refcnt);queue_work(cgwb_release_wq, &wb->release_work);}static void cgwb_kill(struct bdi_writeback  wb){lockdep_assert_held(&cgwb_lock);WARN_ON(!radix_tree_delete(&wb->bdi->cgwb_tree, wb->memcg_css->id));list_del(&wb->memcg_node);list_del(&wb->blkcg_node);list_add(&wb->offline_node, &offline_cgwbs);percpu_ref_kill(&wb->refcnt);}static void cgwb_remove_from_bdi_list(struct bdi_writeback  wb){spin_lock_irq(&cgwb_lock);list_del_rcu(&wb->bdi_node);spin_unlock_irq(&cgwb_lock);}static int cgwb_create(struct backing_dev_info  bdi,       struct cgroup_subsys_state  memcg_css, gfp_t gfp){struct mem_cgroup  memcg;struct cgroup_subsys_state  blkcg_css;struct list_head  memcg_cgwb_list,  blkcg_cgwb_list;struct bdi_writeback  wb;unsigned long flags;int ret = 0;memcg = mem_cgroup_from_css(memcg_css);blkcg_css = cgroup_get_e_css(memcg_css->cgroup, &io_cgrp_subsys);memcg_cgwb_list = &memcg->cgwb_list;blkcg_cgwb_list = blkcg_get_cgwb_list(blkcg_css);  look up again under lock and discard on blkcg mismatch ", "if (!S_ISREG(mapping->host->i_mode))return -EIO;return truncate_inode_folio(mapping, page_folio(page));}EXPORT_SYMBOL(generic_error_remove_page": "generic_error_remove_page(struct address_space  mapping, struct page  page){VM_BUG_ON_PAGE(PageTail(page), page);if (!mapping)return -EINVAL;    Only punch for normal data pages for now.   Handling other types like directories would need more auditing. ", "void truncate_inode_pages_range(struct address_space *mapping,loff_t lstart, loff_t lend)": "truncate_inode_pages_range - truncate range of pages specified by start & end byte offsets   @mapping: mapping to truncate   @lstart: offset from which to truncate   @lend: offset to which to truncate (inclusive)     Truncate the page cache, removing the pages that are between   specified offsets (and zeroing out partial pages   if lstart or lend + 1 is not page aligned).     Truncate takes two passes - the first pass is nonblocking.  It will not   block on page locks and it will not block on writeback.  The second pass   will wait.  This is to prevent as much IO as possible in the affected region.   The first pass will remove most pages, so the search cost of the second pass   is low.     We pass down the cache-hot hint to the page freeing code.  Even if the   mapping is large, it is probably the case that the final pages are the most   recently touched, and freeing happens in ascending file offset order.     Note that since ->invalidate_folio() accepts range to invalidate   truncate_inode_pages_range is able to handle cases where lend + 1 is not   page aligned properly. ", "void truncate_inode_pages_final(struct address_space *mapping)": "truncate_inode_pages_final - truncate  all  pages before inode dies   @mapping: mapping to truncate     Called under (and serialized by) inode->i_rwsem.     Filesystems have to use this in the .evict_inode path to inform the   VM that this is the final truncate and the inode is going away. ", "static void truncate_cleanup_folio(struct folio *folio)": "invalidate_mapping_pages got there first and   c) when tmpfs swizzles a page between a tmpfs inode and swapper_space. ", "void truncate_pagecache(struct inode *inode, loff_t newsize)": "truncate_pagecache - unmap and remove pagecache that has been truncated   @inode: inode   @newsize: new file size     inode's new i_size must already be written before truncate_pagecache   is called.     This function should typically be called before the filesystem   releases resources associated with the freed range (eg. deallocates   blocks). This way, pagecache will always stay logically coherent   with on-disk format, and the filesystem would not have to deal with   situations such as writepage being called for a page that has already   had its underlying blocks deallocated. ", "void truncate_setsize(struct inode *inode, loff_t newsize)": "truncate_setsize - update inode and pagecache for a new file size   @inode: inode   @newsize: new file size     truncate_setsize updates i_size and performs pagecache truncation (if   necessary) to @newsize. It will be typically be called from the filesystem's   setattr function when ATTR_SIZE is passed in.     Must be called with a lock serializing truncates and writes (generally   i_rwsem but e.g. xfs uses a different lock) and before all filesystem   specific block truncation has been performed. ", "void pagecache_isize_extended(struct inode *inode, loff_t from, loff_t to)": "pagecache_isize_extended(inode, oldsize, newsize);truncate_pagecache(inode, newsize);}EXPORT_SYMBOL(truncate_setsize);     pagecache_isize_extended - update pagecache after extension of i_size   @inode:inode for which i_size was extended   @from:original inode size   @to:new inode size     Handle extension of inode size either caused by extending truncate or by   write starting after current i_size. We mark the page straddling current   i_size RO so that page_mkwrite() is called on the nearest write access to   the page.  This way filesystem can be sure that page_mkwrite() is called on   the page before user writes to the page via mmap after the i_size has been   changed.     The function must be called after i_size is updated so that page fault   coming after we unlock the page will already see the new i_size.   The function must be called while we still hold i_rwsem - this not only   makes sure i_size is stable but also that userspace cannot observe new   i_size value before we are prepared to store mmap writes at new inode size. ", "void truncate_pagecache_range(struct inode *inode, loff_t lstart, loff_t lend)": "truncate_pagecache_range - unmap and remove pagecache that is hole-punched   @inode: inode   @lstart: offset of beginning of hole   @lend: offset of last byte of hole     This function should typically be called before the filesystem   releases resources associated with the freed range (eg. deallocates   blocks). This way, pagecache will always stay logically coherent   with on-disk format, and the filesystem would not have to deal with   situations such as writepage being called for a page that has already   had its underlying blocks deallocated. ", "if (WARN_ON_ONCE(addr >= PKMAP_ADDR(0) && addr < PKMAP_ADDR(LAST_PKMAP)))return pte_page(ptep_get(&pkmap_page_table[PKMAP_NR(addr)]));/* kmap_local_page() mappings ": "__kmap_to_page(void  vaddr){unsigned long base = (unsigned long) vaddr & PAGE_MASK;struct kmap_ctrl  kctrl = &current->kmap_ctrl;unsigned long addr = (unsigned long)vaddr;int i;  kmap() mappings ", "#ifdef ARCH_NEEDS_KMAP_HIGH_GET#define lock_kmap()             spin_lock_irq(&kmap_lock)#define unlock_kmap()           spin_unlock_irq(&kmap_lock)#define lock_kmap_any(flags)    spin_lock_irqsave(&kmap_lock, flags)#define unlock_kmap_any(flags)  spin_unlock_irqrestore(&kmap_lock, flags)#else#define lock_kmap()             spin_lock(&kmap_lock)#define unlock_kmap()           spin_unlock(&kmap_lock)#define lock_kmap_any(flags)    \\do ": "kmap_high_get(), so let's abstract   the disabling of IRQ out of the locking in that case to save on a   potential useless overhead. ", "void *kmap_high_get(struct page *page)": "kunmap_high() is necessary.     This can be called from any context. ", "if (!IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP) && !PageHighMem(page))return page_address(page);/* Try kmap_high_get() if architecture has it enabled ": "__kmap_local_page_prot(struct page  page, pgprot_t prot){void  kmap;    To broaden the usage of the actual kmap_local() machinery always map   pages when debugging is enabled and the architecture has no problems   with alias mappings. ", "WARN_ON_ONCE(1);return;}/* * Handle mappings which were obtained by kmap_high_get() * first as the virtual address of such mappings is below * PAGE_OFFSET. Warn for all other addresses which are in * the user space part of the virtual address space. ": "kunmap_local_indexed(const void  vaddr){unsigned long addr = (unsigned long) vaddr & PAGE_MASK;pte_t  kmap_pte;int idx;if (addr < __fix_to_virt(FIX_KMAP_END) ||    addr > __fix_to_virt(FIX_KMAP_BEGIN)) {if (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP)) {  This _should_ never happen! See above. ", "for (;;) ": "page_address(page, NULL);need_flush = 1;}if (need_flush)flush_tlb_kernel_range(PKMAP_ADDR(0), PKMAP_ADDR(LAST_PKMAP));}void __kmap_flush_unused(void){lock_kmap();flush_all_zero_pkmaps();unlock_kmap();}static inline unsigned long map_new_virtual(struct page  page){unsigned long vaddr;int count;unsigned int last_pkmap_nr;unsigned int color = get_pkmap_color(page);start:count = get_pkmap_entries_count(color);  Find an empty entry ", "return vma_alloc_folio(gfp, folio_order(src), vma, address,folio_test_large(src));}#elsestatic int migrate_folio_add(struct folio *folio, struct list_head *foliolist,unsigned long flags)": "vma_alloc_folio() will use task or system default policy ", "if (!static_branch_likely(&vm_numa_stat_key))return page;if (page && page_to_nid(page) == nid) ": "alloc_pages(gfp, order, nid, NULL);  skip NUMA_INTERLEAVE_HIT counter update if numa stats is disabled ", "if (!folio && (gfp & __GFP_DIRECT_RECLAIM))folio = __folio_alloc(gfp, order, hpage_node,      nmask);goto out;}}nmask = policy_nodemask(gfp, pol);preferred_nid = policy_node(gfp, pol, node);folio = __folio_alloc(gfp, order, preferred_nid, nmask);mpol_cond_put(pol);out:return folio;}EXPORT_SYMBOL(vma_alloc_folio);/** * alloc_pages - Allocate pages. * @gfp: GFP flags. * @order: Power of two of number of pages to allocate. * * Allocate 1 << @order contiguous pages.  The physical address of the * first page is naturally aligned (eg an order-3 allocation will be aligned * to a multiple of 8 * PAGE_SIZE bytes).  The NUMA policy of the current * process is honoured when in process context. * * Context: Can be called from any context, providing the appropriate GFP * flags are used. * Return: The page on success or NULL if allocation fails. ": "folio_alloc_node(gfp | __GFP_THISNODE |__GFP_NORETRY, order, hpage_node);    If hugepage allocations are configured to always   synchronous compact or the vma has been madvised   to prefer hugepage backing, retry allowing remote   memory with both reclaim and compact as well. ", "static int get_hwpoison_page(struct page *p, unsigned long flags)": "unpoison_memory(), the caller should already ensure that   the given page has PG_hwpoison. So it's never reused for other page   allocations, and __get_unpoison_page() never races with them.     Return: 0 on failure,           1 on success for in-use pages in a well-defined state,           -EIO for pages on which we can not handle memory errors,           -EBUSY when get_hwpoison_page() has raced with page lifecycle           operations like allocation and free,           -EHWPOISON when the page is hwpoisoned and taken off from buddy. ", "bool page_cma = is_migrate_cma_page(page);int mapcount;char *type = \"\";if (page < head || (page >= head + MAX_ORDER_NR_PAGES)) ": "dump_page(struct page  page){struct folio  folio = page_folio(page);struct page  head = &folio->page;struct address_space  mapping;bool compound = PageCompound(page);    Accessing the pageblock without the zone lock. It could change to   \"isolate\" again in the meantime, but since we are just dumping the   state for debugging, it should be fine to accept a bit of   inaccuracy here due to racing. ", "pgoff_t start_index;pgoff_t end_index;unsigned long nrpages;inode = file_inode(file);if (S_ISFIFO(inode->i_mode))return -ESPIPE;mapping = file->f_mapping;if (!mapping || len < 0)return -EINVAL;bdi = inode_to_bdi(mapping->host);if (IS_DAX(inode) || (bdi == &noop_backing_dev_info)) ": "generic_fadvise(struct file  file, loff_t offset, loff_t len, int advice){struct inode  inode;struct address_space  mapping;struct backing_dev_info  bdi;loff_t endbyte;  inclusive ", "static __fastpath_inline void *slab_alloc_node(struct kmem_cache *s, struct list_lru *lru,gfp_t gfpflags, int node, unsigned long addr, size_t orig_size)": "kmem_cache_alloc)   have the fastpath folded into their functions. So no function call   overhead for requests that can be satisfied on the fastpath.     The fastpath works by first checking if the lockless freelist can be used.   If not then __slab_alloc is called for slow processing.     Otherwise we can simply pick the next object from the lockless free list. ", "static __always_inline void do_slab_free(struct kmem_cache *s,struct slab *slab, void *head, void *tail,int cnt, unsigned long addr)": "kmem_cache_free that   can perform fastpath freeing without additional function calls.     The fastpath is only possible if we are freeing to the current cpu slab   of this processor. This typically the case if we have just allocated   the item before.     If fastpath is not possible then fall back to __slab_free where we deal   with all sorts of special processing.     Bulk free of a freelist with several objects (all pointing to the   same slab) possible by specifying head and tail ptr, plus objects   count (cnt). Bulk free indicated by tail pointer being set. ", "c = slub_get_cpu_ptr(s->cpu_slab);local_lock_irqsave(&s->cpu_slab->lock, irqflags);for (i = 0; i < size; i++) ": "kmem_cache_alloc_bulk(struct kmem_cache  s, gfp_t flags,size_t size, void   p, struct obj_cgroup  objcg){struct kmem_cache_cpu  c;unsigned long irqflags;int i;    Drain objects in the per cpu slab, while disabling local   IRQs, which protects against PREEMPT and interrupts   handlers invoking normal fastpath. ", "if (!folio_test_hugetlb(folio))__page_cache_release(folio);destroy_large_folio(folio);}void __folio_put(struct folio *folio)": "__folio_put_small(struct folio  folio){__page_cache_release(folio);mem_cgroup_uncharge(folio);free_unref_page(&folio->page, 0);}static void __folio_put_large(struct folio  folio){    __page_cache_release() is supposed to be called for thp, not for   hugetlb. This is because hugetlb page does never have PageLRU set   (it's never listed to any LRU lists) and no memcg routines should   be called for hugetlb (it has a separate hugetlb_cgroup.) ", "void put_pages_list(struct list_head *pages)": "put_pages_list() - release a list of pages   @pages: list of pages threaded on page->lru     Release a list of pages which are strung together on page.lru. ", "} else if (!folio_test_active(folio)) ": "folio_mark_accessed(struct folio  folio){if (lru_gen_enabled()) {folio_inc_refs(folio);return;}if (!folio_test_referenced(folio)) {folio_set_referenced(folio);} else if (folio_test_unevictable(folio)) {    Unevictable pages are on the \"LRU_UNEVICTABLE\" list. But,   this list is never rotated or maintained, so marking an   unevictable page accessed has no effect. ", "void folio_add_lru(struct folio *folio)": "folio_add_lru - Add a folio to an LRU list.   @folio: The folio to be added to the LRU.     Queue the folio for addition to the LRU. The decision on whether   to add the page to the [in]active [file|anon] list is deferred until the   folio_batch is drained. This gives a chance for the caller of folio_add_lru()   have the folio added to the active list using folio_mark_accessed(). ", "if (unlikely(folio_test_mlocked(folio))) ": "release_pages() ", "void __folio_batch_release(struct folio_batch *fbatch)": "__folio_batch_release() will drain those queues here.   folio_batch_move_lru() calls folios_put() directly to avoid   mutual recursion. ", "preempt_disable_nested();x = delta + __this_cpu_read(*p);t = __this_cpu_read(pcp->stat_threshold);if (unlikely(abs(x) > t)) ": "mod_zone_page_state(struct zone  zone, enum zone_stat_item item,   long delta){struct per_cpu_zonestat __percpu  pcp = zone->per_cpu_zonestats;s8 __percpu  p = pcp->vm_stat_diff + item;long x;long t;    Accurate vmstat updates require a RMW. On !PREEMPT_RT kernels,   atomicity is provided by IRQs being disabled -- either explicitly   or via local_lock_irq. On PREEMPT_RT, local_lock_irq only disables   CPU migrations and preemption potentially corrupts a counter so   disable preemption. ", "VM_WARN_ON_ONCE(delta & (PAGE_SIZE - 1));delta >>= PAGE_SHIFT;}/* See __mod_node_page_state ": "mod_node_page_state(struct pglist_data  pgdat, enum node_stat_item item,long delta){struct per_cpu_nodestat __percpu  pcp = pgdat->per_cpu_nodestats;s8 __percpu  p = pcp->vm_node_stat_diff + item;long x;long t;if (vmstat_item_in_bytes(item)) {    Only cgroups use subpage accounting right now; at   the global level, these items still change in   multiples of whole pages. Store them as pages   internally to keep the per-cpu counters compact. ", "preempt_disable_nested();v = __this_cpu_dec_return(*p);t = __this_cpu_read(pcp->stat_threshold);if (unlikely(v < - t)) ": "inc_node_page_state(struct page  page, enum node_stat_item item){__inc_node_state(page_pgdat(page), item);}EXPORT_SYMBOL(__inc_node_page_state);void __dec_zone_state(struct zone  zone, enum zone_stat_item item){struct per_cpu_zonestat __percpu  pcp = zone->per_cpu_zonestats;s8 __percpu  p = pcp->vm_stat_diff + item;s8 v, t;  See __mod_node_page_state ", "static inline void mod_zone_state(struct zone *zone,       enum zone_stat_item item, long delta, int overstep_mode)": "dec_node_page_state(struct page  page, enum node_stat_item item){__dec_node_state(page_pgdat(page), item);}EXPORT_SYMBOL(__dec_node_page_state);#ifdef CONFIG_HAVE_CMPXCHG_LOCAL    If we have cmpxchg_local support then we do not need to incur the overhead   that comes with local_irq_saverestore if we use this_cpu_cmpxchg.     mod_state() modifies the zone counter state through atomic per cpu   operations.     Overstep mode specifies how overstep should handled:       0       No overstepping       1       Overstepping half of threshold       -1      Overstepping minus half of threshold", "preempt_disable_nested();v = __this_cpu_inc_return(*p);t = __this_cpu_read(pcp->stat_threshold);if (unlikely(v > t)) ": "inc_node_state(struct pglist_data  pgdat, enum node_stat_item item){struct per_cpu_nodestat __percpu  pcp = pgdat->per_cpu_nodestats;s8 __percpu  p = pcp->vm_node_stat_diff + item;s8 v, t;VM_WARN_ON_ONCE(vmstat_item_in_bytes(item));  See __mod_node_page_state ", "struct kmem_cache *kmem_cache_create_usercopy(const char *name,  unsigned int size, unsigned int align,  slab_flags_t flags,  unsigned int useroffset, unsigned int usersize,  void (*ctor)(void *))": "kmem_cache_create_usercopy - Create a cache with a region suitable   for copying to userspace   @name: A string which is used in procslabinfo to identify this cache.   @size: The size of objects to be created in this cache.   @align: The required alignment for the objects.   @flags: SLAB flags   @useroffset: Usercopy region offset   @usersize: Usercopy region size   @ctor: A constructor for the objects.     Cannot be called within a interrupt, but can be interrupted.   The @ctor is run when new pages are allocated by the cache.     The flags are     %SLAB_POISON - Poison the slab with a known test pattern (a5a5a5a5)   to catch references to uninitialised memory.     %SLAB_RED_ZONE - Insert `Red` zones around the allocated memory to check   for buffer overruns.     %SLAB_HWCACHE_ALIGN - Align the objects in this cache to a hardware   cacheline.  This can be beneficial if you're counting cycles as closely   as davem.     Return: a pointer to the cache on success, NULL on failure. ", "return 0;}#elsestatic inline int kmem_cache_sanity_check(const char *name, unsigned int size)": "kmem_cache_create(%s) integrity check failed\\n\", name);return -EINVAL;}WARN_ON(strchr(name, ' '));  It confuses parsers ", "static void kmem_cache_release(struct kmem_cache *s)": "kmem_cache_destroy() should only be called   once or there will be a use-after-free problem. The actual deletion   and release of the kobject does not need slab_mutex or cpu_hotplug_lock   protection. So they are now done without holding those locks.     Note that there will be a slight delay in the deletion of sysfs files   if kmem_cache_release() is called indrectly from a work function. ", "int kmem_cache_shrink(struct kmem_cache *cachep)": "kmem_cache_shrink - Shrink a cache.   @cachep: The cache to shrink.     Releases as many slabs as possible for a cache.   To help debugging, a zero exit status indicates all slabs were released.     Return: %0 if all slabs were released, non-zero otherwise ", " };EXPORT_SYMBOL(kmalloc_caches": "kmalloc_caches[NR_KMALLOC_TYPES][KMALLOC_SHIFT_HIGH + 1] __ro_after_init ={   initialization for https:bugs.llvm.orgshow_bug.cgi?id=42570 ", "if (unlikely(size == 0))return 0;/* Short-circuit saturated \"too-large\" case. ": "kmalloc_size_roundup(size_t size){struct kmem_cache  c;  Short-circuit the 0 size case. ", "if (IS_ENABLED(CONFIG_MEMCG_KMEM) && (type == KMALLOC_NORMAL))flags |= SLAB_NO_MERGE;if (minalign > ARCH_KMALLOC_MINALIGN) ": "__kmalloc_minalign(void){#ifdef CONFIG_DMA_BOUNCE_UNALIGNED_KMALLOCif (io_tlb_default_mem.nslabs)return ARCH_KMALLOC_MINALIGN;#endifreturn dma_get_cache_alignment();}void __initnew_kmalloc_cache(int idx, enum kmalloc_cache_type type, slab_flags_t flags){unsigned int minalign = __kmalloc_minalign();unsigned int aligned_size = kmalloc_info[idx].size;int aligned_idx = idx;if ((KMALLOC_RECLAIM != KMALLOC_NORMAL) && (type == KMALLOC_RECLAIM)) {flags |= SLAB_RECLAIM_ACCOUNT;} else if (IS_ENABLED(CONFIG_MEMCG_KMEM) && (type == KMALLOC_CGROUP)) {if (mem_cgroup_kmem_disabled()) {kmalloc_caches[type][idx] = kmalloc_caches[KMALLOC_NORMAL][idx];return;}flags |= SLAB_ACCOUNT;} else if (IS_ENABLED(CONFIG_ZONE_DMA) && (type == KMALLOC_DMA)) {flags |= SLAB_CACHE_DMA;}    If CONFIG_MEMCG_KMEM is enabled, disable cache merging for   KMALLOC_NORMAL caches. ", "struct kmem_cache *kmem_cache_create(const char *name, unsigned int size, unsigned int align,slab_flags_t flags, void (*ctor)(void *))": "kfree_const(cache_name);}out_unlock:mutex_unlock(&slab_mutex);if (err) {if (flags & SLAB_PANIC)panic(\"%s: Failed to create slab '%s'. Error %d\\n\",__func__, name, err);else {pr_warn(\"%s(%s) failed with error %d\\n\",__func__, name, err);dump_stack();}return NULL;}return s;}EXPORT_SYMBOL(kmem_cache_create_usercopy);     kmem_cache_create - Create a cache.   @name: A string which is used in procslabinfo to identify this cache.   @size: The size of objects to be created in this cache.   @align: The required alignment for the objects.   @flags: SLAB flags   @ctor: A constructor for the objects.     Cannot be called within a interrupt, but can be interrupted.   The @ctor is run when new pages are allocated by the cache.     The flags are     %SLAB_POISON - Poison the slab with a known test pattern (a5a5a5a5)   to catch references to uninitialised memory.     %SLAB_RED_ZONE - Insert `Red` zones around the allocated memory to check   for buffer overruns.     %SLAB_HWCACHE_ALIGN - Align the objects in this cache to a hardware   cacheline.  This can be beneficial if you're counting cycles as closely   as davem.     Return: a pointer to the cache on success, NULL on failure. ", "void kfree(const void *object)": "kmalloc_large_node(size_t size, gfp_t flags, int node);static __always_inlinevoid  __do_kmalloc_node(size_t size, gfp_t flags, int node, unsigned long caller){struct kmem_cache  s;void  ret;if (unlikely(size > KMALLOC_MAX_CACHE_SIZE)) {ret = __kmalloc_large_node(size, flags, node);trace_kmalloc(caller, ret, size,      PAGE_SIZE << get_order(size), flags, node);return ret;}s = kmalloc_slab(size, flags);if (unlikely(ZERO_OR_NULL_PTR(s)))return s;ret = __kmem_cache_alloc_node(s, flags, node, size, caller);ret = kasan_kmalloc(s, ret, size, flags);trace_kmalloc(caller, ret, size, s->size, flags, node);return ret;}void  __kmalloc_node(size_t size, gfp_t flags, int node){return __do_kmalloc_node(size, flags, node, _RET_IP_);}EXPORT_SYMBOL(__kmalloc_node);void  __kmalloc(size_t size, gfp_t flags){return __do_kmalloc_node(size, flags, NUMA_NO_NODE, _RET_IP_);}EXPORT_SYMBOL(__kmalloc);void  __kmalloc_node_track_caller(size_t size, gfp_t flags,  int node, unsigned long caller){return __do_kmalloc_node(size, flags, node, caller);}EXPORT_SYMBOL(__kmalloc_node_track_caller);     kfree - free previously allocated memory   @object: pointer returned by kmalloc() or kmem_cache_alloc()     If @object is NULL, no operation is performed. ", "if (likely(!ZERO_OR_NULL_PTR(p))) ": "krealloc(const void  p, size_t new_size, gfp_t flags){void  ret;size_t ks;  Check for double-free before calling ksize. ", "void kfree_sensitive(const void *p)": "kfree_sensitive - Clear sensitive information in memory before freeing   @p: object to free memory of     The memory of the object @p points to is zeroed before freed.   If @p is %NULL, kfree_sensitive() does nothing.     Note: this function zeroes the whole allocated buffer which can be a good   deal bigger than the requested buffer size passed to kmalloc(). So be   careful when using this function in performance sensitive code. ", "size_t __ksize(const void *object)": "ksize -- Report full size of underlying allocation   @object: pointer to the object     This should only be used internally to query the true size of allocations.   It is not meant to be a way to discover the usable size of an allocation   after the fact. Instead, use kmalloc_size_roundup(). Using memory beyond   the originally requested allocation size may trigger KASAN, UBSAN_BOUNDS,   andor FORTIFY_SOURCE.     Return: size of the actual memory used by @object in bytes ", "void __ref kmemleak_update_trace(const void *ptr)": "kmemleak_update_trace - update object allocation stack trace   @ptr:pointer to beginning of the object     Override the object allocation stack trace for cases where the actual   allocation place is not always useful. ", "void __ref kmemleak_not_leak(const void *ptr)": "kmemleak_not_leak - mark an allocated object as false positive   @ptr:pointer to beginning of the object     Calling this function on an object will cause the memory block to no longer   be reported as leak and always be scanned. ", "void __ref kmemleak_ignore(const void *ptr)": "kmemleak_ignore - ignore an allocated object   @ptr:pointer to beginning of the object     Calling this function on an object will cause the memory block to be   ignored (not scanned and not reported as a leak). This is usually done when   it is known that the corresponding block is not a leak and does not contain   any references to other allocated memory blocks. ", "struct kmemleak_object ": "kmemleak_scan_area {struct hlist_node node;unsigned long start;size_t size;};#define KMEMLEAK_GREY0#define KMEMLEAK_BLACK-1    Structure holding the metadata for each allocated memory block.   Modifications to such objects should be made while holding the   object->lock. Insertions or deletions from object_list, gray_list or   rb_node are already protected by the corresponding locks or mutex (see   the notes on locking above). These objects are reference-counted   (use_count) and freed using the RCU mechanism. ", "void __ref kmemleak_no_scan(const void *ptr)": "kmemleak_no_scan - do not scan an allocated object   @ptr:pointer to beginning of the object     This function notifies kmemleak not to scan the given memory block. Useful   in situations where it is known that the given object does not contain any   references to other objects. Kmemleak will not scan such objects reducing   the number of false negatives. ", "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt#include <linux/init.h>#include <linux/kernel.h>#include <linux/list.h>#include <linux/sched/signal.h>#include <linux/sched/task.h>#include <linux/sched/task_stack.h>#include <linux/jiffies.h>#include <linux/delay.h>#include <linux/export.h>#include <linux/kthread.h>#include <linux/rbtree.h>#include <linux/fs.h>#include <linux/debugfs.h>#include <linux/seq_file.h>#include <linux/cpumask.h>#include <linux/spinlock.h>#include <linux/module.h>#include <linux/mutex.h>#include <linux/rcupdate.h>#include <linux/stacktrace.h>#include <linux/stackdepot.h>#include <linux/cache.h>#include <linux/percpu.h>#include <linux/memblock.h>#include <linux/pfn.h>#include <linux/mmzone.h>#include <linux/slab.h>#include <linux/thread_info.h>#include <linux/err.h>#include <linux/uaccess.h>#include <linux/string.h>#include <linux/nodemask.h>#include <linux/mm.h>#include <linux/workqueue.h>#include <linux/crc32.h>#include <asm/sections.h>#include <asm/processor.h>#include <linux/atomic.h>#include <linux/kasan.h>#include <linux/kfence.h>#include <linux/kmemleak.h>#include <linux/memory_hotplug.h>/* * Kmemleak configuration and common defines. ": "kmemleak_alloc_phys()) callback and removed in delete_object() called from     the kmemleak_free() callback   - kmemleak_object.lock (raw_spinlock_t): protects a kmemleak_object.     Accesses to the metadata (e.g. count) are protected by this lock. Note     that some members of this structure may be protected by other means     (atomic or kmemleak_lock). This lock is also held when scanning the     corresponding memory block to avoid the kernel freeing it via the     kmemleak_free() callback. This is less heavyweight than holding a global     lock like kmemleak_lock during scanning.   - scan_mutex (mutex): ensures that only one thread may scan the memory for     unreferenced objects at a time. The gray_list contains the objects which     are already referenced or marked as false positives and need to be     scanned. This list is only modified during a scanning episode when the     scan_mutex is held. At the end of a scan, the gray_list is always empty.     Note that the kmemleak_object.use_count is incremented when an object is     added to the gray_list and therefore cannot be freed. This mutex also     prevents multiple users of the \"kmemleak\" debugfs file together with     modifications to the memory scanning parameters including the scan_thread     pointer     Locks and mutexes are acquirednested in the following order:       scan_mutex [-> object->lock] -> kmemleak_lock -> other_object->lock (SINGLE_DEPTH_NESTING)     No kmemleak_lock and object->lock nesting is allowed outside scan_mutex   regions.     The kmemleak_object structures have a use_count incremented or decremented   using the get_object()put_object() functions. When the use_count becomes   0, this count can no longer be incremented and put_object() schedules the   kmemleak_object freeing via an RCU callback. All calls to the get_object()   function must be protected by rcu_read_lock() to avoid accessing a freed   structure. ", "void __ref kmemleak_free_part_phys(phys_addr_t phys, size_t size)": "kmemleak_free_part_phys - similar to kmemleak_free_part but taking a       physical address argument   @phys:physical address if the beginning or inside an object. This  also represents the start of the range to be freed   @size:size to be unregistered ", "void __ref kmemleak_ignore_phys(phys_addr_t phys)": "kmemleak_ignore_phys - similar to kmemleak_ignore but taking a physical    address argument   @phys:physical address of the object ", "void zpool_register_driver(struct zpool_driver *driver)": "zpool_register_driver() - register a zpool implementation.   @driver:driver to register ", "int zpool_unregister_driver(struct zpool_driver *driver)": "zpool_unregister_driver() - unregister a zpool implementation.   @driver:driver to unregister.     Module usage counting is used to prevent using a driver   whileafter unloading, so if this is called from module   exit function, this should never fail; if called from   other than the module exit function, and this returns   failure, the driver is in use and must remain available. ", "bool zpool_has_pool(char *type)": "zpool_has_pool() - Check if the pool driver is available   @type:The type of the zpool to check (e.g. zbud, zsmalloc)     This checks if the @type pool driver is available.  This will try to load   the requested module, if needed, but there is no guarantee the module will   still be loaded and available immediately after calling.  If this returns   true, the caller should assume the pool is available, but must be prepared   to handle the @zpool_create_pool() returning failure.  However if this   returns false, the caller should assume the requested pool type is not   available; either the requested pool type module does not exist, or could   not be loaded, and calling @zpool_create_pool() with the pool type will   fail.     The @type string must be null-terminated.     Returns: true if @type pool is available, false if not ", "struct mem_cgroup *memcg;pg_data_t *pgdat = page_pgdat(page);struct lruvec *lruvec;rcu_read_lock();memcg = page_memcg(head);/* Untracked pages have no memcg, no lruvec. Update only the node ": "__mod_lruvec_page_state(struct page  page, enum node_stat_item idx,     int val){struct page  head = compound_head(page);   rmap on tail pages ", "if (unlikely(!p))return NULL;return mem_cgroup_from_css(task_css(p, memory_cgrp_id));}EXPORT_SYMBOL(mem_cgroup_from_task": "mem_cgroup_from_task(struct task_struct  p){    mm_update_next_owner() may clear mm->owner to NULL   if it races with swapoff, page migration, etc.   So this can be called with p == NULL. ", "struct mem_cgroup *get_mem_cgroup_from_mm(struct mm_struct *mm)": "get_mem_cgroup_from_mm: Obtain a reference on given mm_struct's memcg.   @mm: mm from which memcg should be extracted. It can be NULL.     Obtain a reference on mm->memcg and returns it if successful. If mm   is NULL, then the memcg is chosen as follows:   1) The active memcg, if set.   2) current->mm->memcg, if available   3) root memcg   If mem_cgroup is disabled, NULL is returned. ", "static ssize_t mem_cgroup_write(struct kernfs_open_file *of,char *buf, size_t nbytes, loff_t off)": "memcg_sockets_enabled_key);memcg->tcpmem_active = true;}out:mutex_unlock(&memcg_max_mutex);return ret;}    The user of this function is...   RES_LIMIT. ", "static struct slab *cache_grow_begin(struct kmem_cache *cachep,gfp_t flags, int nodeid)": "kmem_cache_alloc() when there are no active objs left in a cache. ", "slab_post_alloc_hook(s, objcg, flags, size, p,slab_want_init_on_alloc(flags, s), s->object_size);/* FIXME: Trace call missing. Christoph would like a bulk variant ": "kmem_cache_alloc_bulk(struct kmem_cache  s, gfp_t flags, size_t size,  void   p){struct obj_cgroup  objcg = NULL;unsigned long irqflags;size_t i;s = slab_pre_alloc_hook(s, NULL, &objcg, size, flags);if (!s)return 0;local_irq_save(irqflags);for (i = 0; i < size; i++) {void  objp = kfence_alloc(s, s->object_size, flags) ?:     __do_cache_alloc(s, flags, NUMA_NO_NODE);if (unlikely(!objp))goto error;p[i] = objp;}local_irq_restore(irqflags);cache_alloc_debugcheck_after_bulk(s, flags, size, p, _RET_IP_);    memcg and kmem_cache debug support and memory initialization.   Done outside of the IRQ disabled section. ", "void *kmem_cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid)": "kmem_cache_free_bulk(s, i, p);return 0;}EXPORT_SYMBOL(kmem_cache_alloc_bulk);     kmem_cache_alloc_node - Allocate an object on the specified node   @cachep: The cache to allocate from.   @flags: See kmalloc().   @nodeid: node number of the target node.     Identical to kmem_cache_alloc but it will allocate memory on the given   node, which can improve the performance for cpu bound structures.     Fallback to other node is possible if __GFP_THISNODE is not set.     Return: pointer to the new object or %NULL in case of error ", "#include<linux/slab.h>#include<linux/mm.h>#include<linux/poison.h>#include<linux/swap.h>#include<linux/cache.h>#include<linux/interrupt.h>#include<linux/init.h>#include<linux/compiler.h>#include<linux/cpuset.h>#include<linux/proc_fs.h>#include<linux/seq_file.h>#include<linux/notifier.h>#include<linux/kallsyms.h>#include<linux/kfence.h>#include<linux/cpu.h>#include<linux/sysctl.h>#include<linux/module.h>#include<linux/rcupdate.h>#include<linux/string.h>#include<linux/uaccess.h>#include<linux/nodemask.h>#include<linux/kmemleak.h>#include<linux/mempolicy.h>#include<linux/mutex.h>#include<linux/fault-inject.h>#include<linux/rtmutex.h>#include<linux/reciprocal_div.h>#include<linux/debugobjects.h>#include<linux/memory.h>#include<linux/prefetch.h>#include<linux/sched/task_stack.h>#include<net/sock.h>#include<asm/cacheflush.h>#include<asm/tlbflush.h>#include<asm/page.h>#include <trace/events/kmem.h>#include\"internal.h\"#include\"slab.h\"/* * DEBUG- 1 for kmem_cache_create() to honour; SLAB_RED_ZONE & SLAB_POISON. *  0 for faster, smaller code (especially in the critical paths). * * STATS- 1 to collect stats for /proc/slabinfo. *  0 for faster, smaller code (especially in the critical paths). * * FORCED_DEBUG- 1 enables SLAB_RED_ZONE and SLAB_POISON (if possible) ": "kmem_cache_free.     Each cache can only support one memory type (GFP_DMA, GFP_HIGHMEM,   normal). If you need a special memory type, then must create a new   cache for that memory type.     In order to reduce fragmentation, the slabs are sorted in 3 groups:     full slabs with 0 free objects     partial slabs     empty slabs with no allocated objects     If partial slabs exist, then new allocations come from these slabs,   otherwise from empty slabs or new slabs are allocated.     kmem_cache_destroy() CAN CRASH if you try to allocate from the cache   during kmem_cache_destroy(). The caller must prevent concurrent allocs.     Each cache has a short per-cpu head array, most allocs   and frees go into that array, and if that array overflows, then 12   of the entries in the array are given back into the global cache.   The head array is strictly LIFO and should improve the cache hit rates.   On SMP, it additionally reduces the spinlock operations.     The c_cpuarray may not be read with enabled local interrupts -   it's changed with a smp_call_function().     SMP synchronization:    constructors and destructors are called without any locking.    Several members in struct kmem_cache and struct slab never change, they  are accessed without any locking.    The per-cpu arrays are never accessed from the wrong cpu, no locking,    and local interrupts are disabled so slab code is preempt-safe.    The non-constant members are protected with a per-cache irq spinlock.     Many thanks to Mark Hemment, who wrote another per-cpu slab patch   in 2000 - many ideas in the current implementation are derived from   his patch.     Further notes from the original documentation:     11 April '97.  Started multi-threading - markhe  The global cache-chain is protected by the mutex 'slab_mutex'.  The sem is only needed when accessingextending the cache-chain, which  can never happen inside an interrupt (kmem_cache_create(),  kmem_cache_shrink() and kmem_cache_reap()).    At present, each engine can be growing a cache.  This should be blocked.     15 March 2005. NUMA slab allocator.  Shai Fultheim <shai@scalex86.org>.  Shobhit Dayal <shobhit@calsoftinc.com>  Alok N Kataria <alokk@calsoftinc.com>  Christoph Lameter <christoph@lameter.com>    Modified the slab allocator to be node aware on NUMA systems.  Each node has its own list of partial, free and full slabs.  All object allocations for a node occur from node specific slab lists. ", "#define SHMEM_INO_BATCH 1024static int shmem_reserve_inode(struct super_block *sb, ino_t *inop)": "shmem_aops;static const struct file_operations shmem_file_operations;static const struct inode_operations shmem_inode_operations;static const struct inode_operations shmem_dir_inode_operations;static const struct inode_operations shmem_special_inode_operations;static const struct vm_operations_struct shmem_vm_ops;static const struct vm_operations_struct shmem_anon_vm_ops;static struct file_system_type shmem_fs_type;bool vma_is_anon_shmem(struct vm_area_struct  vma){return vma->vm_ops == &shmem_anon_vm_ops;}bool vma_is_shmem(struct vm_area_struct  vma){return vma_is_anon_shmem(vma) || vma->vm_ops == &shmem_vm_ops;}static LIST_HEAD(shmem_swaplist);static DEFINE_MUTEX(shmem_swaplist_mutex);    shmem_reserve_inode() performs bookkeeping to reserve a shmem inode, and   produces a novel ino for the newly allocated inode.     It may also be called when making a hard link to permit the space needed by   each dentry. However, in that case, no new inode number is needed since that   internally draws from another pool of inode numbers (currently global   get_next_ino()). This case is indicated by passing NULL as inop. ", "void kfree_const(const void *x)": "kfree_const - conditionally free memory   @x: pointer to the memory     Function calls kfree only if @x is not in .rodata section. ", "noinlinechar *kstrdup(const char *s, gfp_t gfp)": "kstrdup - allocate space for and copy an existing string   @s: the string to duplicate   @gfp: the GFP mask used in the kmalloc() call when allocating memory     Return: newly allocated copy of @s or %NULL in case of error ", "const char *kstrdup_const(const char *s, gfp_t gfp)": "kstrdup_const - conditionally duplicate an existing const string   @s: the string to duplicate   @gfp: the GFP mask used in the kmalloc() call when allocating memory     Note: Strings allocated by kstrdup_const should be freed by kfree_const and   must not be passed to krealloc().     Return: source string if it is in .rodata section otherwise   fallback to kstrdup. ", "char *kstrndup(const char *s, size_t max, gfp_t gfp)": "kmemdup_nul() instead if the size is known exactly.     Return: newly allocated copy of @s or %NULL in case of error ", "void *kvmemdup(const void *src, size_t len, gfp_t gfp)": "kvfree() to free. ", "void *memdup_user(const void __user *src, size_t len)": "memdup_user - duplicate memory region from user space     @src: source address in user space   @len: number of bytes to copy     Return: an ERR_PTR() on failure.  Result is physically   contiguous, to be freed by kfree(). ", "void *vmemdup_user(const void __user *src, size_t len)": "vmemdup_user - duplicate memory region from user space     @src: source address in user space   @len: number of bytes to copy     Return: an ERR_PTR() on failure.  Result may be not   physically contiguous.  Use kvfree() to free. ", "char *strndup_user(const char __user *s, long n)": "strndup_user - duplicate an existing string from user space   @s: The string to duplicate   @n: Maximum number of bytes to copy, including the trailing NUL.     Return: newly allocated copy of @s or an ERR_PTR() in case of error ", "void *memdup_user_nul(const void __user *src, size_t len)": "memdup_user_nul - duplicate memory region from user space and NUL-terminate     @src: source address in user space   @len: number of bytes to copy     Return: an ERR_PTR() on failure. ", "get_file(file);swap(vma->vm_file, file);fput(file);}EXPORT_SYMBOL(vma_set_file": "vma_set_file(struct vm_area_struct  vma, struct file  file){  Changing an anonymous vma with this is illegal ", "void *kvmalloc_node(size_t size, gfp_t flags, int node)": "kvmalloc_node - attempt to allocate physically contiguous memory, but upon   failure, fall back to non-contiguous (vmalloc) allocation.   @size: size of the request.   @flags: gfp mask for the allocation - must be compatible (superset) with GFP_KERNEL.   @node: numa node to allocate from     Uses kmalloc to get the memory but if the allocation fails then falls back   to the vmalloc allocator. Use kvfree for freeing the memory.     GFP_NOWAIT and GFP_ATOMIC are not supported, neither is the __GFP_NORETRY modifier.   __GFP_RETRY_MAYFAIL is supported, and it should be used only if kmalloc is   preferable to the vmalloc fallback, due to visible performance drawbacks.     Return: pointer to the allocated memory of %NULL in case of failure ", "void kvfree_sensitive(const void *addr, size_t len)": "kvfree_sensitive - Free a data object containing sensitive information.   @addr: address of the data object to be freed.   @len: length of the data object.     Use the special memzero_explicit() function to clear the content of a   kvmalloc'ed object containing sensitive data to make sure that the   compiler won't optimize out the data clearing. ", "void *__vmalloc_array(size_t n, size_t size, gfp_t flags)": "vmalloc_array - allocate memory for a virtually contiguous array.   @n: number of elements.   @size: element size.   @flags: the type of memory to allocate (see kmalloc). ", "void *__vcalloc(size_t n, size_t size, gfp_t flags)": "vcalloc - allocate and zero memory for a virtually contiguous array.   @n: number of elements.   @size: element size.   @flags: the type of memory to allocate (see kmalloc). ", "struct address_space *folio_mapping(struct folio *folio)": "folio_mapping - Find the mapping where this folio is stored.   @folio: The folio.     For folios which are in the page cache, return the mapping that this   page belongs to.  Folios in the swap cache return the swap mapping   this page is stored in (which is different from the mapping for the   swap file or swap device where the data is stored).     You can call this for folios which aren't in the swap cache or page   cache and it will return NULL. ", "static DECLARE_RWSEM(page_offline_rwsem);void page_offline_freeze(void)": "page_offline_end() is used by drivers that care about   such races when setting a page PageOffline(). ", "if (test_bit(AS_ENOSPC, &mapping->flags) &&    test_and_clear_bit(AS_ENOSPC, &mapping->flags))ret = -ENOSPC;if (test_bit(AS_EIO, &mapping->flags) &&    test_and_clear_bit(AS_EIO, &mapping->flags))ret = -EIO;return ret;}EXPORT_SYMBOL(filemap_check_errors": "filemap_check_errors(struct address_space  mapping){int ret = 0;  Check for outstanding write errors ", "int filemap_fdatawrite_wbc(struct address_space *mapping,   struct writeback_control *wbc)": "filemap_fdatawrite_wbc - start writeback on mapping dirty pages in range   @mapping:address space structure to write   @wbc:the writeback_control controlling the writeout     Call writepages on the mapping using the provided wbc to control the   writeout.     Return: %0 on success, negative error code otherwise. ", "int __filemap_fdatawrite_range(struct address_space *mapping, loff_t start,loff_t end, int sync_mode)": "filemap_fdatawrite_range - start writeback on mapping dirty pages in range   @mapping:address space structure to write   @start:offset in bytes where the range starts   @end:offset in bytes where the range ends (inclusive)   @sync_mode:enable synchronous operation     Start writeback against all of a mapping's dirty pages that lie   within the byte offsets <start, end> inclusive.     If sync_mode is WB_SYNC_ALL then this is a \"data integrity\" operation, as   opposed to a regular memory cleansing writeback.  The difference between   these two operations is that if a dirty pagebuffer is encountered, it must   be waited upon, and not just skipped over.     Return: %0 on success, negative error code otherwise. ", "int filemap_flush(struct address_space *mapping)": "filemap_flush - mostly a non-blocking flush   @mapping:target address_space     This is a mostly non-blocking flush.  Not suitable for data-integrity   purposes - IO may not be started against all dirty pages.     Return: %0 on success, negative error code otherwise. ", "bool filemap_range_has_page(struct address_space *mapping,   loff_t start_byte, loff_t end_byte)": "filemap_range_has_page - check if a page exists in range.   @mapping:           address space within which to check   @start_byte:        offset in bytes where the range starts   @end_byte:          offset in bytes where the range ends (inclusive)     Find at least one page in the range supplied, usually used to check if   direct writing in this range will trigger a writeback.     Return: %true if at least one page exists in the specified range,   %false otherwise. ", "int filemap_fdatawait_range(struct address_space *mapping, loff_t start_byte,    loff_t end_byte)": "filemap_get_folios_tag(mapping, &index, end,PAGECACHE_TAG_WRITEBACK, &fbatch);if (!nr_folios)break;for (i = 0; i < nr_folios; i++) {struct folio  folio = fbatch.folios[i];folio_wait_writeback(folio);folio_clear_error(folio);}folio_batch_release(&fbatch);cond_resched();}}     filemap_fdatawait_range - wait for writeback to complete   @mapping:address space structure to wait for   @start_byte:offset in bytes where the range starts   @end_byte:offset in bytes where the range ends (inclusive)     Walk the list of under-writeback pages of the given address space   in the given range and wait for all of them.  Check error status of   the address space and return it.     Since the error status of the address space is cleared by this function,   callers are responsible for checking the return value and handling andor   reporting the error.     Return: error status of the address space. ", "int filemap_fdatawait_range_keep_errors(struct address_space *mapping,loff_t start_byte, loff_t end_byte)": "filemap_fdatawait_range_keep_errors - wait for writeback to complete   @mapping:address space structure to wait for   @start_byte:offset in bytes where the range starts   @end_byte:offset in bytes where the range ends (inclusive)     Walk the list of under-writeback pages of the given address space in the   given range and wait for all of them.  Unlike filemap_fdatawait_range(),   this function does not clear error status of the address space.     Use this function if callers don't handle errors themselves.  Expected   call sites are system-wide  filesystem-wide data flushers: e.g. sync(2),   fsfreeze(8) ", "int file_fdatawait_range(struct file *file, loff_t start_byte, loff_t end_byte)": "file_fdatawait_range - wait for writeback to complete   @file:file pointing to address space structure to wait for   @start_byte:offset in bytes where the range starts   @end_byte:offset in bytes where the range ends (inclusive)     Walk the list of under-writeback pages of the address space that file   refers to, in the given range and wait for all of them.  Check error   status of the address space vs. the file->f_wb_err cursor and return it.     Since the error status of the file is advanced by this function,   callers are responsible for checking the return value and handling andor   reporting the error.     Return: error status of the address space vs. the file->f_wb_err cursor. ", "int filemap_fdatawait_keep_errors(struct address_space *mapping)": "file_check_and_advance_wb_err(file);}EXPORT_SYMBOL(file_fdatawait_range);     filemap_fdatawait_keep_errors - wait for writeback without clearing errors   @mapping: address space structure to wait for     Walk the list of under-writeback pages of the given address space   and wait for all of them.  Unlike filemap_fdatawait(), this function   does not clear error status of the address space.     Use this function if callers don't handle errors themselves.  Expected   call sites are system-wide  filesystem-wide data flushers: e.g. sync(2),   fsfreeze(8)     Return: error status of the address space. ", "int filemap_write_and_wait_range(struct address_space *mapping, loff_t lstart, loff_t lend)": "filemap_write_and_wait_range - write out & wait on a file range   @mapping:the address_space for the pages   @lstart:offset in bytes where the range starts   @lend:offset in bytes where the range ends (inclusive)     Write out and wait upon file offsets lstart->lend, inclusive.     Note that @lend is inclusive (describes the last byte to be written) so   that this function can be used to write to the very end-of-file (end = -1).     Return: error status of the address space. ", "int file_write_and_wait_range(struct file *file, loff_t lstart, loff_t lend)": "file_write_and_wait_range - write out & wait on a file range   @file:file pointing to address_space with pages   @lstart:offset in bytes where the range starts   @lend:offset in bytes where the range ends (inclusive)     Write out and wait upon file offsets lstart->lend, inclusive.     Note that @lend is inclusive (describes the last byte to be written) so   that this function can be used to write to the very end-of-file (end = -1).     After writing out and waiting on the data, we check and advance the   f_wb_err cursor to the latest value, and return any errors detected there.     Return: %0 on success, negative error code otherwise. ", "void filemap_invalidate_lock_two(struct address_space *mapping1, struct address_space *mapping2)": "filemap_invalidate_lock_two - lock invalidate_lock for two mappings     Lock exclusively invalidate_lock of any passed mapping that is not NULL.     @mapping1: the first mapping to lock   @mapping2: the second mapping to lock ", "void filemap_invalidate_unlock_two(struct address_space *mapping1,   struct address_space *mapping2)": "filemap_invalidate_unlock_two - unlock invalidate_lock for two mappings     Unlock exclusive invalidate_lock of any passed mapping that is not NULL.     @mapping1: the first mapping to unlock   @mapping2: the second mapping to unlock ", "smp_store_release(&wait->flags, flags | WQ_FLAG_WOKEN);wake_up_state(wait->private, mode);/* * Ok, we have successfully done what we're waiting for, * and we can unconditionally remove the wait entry. * * Note that this pairs with the \"finish_wait()\" in the * waiter, and has to be the absolute last thing we do. * After this list_del_init(&wait->entry) the wait entry * might be de-allocated and the process might even have * exited. ": "folio_wait_bit_common(). ", "void folio_unlock(struct folio *folio)": "folio_unlock - Unlock a locked folio.   @folio: The folio.     Unlocks the folio and wakes up any thread sleeping on the page lock.     Context: May be called from interrupt or process context.  May not be   called from NMI context. ", "void folio_end_private_2(struct folio *folio)": "folio_end_private_2 - Clear PG_private_2 and wake any waiters.   @folio: The folio.     Clear the PG_private_2 bit on a folio and wake up any sleepers waiting for   it.  The folio reference held for PG_private_2 being set is released.     This is, for example, used when a netfs folio is being written to a local   disk cache, thereby allowing writes to the cache for the same folio to be   serialised. ", "void folio_wait_private_2(struct folio *folio)": "folio_wait_private_2 - Wait for PG_private_2 to be cleared on a folio.   @folio: The folio to wait on.     Wait for PG_private_2 (aka PG_fscache) to be cleared on a folio. ", "int folio_wait_private_2_killable(struct folio *folio)": "folio_wait_private_2_killable - Wait for PG_private_2 to be cleared on a folio.   @folio: The folio to wait on.     Wait for PG_private_2 (aka PG_fscache) to be cleared on a folio or until a   fatal signal is received by the calling task.     Return:   - 0 if successful.   - -EINTR if a fatal signal was encountered. ", "void folio_end_writeback(struct folio *folio)": "folio_end_writeback - End writeback against a folio.   @folio: The folio. ", "SHARED,/* Hold ref to page and check the bit when woken, like * folio_wait_writeback() waiting on PG_writeback. ": "__folio_lock() waiting on then setting PG_locked. ", "pgoff_t page_cache_next_miss(struct address_space *mapping,     pgoff_t index, unsigned long max_scan)": "page_cache_next_miss() - Find the next gap in the page cache.   @mapping: Mapping.   @index: Index.   @max_scan: Maximum range to search.     Search the range [index, min(index + max_scan - 1, ULONG_MAX)] for the   gap with the lowest index.     This function may be called under the rcu_read_lock.  However, this will   not atomically search a snapshot of the cache at a single point in time.   For example, if a gap is created at index 5, then subsequently a gap is   created at index 10, page_cache_next_miss covering both indices may   return 10 if called under the rcu_read_lock.     Return: The index of the gap if found, otherwise an index outside the   range specified (in which case 'return - index >= max_scan' will be true).   In the rare case of index wrap-around, 0 will be returned. ", "pgoff_t page_cache_prev_miss(struct address_space *mapping,     pgoff_t index, unsigned long max_scan)": "page_cache_prev_miss() - Find the previous gap in the page cache.   @mapping: Mapping.   @index: Index.   @max_scan: Maximum range to search.     Search the range [max(index - max_scan + 1, 0), index] for the   gap with the highest index.     This function may be called under the rcu_read_lock.  However, this will   not atomically search a snapshot of the cache at a single point in time.   For example, if a gap is created at index 10, then subsequently a gap is   created at index 5, page_cache_prev_miss() covering both indices may   return 5 if called under the rcu_read_lock.     Return: The index of the gap if found, otherwise an index outside the   range specified (in which case 'index - return >= max_scan' will be true).   In the rare case of wrap-around, ULONG_MAX will be returned. ", "struct folio *__filemap_get_folio(struct address_space *mapping, pgoff_t index,int fgp_flags, gfp_t gfp)": "__filemap_get_folio - Find and get a reference to a folio.   @mapping: The address_space to search.   @index: The page index.   @fgp_flags: %FGP flags modify how the folio is returned.   @gfp: Memory allocation flags to use if %FGP_CREAT is specified.     Looks up the page cache entry at @mapping & @index.     @fgp_flags can be zero or more of these flags:       %FGP_ACCESSED - The folio will be marked accessed.     %FGP_LOCK - The folio is returned locked.     %FGP_CREAT - If no page is present then a new page is allocated using     @gfp and added to the page cache and the VM's LRU list.     The page is returned locked and with an increased refcount.     %FGP_FOR_MMAP - The caller wants to do its own locking dance if the     page is already in cache.  If the page was allocated, unlock it before     returning so the caller can do the same dance.     %FGP_WRITE - The page will be written to by the caller.     %FGP_NOFS - __GFP_FS will get cleared in gfp.     %FGP_NOWAIT - Don't get blocked by page lock.     %FGP_STABLE - Wait for the folio to be stable (finished writeback)     If %FGP_LOCK or %FGP_CREAT are specified then the function may sleep even   if the %GFP flags specified for %FGP_CREAT are atomic.     If there is a page cache page, it is returned with an increased refcount.     Return: The found folio or an ERR_PTR() otherwise. ", "unsigned filemap_get_folios_contig(struct address_space *mapping,pgoff_t *start, pgoff_t end, struct folio_batch *fbatch)": "filemap_get_folios_contig - Get a batch of contiguous folios   @mapping:The address_space to search   @start:The starting page index   @end:The final page index (inclusive)   @fbatch:The batch to fill     filemap_get_folios_contig() works exactly like filemap_get_folios(),   except the returned folios are guaranteed to be contiguous. This may   not return all contiguous folios if the batch gets filled up.     Return: The number of folios found.   Also update @start to be positioned for traversal of the next folio. ", "ssize_tgeneric_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)": "generic_file_read_iter - generic filesystem read routine   @iocb:kernel IO control block   @iter:destination for the data read     This is the \"read_iter()\" routine for all filesystems   that can use the page cache directly.     The IOCB_NOWAIT flag in iocb->ki_flags indicates that -EAGAIN shall   be returned when no data can be read without waiting for IO requests   to complete; it doesn't prevent readahead.     The IOCB_NOIO flag in iocb->ki_flags indicates that no new IO   requests shall be made for the read or for readahead.  When no data   can be read, -EAGAIN shall be returned.  When readahead would be   triggered, a partial, possibly empty read shall be returned.     Return:     number of bytes copied, even for partial reads     negative error code (or 0 if IOCB_NOIO) if nothing was read ", "ssize_t filemap_splice_read(struct file *in, loff_t *ppos,    struct pipe_inode_info *pipe,    size_t len, unsigned int flags)": "filemap_splice_read -  Splice data from a file's pagecache into a pipe   @in: The file to read from   @ppos: Pointer to the file position to read from   @pipe: The pipe to splice into   @len: The amount to splice   @flags: The SPLICE_F_  flags     This function gets folios from a file's pagecache and splices them into the   pipe.  Readahead will be called as necessary to fill more folios.  This may   be used for blockdevs also.     Return: On success, the number of bytes read will be returned and  @ppos   will be updated if appropriate; 0 will be returned if there is no more data   to be read; -EAGAIN will be returned if the pipe had no space, and some   other negative error code will be returned on error.  A short read may occur   if the pipe has insufficient space, we reach the end of the data or we hit a   hole. ", "static void page_cache_delete(struct address_space *mapping,   struct folio *folio, void *shadow)": "generic_perform_write)      ->mmap_lock(fault_in_readable->do_page_fault)      bdi->wb.list_lock      sb_lock(fsfs-writeback.c)      ->i_pages lock(__sync_single_inode)      ->i_mmap_rwsem      ->anon_vma.lock(vma_merge)      ->anon_vma.lock      ->page_table_lock or pte_lock(anon_vma_prepare and various)      ->page_table_lock or pte_lock      ->swap_lock(try_to_unmap_one)      ->private_lock(try_to_unmap_one)      ->i_pages lock(try_to_unmap_one)      ->lruvec->lru_lock(follow_page->mark_page_accessed)      ->lruvec->lru_lock(check_pte_range->isolate_lru_page)      ->private_lock(page_remove_rmap->set_page_dirty)      ->i_pages lock(page_remove_rmap->set_page_dirty)      bdi.wb->list_lock(page_remove_rmap->set_page_dirty)      ->inode->i_lock(page_remove_rmap->set_page_dirty)      ->memcg->move_lock(page_remove_rmap->folio_memcg_lock)      bdi.wb->list_lock(zap_pte_range->set_page_dirty)      ->inode->i_lock(zap_pte_range->set_page_dirty)      ->private_lock(zap_pte_range->block_dirty_folio)     ->i_mmap_rwsem     ->tasklist_lock            (memory_failure, collect_procs_ao) ", "if (!pte_none(ptep_get(vmf->pte)))goto unlock;/* We're about to handle the fault ": "filemap_map_pages(struct vm_fault  vmf,     pgoff_t start_pgoff, pgoff_t end_pgoff){struct vm_area_struct  vma = vmf->vma;struct file  file = vma->vm_file;struct address_space  mapping = file->f_mapping;pgoff_t last_pgoff = start_pgoff;unsigned long addr;XA_STATE(xas, &mapping->i_pages, start_pgoff);struct folio  folio;struct page  page;unsigned int mmap_miss = READ_ONCE(file->f_ra.mmap_miss);vm_fault_t ret = 0;rcu_read_lock();folio = first_map_page(mapping, &xas, end_pgoff);if (!folio)goto out;if (filemap_map_pmd(vmf, folio, start_pgoff)) {ret = VM_FAULT_NOPAGE;goto out;}addr = vma->vm_start + ((start_pgoff - vma->vm_pgoff) << PAGE_SHIFT);vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, addr, &vmf->ptl);if (!vmf->pte) {folio_unlock(folio);folio_put(folio);goto out;}do {again:page = folio_file_page(folio, xas.xa_index);if (PageHWPoison(page))goto unlock;if (mmap_miss > 0)mmap_miss--;addr += (xas.xa_index - last_pgoff) << PAGE_SHIFT;vmf->pte += xas.xa_index - last_pgoff;last_pgoff = xas.xa_index;    NOTE: If there're PTE markers, we'll leave them to be   handled in the specific fault path, and it'll prohibit the   fault-around logic. ", "folio_mark_dirty(folio);folio_wait_stable(folio);out:sb_end_pagefault(mapping->host->i_sb);return ret;}const struct vm_operations_struct generic_file_vm_ops = ": "filemap_page_mkwrite(struct vm_fault  vmf){struct address_space  mapping = vmf->vma->vm_file->f_mapping;struct folio  folio = page_folio(vmf->page);vm_fault_t ret = VM_FAULT_LOCKED;sb_start_pagefault(mapping->host->i_sb);file_update_time(vmf->vma->vm_file);folio_lock(folio);if (folio->mapping != mapping) {folio_unlock(folio);ret = VM_FAULT_NOPAGE;goto out;}    We mark the folio dirty already here so that when freeze is in   progress, we are guaranteed that writeback during freezing will   see the dirty folio and writeprotect it again. ", "int generic_file_readonly_mmap(struct file *file, struct vm_area_struct *vma)": "generic_file_mmap(struct file  file, struct vm_area_struct  vma){struct address_space  mapping = file->f_mapping;if (!mapping->a_ops->read_folio)return -ENOEXEC;file_accessed(file);vma->vm_ops = &generic_file_vm_ops;return 0;}    This is for filesystems which do not implement ->writepage. ", "EXPORT_SYMBOL(filemap_page_mkwrite);EXPORT_SYMBOL(generic_file_mmap);EXPORT_SYMBOL(generic_file_readonly_mmap": "generic_file_readonly_mmap(struct file  file, struct vm_area_struct  vma){if ((vma->vm_flags & VM_SHARED) && (vma->vm_flags & VM_MAYWRITE))return -EINVAL;return generic_file_mmap(file, vma);}#elsevm_fault_t filemap_page_mkwrite(struct vm_fault  vmf){return VM_FAULT_SIGBUS;}int generic_file_mmap(struct file  file, struct vm_area_struct  vma){return -ENOSYS;}int generic_file_readonly_mmap(struct file  file, struct vm_area_struct  vma){return -ENOSYS;}#endif   CONFIG_MMU ", "return ERR_PTR(err);}goto filler;}if (folio_test_uptodate(folio))goto out;if (!folio_trylock(folio)) ": "read_cache_folio(struct address_space  mapping,pgoff_t index, filler_t filler, struct file  file, gfp_t gfp){struct folio  folio;int err;if (!filler)filler = mapping->a_ops->read_folio;repeat:folio = filemap_get_folio(mapping, index);if (IS_ERR(folio)) {folio = filemap_alloc_folio(gfp, 0);if (!folio)return ERR_PTR(-ENOMEM);err = filemap_add_folio(mapping, folio, index, gfp);if (unlikely(err)) {folio_put(folio);if (err == -EEXIST)goto repeat;  Presumably ENOMEM for xarray node ", "struct folio *mapping_read_folio_gfp(struct address_space *mapping,pgoff_t index, gfp_t gfp)": "mapping_read_folio_gfp - Read into page cache, using specified allocation flags.   @mapping:The address_space for the folio.   @index:The index that the allocated folio will contain.   @gfp:The page allocator flags to use if allocating.     This is the same as \"read_cache_folio(mapping, index, NULL, NULL)\", but with   any new memory allocations done using the specified allocation flags.     The most likely error from this function is EIO, but ENOMEM is   possible and so is EINTR.  If ->read_folio returns another error,   that will be returned to the caller.     The function expects mapping->invalidate_lock to be already held.     Return: Uptodate folio on success, ERR_PTR() on failure. ", "struct page *read_cache_page_gfp(struct address_space *mapping,pgoff_t index,gfp_t gfp)": "read_cache_page_gfp - read into page cache, using specified page allocation flags.   @mapping:the page's address_space   @index:the page index   @gfp:the page allocator flags to use if allocating     This is the same as \"read_mapping_page(mapping, index, NULL)\", but with   any new page allocations done using the specified allocation flags.     If the page does not get brought uptodate, return -EIO.     The function expects mapping->invalidate_lock to be already held.     Return: up to date page on success, ERR_PTR() on failure. ", "written = kiocb_invalidate_pages(iocb, write_len);if (written) ": "generic_file_direct_write(struct kiocb  iocb, struct iov_iter  from){struct address_space  mapping = iocb->ki_filp->f_mapping;size_t write_len = iov_iter_count(from);ssize_t written;    If a page can not be invalidated, return 0 to fall back   to buffered write. ", "ssize_t __generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)": "generic_file_write_iter - write data to a file   @iocb:IO state structure (file, offset, etc.)   @from:iov_iter with data to write     This function does all the work needed for actually writing data to a   file. It does all basic checks, removes SUID from the file, updates   modification times and calls proper subroutines depending on whether we   do direct IO or a standard buffered write.     It expects i_rwsem to be grabbed unless we work on a block device or similar   object which does not need locking at all.     This function does  not  take care of syncing data in case of O_SYNC write.   A caller has to handle it. This is mainly due to the fact that we want to   avoid syncing under i_rwsem.     Return:     number of bytes written, even for truncated writes     negative error code if no data has been written at all ", "bool filemap_release_folio(struct folio *folio, gfp_t gfp)": "filemap_release_folio() - Release fs-specific metadata on a folio.   @folio: The folio which the kernel is trying to free.   @gfp: Memory allocation flags (and IO mode).     The address_space is trying to release any data attached to a folio   (presumably at folio->private).     This will also be called if the private_2 flag is set on a page,   indicating that the folio has other metadata associated with it.     The @gfp argument specifies whether IO may be performed to release   this page (__GFP_IO), and whether the call may block   (__GFP_RECLAIM & __GFP_FS).     Return: %true if the release was successful, otherwise %false. ", "last_addr = phys_addr + size - 1;if (!size || last_addr < phys_addr)return NULL;/* Page-align mappings ": "ioremap_prot(phys_addr_t phys_addr, size_t size,   unsigned long prot){unsigned long offset, vaddr;phys_addr_t last_addr;struct vm_struct  area;  Disallow wrap-around or zero size ", "int follow_pfn(struct vm_area_struct *vma, unsigned long address,unsigned long *pfn)": "follow_pfn - look up PFN at a user virtual address   @vma: memory mapping   @address: user virtual address   @pfn: location to store found PFN     Only IO mappings and raw PFN mappings are allowed.     Returns zero and the pfn at @pfn on success, -ve otherwise. ", "return kmalloc(size, (gfp_mask | __GFP_COMP) & ~__GFP_HIGHMEM);}EXPORT_SYMBOL(__vmalloc": "__vmalloc(unsigned long size, gfp_t gfp_mask){     You can't specify __GFP_HIGHMEM with kmalloc() since kmalloc()   returns only a logical address. ", "static struct kmem_cache *vm_region_jar;struct rb_root nommu_region_tree = RB_ROOT;DECLARE_RWSEM(nommu_region_sem);const struct vm_operations_struct generic_file_vm_ops = ": "vmalloc.h>#include <linuxbacking-dev.h>#include <linuxcompiler.h>#include <linuxmount.h>#include <linuxpersonality.h>#include <linuxsecurity.h>#include <linuxsyscalls.h>#include <linuxaudit.h>#include <linuxprintk.h>#include <linuxuaccess.h>#include <linuxuio.h>#include <asmtlb.h>#include <asmtlbflush.h>#include <asmmmu_context.h>#include \"internal.h\"void  high_memory;EXPORT_SYMBOL(high_memory);struct page  mem_map;unsigned long max_mapnr;EXPORT_SYMBOL(max_mapnr);unsigned long highest_memmap_pfn;int sysctl_nr_trim_pages = CONFIG_NOMMU_INITIAL_TRIM_EXCESS;int heap_stack_gap = 0;atomic_long_t mmap_pages_allocated;EXPORT_SYMBOL(mem_map);  list of mapped, potentially shareable regions ", "if ((unsigned long) addr + count < count)count = -(unsigned long) addr;return copy_to_iter(addr, count, iter);}/* *vmalloc  -  allocate virtually contiguous memory * *@size:allocation size * *Allocate enough pages to cover @size from the page level *allocator and map them into contiguous kernel virtual space. * *For tight control over page level allocator and protection flags *use __vmalloc() instead. ": "vmalloc_node_range(unsigned long size, unsigned long align,unsigned long start, unsigned long end, gfp_t gfp_mask,pgprot_t prot, unsigned long vm_flags, int node,const void  caller){return __vmalloc(size, gfp_mask);}void  __vmalloc_node(unsigned long size, unsigned long align, gfp_t gfp_mask,int node, const void  caller){return __vmalloc(size, gfp_mask);}static void  __vmalloc_user_flags(unsigned long size, gfp_t flags){void  ret;ret = __vmalloc(size, flags);if (ret) {struct vm_area_struct  vma;mmap_write_lock(current->mm);vma = find_vma(current->mm, (unsigned long)ret);if (vma)vm_flags_set(vma, VM_USERMAP);mmap_write_unlock(current->mm);}return ret;}void  vmalloc_user(unsigned long size){return __vmalloc_user_flags(size, GFP_KERNEL | __GFP_ZERO);}EXPORT_SYMBOL(vmalloc_user);struct page  vmalloc_to_page(const void  addr){return virt_to_page(addr);}EXPORT_SYMBOL(vmalloc_to_page);unsigned long vmalloc_to_pfn(const void  addr){return page_to_pfn(virt_to_page(addr));}EXPORT_SYMBOL(vmalloc_to_pfn);long vread_iter(struct iov_iter  iter, const char  addr, size_t count){  Don't allow overflow ", "return kmalloc(size, (gfp_mask | __GFP_COMP) & ~__GFP_HIGHMEM);}EXPORT_SYMBOL(__vmalloc);void *__vmalloc_node_range(unsigned long size, unsigned long align,unsigned long start, unsigned long end, gfp_t gfp_mask,pgprot_t prot, unsigned long vm_flags, int node,const void *caller)": "vmap_area_list);void vfree(const void  addr){kfree(addr);}EXPORT_SYMBOL(vfree);void  __vmalloc(unsigned long size, gfp_t gfp_mask){     You can't specify __GFP_HIGHMEM with kmalloc() since kmalloc()   returns only a logical address. ", "return page_size(page);}/** * follow_pfn - look up PFN at a user virtual address * @vma: memory mapping * @address: user virtual address * @pfn: location to store found PFN * * Only IO mappings and raw PFN mappings are allowed. * * Returns zero and the pfn at @pfn on success, -ve otherwise. ": "find_vma(current->mm, (unsigned long)objp);if (vma)return vma->vm_end - vma->vm_start;}    The ksize() function is only guaranteed to work for pointers   returned by kmalloc(). So handle arbitrary pointers here. ", "void __noreturn usercopy_abort(const char *name, const char *detail,       bool to_user, unsigned long offset,       unsigned long len)": "__check_object_size() function. Normal stack buffer usage should never   trip the checks, and kernel text addressing will always trip the check.   For cache objects, it is checking that only the whitelisted range of   bytes for a given cache is being accessed (via the cache's usersize and   useroffset fields). To adjust a cache whitelist, use the usercopy-aware   kmem_cache_create_usercopy() function to create the cache (and   carefully audit the whitelist range). ", "addr = kasan_reset_tag(addr);/* Skip KFENCE memory if called explicitly outside of sl*b. ": "kasan_poison(const void  addr, size_t size, u8 value, bool init){void  shadow_start,  shadow_end;if (!kasan_arch_is_ready())return;    Perform shadow offset calculation based on untagged address, as   some of the callers (e.g. kasan_poison_object_data) pass tagged   addresses to this function. ", "/* * The redzone has byte-level precision for the generic mode. * Partially poison the last object granule to cover the unaligned * part of the redzone. ": "__kasan_kmalloc(struct kmem_cache  cache,const void  object, size_t size, gfp_t flags){unsigned long redzone_start;unsigned long redzone_end;if (gfpflags_allow_blocking(flags))kasan_quarantine_reduce();if (unlikely(object == NULL))return NULL;if (is_kfence_address(kasan_reset_tag(object)))return (void  )object;    The object has already been unpoisoned by kasan_slab_alloc() for   kmalloc() or by kasan_krealloc() for krealloc(). ", " false);return;}kmsan_enter_runtime();__memcpy(shadow_ptr_for(dst), shadow_ptr_for(src), PAGE_SIZE);__memcpy(origin_ptr_for(dst), origin_ptr_for(src), PAGE_SIZE);kmsan_leave_runtime();}EXPORT_SYMBOL(kmsan_copy_page_meta": "kmsan_copy_page_meta(struct page  dst, struct page  src){if (!kmsan_enabled || kmsan_in_runtime())return;if (!dst || !page_has_metadata(dst))return;if (!src || !page_has_metadata(src)) {kmsan_internal_unpoison_memory(page_address(dst), PAGE_SIZE,        checked", " false);}EXPORT_SYMBOL(__msan_metadata_ptr_for_load_n": "__msan_metadata_ptr_for_load_n(void  addr,uintptr_t size);struct shadow_origin_ptr __msan_metadata_ptr_for_load_n(void  addr,uintptr_t size){return get_shadow_origin_ptr(addr, size,  store", " true);}EXPORT_SYMBOL(__msan_metadata_ptr_for_store_n": "__msan_metadata_ptr_for_store_n(void  addr, uintptr_t size);struct shadow_origin_ptr __msan_metadata_ptr_for_store_n(void  addr, uintptr_t size){return get_shadow_origin_ptr(addr, size,  store", "void __msan_instrument_asm_store(void *addr, uintptr_t size);void __msan_instrument_asm_store(void *addr, uintptr_t size)": "__msan_instrument_asm_store() may be called for inline assembly code when   entering or leaving IRQ. We omit the check for kmsan_in_runtime() to ensure   the memory written to in these cases is also marked as initialized. ", "return result;if (!kmsan_enabled || kmsan_in_runtime())return result;kmsan_enter_runtime();kmsan_internal_memmove_metadata(dst, (void *)src, n);kmsan_leave_runtime();set_retval_metadata(shadow, origin);return result;}EXPORT_SYMBOL(__msan_memmove": "__msan_memmove(void  dst, const void  src, uintptr_t n);void  __msan_memmove(void  dst, const void  src, uintptr_t n){depot_stack_handle_t origin;void  result;u64 shadow;get_param0_metadata(&shadow, &origin);result = __memmove(dst, src, n);if (!n)  Some people call memmove() with zero length. ", "return result;if (!kmsan_enabled || kmsan_in_runtime())return result;kmsan_enter_runtime();/* Using memmove instead of memcpy doesn't affect correctness. ": "__msan_memcpy(void  dst, const void  src, uintptr_t n);void  __msan_memcpy(void  dst, const void  src, uintptr_t n){depot_stack_handle_t origin;void  result;u64 shadow;get_param0_metadata(&shadow, &origin);result = __memcpy(dst, src, n);if (!n)  Some people call memcpy() with zero length. ", "kmsan_internal_unpoison_memory(dst, n, /*checked": "__msan_memset(void  dst, int c, uintptr_t n);void  __msan_memset(void  dst, int c, uintptr_t n){depot_stack_handle_t origin;void  result;u64 shadow;get_param0_metadata(&shadow, &origin);result = __memset(dst, c, n);if (!kmsan_enabled || kmsan_in_runtime())return result;kmsan_enter_runtime();    Clang doesn't pass parameter metadata here, so it is impossible to   use shadow of @c to set up the shadow for @dst. ", "kmsan_enter_runtime();ret = kmsan_internal_chain_origin(origin);kmsan_leave_runtime();user_access_restore(ua_flags);return ret;}EXPORT_SYMBOL(__msan_chain_origin": "__msan_chain_origin(depot_stack_handle_t origin);depot_stack_handle_t __msan_chain_origin(depot_stack_handle_t origin){depot_stack_handle_t ret = 0;unsigned long ua_flags;if (!kmsan_enabled || kmsan_in_runtime())return ret;ua_flags = user_access_save();  Creating new origins may allocate memory. ", "if (IS_ENABLED(CONFIG_UNWINDER_FRAME_POINTER))entries[3] = (u64)__builtin_return_address(1);elseentries[3] = 0;/* stack_depot_save() may allocate memory. ": "__msan_poison_alloca(void  address, uintptr_t size, char  descr);void __msan_poison_alloca(void  address, uintptr_t size, char  descr){depot_stack_handle_t handle;unsigned long entries[4];unsigned long ua_flags;if (!kmsan_enabled || kmsan_in_runtime())return;ua_flags = user_access_save();entries[0] = KMSAN_ALLOCA_MAGIC_ORIGIN;entries[1] = (u64)descr;entries[2] = (u64)__builtin_return_address(0);    With frame pointers enabled, it is possible to quickly fetch the   second frame of the caller stack without calling the unwinder.   Without them, simply do not bother. ", " true);kmsan_leave_runtime();}EXPORT_SYMBOL(__msan_unpoison_alloca": "__msan_unpoison_alloca(void  address, uintptr_t size);void __msan_unpoison_alloca(void  address, uintptr_t size){if (!kmsan_enabled || kmsan_in_runtime())return;kmsan_enter_runtime();kmsan_internal_unpoison_memory(address, size,  checked", " 0, /*size": "__msan_warning(u32 origin);void __msan_warning(u32 origin){if (!kmsan_enabled || kmsan_in_runtime())return;kmsan_enter_runtime();kmsan_report(origin,  address", "/* copy_to_user() may copy zero bytes. No need to check. ": "kmsan_copy_to_user(void __user  to, const void  from, size_t to_copy,size_t left){unsigned long ua_flags;if (!kmsan_enabled || kmsan_in_runtime())return;    At this point we've copied the memory already. It's hard to check it   before copying, as the size of actually copied buffer is unknown. ", "kmsan_internal_poison_memory((void *)address, size, flags,     KMSAN_POISON_NOCHECK);kmsan_leave_runtime();}EXPORT_SYMBOL(kmsan_poison_memory": "kmsan_poison_memory(const void  address, size_t size, gfp_t flags){if (!kmsan_enabled || kmsan_in_runtime())return;kmsan_enter_runtime();  The users may want to poisonunpoison random memory. ", "kmsan_internal_unpoison_memory((void *)address, size,       KMSAN_POISON_NOCHECK);kmsan_leave_runtime();user_access_restore(ua_flags);}EXPORT_SYMBOL(kmsan_unpoison_memory": "kmsan_unpoison_memory(const void  address, size_t size){unsigned long ua_flags;if (!kmsan_enabled || kmsan_in_runtime())return;ua_flags = user_access_save();kmsan_enter_runtime();  The users may want to poisonunpoison random memory. ", " 0,   REASON_ANY);}EXPORT_SYMBOL(kmsan_check_memory": "kmsan_check_memory(const void  addr, size_t size){if (!kmsan_enabled)return;return kmsan_internal_check_memory((void  )addr, size,  user_addr", "invalidate_mapping_pages(mapping, 0, -1);}}EXPORT_SYMBOL(invalidate_bdev": "invalidate_bdev(struct block_device  bdev){struct address_space  mapping = bdev->bd_inode->i_mapping;if (mapping->nrpages) {invalidate_bh_lrus();lru_add_drain_all();  make sure all lru add caches are flushed ", "if (size > PAGE_SIZE || size < 512 || !is_power_of_2(size))return -EINVAL;/* Size cannot be smaller than the size supported by the device ": "set_blocksize(struct block_device  bdev, int size){  Size must be a power of two, and between 512 and PAGE_SIZE ", "sb->s_blocksize = size;sb->s_blocksize_bits = blksize_bits(size);return sb->s_blocksize;}EXPORT_SYMBOL(sb_set_blocksize": "sb_set_blocksize(struct super_block  sb, int size){if (set_blocksize(sb->s_bdev, size))return 0;  If we get here, we know size is power of two   and it's value is between 512 and PAGE_SIZE ", "sb->s_blocksize = size;sb->s_blocksize_bits = blksize_bits(size);return sb->s_blocksize;}EXPORT_SYMBOL(sb_set_blocksize);int sb_min_blocksize(struct super_block *sb, int size)": "sync_blockdev(bdev);bdev->bd_inode->i_blkbits = blksize_bits(size);kill_bdev(bdev);}return 0;}EXPORT_SYMBOL(set_blocksize);int sb_set_blocksize(struct super_block  sb, int size){if (set_blocksize(sb->s_bdev, size))return 0;  If we get here, we know size is power of two   and it's value is between 512 and PAGE_SIZE ", "int freeze_bdev(struct block_device *bdev)": "thaw_bdev(). When it becomes 0, thaw_bdev() will unfreeze   actually. ", "return invalidate_inode_pages2_range(bdev->bd_inode->i_mapping,     lstart >> PAGE_SHIFT,     lend >> PAGE_SHIFT);}static void set_init_blocksize(struct block_device *bdev)": "bd_abort_claiming(bdev, truncate_bdev_range);return 0;invalidate:    Someone else has handle exclusively open. Try invalidating instead.   The 'end' argument is inclusive so the rounding is safe. ", "struct block_device *blkdev_get_by_dev(dev_t dev, blk_mode_t mode, void *holder,const struct blk_holder_ops *hops)": "blkdev_get_by_path().     CONTEXT:   Might sleep.     RETURNS:   Reference to the block_device on success, ERR_PTR(-errno) on failure. ", "bdev = &BDEV_I(inode)->bdev;if (!kobject_get_unless_zero(&bdev->bd_device.kobj))bdev = NULL;iput(inode);return bdev;}void blkdev_put_no_open(struct block_device *bdev)": "blkdev_put_whole(struct block_device  bdev){if (atomic_dec_and_test(&bdev->bd_openers))blkdev_flush_mapping(bdev);if (bdev->bd_disk->fops->release)bdev->bd_disk->fops->release(bdev->bd_disk);}static int blkdev_get_part(struct block_device  part, blk_mode_t mode){struct gendisk  disk = part->bd_disk;int ret;ret = blkdev_get_whole(bdev_whole(part), mode);if (ret)return ret;ret = -ENXIO;if (!bdev_nr_sectors(part))goto out_blkdev_put;if (!atomic_read(&part->bd_openers)) {disk->open_partitions++;set_init_blocksize(part);}atomic_inc(&part->bd_openers);return 0;out_blkdev_put:blkdev_put_whole(bdev_whole(part));return ret;}static void blkdev_put_part(struct block_device  part){struct block_device  whole = bdev_whole(part);if (atomic_dec_and_test(&part->bd_openers)) {blkdev_flush_mapping(part);whole->bd_disk->open_partitions--;}blkdev_put_whole(whole);}struct block_device  blkdev_get_no_open(dev_t dev){struct block_device  bdev;struct inode  inode;inode = ilookup(blockdev_superblock, dev);if (!inode && IS_ENABLED(CONFIG_BLOCK_LEGACY_AUTOLOAD)) {blk_request_module(dev);inode = ilookup(blockdev_superblock, dev);if (inode)pr_warn_ratelimited(\"block device autoloading is deprecated and will be removed.\\n\");}if (!inode)return NULL;  switch from the inode reference to a device mode one: ", "if (atomic_read(&bdev->bd_openers) == 1)sync_blockdev(bdev);mutex_lock(&disk->open_mutex);if (holder)bd_end_claim(bdev, holder);/* * Trigger event checking and tell drivers to flush MEDIA_CHANGE * event.  This is to ensure detection of media removal commanded * from userland - e.g. eject(1). ": "lookup_bdev(path, &dev);if (error)return ERR_PTR(error);bdev = blkdev_get_by_dev(dev, mode, holder, hops);if (!IS_ERR(bdev) && (mode & BLK_OPEN_WRITE) && bdev_read_only(bdev)) {blkdev_put(bdev, holder);return ERR_PTR(-EACCES);}return bdev;}EXPORT_SYMBOL(blkdev_get_by_path);void blkdev_put(struct block_device  bdev, void  holder){struct gendisk  disk = bdev->bd_disk;    Sync early if it looks like we're the last one.  If someone else   opens the block device between now and the decrement of bd_openers   then we did a sync that we didn't need to, but that's not the end   of the world and we want to avoid long (could be several minute)   syncs while holding the mutex. ", "shrink_dcache_sb(sb);res = invalidate_inodes(sb, kill_dirty);drop_super(sb);}invalidate_bdev(bdev);return res;}EXPORT_SYMBOL(__invalidate_device": "__invalidate_device(struct block_device  bdev, bool kill_dirty){struct super_block  sb = get_super(bdev);int res = 0;if (sb) {    no need to lock the super, get_super holds the   read mutex so the filesystem cannot go away   under us (->put_super runs with the write lock   hold). ", "void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,busy_tag_iter_fn *fn, void *priv)": "blk_mq_tagset_busy_iter - iterate over all started requests in a tag set   @tagset:Tag set to iterate over.   @fn:Pointer to the function that will be called for each started  request. @fn will be called as follows: @fn(rq, @priv,  reserved) where rq is a pointer to a request. 'reserved'  indicates whether or not @rq is a reserved request. Return  true to continue iterating tags, false to stop.   @priv:Will be passed as second argument to @fn.     We grab one request reference before calling @fn and release it after   @fn returns. ", "void blk_mq_tagset_wait_completed_request(struct blk_mq_tag_set *tagset)": "blk_mq_tagset_wait_completed_request - Wait until all scheduled request   completions have finished.   @tagset:Tag set to drain completed request     Note: This function has to be run after all IO queues are shutdown ", "u32 blk_mq_unique_tag(struct request *rq)": "blk_mq_unique_tag() - return a tag that is unique queue-wide   @rq: request for which to compute a unique tag     The tag field in struct request is unique per hardware queue but not over   all hardware queues. Hence this function that returns a tag with the   hardware context index in the upper bits and the per hardware queue tag in   the lower bits.     Note: When called for a request that is queued on a non-multiqueue request   queue, the hardware context index is set to zero. ", "int blkdev_issue_flush(struct block_device *bdev)": "blkdev_issue_flush - queue a flush   @bdev:blockdev to issue flush for     Description:      Issue a flush for the block device in question. ", "struct bio_set fs_bio_set;EXPORT_SYMBOL(fs_bio_set": "fs_bio_set is the bio_set containing bio and iovec memory pools used by   IO code that does not need private memory pools. ", "void bio_init(struct bio *bio, struct block_device *bdev, struct bio_vec *table,      unsigned short max_vecs, blk_opf_t opf)": "bio_init() with bio_uninit()   when IO has completed, or when the bio is released. ", "void bio_reset(struct bio *bio, struct block_device *bdev, blk_opf_t opf)": "bio_alloc_bioset() - the only fields that are     preserved are the ones that are initialized by bio_alloc_bioset(). See     comment in struct bio. ", "void bio_chain(struct bio *bio, struct bio *parent)": "bio_endio(__bio_chain_endio(bio));}     bio_chain - chain bio completions   @bio: the target bio   @parent: the parent bio of @bio     The caller won't have a bi_end_io called when @bio completes - instead,   @parent's bi_end_io won't be called until both @parent and @bio have   completed; the chained bio will also be freed when it completes.     The caller must not set bi_private or bi_end_io in @bio. ", "struct bio *bio_kmalloc(unsigned short nr_vecs, gfp_t gfp_mask)": "bio_kmalloc - kmalloc a bio   @nr_vecs:number of bio_vecs to allocate   @gfp_mask:   the GFP_  mask given to the slab allocator     Use kmalloc to allocate a bio (including bvecs).  The bio must be initialized   using bio_init() before use.  To free a bio returned from this function use   kfree() after calling bio_uninit().  A bio returned from this function can   be reused by calling bio_uninit() before calling bio_init() again.     Note that unlike bio_alloc() or bio_alloc_bioset() allocations from this   function are not backed by a mempool can fail.  Do not use this function   for allocations in the file system IO path.     Returns: Pointer to new bio on success, NULL on failure. ", "static inline gfp_t bvec_alloc_gfp(gfp_t gfp)": "bio_put_slab(struct bio_set  bs){struct bio_slab  bslab = NULL;unsigned int slab_size = bs_bio_slab_size(bs);mutex_lock(&bio_slab_lock);bslab = xa_load(&bio_slabs, slab_size);if (WARN(!bslab, KERN_ERR \"bio: unable to find slab!\\n\"))goto out;WARN_ON_ONCE(bslab->slab != bs->bio_slab);WARN_ON(!bslab->slab_ref);if (--bslab->slab_ref)goto out;xa_erase(&bio_slabs, slab_size);kmem_cache_destroy(bslab->slab);kfree(bslab);out:mutex_unlock(&bio_slab_lock);}void bvec_free(mempool_t  pool, struct bio_vec  bv, unsigned short nr_vecs){BUG_ON(nr_vecs > BIO_MAX_VECS);if (nr_vecs == BIO_MAX_VECS)mempool_free(bv, pool);else if (nr_vecs > BIO_INLINE_VECS)kmem_cache_free(biovec_slab(nr_vecs)->slab, bv);}    Make the first allocation restricted and don't dump info on allocation   failures, since we'll fall back to the mempool in case of failure. ", "struct bio *bio_alloc_clone(struct block_device *bdev, struct bio *bio_src,gfp_t gfp, struct bio_set *bs)": "bio_alloc_clone - clone a bio that shares the original bio's biovec   @bdev: block_device to clone onto   @bio_src: bio to clone from   @gfp: allocation priority   @bs: bio_set to allocate from     Allocate a new bio that is a clone of @bio_src. The caller owns the returned   bio, but not the actual data it points to.     The caller must ensure that the return bio is not freed before @bio_src. ", "int bio_init_clone(struct block_device *bdev, struct bio *bio,struct bio *bio_src, gfp_t gfp)": "bio_init_clone - clone a bio that shares the original bio's biovec   @bdev: block_device to clone onto   @bio: bio to clone into   @bio_src: bio to clone from   @gfp: allocation priority     Initialize a new bio in caller provided memory that is a clone of @bio_src.   The caller owns the returned bio, but not the actual data it points to.     The caller must ensure that @bio_src is not freed before @bio. ", "int bio_add_pc_page(struct request_queue *q, struct bio *bio,struct page *page, unsigned int len, unsigned int offset)": "bio_add_pc_page- attempt to add page to passthrough bio   @q: the target queue   @bio: destination bio   @page: page to add   @len: vec entry length   @offset: vec entry offset     Attempt to add a page to the bio_vec maplist. This can fail for a   number of reasons, such as the bio being full or target block device   limitations. The target block device must allow bio's up to PAGE_SIZE,   so it is always possible to add a single page to an empty bio.     This should only be used by passthrough bios. ", "void __bio_add_page(struct bio *bio, struct page *page,unsigned int len, unsigned int off)": "bio_add_page - add page(s) to a bio in a new segment   @bio: destination bio   @page: start page to add   @len: length of the data to add, may cross pages   @off: offset of the data relative to @page, may cross pages     Add the data at @page + @off to @bio as a new bvec.  The caller must ensure   that @bio has space for another bvec. ", "bool bio_add_folio(struct bio *bio, struct folio *folio, size_t len,   size_t off)": "bio_add_folio_nofail(struct bio  bio, struct folio  folio, size_t len,  size_t off){WARN_ON_ONCE(len > UINT_MAX);WARN_ON_ONCE(off > UINT_MAX);__bio_add_page(bio, &folio->page, len, off);}     bio_add_folio - Attempt to add part of a folio to a bio.   @bio: BIO to add to.   @folio: Folio to add.   @len: How many bytes from the folio to add.   @off: First byte in this folio to add.     Filesystems that use folios can call this function instead of calling   bio_add_page() for each page in the folio.  If @off is bigger than   PAGE_SIZE, this function can create a bio_vec that starts in a page   after the bv_page.  BIOs do not support folios that are 4GiB or larger.     Return: Whether the addition was successful. ", "int submit_bio_wait(struct bio *bio)": "submit_bio_wait_endio(struct bio  bio){complete(bio->bi_private);}     submit_bio_wait - submit a bio, and wait until it completes   @bio: The &struct bio which describes the IO     Simple wrapper around submit_bio(). Returns 0 on success, or the error from   bio_endio() on failure.     WARNING: Unlike to how submit_bio() is usually used, this function does not   result in bio reference to be consumed. The caller must drop the reference   on his own. ", "struct bio *bio_split(struct bio *bio, int sectors,      gfp_t gfp, struct bio_set *bs)": "bio_split - split a bio   @bio:bio to split   @sectors:number of sectors to split from the front of @bio   @gfp:gfp mask   @bs:bio set to allocate from     Allocates and returns a new bio which represents @sectors from the start of   @bio, and updates @bio to represent the remaining sectors.     Unless this is a discard request the newly allocated bio will point   to @bio's bi_io_vec. It is the caller's responsibility to ensure that   neither @bio nor @bs are freed before the split bio. ", "void bioset_exit(struct bio_set *bs)": "bioset_init()     May be called on a zeroed but uninitialized bioset (i.e. allocated with   kzalloc()). ", "bool disk_check_media_change(struct gendisk *disk)": "disk_check_media_change - check if a removable media has been changed   @disk: gendisk to check     Check whether a removable media has been changed, and attempt to free all   dentries and inodes and invalidates all block device page cache entries in   that case.     Returns %true if the media has changed, or %false if not. ", "int blk_rq_count_integrity_sg(struct request_queue *q, struct bio *bio)": "blk_rq_count_integrity_sg - Count number of integrity scatterlist elements   @q:request queue   @bio:bio with integrity metadata attached     Description: Returns the number of elements required in a   scatterlist corresponding to the integrity metadata in a bio. ", "int blk_rq_map_integrity_sg(struct request_queue *q, struct bio *bio,    struct scatterlist *sglist)": "blk_rq_map_integrity_sg - Map integrity metadata into a scatterlist   @q:request queue   @bio:bio with integrity metadata attached   @sglist:target scatterlist     Description: Map the integrity vectors in request into a   scatterlist.  The scatterlist must be big enough to hold all   elements.  I.e. sized using blk_rq_count_integrity_sg(). ", "int blk_integrity_compare(struct gendisk *gd1, struct gendisk *gd2)": "blk_integrity_compare - Compare integrity profile of two disks   @gd1:Disk to compare   @gd2:Disk to compare     Description: Meta-devices like DM and MD need to verify that all   sub-devices use the same integrity format before advertising to   upper layers that they can sendreceive integrity metadata.  This   function can be used to check whether two gendisk devices have   compatible integrity formats. ", "void blk_integrity_register(struct gendisk *disk, struct blk_integrity *template)": "blk_integrity_register - Register a gendisk as being integrity-capable   @disk:struct gendisk pointer to make integrity-aware   @template:block integrity profile to register     Description: When a device needs to advertise itself as being able to   sendreceive integrity metadata it must use this function to register   the capability with the block layer. The template is a blk_integrity   struct with values appropriate for the underlying hardware. See   Documentationblockdata-integrity.rst. ", "void blk_integrity_unregister(struct gendisk *disk)": "blk_integrity_unregister - Unregister block integrity profile   @disk:disk whose integrity profile to unregister     Description: This function unregisters the integrity capability from   a block device. ", "void blk_pm_runtime_init(struct request_queue *q, struct device *dev)": "blk_pm_runtime_init - Block layer runtime PM initialization routine   @q: the queue of the device   @dev: the device the queue belongs to     Description:      Initialize runtime-PM-related fields for @q and start auto suspend for      @dev. Drivers that want to take advantage of request-based runtime PM      should call this function after @dev has been initialized, and its      request queue @q has been allocated, and runtime PM for it can not happen      yet(either due to disabledforbidden or its usage_count > 0). In most      cases, driver should call this function before any IO has taken place.        This function takes care of setting up using auto suspend for the device,      the autosuspend delay is set to -1 to make runtime suspend impossible      until an updated value is either set by user or by driver. Drivers do      not need to touch other autosuspend settings.        The block layer runtime PM is request based, so only works for drivers      that use request as their IO unit instead of those directly use bio's. ", "int blk_pre_runtime_suspend(struct request_queue *q)": "blk_pre_runtime_suspend - Pre runtime suspend check   @q: the queue of the device     Description:      This function will check if runtime suspend is allowed for the device      by examining if there are any requests pending in the queue. If there      are requests pending, the device can not be runtime suspended; otherwise,      the queue's status will be updated to SUSPENDING and the driver can      proceed to suspend the device.        For the not allowed case, we mark last busy for the device so that      runtime PM core will try to autosuspend it some time later.        This function should be called near the start of the device's      runtime_suspend callback.     Return:      0- OK to runtime suspend the device      -EBUSY- Device should not be runtime suspended ", "void blk_post_runtime_suspend(struct request_queue *q, int err)": "blk_post_runtime_suspend - Post runtime suspend processing   @q: the queue of the device   @err: return value of the device's runtime_suspend function     Description:      Update the queue's runtime status according to the return value of the      device's runtime suspend function and mark last busy for the device so      that PM core will try to auto suspend the device at a later time.        This function should be called near the end of the device's      runtime_suspend callback. ", "void blk_pre_runtime_resume(struct request_queue *q)": "blk_pre_runtime_resume - Pre runtime resume processing   @q: the queue of the device     Description:      Update the queue's runtime status to RESUMING in preparation for the      runtime resume of the device.        This function should be called near the start of the device's      runtime_resume callback. ", "void blk_post_runtime_resume(struct request_queue *q)": "blk_set_runtime_active()      to do the real work of restarting the queue.  It does this regardless of      whether the device's runtime-resume succeeded; even if it failed the      driver or error handler will need to communicate with the device.        This function should be called near the end of the device's      runtime_resume callback. ", "struct bio_integrity_payload *bio_integrity_alloc(struct bio *bio,  gfp_t gfp_mask,  unsigned int nr_vecs)": "bio_integrity_alloc - Allocate integrity payload and attach it to bio   @bio:bio to attach integrity metadata to   @gfp_mask:Memory allocation mask   @nr_vecs:Number of integrity metadata scatter-gather elements     Description: This function prepares a bio for attaching integrity   metadata.  nr_vecs specifies the maximum number of pages containing   integrity metadata that can be attached. ", "int bio_integrity_add_page(struct bio *bio, struct page *page,   unsigned int len, unsigned int offset)": "bio_integrity_add_page - Attach integrity metadata   @bio:bio to update   @page:page containing integrity metadata   @len:number of bytes of integrity metadata in page   @offset:start offset within page     Description: Attach a page containing integrity metadata to bio. ", "bool bio_integrity_prep(struct bio *bio)": "bio_integrity_prep - Prepare bio for integrity IO   @bio:bio to prepare     Description:  Checks if the bio already has an integrity payload attached.   If it does, the payload has been generated by another kernel subsystem,   and we just pass it through. Otherwise allocates integrity payload.   The bio must have data direction, target device and start sector set priot   to calling.  In the WRITE case, integrity metadata will be generated using   the block device's integrity function.  In the READ case, the buffer   will be prepared for DMA and a suitable end_io handler set up. ", "void bio_integrity_trim(struct bio *bio)": "bio_integrity_trim - Trim integrity vector   @bio:bio whose integrity vector to update     Description: Used to trim the integrity vector in a cloned bio. ", "if (WARN_ON_ONCE(!bdev_discard_granularity(bdev))) ": "blkdev_issue_discard(struct block_device  bdev, sector_t sector,sector_t nr_sects, gfp_t gfp_mask, struct bio   biop){struct bio  bio =  biop;sector_t bs_mask;if (bdev_read_only(bdev))return -EPERM;if (!bdev_max_discard_sectors(bdev))return -EOPNOTSUPP;  In case the discard granularity isn't set by buggy device driver ", "int __blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,sector_t nr_sects, gfp_t gfp_mask, struct bio **biop,unsigned flags)": "blkdev_issue_zeroout - generate number of zero filed write bios   @bdev:blockdev to issue   @sector:start sector   @nr_sects:number of sectors to write   @gfp_mask:memory allocation flags (for bio_alloc)   @biop:pointer to anchor bio   @flags:controls detailed behavior     Description:    Zero-fill a block range, either using hardware offload or by explicitly    writing zeroes to the device.      If a device is using logical block provisioning, the underlying space will    not be released if %flags contains BLKDEV_ZERO_NOUNMAP.      If %flags contains BLKDEV_ZERO_NOFALLBACK, the function will return    -EOPNOTSUPP if no explicit hardware offload for zeroing is provided. ", "if (max_sectors > UINT_MAX >> SECTOR_SHIFT)max_sectors = UINT_MAX >> SECTOR_SHIFT;max_sectors &= ~bs_mask;if (max_sectors == 0)return -EOPNOTSUPP;if ((sector | nr_sects) & bs_mask)return -EINVAL;if (bdev_read_only(bdev))return -EPERM;blk_start_plug(&plug);for (;;) ": "blkdev_issue_secure_erase(struct block_device  bdev, sector_t sector,sector_t nr_sects, gfp_t gfp){sector_t bs_mask = (bdev_logical_block_size(bdev) >> 9) - 1;unsigned int max_sectors = bdev_max_secure_erase_sectors(bdev);struct bio  bio = NULL;struct blk_plug plug;int ret = 0;  make sure that \"len << SECTOR_SHIFT\" doesn't overflow ", "struct io_cq *ioc_lookup_icq(struct request_queue *q)": "ioc_lookup_icq - lookup io_cq from ioc   @q: the associated request_queue     Look up io_cq associated with @ioc - @q pair from @ioc.  Must be called   with @q->queue_lock held. ", "int __register_blkdev(unsigned int major, const char *name,void (*probe)(dev_t devt))": "__register_blkdev - register a new block device     @major: the requested major device number [1..BLKDEV_MAJOR_MAX-1]. If           @major = 0, try to allocate any unused major number.   @name: the name of the new block device as a zero terminated string   @probe: pre-devtmpfs  pre-udev callback used to create disks when their     pre-created device node is accessed. When a probe call uses     add_disk() and it fails the driver must cleanup resources. This     interface may soon be removed.     The @name must be unique within the system.     The return value depends on the @major input parameter:      - if a major device number was requested in range [1..BLKDEV_MAJOR_MAX-1]      then the function returns zero on success, or a negative error code    - if any unused major number was requested with @major = 0 parameter      then the return value is the allocated major number in range      [1..BLKDEV_MAJOR_MAX-1] or a negative error code otherwise     See Documentationadmin-guidedevices.txt for the list of allocated   major numbers.     Use register_blkdev instead for any new code. ", "int __must_check device_add_disk(struct device *parent, struct gendisk *disk, const struct attribute_group **groups)": "device_add_disk - add disk information to kernel list   @parent: parent device for the disk   @disk: per-device partitioning information   @groups: Additional per-device sysfs groups     This function registers the partitioning information in @disk   with the kernel. ", "void del_gendisk(struct gendisk *disk)": "put_disk(), which should be called after del_gendisk(), if   __device_add_disk() was used.     Drivers exist which depend on the release of the gendisk to be synchronous,   it should not be deferred.     Context: can sleep ", "void invalidate_disk(struct gendisk *disk)": "invalidate_disk - invalidate the disk   @disk: the struct gendisk to invalidate     A helper to invalidates the disk. It will clean the disk's associated   bufferpage caches and reset its internal states so that the disk   can be reused by the drivers.     Context: can sleep ", "void set_disk_ro(struct gendisk *disk, bool read_only)": "set_disk_ro_uevent(struct gendisk  gd, int ro){char event[] = \"DISK_RO=1\";char  envp[] = { event, NULL };if (!ro)event[8] = '0';kobject_uevent_env(&disk_to_dev(gd)->kobj, KOBJ_CHANGE, envp);}     set_disk_ro - set a gendisk read-only   @disk:gendisk to operate on   @read_only:%true to set the disk read-only, %false set the disk readwrite     This function is used to indicate whether a given disk device should have its   read-only flag set. set_disk_ro() is typically used by device drivers to   indicate whether the underlying physical device is write-protected. ", "void blk_queue_flag_set(unsigned int flag, struct request_queue *q)": "blk_queue_flag_set - atomically set a queue flag   @flag: flag to be set   @q: request queue ", "void blk_queue_flag_clear(unsigned int flag, struct request_queue *q)": "blk_queue_flag_clear - atomically clear a queue flag   @flag: flag to be cleared   @q: request queue ", "void blk_sync_queue(struct request_queue *q)": "submit_bio will not re-add plugging prior to calling       this function.         This function does not cancel any asynchronous activity arising       out of elevator or throttling code. That would require elevator_exit()       and blkcg_exit_queue() to be called with queue lock initialized.   ", "void blk_put_queue(struct request_queue *q)": "blk_put_queue - decrement the request_queue refcount   @q: the request_queue structure to decrement the refcount for     Decrements the refcount of the request_queue and free it when the refcount   reaches 0. ", "bool blk_get_queue(struct request_queue *q)": "blk_get_queue - increment the request_queue refcount   @q: the request_queue structure to increment the refcount for     Increment the refcount of the request_queue kobject.     Context: Any context. ", "static void __submit_bio_noacct(struct bio *bio)": "submit_bio_noacct.  If it did, we find a      non-NULL value in bio_list and re-enter the loop from the top.    - In this case we really did just take the bio of the top of the list (no      pretending) and so remove it from bio_list, and call into ->submit_bio()      again.     bio_list_on_stack[0] contains bios submitted by the current ->submit_bio.   bio_list_on_stack[1] contains bios that were submitted before the current  ->submit_bio, but that haven't been processed yet. ", "if (percpu_ref_init(&q->q_usage_counter,blk_queue_usage_counter_release,PERCPU_REF_INIT_ATOMIC, GFP_KERNEL))goto fail_stats;blk_set_default_limits(&q->limits);q->nr_requests = BLKDEV_DEFAULT_RQ;return q;fail_stats:blk_free_queue_stats(q->stats);fail_id:ida_free(&blk_queue_ida, q->id);fail_q:kmem_cache_free(blk_requestq_cachep, q);return NULL;}/** * blk_get_queue - increment the request_queue refcount * @q: the request_queue structure to increment the refcount for * * Increment the refcount of the request_queue kobject. * * Context: Any context. ": "kblockd_schedule_work(&q->timeout_work);}static void blk_timeout_work(struct work_struct  work){}struct request_queue  blk_alloc_queue(int node_id){struct request_queue  q;q = kmem_cache_alloc_node(blk_requestq_cachep, GFP_KERNEL | __GFP_ZERO,  node_id);if (!q)return NULL;q->last_merge = NULL;q->id = ida_alloc(&blk_queue_ida, GFP_KERNEL);if (q->id < 0)goto fail_q;q->stats = blk_alloc_queue_stats();if (!q->stats)goto fail_id;q->node = node_id;atomic_set(&q->nr_active_requests_shared_tags, 0);timer_setup(&q->timeout, blk_rq_timed_out_timer, 0);INIT_WORK(&q->timeout_work, blk_timeout_work);INIT_LIST_HEAD(&q->icq_list);refcount_set(&q->refs, 1);mutex_init(&q->debugfs_mutex);mutex_init(&q->sysfs_lock);mutex_init(&q->sysfs_dir_lock);mutex_init(&q->rq_qos_mutex);spin_lock_init(&q->queue_lock);init_waitqueue_head(&q->mq_freeze_wq);mutex_init(&q->mq_freeze_lock);    Init percpu_ref in atomic mode so that it's faster to shutdown.   See blk_register_queue() for details. ", "if (tsk->plug)return;plug->mq_list = NULL;plug->cached_rq = NULL;plug->nr_ios = min_t(unsigned short, nr_ios, BLK_MAX_REQUEST_COUNT);plug->rq_count = 0;plug->multiple_queues = false;plug->has_elevator = false;plug->nowait = false;INIT_LIST_HEAD(&plug->cb_list);/* * Store ordering should not be needed here, since a potential * preempt will imply a full memory barrier ": "blk_start_plug_nr_ios(struct blk_plug  plug, unsigned short nr_ios){struct task_struct  tsk = current;    If this is a nested plug, don't actually assign it. ", "BUG_ON(size < sizeof(*cb));cb = kzalloc(size, GFP_ATOMIC);if (cb) ": "blk_check_plugged(blk_plug_cb_fn unplug, void  data,      int size){struct blk_plug  plug = current->plug;struct blk_plug_cb  cb;if (!plug)return NULL;list_for_each_entry(cb, &plug->cb_list, list)if (cb->callback == unplug && cb->data == data)return cb;  Not currently on the callback list ", "void blk_start_plug(struct blk_plug *plug)": "blk_finish_plug()     is called.  However, the block layer may choose to submit requests     before a call to blk_finish_plug() if the number of queued IOs     exceeds %BLK_MAX_REQUEST_COUNT, or if the size of the IO is larger than     %BLK_PLUG_FLUSH_SIZE.  The queued IOs may also be submitted early if     the task schedules (see below).       Tracking blk_plug inside the task_struct will help with auto-flushing the     pending IO should the task end up blocking between blk_start_plug() and     blk_finish_plug(). This is important from a performance perspective, but     also ensures that we don't deadlock. For instance, if the task is blocking     for a memory allocation, memory reclaim could end up wanting to free a     page belonging to that request that is currently residing in our private     plug. By flushing the pending IO when the process goes to sleep, we avoid     this kind of deadlock. ", "struct bio *__bio_split_to_limits(struct bio *bio,  const struct queue_limits *lim,  unsigned int *nr_segs)": "bio_split_to_limits - split a bio to fit the queue limits   @bio:     bio to be split   @lim:     queue limits to split based on   @nr_segs: returns the number of segments in the returned bio     Check if @bio needs splitting based on the queue limits, and if so split off   a bio fitting the limits from the beginning of @bio and return it.  @bio is   shortened to the remainder and re-submitted.     The split bio is allocated from @q->bio_split, which is provided by the   block layer. ", "WARN_ON(nsegs > blk_rq_nr_phys_segments(rq));return nsegs;}EXPORT_SYMBOL(__blk_rq_map_sg": "__blk_rq_map_sg(struct request_queue  q, struct request  rq,struct scatterlist  sglist, struct scatterlist   last_sg){int nsegs = 0;if (rq->rq_flags & RQF_SPECIAL_PAYLOAD)nsegs = __blk_bvec_map_sg(rq->special_vec, sglist, last_sg);else if (rq->bio)nsegs = __blk_bios_map_sg(q, rq->bio, sglist, last_sg);if ( last_sg)sg_mark_end( last_sg);    Something must have been wrong if the figured number of   segment is bigger than number of req's physical segments ", "dev->cmd = kmalloc(IO_BUFFER_LENGTH, GFP_KERNEL);if (!dev->cmd)goto err_free_dev;dev->resp = kmalloc(IO_BUFFER_LENGTH, GFP_KERNEL);if (!dev->resp)goto err_free_cmd;INIT_LIST_HEAD(&dev->unlk_lst);mutex_init(&dev->dev_lock);dev->flags = 0;dev->data = data;dev->send_recv = send_recv;if (check_opal_support(dev) != 0) ": "init_opal_dev(void  data, sec_send_recv  send_recv){struct opal_dev  dev;dev = kmalloc(sizeof( dev), GFP_KERNEL);if (!dev)return NULL;    Presumably DMA-able buffers must be cache-aligned. Kmalloc makes   sure the allocated buffer is DMA-safe in that regard. ", "while (j < npages)bio_release_page(bio, pages[j++]);if (pages != stack_pages)kvfree(pages);/* couldn't stuff something into bio? ": "blk_rq_append_bio(rq, bio);if (ret)goto cleanup;return 0;cleanup:if (!map_data)bio_free_pages(bio);bio_uninit(bio);kfree(bio);out_bmd:kfree(bmd);return ret;}static void blk_mq_map_bio_put(struct bio  bio){if (bio->bi_opf & REQ_ALLOC_CACHE) {bio_put(bio);} else {bio_uninit(bio);kfree(bio);}}static struct bio  blk_rq_map_bio_alloc(struct request  rq,unsigned int nr_vecs, gfp_t gfp_mask){struct bio  bio;if (rq->cmd_flags & REQ_ALLOC_CACHE && (nr_vecs <= BIO_INLINE_VECS)) {bio = bio_alloc_bioset(NULL, nr_vecs, rq->cmd_flags, gfp_mask,&fs_bio_set);if (!bio)return NULL;} else {bio = bio_kmalloc(nr_vecs, gfp_mask);if (!bio)return NULL;bio_init(bio, NULL, bio->bi_inline_vecs, nr_vecs, req_op(rq));}return bio;}static int bio_map_user_iov(struct request  rq, struct iov_iter  iter,gfp_t gfp_mask){iov_iter_extraction_t extraction_flags = 0;unsigned int max_sectors = queue_max_hw_sectors(rq->q);unsigned int nr_vecs = iov_iter_npages(iter, BIO_MAX_VECS);struct bio  bio;int ret;int j;if (!iov_iter_count(iter))return -EINVAL;bio = blk_rq_map_bio_alloc(rq, nr_vecs, gfp_mask);if (bio == NULL)return -ENOMEM;if (blk_queue_pci_p2pdma(rq->q))extraction_flags |= ITER_ALLOW_P2PDMA;if (iov_iter_extract_will_pin(iter))bio_set_flag(bio, BIO_PAGE_PINNED);while (iov_iter_count(iter)) {struct page  stack_pages[UIO_FASTIOV];struct page   pages = stack_pages;ssize_t bytes;size_t offs;int npages;if (nr_vecs > ARRAY_SIZE(stack_pages))pages = NULL;bytes = iov_iter_extract_pages(iter, &pages, LONG_MAX,       nr_vecs, extraction_flags, &offs);if (unlikely(bytes <= 0)) {ret = bytes ? bytes : -EFAULT;goto out_unmap;}npages = DIV_ROUND_UP(offs + bytes, PAGE_SIZE);if (unlikely(offs & queue_dma_alignment(rq->q)))j = 0;else {for (j = 0; j < npages; j++) {struct page  page = pages[j];unsigned int n = PAGE_SIZE - offs;bool same_page = false;if (n > bytes)n = bytes;if (!bio_add_hw_page(rq->q, bio, page, n, offs,     max_sectors, &same_page)) {if (same_page)bio_release_page(bio, page);break;}bytes -= n;offs = 0;}}    release the pages we didn't map into the bio, if any ", "int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,struct rq_map_data *map_data,const struct iov_iter *iter, gfp_t gfp_mask)": "blk_rq_unmap_user() must be issued at the end of IO, while      still in process context. ", "bio = blk_rq_map_bio_alloc(rq, 0, GFP_KERNEL);if (bio == NULL)return -ENOMEM;bio_iov_bvec_set(bio, (struct iov_iter *)iter);blk_rq_bio_prep(rq, bio, nr_segs);/* loop to perform a bunch of sanity checks ": "blk_rq_map_user_bvec(struct request  rq, const struct iov_iter  iter){struct request_queue  q = rq->q;size_t nr_iter = iov_iter_count(iter);size_t nr_segs = iter->nr_segs;struct bio_vec  bvecs,  bvprvp = NULL;const struct queue_limits  lim = &q->limits;unsigned int nsegs = 0, bytes = 0;struct bio  bio;size_t i;if (!nr_iter || (nr_iter >> SECTOR_SHIFT) > queue_max_hw_sectors(q))return -EINVAL;if (nr_segs > queue_max_segments(q))return -EINVAL;  no iovecs to alloc, as we already have a BVEC iterator ", "int blk_rq_map_kern(struct request_queue *q, struct request *rq, void *kbuf,    unsigned int len, gfp_t gfp_mask)": "blk_rq_map_kern - map kernel data to a request, for passthrough requests   @q:request queue where request should be inserted   @rq:request to fill   @kbuf:the kernel buffer   @len:length of user data   @gfp_mask:memory allocation flags     Description:      Data will be mapped directly if possible. Otherwise a bounce      buffer is used. Can be called multiple times to append multiple      buffers. ", "void blk_set_stacking_limits(struct queue_limits *lim)": "blk_set_stacking_limits - set default limits for stacking devices   @lim:  the queue_limits structure to reset     Description:     Returns a queue_limit struct to its default state. Should be used     by stacking drivers like DM that have no internal limits. ", "void blk_queue_bounce_limit(struct request_queue *q, enum blk_bounce bounce)": "blk_queue_bounce_limit - set bounce buffer limit for queue   @q: the request queue for the device   @bounce: bounce limit to enforce     Description:      Force bouncing for ISA DMA ranges or highmem.        DEPRECATED, don't use in new code.  ", "void blk_queue_max_hw_sectors(struct request_queue *q, unsigned int max_hw_sectors)": "blk_queue_max_hw_sectors - set max sectors for a request for this queue   @q:  the request queue for the device   @max_hw_sectors:  max hardware sectors in the usual 512b unit     Description:      Enables a low level driver to set a hard upper limit,      max_hw_sectors, on the size of requests.  max_hw_sectors is set by      the device driver based upon the capabilities of the IO      controller.        max_dev_sectors is a hard limit imposed by the storage device for      READWRITE requests. It is set by the disk driver.        max_sectors is a soft limit imposed by the block layer for      filesystem type requests.  This value can be overridden on a      per-device basis in sysblock<device>queuemax_sectors_kb.      The soft limit can not exceed max_hw_sectors.  ", "void blk_queue_chunk_sectors(struct request_queue *q, unsigned int chunk_sectors)": "blk_queue_chunk_sectors - set size of the chunk for this queue   @q:  the request queue for the device   @chunk_sectors:  chunk sectors in the usual 512b unit     Description:      If a driver doesn't want IOs to cross a given chunk size, it can set      this limit and prevent merging across chunks. Note that the block layer      must accept a page worth of data at any offset. So if the crossing of      chunks is a hard limitation in the driver, it must still be prepared      to split single page bios.  ", "void blk_queue_max_discard_sectors(struct request_queue *q,unsigned int max_discard_sectors)": "blk_queue_max_discard_sectors - set max sectors for a single discard   @q:  the request queue for the device   @max_discard_sectors: maximum number of sectors to discard  ", "void blk_queue_max_secure_erase_sectors(struct request_queue *q,unsigned int max_sectors)": "blk_queue_max_secure_erase_sectors - set max sectors for a secure erase   @q:  the request queue for the device   @max_sectors: maximum number of sectors to secure_erase  ", "void blk_queue_max_write_zeroes_sectors(struct request_queue *q,unsigned int max_write_zeroes_sectors)": "blk_queue_max_write_zeroes_sectors - set max sectors for a single                                        write zeroes   @q:  the request queue for the device   @max_write_zeroes_sectors: maximum number of sectors to write per command  ", "void blk_queue_max_segments(struct request_queue *q, unsigned short max_segments)": "blk_queue_max_segments - set max hw segments for a request for this queue   @q:  the request queue for the device   @max_segments:  max number of segments     Description:      Enables a low level driver to set an upper limit on the number of      hw data segments in a request.  ", "void blk_queue_max_segment_size(struct request_queue *q, unsigned int max_size)": "blk_queue_max_segment_size - set max segment size for blk_rq_map_sg   @q:  the request queue for the device   @max_size:  max size of segment in bytes     Description:      Enables a low level driver to set an upper limit on the size of a      coalesced segment  ", "void blk_queue_logical_block_size(struct request_queue *q, unsigned int size)": "blk_queue_logical_block_size - set logical block size for the queue   @q:  the request queue for the device   @size:  the logical block size, in bytes     Description:     This should be set to the lowest possible block size that the     storage device can address.  The default of 512 covers most     hardware.  ", "void blk_queue_physical_block_size(struct request_queue *q, unsigned int size)": "blk_queue_physical_block_size - set physical block size for the queue   @q:  the request queue for the device   @size:  the physical block size, in bytes     Description:     This should be set to the lowest possible sector size that the     hardware can operate on without reverting to read-modify-write     operations. ", "void blk_queue_alignment_offset(struct request_queue *q, unsigned int offset)": "blk_queue_alignment_offset - set physical block alignment offset   @q:the request queue for the device   @offset: alignment offset in bytes     Description:     Some devices are naturally misaligned to compensate for things like     the legacy DOS partition table 63-sector offset.  Low-level drivers     should call this function for devices whose first sector is not     naturally aligned. ", "void blk_limits_io_min(struct queue_limits *limits, unsigned int min)": "blk_limits_io_min - set minimum request size for a device   @limits: the queue limits   @min:  smallest IO size in bytes     Description:     Some devices have an internal block size bigger than the reported     hardware sector size.  This function can be used to signal the     smallest IO the device can perform without incurring a performance     penalty. ", "void blk_queue_io_min(struct request_queue *q, unsigned int min)": "blk_queue_io_min - set minimum request size for the queue   @q:the request queue for the device   @min:  smallest IO size in bytes     Description:     Storage devices may report a granularity or preferred minimum IO     size which is the smallest request the device can perform without     incurring a performance penalty.  For disk drives this is often the     physical block size.  For RAID arrays it is often the stripe chunk     size.  A properly aligned multiple of minimum_io_size is the     preferred request size for workloads where a high number of IO     operations is desired. ", "void blk_limits_io_opt(struct queue_limits *limits, unsigned int opt)": "blk_limits_io_opt - set optimal request size for a device   @limits: the queue limits   @opt:  smallest IO size in bytes     Description:     Storage devices may report an optimal IO size, which is the     device's preferred unit for sustained IO.  This is rarely reported     for disk drives.  For RAID arrays it is usually the stripe width or     the internal track size.  A properly aligned multiple of     optimal_io_size is the preferred request size for workloads where     sustained throughput is desired. ", "void blk_queue_io_opt(struct request_queue *q, unsigned int opt)": "blk_queue_io_opt - set optimal request size for the queue   @q:the request queue for the device   @opt:  optimal request size in bytes     Description:     Storage devices may report an optimal IO size, which is the     device's preferred unit for sustained IO.  This is rarely reported     for disk drives.  For RAID arrays it is usually the stripe width or     the internal track size.  A properly aligned multiple of     optimal_io_size is the preferred request size for workloads where     sustained throughput is desired. ", "int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,     sector_t start)": "blk_stack_limits - adjust queue_limits for stacked devices   @t:the stacking driver limits (top device)   @b:  the underlying queue limits (bottom, component device)   @start:  first data sector within component device     Description:      This function is used by stacking drivers like MD and DM to ensure      that all component devices have compatible block sizes and      alignments.  The stacking driver must provide a queue_limits      struct (top) and then iteratively call the stacking function for      all component (bottom) devices.  The stacking function will      attempt to combine the values and ensure proper alignment.        Returns 0 if the top and bottom queue_limits are compatible.  The      top device's block sizes and alignment offsets may be adjusted to      ensure alignment with the bottom device. If no compatible sizes      and alignments exist, -1 is returned and the resulting top      queue_limits will have the misaligned flag set to indicate that      the alignment_offset is undefined. ", "void disk_stack_limits(struct gendisk *disk, struct block_device *bdev,       sector_t offset)": "disk_stack_limits - adjust queue limits for stacked drivers   @disk:  MDDM gendisk (top)   @bdev:  the underlying block device (bottom)   @offset:  offset to beginning of data within component device     Description:      Merges the limits for a top level gendisk and a bottom level      block_device. ", "void blk_queue_update_dma_pad(struct request_queue *q, unsigned int mask)": "blk_queue_update_dma_pad - update pad mask   @q:     the request queue for the device   @mask:  pad mask     Update dma pad mask.     Appending pad buffer to a request modifies the last entry of a   scatter list such that it includes the pad buffer.  ", "void blk_queue_segment_boundary(struct request_queue *q, unsigned long mask)": "blk_queue_segment_boundary - set boundary rules for segment merging   @q:  the request queue for the device   @mask:  the memory boundary mask  ", "WARN_ON_ONCE(q->limits.virt_boundary_mask);q->limits.max_segment_size = max_size;}EXPORT_SYMBOL(blk_queue_max_segment_size);/** * blk_queue_logical_block_size - set logical block size for the queue * @q:  the request queue for the device * @size:  the logical block size, in bytes * * Description: *   This should be set to the lowest possible block size that the *   storage device can address.  The default of 512 covers most *   hardware. *": "blk_queue_virt_boundary() for the explanation ", "void blk_queue_dma_alignment(struct request_queue *q, int mask)": "blk_queue_dma_alignment - set dma length and memory alignment   @q:     the request queue for the device   @mask:  alignment mask     description:      set required memory and length alignment for direct dma transactions.      this is used when building direct io requests for the queue.    ", "void blk_queue_update_dma_alignment(struct request_queue *q, int mask)": "blk_queue_update_dma_alignment - update dma length and memory alignment   @q:     the request queue for the device   @mask:  alignment mask     description:      update required memory and length alignment for direct dma transactions.      If the requested alignment is larger than the current alignment, then      the current queue alignment is updated to the new value, otherwise it      is left alone.  The design of this is to allow multiple objects      (driver, device, transport etc) to set their respective      alignments without having them interfere.    ", "void blk_set_queue_depth(struct request_queue *q, unsigned int depth)": "blk_set_queue_depth - tell the block layer about the device queue depth   @q:the request queue for the device   @depth:queue depth   ", "percpu_ref_get_many(&data->q->q_usage_counter, nr - 1);data->nr_tags -= nr;return rq_list_pop(data->cached_rq);}static struct request *__blk_mq_alloc_requests(struct blk_mq_alloc_data *data)": "blk_mq_alloc_requests_batch(struct blk_mq_alloc_data  data){unsigned int tag, tag_offset;struct blk_mq_tags  tags;struct request  rq;unsigned long tag_mask;int i, nr = 0;tag_mask = blk_mq_get_tags(data, data->nr_tags, &tag_offset);if (unlikely(!tag_mask))return NULL;tags = blk_mq_tags_from_data(data);for (i = 0; tag_mask; i++) {if (!(tag_mask & (1UL << i)))continue;tag = tag_offset + i;prefetch(tags->static_rqs[tag]);tag_mask &= ~(1UL << i);rq = blk_mq_rq_ctx_init(data, tags, tag);rq_list_add(data->cached_rq, rq);nr++;}  caller already holds a reference, add for remainder ", "if (force_irqthreads())return false;/* same CPU or cache domain?  Complete locally ": "blk_mq_complete_request_remote(void  data){__raise_softirq_irqoff(BLOCK_SOFTIRQ);}static inline bool blk_mq_complete_need_ipi(struct request  rq){int cpu = raw_smp_processor_id();if (!IS_ENABLED(CONFIG_SMP) ||    !test_bit(QUEUE_FLAG_SAME_COMP, &rq->q->queue_flags))return false;    With force threaded interrupts enabled, raising softirq from an SMP   function call will always result in waking the ksoftirqd thread.   This is probably worse than completing the request on a different   cache domain. ", "void blk_mq_start_request(struct request *rq)": "blk_mq_start_request - Start processing a request   @rq: Pointer to request to be started     Function used by device drivers to notify the block layer that a request   is going to be processed now, so blk layer can do proper initializations   such as starting the timeout timer. ", "void blk_execute_rq_nowait(struct request *rq, bool at_head)": "blk_execute_rq_nowait - insert a request to IO scheduler for execution   @rq:request to insert   @at_head:    insert request at head or tail of queue     Description:      Insert a fully prepared request at the back of the IO scheduler queue      for execution.  Don't wait for completion.     Note:      This function will invoke @done directly if the queue is dead. ", "blk_mq_sched_requeue_request(rq);spin_lock_irqsave(&q->requeue_lock, flags);list_add_tail(&rq->queuelist, &q->requeue_list);spin_unlock_irqrestore(&q->requeue_lock, flags);if (kick_requeue_list)blk_mq_kick_requeue_list(q);}EXPORT_SYMBOL(blk_mq_requeue_request": "blk_mq_requeue_request(struct request  rq){struct request_queue  q = rq->q;blk_mq_put_driver_tag(rq);trace_block_rq_requeue(rq);rq_qos_requeue(q, rq);if (blk_mq_request_started(rq)) {WRITE_ONCE(rq->state, MQ_RQ_IDLE);rq->rq_flags &= ~RQF_TIMED_OUT;}}void blk_mq_requeue_request(struct request  rq, bool kick_requeue_list){struct request_queue  q = rq->q;unsigned long flags;__blk_mq_requeue_request(rq);  this request will be re-inserted to io scheduler queue ", "if (rq->rq_flags & RQF_DONTPREP) ": "blk_mq_kick_requeue_list(q);}EXPORT_SYMBOL(blk_mq_requeue_request);static void blk_mq_requeue_work(struct work_struct  work){struct request_queue  q =container_of(work, struct request_queue, requeue_work.work);LIST_HEAD(rq_list);LIST_HEAD(flush_list);struct request  rq;spin_lock_irq(&q->requeue_lock);list_splice_init(&q->requeue_list, &rq_list);list_splice_init(&q->flush_list, &flush_list);spin_unlock_irq(&q->requeue_lock);while (!list_empty(&rq_list)) {rq = list_entry(rq_list.next, struct request, queuelist);    If RQF_DONTPREP ist set, the request has been started by the   driver already and might have driver-specific data allocated   already.  Insert it into the hctx dispatch list to avoid   block layer merges for the request. ", "static int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)": "blk_mq_delay_run_hw_queue(hctx, BLK_MQ_RESOURCE_DELAY);blk_mq_update_dispatch_busy(hctx, true);return false;}blk_mq_update_dispatch_busy(hctx, false);return true;}static inline int blk_mq_first_mapped_cpu(struct blk_mq_hw_ctx  hctx){int cpu = cpumask_first_and(hctx->cpumask, cpu_online_mask);if (cpu >= nr_cpu_ids)cpu = cpumask_first(hctx->cpumask);return cpu;}    It'd be great if the workqueue API had a way to pass   in a mask and had some smarts for more clever placement.   For now we just round-robin here, switching for every   BLK_MQ_CPU_WORK_BATCH queued items. ", "void blk_freeze_queue(struct request_queue *q)": "blk_mq_run_hw_queues(q, false);} else {mutex_unlock(&q->mq_freeze_lock);}}EXPORT_SYMBOL_GPL(blk_freeze_queue_start);void blk_mq_freeze_queue_wait(struct request_queue  q){wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));}EXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait);int blk_mq_freeze_queue_wait_timeout(struct request_queue  q,     unsigned long timeout){return wait_event_timeout(q->mq_freeze_wq,percpu_ref_is_zero(&q->q_usage_counter),timeout);}EXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait_timeout);    Guarantee no request is in use, so we can change any data structure of   the queue afterward. ", "void blk_mq_delay_run_hw_queues(struct request_queue *q, unsigned long msecs)": "blk_mq_delay_run_hw_queues - Run all hardware queues asynchronously.   @q: Pointer to the request queue to run.   @msecs: Milliseconds of delay to wait before running the queues. ", "void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx)": "blk_mq_stop_hw_queue() returns. Please use   blk_mq_quiesce_queue() for that requirement. ", "void blk_mq_stop_hw_queues(struct request_queue *q)": "blk_mq_stop_hw_queues() returns. Please use   blk_mq_quiesce_queue() for that requirement. ", "void blk_mq_destroy_queue(struct request_queue *q)": "blk_mq_init_allocated_queue(set, q);if (ret) {blk_put_queue(q);return ERR_PTR(ret);}return q;}struct request_queue  blk_mq_init_queue(struct blk_mq_tag_set  set){return blk_mq_init_queue_data(set, NULL);}EXPORT_SYMBOL(blk_mq_init_queue);     blk_mq_destroy_queue - shutdown a request queue   @q: request queue to shutdown     This shuts down a request queue allocated by blk_mq_init_queue(). All future   requests will be failed with -ENODEV. The caller is responsible for dropping   the reference from blk_mq_init_queue() by calling blk_put_queue().     Context: can sleep ", "if (is_kdump_kernel()) ": "blk_mq_alloc_tag_set(struct blk_mq_tag_set  set){int i, ret;BUILD_BUG_ON(BLK_MQ_MAX_DEPTH > 1 << BLK_MQ_UNIQUE_TAG_BITS);if (!set->nr_hw_queues)return -EINVAL;if (!set->queue_depth)return -EINVAL;if (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN)return -EINVAL;if (!set->ops->queue_rq)return -EINVAL;if (!set->ops->get_budget ^ !set->ops->put_budget)return -EINVAL;if (set->queue_depth > BLK_MQ_MAX_DEPTH) {pr_info(\"blk-mq: reduced tag depth to %u\\n\",BLK_MQ_MAX_DEPTH);set->queue_depth = BLK_MQ_MAX_DEPTH;}if (!set->nr_maps)set->nr_maps = 1;else if (set->nr_maps > HCTX_MAX_TYPES)return -EINVAL;    If a crashdump is active, then we are potentially in a very   memory constrained environment. Limit us to 1 queue and   64 tags to prevent using too much memory. ", "EXPORT_SYMBOL(EISA_bus": "EISA_bus;  for legacy drivers ", "int xen_alloc_unpopulated_pages(unsigned int nr_pages, struct page **pages)": "xen_alloc_unpopulated_pages - alloc unpopulated pages   @nr_pages: Number of pages   @pages: pages returned   @return 0 on success, error otherwise ", "void xen_free_unpopulated_pages(unsigned int nr_pages, struct page **pages)": "xen_free_unpopulated_pages - return unpopulated pages   @nr_pages: Number of pages   @pages: pages to return ", "int xen_alloc_ballooned_pages(unsigned int nr_pages, struct page **pages)": "xen_alloc_ballooned_pages - get pages that have been ballooned out   @nr_pages: Number of pages to get   @pages: pages returned   @return 0 on success, error otherwise ", "balloon_stats.target_unpopulated -= nr_pages - pgno;return ret;}EXPORT_SYMBOL(xen_alloc_ballooned_pages);/** * xen_free_ballooned_pages - return pages retrieved with get_ballooned_pages * @nr_pages: Number of pages * @pages: pages to return ": "xen_free_ballooned_pages(pgno, pages);    NB: xen_free_ballooned_pages will only subtract pgno pages, but since   target_unpopulated is incremented with nr_pages at the start we need   to remove the remaining ones also, or accounting will be screwed. ", "r = st21nfca_hci_control_se(hdev, se_idx, ST21NFCA_SE_MODE_ON);if (r == ST21NFCA_ESE_HOST_ID) ": "st21nfca_hci_enable_se(struct nfc_hci_dev  hdev, u32 se_idx){int r;    According to upper layer, se_idx == NFC_SE_UICC when   info->se_status->is_uicc_enable is true should never happen.   Same for eSE. ", "r = st21nfca_hci_control_se(hdev, se_idx, ST21NFCA_SE_MODE_OFF);if (r < 0)return r;return 0;}EXPORT_SYMBOL(st21nfca_hci_disable_se": "st21nfca_hci_disable_se(struct nfc_hci_dev  hdev, u32 se_idx){int r;    According to upper layer, se_idx == NFC_SE_UICC when   info->se_status->is_uicc_enable is true should never happen   Same for eSE. ", "kfree(cb_context);return -ENODEV;}}EXPORT_SYMBOL(st21nfca_hci_se_io": "st21nfca_hci_se_io(struct nfc_hci_dev  hdev, u32 se_idx,u8  apdu, size_t apdu_length,se_io_cb_t cb, void  cb_context){struct st21nfca_hci_info  info = nfc_hci_get_clientdata(hdev);pr_debug(\"se_io %x\\n\", se_idx);switch (se_idx) {case ST21NFCA_ESE_HOST_ID:info->se_info.cb = cb;info->se_info.cb_context = cb_context;mod_timer(&info->se_info.bwi_timer, jiffies +  msecs_to_jiffies(info->se_info.wt_timeout));info->se_info.bwi_active = true;return nfc_hci_send_event(hdev, ST21NFCA_APDU_READER_GATE,ST21NFCA_EVT_TRANSMIT_DATA,apdu, apdu_length);default:  Need to free cb_context here as at the moment we can't   clearly indicate to the caller if the callback function   would be called (and free it) or not. In both cases a   negative value may be returned to the caller. ", "if (skb->len < 2 || skb->data[0] != NFC_EVT_TRANSACTION_AID_TAG)return -EPROTO;aid_len = skb->data[1];if (skb->len < aid_len + 4 || aid_len > sizeof(transaction->aid))return -EPROTO;params_len = skb->data[aid_len + 3];/* Verify PARAMETERS tag is (82), and final check that there is enough * space in the packet to read everything. ": "st21nfca_connectivity_event_received(struct nfc_hci_dev  hdev, u8 host,u8 event, struct sk_buff  skb){int r = 0;struct device  dev = &hdev->ndev->dev;struct nfc_evt_transaction  transaction;u32 aid_len;u8 params_len;pr_debug(\"connectivity gate event: %x\\n\", event);switch (event) {case ST21NFCA_EVT_CONNECTIVITY:r = nfc_se_connectivity(hdev->ndev, host);break;case ST21NFCA_EVT_TRANSACTION:  According to specification etsi 102 622   11.2.2.4 EVT_TRANSACTION Table 52   DescriptionTagLength   AID815 to 16   PARAMETERS820 to 255     The key differences are aid storage length is variably sized   in the packet, but fixed in nfc_evt_transaction, and that the aid_len   is u8 in the packet, but u32 in the structure, and the tags in   the packet are not included in nfc_evt_transaction.     size in bytes: 1          1       5-16 1             1           0-255   offset:        0          1       2    aid_len + 2   aid_len + 3 aid_len + 4   member name:   aid_tag(M) aid_len aid  params_tag(M) params_len  params   example:       0x81       5-16    X    0x82 0-255    X ", "timer_setup(&info->se_info.bwi_timer, st21nfca_se_wt_timeout, 0);info->se_info.bwi_active = false;timer_setup(&info->se_info.se_active_timer,    st21nfca_se_activation_timeout, 0);info->se_info.se_active = false;info->se_info.count_pipes = 0;info->se_info.expected_pipes = 0;info->se_info.xch_error = false;info->se_info.wt_timeout =ST21NFCA_BWI_TO_TIMEOUT(ST21NFCA_ATR_DEFAULT_BWI);}EXPORT_SYMBOL(st21nfca_se_init": "st21nfca_se_init(struct nfc_hci_dev  hdev){struct st21nfca_hci_info  info = nfc_hci_get_clientdata(hdev);init_completion(&info->se_info.req_completion);INIT_WORK(&info->se_info.timeout_work, st21nfca_se_wt_work);  initialize timers ", "dev_num = find_first_zero_bit(dev_mask, ST21NFCA_NUM_DEVICES);if (dev_num >= ST21NFCA_NUM_DEVICES) ": "st21nfca_hci_probe(void  phy_id, const struct nfc_phy_ops  phy_ops,       char  llc_name, int phy_headroom, int phy_tailroom,       int phy_payload, struct nfc_hci_dev   hdev,   struct st21nfca_se_status  se_status){struct st21nfca_hci_info  info;int r = 0;int dev_num;u32 protocols;struct nfc_hci_init_data init_data;unsigned long quirks = 0;info = kzalloc(sizeof(struct st21nfca_hci_info), GFP_KERNEL);if (!info)return -ENOMEM;info->phy_ops = phy_ops;info->phy_id = phy_id;info->state = ST21NFCA_ST_COLD;mutex_init(&info->info_lock);init_data.gate_count = ARRAY_SIZE(st21nfca_gates);memcpy(init_data.gates, st21nfca_gates, sizeof(st21nfca_gates));    Session id must include the driver name + i2c bus addr   persistent info to discriminate 2 identical chips ", "info->async_cb_type = ST21NFCA_CB_TYPE_READER_F;info->async_cb_context = info;info->async_cb = st21nfca_im_recv_atr_res_cb;info->dep_info.bri = atr_req->bri;info->dep_info.bsi = atr_req->bsi;info->dep_info.lri = ST21NFCA_PP2LRI(atr_req->ppi);return nfc_hci_send_cmd_async(hdev, ST21NFCA_RF_READER_F_GATE,ST21NFCA_WR_XCHG_DATA, skb->data,skb->len, info->async_cb, info);}EXPORT_SYMBOL(st21nfca_im_send_atr_req": "st21nfca_im_send_atr_req(struct nfc_hci_dev  hdev, u8  gb, size_t gb_len){struct sk_buff  skb;struct st21nfca_hci_info  info = nfc_hci_get_clientdata(hdev);struct st21nfca_atr_req  atr_req;struct nfc_target  target;uint size;info->dep_info.to = ST21NFCA_DEFAULT_TIMEOUT;size = ST21NFCA_ATR_REQ_MIN_SIZE + gb_len;if (size > ST21NFCA_ATR_REQ_MAX_SIZE) {PROTOCOL_ERR(\"14.6.1.1\");return -EINVAL;}skb =    alloc_skb(sizeof(struct st21nfca_atr_req) + gb_len + 1, GFP_KERNEL);if (!skb)return -ENOMEM;skb_reserve(skb, 1);skb_put(skb, sizeof(struct st21nfca_atr_req));atr_req = (struct st21nfca_atr_req  )skb->data;memset(atr_req, 0, sizeof(struct st21nfca_atr_req));atr_req->cmd0 = ST21NFCA_NFCIP1_REQ;atr_req->cmd1 = ST21NFCA_NFCIP1_ATR_REQ;memset(atr_req->nfcid3, 0, NFC_NFCID3_MAXSIZE);target = hdev->ndev->targets;if (target->sensf_res_len > 0)memcpy(atr_req->nfcid3, target->sensf_res,target->sensf_res_len);elseget_random_bytes(atr_req->nfcid3, NFC_NFCID3_MAXSIZE);atr_req->did = 0x0;atr_req->bsi = 0x00;atr_req->bri = 0x00;atr_req->ppi = ST21NFCA_LR_BITS_PAYLOAD_SIZE_254B;if (gb_len) {atr_req->ppi |= ST21NFCA_GB_BIT;skb_put_data(skb, gb, gb_len);}atr_req->length = sizeof(struct st21nfca_atr_req) + hdev->gb_len; (u8  )skb_push(skb, 1) = info->dep_info.to | 0x10;   timeout ", "strcpy(init_data.session_id, \"ID544HCI\");protocols = NFC_PROTO_JEWEL_MASK |    NFC_PROTO_MIFARE_MASK |    NFC_PROTO_FELICA_MASK |    NFC_PROTO_ISO14443_MASK |    NFC_PROTO_ISO14443_B_MASK |    NFC_PROTO_NFC_DEP_MASK;info->hdev = nfc_hci_allocate_device(&pn544_hci_ops, &init_data, 0,     protocols, llc_name,     phy_headroom + PN544_CMDS_HEADROOM,     phy_tailroom, phy_payload);if (!info->hdev) ": "pn544_hci_probe(void  phy_id, const struct nfc_phy_ops  phy_ops,    char  llc_name, int phy_headroom, int phy_tailroom,    int phy_payload, fw_download_t fw_download,    struct nfc_hci_dev   hdev){struct pn544_hci_info  info;u32 protocols;struct nfc_hci_init_data init_data;int r;info = kzalloc(sizeof(struct pn544_hci_info), GFP_KERNEL);if (!info) {r = -ENOMEM;goto err_info_alloc;}info->phy_ops = phy_ops;info->phy_id = phy_id;info->fw_download = fw_download;info->state = PN544_ST_COLD;mutex_init(&info->info_lock);init_data.gate_count = ARRAY_SIZE(pn544_gates);memcpy(init_data.gates, pn544_gates, sizeof(pn544_gates));    TODO: Session id must include the driver name + some bus addr   persistent info to discriminate 2 identical chips ", "kfree(cb_context);return -ENODEV;}}EXPORT_SYMBOL(st_nci_se_io": "st_nci_se_io(struct nci_dev  ndev, u32 se_idx,       u8  apdu, size_t apdu_length,       se_io_cb_t cb, void  cb_context){struct st_nci_info  info = nci_get_drvdata(ndev);switch (se_idx) {case ST_NCI_ESE_HOST_ID:info->se_info.cb = cb;info->se_info.cb_context = cb_context;mod_timer(&info->se_info.bwi_timer, jiffies +  msecs_to_jiffies(info->se_info.wt_timeout));info->se_info.bwi_active = true;return nci_hci_send_event(ndev, ST_NCI_APDU_READER_GATE,ST_NCI_EVT_TRANSMIT_DATA, apdu,apdu_length);default:  Need to free cb_context here as at the moment we can't   clearly indicate to the caller if the callback function   would be called (and free it) or not. In both cases a   negative value may be returned to the caller. ", "timer_setup(&info->se_info.bwi_timer, st_nci_se_wt_timeout, 0);info->se_info.bwi_active = false;timer_setup(&info->se_info.se_active_timer,    st_nci_se_activation_timeout, 0);info->se_info.se_active = false;info->se_info.xch_error = false;info->se_info.wt_timeout =ST_NCI_BWI_TO_TIMEOUT(ST_NCI_ATR_DEFAULT_BWI);info->se_info.se_status = se_status;return 0;}EXPORT_SYMBOL(st_nci_se_init": "st_nci_se_init(struct nci_dev  ndev, struct st_nci_se_status  se_status){struct st_nci_info  info = nci_get_drvdata(ndev);init_completion(&info->se_info.req_completion);  initialize timers ", "ndlc->ops->enable(ndlc->phy_id);ndlc->powered = 1;return 0;}EXPORT_SYMBOL(ndlc_open": "ndlc_open(struct llt_ndlc  ndlc){  toggle reset pin ", "ndlc->ops->enable(ndlc->phy_id);nci_prop_cmd(ndlc->ndev, ST_NCI_CORE_PROP,     sizeof(struct nci_mode_set_cmd), (__u8 *)&cmd);ndlc->powered = 0;ndlc->ops->disable(ndlc->phy_id);}EXPORT_SYMBOL(ndlc_close": "ndlc_close(struct llt_ndlc  ndlc){struct nci_mode_set_cmd cmd;cmd.cmd_type = ST_NCI_SET_NFC_MODE;cmd.mode = 0;  toggle reset pin ", "u8 pcb = PCB_TYPE_DATAFRAME | PCB_DATAFRAME_RETRANSMIT_NO |PCB_FRAME_CRC_INFO_NOTPRESENT;*(u8 *)skb_push(skb, 1) = pcb;skb_queue_tail(&ndlc->send_q, skb);schedule_work(&ndlc->sm_work);return 0;}EXPORT_SYMBOL(ndlc_send": "ndlc_send(struct llt_ndlc  ndlc, struct sk_buff  skb){  add ndlc header ", "timer_setup(&ndlc->t1_timer, ndlc_t1_timeout, 0);timer_setup(&ndlc->t2_timer, ndlc_t2_timeout, 0);skb_queue_head_init(&ndlc->rcv_q);skb_queue_head_init(&ndlc->send_q);skb_queue_head_init(&ndlc->ack_pending_q);INIT_WORK(&ndlc->sm_work, llt_ndlc_sm_work);return st_nci_probe(ndlc, phy_headroom, phy_tailroom, se_status);}EXPORT_SYMBOL(ndlc_probe": "ndlc_probe(void  phy_id, const struct nfc_phy_ops  phy_ops,       struct device  dev, int phy_headroom, int phy_tailroom,       struct llt_ndlc   ndlc_id, struct st_nci_se_status  se_status){struct llt_ndlc  ndlc;ndlc = devm_kzalloc(dev, sizeof(struct llt_ndlc), GFP_KERNEL);if (!ndlc)return -ENOMEM;ndlc->ops = phy_ops;ndlc->phy_id = phy_id;ndlc->dev = dev;ndlc->powered = 0; ndlc_id = ndlc;  initialize timers ", "del_timer_sync(&ndlc->t1_timer);del_timer_sync(&ndlc->t2_timer);ndlc->t2_active = false;ndlc->t1_active = false;/* cancel work ": "ndlc_remove(struct llt_ndlc  ndlc){  cancel timers ", "int tc_register_driver(struct tc_driver *tdrv)": "tc_register_driver - register a new TC driver   @drv: the driver structure to register     Adds the driver structure to the list of registered drivers   Returns a negative value on error, otherwise 0.   If no error occurred, the driver remains registered even if   no device was claimed during registration. ", "void tc_unregister_driver(struct tc_driver *tdrv)": "tc_unregister_driver - unregister a TC driver   @drv: the driver structure to unregister     Deletes the driver structure from the list of registered TC drivers,   gives it a chance to clean up by calling its remove() function for   each device it was responsible for, and marks those devices as   driverless. ", "struct se_node_acl *__core_tpg_get_initiator_node_acl(struct se_portal_group *tpg,const char *initiatorname)": "core_tpg_get_initiator_node_acl():    mutex_lock(&tpg->acl_node_mutex); must be held when calling ", "kref_get(&acl->acl_kref);acl->dynamic_node_acl = 1;/* * Here we only create demo-mode MappedLUNs from the active * TPG LUNs if the fabric is not explicitly asking for * tpg_check_demo_mode_login_only() == 1. ": "core_tpg_check_initiator_node_acl(struct se_portal_group  tpg,unsigned char  initiatorname){struct se_node_acl  acl;acl = core_tpg_get_initiator_node_acl(tpg, initiatorname);if (acl)return acl;if (!tpg->se_tpg_tfo->tpg_check_demo_mode(tpg))return NULL;acl = target_alloc_node_acl(tpg, initiatorname);if (!acl)return NULL;    When allocating a dynamically generated node_acl, go ahead   and take the extra kref now before returning to the fabric   driver caller.     Note this reference will be released at session shutdown   time within transport_free_session() code. ", "int core_tpg_set_initiator_node_queue_depth(struct se_node_acl *acl,u32 queue_depth)": "core_tpg_set_initiator_node_queue_depth():     ", "int core_tpg_set_initiator_node_tag(struct se_portal_group *tpg,struct se_node_acl *acl,const char *new_tag)": "core_tpg_set_initiator_node_tag():    Initiator nodeacl tags are not used internally, but may be used by  userspace to emulate aliases or groups.  Returns length of newly-set tag or -EINVAL. ", "if (se_wwn)se_tpg->se_tpg_tfo = se_wwn->wwn_tf->tf_ops;if (!se_tpg->se_tpg_tfo) ": "core_tpg_register(struct se_wwn  se_wwn,struct se_portal_group  se_tpg,int proto_id){int ret;if (!se_tpg)return -EINVAL;    For the typical case where core_tpg_register() is called by a   fabric driver from target_core_fabric_ops->fabric_make_tpg()   configfs context, use the original tf_ops pointer already saved   by target-core in target_fabric_make_wwn().     Otherwise, for special cases like iscsi-target discovery TPGs   the caller is responsible for setting ->se_tpg_tfo ahead of   calling core_tpg_register(). ", "list_for_each_entry_safe(nacl, nacl_tmp, &node_list, acl_list) ": "core_tpg_deregister(struct se_portal_group  se_tpg){const struct target_core_fabric_ops  tfo = se_tpg->se_tpg_tfo;struct se_node_acl  nacl,  nacl_tmp;LIST_HEAD(node_list);pr_debug(\"TARGET_CORE[%s]: Deallocating portal_group for endpoint: %s, \" \"Proto: %d, Portal Tag: %u\\n\", tfo->fabric_name,tfo->tpg_get_wwn(se_tpg) ? tfo->tpg_get_wwn(se_tpg) : NULL,se_tpg->proto_id, tfo->tpg_get_tag(se_tpg));while (atomic_read(&se_tpg->tpg_pr_ref_count) != 0)cpu_relax();mutex_lock(&se_tpg->acl_node_mutex);list_splice_init(&se_tpg->acl_node_list, &node_list);mutex_unlock(&se_tpg->acl_node_mutex);    Release any remaining demo-mode generated se_node_acl that have   not been released because of TFO->tpg_check_demo_mode_cache() == 1   in transport_deregister_session(). ", "if (dev->transport->get_device_type(dev) == TYPE_TAPE)buf[1] = 0x80;buf[2] = 0x06; /* SPC-4 ": "spc_emulate_inquiry_std(struct se_cmd  cmd, unsigned char  buf){struct se_lun  lun = cmd->se_lun;struct se_portal_group  tpg = lun->lun_tpg;struct se_device  dev = cmd->se_dev;struct se_session  sess = cmd->se_sess;  Set RMB (removable media) for tape devices ", "if (!(dev->dev_flags & DF_EMULATED_VPD_UNIT_SERIAL))goto check_t10_vend_desc;/* CODE SET == Binary ": "spc_emulate_evpd_83(struct se_cmd  cmd, unsigned char  buf){struct se_device  dev = cmd->se_dev;struct se_lun  lun = cmd->se_lun;struct se_portal_group  tpg = NULL;struct t10_alua_lu_gp_member  lu_gp_mem;struct t10_alua_tg_pt_gp  tg_pt_gp;unsigned char  prod = &dev->t10_wwn.model[0];u32 off = 0;u16 len = 0, id_len;off = 4;    NAA IEEE Registered Extended Assigned designator format, see   spc4r17 section 7.7.3.6.5     We depend upon a target_core_modConfigFS provided   syskernelconfigtargetcore$HBA$DEVwwnvpd_unit_serial   value in order to return the NAA id. ", "if (!sess)goto done;nacl = sess->se_node_acl;rcu_read_lock();hlist_for_each_entry_rcu(deve, &nacl->lun_entry_hlist, link) ": "spc_emulate_report_luns(struct se_cmd  cmd){struct se_dev_entry  deve;struct se_session  sess = cmd->se_sess;struct se_node_acl  nacl;struct scsi_lun slun;unsigned char  buf;u32 lun_count = 0, offset = 8;__be32 len;buf = transport_kmap_data_sg(cmd);if (cmd->data_length && !buf)return TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;    If no struct se_session pointer is present, this struct se_cmd is   coming via a target_core_mod PASSTHROUGH op, and not through   a $FABRIC_MOD.  In that case, report LUN=0 only. ", "if (cdb[0] == RESERVE_10)*size = get_unaligned_be16(&cdb[7]);else*size = cmd->data_length;cmd->execute_cmd = target_scsi2_reservation_reserve;break;case REQUEST_SENSE:*size = cdb[4];cmd->execute_cmd = spc_emulate_request_sense;break;case INQUIRY:*size = get_unaligned_be16(&cdb[3]);/* * Do implicit HEAD_OF_QUEUE processing for INQUIRY. * See spc4r17 section 5.3 ": "spc_parse_cdb(struct se_cmd  cmd, unsigned int  size){struct se_device  dev = cmd->se_dev;unsigned char  cdb = cmd->t_task_cdb;switch (cdb[0]) {case RESERVE:case RESERVE_10:case RELEASE:case RELEASE_10:if (!dev->dev_attrib.emulate_pr)return TCM_UNSUPPORTED_SCSI_OPCODE;if (dev->transport_flags & TRANSPORT_FLAG_PASSTHROUGH_PGR)return TCM_UNSUPPORTED_SCSI_OPCODE;break;case PERSISTENT_RESERVE_IN:case PERSISTENT_RESERVE_OUT:if (!dev->dev_attrib.emulate_pr)return TCM_UNSUPPORTED_SCSI_OPCODE;break;}switch (cdb[0]) {case MODE_SELECT: size = cdb[4];cmd->execute_cmd = spc_emulate_modeselect;break;case MODE_SELECT_10: size = get_unaligned_be16(&cdb[7]);cmd->execute_cmd = spc_emulate_modeselect;break;case MODE_SENSE: size = cdb[4];cmd->execute_cmd = spc_emulate_modesense;break;case MODE_SENSE_10: size = get_unaligned_be16(&cdb[7]);cmd->execute_cmd = spc_emulate_modesense;break;case LOG_SELECT:case LOG_SENSE: size = get_unaligned_be16(&cdb[7]);break;case PERSISTENT_RESERVE_IN: size = get_unaligned_be16(&cdb[7]);cmd->execute_cmd = target_scsi3_emulate_pr_in;break;case PERSISTENT_RESERVE_OUT: size = get_unaligned_be32(&cdb[5]);cmd->execute_cmd = target_scsi3_emulate_pr_out;break;case RELEASE:case RELEASE_10:if (cdb[0] == RELEASE_10) size = get_unaligned_be16(&cdb[7]);else size = cmd->data_length;cmd->execute_cmd = target_scsi2_reservation_release;break;case RESERVE:case RESERVE_10:    The SPC-2 RESERVE does not contain a size in the SCSI CDB.   Assume the passthrough or $FABRIC_MOD will tell us about it. ", "void transport_init_session(struct se_session *se_sess)": "transport_init_session - initialize a session object   @se_sess: Session object pointer.     The caller must have zero-initialized @se_sess before calling this function. ", "struct se_session *transport_alloc_session(enum target_prot_op sup_prot_ops)": "transport_alloc_session - allocate a session object and initialize it   @sup_prot_ops: bitmask that defines which T10-PI modes are supported. ", "int transport_alloc_session_tags(struct se_session *se_sess,         unsigned int tag_num, unsigned int tag_size)": "transport_alloc_session_tags - allocate target driver private data   @se_sess:  Session pointer.   @tag_num:  Maximum number of in-flight commands between initiator and target.   @tag_size: Size in bytes of the private data a target driver associates with        each command. ", "if (se_nacl) ": "transport_register_session(struct se_portal_group  se_tpg,struct se_node_acl  se_nacl,struct se_session  se_sess,void  fabric_sess_ptr){const struct target_core_fabric_ops  tfo = se_tpg->se_tpg_tfo;unsigned char buf[PR_REG_ISID_LEN];unsigned long flags;se_sess->se_tpg = se_tpg;se_sess->fabric_sess_ptr = fabric_sess_ptr;    Used by struct se_node_acl's under ConfigFS to locate active se_session-t     Only set for struct se_session's that will actually be moving IO.   eg:  NOT  discovery sessions. ", "if (tag_num != 0)sess = transport_init_session_tags(tag_num, tag_size, prot_op);elsesess = transport_alloc_session(prot_op);if (IS_ERR(sess)) ": "target_setup_session(struct se_portal_group  tpg,     unsigned int tag_num, unsigned int tag_size,     enum target_prot_op prot_op,     const char  initiatorname, void  private,     int ( callback)(struct se_portal_group  ,     struct se_session  , void  )){struct target_cmd_counter  cmd_cnt;struct se_session  sess;int rc;cmd_cnt = target_alloc_cmd_counter();if (!cmd_cnt)return ERR_PTR(-ENOMEM);    If the fabric driver is using percpu-ida based pre allocation   of IO descriptor tags, go ahead and perform that setup now.. ", "}spin_unlock_bh(&se_tpg->session_lock);return len;}EXPORT_SYMBOL(target_show_dynamic_sessions": "target_show_dynamic_sessions(struct se_portal_group  se_tpg, char  page){struct se_session  se_sess;ssize_t len = 0;spin_lock_bh(&se_tpg->session_lock);list_for_each_entry(se_sess, &se_tpg->tpg_sess_list, sess_list) {if (!se_sess->se_node_acl)continue;if (!se_sess->se_node_acl->dynamic_node_acl)continue;if (strlen(se_sess->se_node_acl->initiatorname) + 1 + len > PAGE_SIZE)break;len += snprintf(page + len, PAGE_SIZE - len, \"%s\\n\",se_sess->se_node_acl->initiatorname);len += 1;   Include NULL terminator ", "se_nacl = se_sess->se_node_acl;if (se_nacl) ": "transport_deregister_session_configfs(struct se_session  se_sess){struct se_node_acl  se_nacl;unsigned long flags;    Used by struct se_node_acl's under ConfigFS to locate active struct se_session ", "void __transport_register_session(struct se_portal_group *se_tpg,struct se_node_acl *se_nacl,struct se_session *se_sess,void *fabric_sess_ptr)": "transport_free_session(se_sess);return ERR_PTR(-ENOMEM);}return se_sess;}    Called with spin_lock_irqsave(&struct se_portal_group->session_lock called. ", " if (page_83[1] & 0x80) ": "transport_set_vpd_proto_id(struct t10_vpd  vpd, unsigned char  page_83){    Check if the Protocol Identifier Valid (PIV) bit is set..     from spc3r23.pdf section 7.5.1 ", "vpd->association = (page_83[1] & 0x30);return transport_dump_vpd_assoc(vpd, NULL, 0);}EXPORT_SYMBOL(transport_set_vpd_assoc": "transport_set_vpd_assoc(struct t10_vpd  vpd, unsigned char  page_83){    The VPD identification association..     from spc3r23.pdf Section 7.6.3.1 Table 297 ", "vpd->device_identifier_type = (page_83[1] & 0x0f);return transport_dump_vpd_ident_type(vpd, NULL, 0);}EXPORT_SYMBOL(transport_set_vpd_ident_type": "transport_set_vpd_ident_type(struct t10_vpd  vpd, unsigned char  page_83){    The VPD identifier type..     from spc3r23.pdf Section 7.6.3.1 Table 298 ", "vpd->device_identifier_type = (page_83[1] & 0x0f);return transport_dump_vpd_ident_type(vpd, NULL, 0);}EXPORT_SYMBOL(transport_set_vpd_ident": "transport_set_vpd_ident_type(struct t10_vpd  vpd, unsigned char  page_83){    The VPD identifier type..     from spc3r23.pdf Section 7.6.3.1 Table 298 ", "sense_reason_ttarget_cmd_size_check(struct se_cmd *cmd, unsigned int size)": "__target_init_cmd().     Return: TCM_NO_SENSE ", "if (scsi_command_size(cdb) > SCSI_MAX_VARLEN_CDB_SIZE) ": "target_cmd_init_cdb(struct se_cmd  cmd, unsigned char  cdb, gfp_t gfp){sense_reason_t ret;    Ensure that the received CDB is less than the max (252 + 8) bytes   for VARIABLE_LENGTH_CMD ", "cmd->t_state = TRANSPORT_NEW_CMD;cmd->transport_state |= CMD_T_ACTIVE;/* * transport_generic_new_cmd() is already handling QUEUE_FULL, * so follow TRANSPORT_NEW_CMD processing thread context usage * and call transport_generic_request_failure() if necessary.. ": "transport_handle_cdb_direct(struct se_cmd  cmd){sense_reason_t ret;might_sleep();if (!cmd->se_lun) {dump_stack();pr_err(\"cmd->se_lun is NULL\\n\");return -EINVAL;}    Set TRANSPORT_NEW_CMD state and CMD_T_ACTIVE to ensure that   outstanding descriptors are handled correctly during shutdown via   transport_wait_for_tasks()     Also, we don't take cmd->t_state_lock here as we only expect   this to be called for initial descriptor submission. ", "void target_submit_cmd(struct se_cmd *se_cmd, struct se_session *se_sess,unsigned char *cdb, unsigned char *sense, u64 unpacked_lun,u32 data_length, int task_attr, int data_dir, int flags)": "target_submit_cmd - lookup unpacked lun and submit uninitialized se_cmd     @se_cmd: command descriptor to submit   @se_sess: associated se_sess for endpoint   @cdb: pointer to SCSI CDB   @sense: pointer to SCSI sense buffer   @unpacked_lun: unpacked LUN to reference for struct se_lun   @data_length: fabric expected data transfer length   @task_attr: SAM task attribute   @data_dir: DMA data direction   @flags: flags for command submission from target_sc_flags_tables     Task tags are supported if the caller has set @se_cmd->tag.     This may only be called from process context, and also currently   assumes internal allocation of fabric payload buffer by target-core.     It also assumes interal target core SGL memory allocation.     This function must only be used by drivers that do their own   sync during shutdown and does not use target_stop_session. If there   is a failure this function will call into the fabric driver's   queue_status with a CHECK_CONDITION. ", "int target_submit_tmr(struct se_cmd *se_cmd, struct se_session *se_sess,unsigned char *sense, u64 unpacked_lun,void *fabric_tmr_ptr, unsigned char tm_type,gfp_t gfp, u64 tag, int flags)": "target_submit_tmr - lookup unpacked lun and submit uninitialized se_cmd                       for TMR CDBs     @se_cmd: command descriptor to submit   @se_sess: associated se_sess for endpoint   @sense: pointer to SCSI sense buffer   @unpacked_lun: unpacked LUN to reference for struct se_lun   @fabric_tmr_ptr: fabric context for TMR req   @tm_type: Type of TM request   @gfp: gfp type for caller   @tag: referenced task tag for TMR_ABORT_TASK   @flags: submit cmd flags     Callable from all contexts.  ", "static unsigned char *transport_get_sense_buffer(struct se_cmd *cmd)": "transport_generic_request_failure(cmd, cmd->sense_reason);}    Used when asking transport to copy Sense Data from the underlying   LinuxSCSI struct scsi_cmnd ", "ret = target_scsi3_ua_check(cmd);if (ret)goto err;ret = target_alua_state_check(cmd);if (ret)goto err;ret = target_check_reservation(cmd);if (ret) ": "target_execute_cmd(struct se_cmd  cmd, bool do_checks){sense_reason_t ret;if (!cmd->execute_cmd) {ret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;goto err;}if (do_checks) {    Check for an existing UNIT ATTENTION condition after   target_handle_task_attr() has done SAM task attr   checking, and possibly have already defered execution   out to target_restart_delayed_cmds() context. ", "if (!cmd->t_data_nents)return NULL;BUG_ON(!sg);if (cmd->t_data_nents == 1)return kmap(sg_page(sg)) + sg->offset;/* >1 page. use vmap ": "transport_kmap_data_sg(struct se_cmd  cmd){struct scatterlist  sg = cmd->t_data_sg;struct page   pages;int i;    We need to take into account a possible offset here for fabrics like   tcm_loop who may be using a contig buffer from the SCSI midlayer for   control CDBs passed as SGLs via transport_generic_map_mem_to_cmd() ", "ret = transport_generic_new_cmd(cmd);if (ret)transport_generic_request_failure(cmd, ret);return 0;}EXPORT_SYMBOL(transport_handle_cdb_direct);sense_reason_ttransport_generic_map_mem_to_cmd(struct se_cmd *cmd, struct scatterlist *sgl,u32 sgl_count, struct scatterlist *sgl_bidi, u32 sgl_bidi_count)": "transport_generic_new_cmd() is already handling QUEUE_FULL,   so follow TRANSPORT_NEW_CMD processing thread context usage   and call transport_generic_request_failure() if necessary.. ", "int transport_generic_free_cmd(struct se_cmd *cmd, int wait_for_tasks)": "transport_generic_free_cmd() skips its call to target_put_sess_cmd().   - For aborted commands for which CMD_T_TAS has been set .queue_status() will     be called and will drop a reference.   - For aborted commands for which CMD_T_TAS has not been set .aborted_task()     will be called. target_handle_abort() will drop the final reference. ", "int target_submit_prep(struct se_cmd *se_cmd, unsigned char *cdb,       struct scatterlist *sgl, u32 sgl_count,       struct scatterlist *sgl_bidi, u32 sgl_bidi_count,       struct scatterlist *sgl_prot, u32 sgl_prot_count,       gfp_t gfp)": "target_get_sess_cmd(se_cmd, flags & TARGET_SCF_ACK_KREF);}EXPORT_SYMBOL_GPL(target_init_cmd);     target_submit_prep - prepare cmd for submission   @se_cmd: command descriptor to prep   @cdb: pointer to SCSI CDB   @sgl: struct scatterlist memory for unidirectional mapping   @sgl_count: scatterlist count for unidirectional mapping   @sgl_bidi: struct scatterlist memory for bidirectional READ mapping   @sgl_bidi_count: scatterlist count for bidirectional READ mapping   @sgl_prot: struct scatterlist memory protection information   @sgl_prot_count: scatterlist count for protection information   @gfp: gfp allocation type     Returns:  - less than zero to signal failure.  - zero on success.     If failure is returned, lio will the callers queue_status to complete   the cmd. ", "}WARN_ON_ONCE(kref_read(&cmd->cmd_kref) == 0);transport_lun_remove_cmd(cmd);transport_cmd_check_stop_to_fabric(cmd);}static void target_abort_work(struct work_struct *work)": "target_put_sess_cmd(cmd) != 0);    To do: establish a unit attention condition on the I_T   nexus associated with cmd. See also the paragraph \"Aborting   commands\" in SAM. ", "transport_complete_task_attr(cmd);if (cmd->transport_complete_callback)cmd->transport_complete_callback(cmd, false, &post_ret);if (cmd->transport_state & CMD_T_ABORTED) ": "target_show_cmd(\"-----[ \", cmd);    For SAM Task Attribute emulation for failed struct se_cmd ", "if (!atomic_read(&cmd_cnt->stopped))percpu_ref_put(&cmd_cnt->refcnt);percpu_ref_exit(&cmd_cnt->refcnt);}EXPORT_SYMBOL_GPL(target_free_cmd_counter);/** * transport_init_session - initialize a session object * @se_sess: Session object pointer. * * The caller must have zero-initialized @se_sess before calling this function. ": "target_stop_session during session   shutdown so we have to drop the ref taken at init time here. ", "void target_wait_for_sess_cmds(struct se_session *se_sess)": "target_wait_for_sess_cmds - Wait for outstanding commands   @se_sess: session to wait for active IO ", "static int transport_cmd_check_stop_to_fabric(struct se_cmd *cmd)": "transport_wait_for_tasks() for t_transport_stop_comp. ", "void target_submit(struct se_cmd *se_cmd)": "transport_send_check_condition_and_sense(se_cmd, rc, 0);target_put_sess_cmd(se_cmd);return -EIO;generic_fail:transport_generic_request_failure(se_cmd, rc);return -EIO;}EXPORT_SYMBOL_GPL(target_submit_prep);     target_submit - perform final initialization and submit cmd to LIO core   @se_cmd: command descriptor to submit     target_submit_prep must have been called on the cmd, and this must be   called from process context. ", "int target_send_busy(struct se_cmd *cmd)": "target_send_busy - Send SCSI BUSY status back to the initiator   @cmd: SCSI command for which to send a BUSY reply.     Note: Only call this function if target_submit_cmd () failed. ", "failure:INIT_WORK(&se_cmd->work, target_complete_tmr_failure);schedule_work(&se_cmd->work);return 0;}EXPORT_SYMBOL(target_submit_tmr);/* * Handle SAM-esque emulation for generic transport request failures. ": "transport_generic_handle_tmr(se_cmd);return 0;    For callback during failure handling, push this work off   to process context with TMR_LUN_DOES_NOT_EXIST status. ", "num_blocks = get_unaligned_be32(&cmd->t_task_cdb[28]);/* * Use the explicit range when non zero is supplied, otherwise calculate * the remaining range based on ->get_blocks() - starting LBA. ": "sbc_get_write_same_sectors(struct se_cmd  cmd){u32 num_blocks;if (cmd->t_task_cdb[0] == WRITE_SAME)num_blocks = get_unaligned_be16(&cmd->t_task_cdb[7]);else if (cmd->t_task_cdb[0] == WRITE_SAME_16)num_blocks = get_unaligned_be32(&cmd->t_task_cdb[10]);else   WRITE_SAME_32 via VARIABLE_LENGTH_CMD ", "if (sectors > 1) ": "sbc_parse_cdb(struct se_cmd  cmd, struct exec_cmd_ops  ops){struct se_device  dev = cmd->se_dev;unsigned char  cdb = cmd->t_task_cdb;unsigned int size;u32 sectors = 0;sense_reason_t ret;cmd->protocol_data = ops;switch (cdb[0]) {case READ_6:sectors = transport_get_sectors_6(cdb);cmd->t_task_lba = transport_lba_21(cdb);cmd->se_cmd_flags |= SCF_SCSI_DATA_CDB;cmd->execute_cmd = sbc_execute_rw;break;case READ_10:sectors = transport_get_sectors_10(cdb);cmd->t_task_lba = transport_lba_32(cdb);if (sbc_check_dpofua(dev, cmd, cdb))return TCM_INVALID_CDB_FIELD;ret = sbc_check_prot(dev, cmd, cdb[1] >> 5, sectors, false);if (ret)return ret;cmd->se_cmd_flags |= SCF_SCSI_DATA_CDB;cmd->execute_cmd = sbc_execute_rw;break;case READ_12:sectors = transport_get_sectors_12(cdb);cmd->t_task_lba = transport_lba_32(cdb);if (sbc_check_dpofua(dev, cmd, cdb))return TCM_INVALID_CDB_FIELD;ret = sbc_check_prot(dev, cmd, cdb[1] >> 5, sectors, false);if (ret)return ret;cmd->se_cmd_flags |= SCF_SCSI_DATA_CDB;cmd->execute_cmd = sbc_execute_rw;break;case READ_16:sectors = transport_get_sectors_16(cdb);cmd->t_task_lba = transport_lba_64(cdb);if (sbc_check_dpofua(dev, cmd, cdb))return TCM_INVALID_CDB_FIELD;ret = sbc_check_prot(dev, cmd, cdb[1] >> 5, sectors, false);if (ret)return ret;cmd->se_cmd_flags |= SCF_SCSI_DATA_CDB;cmd->execute_cmd = sbc_execute_rw;break;case WRITE_6:sectors = transport_get_sectors_6(cdb);cmd->t_task_lba = transport_lba_21(cdb);cmd->se_cmd_flags |= SCF_SCSI_DATA_CDB;cmd->execute_cmd = sbc_execute_rw;break;case WRITE_10:case WRITE_VERIFY:sectors = transport_get_sectors_10(cdb);cmd->t_task_lba = transport_lba_32(cdb);if (sbc_check_dpofua(dev, cmd, cdb))return TCM_INVALID_CDB_FIELD;ret = sbc_check_prot(dev, cmd, cdb[1] >> 5, sectors, true);if (ret)return ret;cmd->se_cmd_flags |= SCF_SCSI_DATA_CDB;cmd->execute_cmd = sbc_execute_rw;break;case WRITE_12:sectors = transport_get_sectors_12(cdb);cmd->t_task_lba = transport_lba_32(cdb);if (sbc_check_dpofua(dev, cmd, cdb))return TCM_INVALID_CDB_FIELD;ret = sbc_check_prot(dev, cmd, cdb[1] >> 5, sectors, true);if (ret)return ret;cmd->se_cmd_flags |= SCF_SCSI_DATA_CDB;cmd->execute_cmd = sbc_execute_rw;break;case WRITE_16:case WRITE_VERIFY_16:sectors = transport_get_sectors_16(cdb);cmd->t_task_lba = transport_lba_64(cdb);if (sbc_check_dpofua(dev, cmd, cdb))return TCM_INVALID_CDB_FIELD;ret = sbc_check_prot(dev, cmd, cdb[1] >> 5, sectors, true);if (ret)return ret;cmd->se_cmd_flags |= SCF_SCSI_DATA_CDB;cmd->execute_cmd = sbc_execute_rw;break;case VARIABLE_LENGTH_CMD:{u16 service_action = get_unaligned_be16(&cdb[8]);switch (service_action) {case WRITE_SAME_32:sectors = transport_get_sectors_32(cdb);if (!sectors) {pr_err(\"WSNZ=1, WRITE_SAME wsectors=0 not\"       \" supported\\n\");return TCM_INVALID_CDB_FIELD;}size = sbc_get_size(cmd, 1);cmd->t_task_lba = get_unaligned_be64(&cdb[12]);ret = sbc_setup_write_same(cmd, cdb[10], ops);if (ret)return ret;break;default:pr_err(\"VARIABLE_LENGTH_CMD service action\"\" 0x%04x not supported\\n\", service_action);return TCM_UNSUPPORTED_SCSI_OPCODE;}break;}case COMPARE_AND_WRITE:if (!dev->dev_attrib.emulate_caw) {pr_err_ratelimited(\"se_device %s%s (vpd_unit_serial %s) reject COMPARE_AND_WRITE\\n\",   dev->se_hba->backend->ops->name,   config_item_name(&dev->dev_group.cg_item),   dev->t10_wwn.unit_serial);return TCM_UNSUPPORTED_SCSI_OPCODE;}sectors = cdb[13];    Currently enforce COMPARE_AND_WRITE for a single sector ", "if (!cmd->alua_nonop_delay)return 0;/* * struct se_cmd->alua_nonop_delay gets set by a target port group * defined interval in core_alua_state_nonoptimized() ": "core_alua_check_nonop_delay(struct se_cmd  cmd){if (!(cmd->se_cmd_flags & SCF_ALUA_NON_OPTIMIZED))return 0;    The ALUA ActiveNonOptimized access state delay can be disabled   in via configfs with a value of zero ", "if (se_cmd->orig_fe_lun != 0) ": "target_nacl_find_deve(nacl, se_cmd->orig_fe_lun);if (deve) {atomic_long_inc(&deve->total_cmds);if (se_cmd->data_direction == DMA_TO_DEVICE)atomic_long_add(se_cmd->data_length,&deve->write_bytes);else if (se_cmd->data_direction == DMA_FROM_DEVICE)atomic_long_add(se_cmd->data_length,&deve->read_bytes);if ((se_cmd->data_direction == DMA_TO_DEVICE) &&    deve->lun_access_ro) {pr_err(\"TARGET_CORE[%s]: Detected WRITE_PROTECTED LUN\"\" Access for 0x%08llx\\n\",se_cmd->se_tfo->fabric_name,se_cmd->orig_fe_lun);rcu_read_unlock();return TCM_WRITE_PROTECTED;}se_lun = deve->se_lun;if (!percpu_ref_tryget_live(&se_lun->lun_ref)) {se_lun = NULL;goto out_unlock;}se_cmd->se_lun = se_lun;se_cmd->pr_res_key = deve->pr_res_key;se_cmd->se_cmd_flags |= SCF_SE_LUN_CMD;se_cmd->lun_ref_active = true;}out_unlock:rcu_read_unlock();if (!se_lun) {    Use the se_portal_group->tpg_virt_lun0 to allow for   REPORT_LUNS, et al to be returned when no active   MappedLUN=0 exists for this Initiator Port. ", "attrib->max_unmap_block_desc_count = 1;attrib->unmap_granularity = bdev_discard_granularity(bdev) / block_size;attrib->unmap_granularity_alignment =bdev_discard_alignment(bdev) / block_size;return true;}EXPORT_SYMBOL(target_configure_unmap_from_queue": "target_configure_unmap_from_queue(struct se_dev_attrib  attrib,       struct block_device  bdev){int block_size = bdev_logical_block_size(bdev);if (!bdev_max_discard_sectors(bdev))return false;attrib->max_unmap_lba_count =bdev_max_discard_sectors(bdev) >> (ilog2(block_size) - 9);    Currently hardcoded to 1 in LinuxSCSI code.. ", "if (cdb[0] == REPORT_LUNS) ": "passthrough_parse_cdb(struct se_cmd  cmd,sense_reason_t ( exec_cmd)(struct se_cmd  cmd)){unsigned char  cdb = cmd->t_task_cdb;struct se_device  dev = cmd->se_dev;unsigned int size;    For REPORT LUNS we always need to emulate the response, for everything   else, pass it up. ", "rcu_barrier();kfree(t->tf_tpg_base_cit.ct_attrs);kfree(t->tf_ops);kfree(t);return;}}mutex_unlock(&g_tf_lock);}EXPORT_SYMBOL(target_unregister_template": "target_unregister_template(const struct target_core_fabric_ops  fo){struct target_fabric_configfs  t;mutex_lock(&g_tf_lock);list_for_each_entry(t, &g_tf_list, tf_list) {if (!strcmp(t->tf_ops->fabric_name, fo->fabric_name)) {BUG_ON(atomic_read(&t->tf_access_cnt));list_del(&t->tf_list);mutex_unlock(&g_tf_lock);    Wait for any outstanding fabric se_deve_entry->rcu_head   callbacks to complete post kfree_rcu(), before allowing   fabric driver unload of TFO->module to proceed. ", "rcu_barrier();kfree(tb);return;}}mutex_unlock(&backend_mutex);}EXPORT_SYMBOL(target_backend_unregister": "target_backend_unregister(const struct target_backend_ops  ops){struct target_backend  tb;mutex_lock(&backend_mutex);list_for_each_entry(tb, &backend_list, list) {if (tb->ops == ops) {list_del(&tb->list);mutex_unlock(&backend_mutex);    Wait for any outstanding backend driver ->rcu_head   callbacks to complete post TBO->free_device() ->   call_rcu(), before allowing backend driver module   unload of target_backend_ops->owner to proceed. ", "ret = CMDSN_LOWER_THAN_EXP;break;}mutex_unlock(&conn->sess->cmdsn_mutex);if (reject)iscsit_reject_cmd(cmd, reason, buf);return ret;}EXPORT_SYMBOL(iscsit_sequence_cmd": "iscsit_sequence_cmd(struct iscsit_conn  conn, struct iscsit_cmd  cmd,unsigned char  buf, __be32 cmdsn){int ret, cmdsn_ret;bool reject = false;u8 reason = ISCSI_REASON_BOOKMARK_NO_RESOURCES;mutex_lock(&conn->sess->cmdsn_mutex);cmdsn_ret = iscsit_check_received_cmdsn(conn->sess, be32_to_cpu(cmdsn));switch (cmdsn_ret) {case CMDSN_NORMAL_OPERATION:ret = iscsit_execute_cmd(cmd, 0);if ((ret >= 0) && !list_empty(&conn->sess->sess_ooo_cmdsn_list))iscsit_execute_ooo_cmdsns(conn->sess);else if (ret < 0) {reject = true;ret = CMDSN_ERROR_CANNOT_RECOVER;}break;case CMDSN_HIGHER_THAN_EXP:ret = iscsit_handle_ooo_cmdsn(conn->sess, cmd, be32_to_cpu(cmdsn));if (ret < 0) {reject = true;ret = CMDSN_ERROR_CANNOT_RECOVER;break;}ret = CMDSN_HIGHER_THAN_EXP;break;case CMDSN_LOWER_THAN_EXP:case CMDSN_MAXCMDSN_OVERRUN:default:cmd->i_state = ISTATE_REMOVE;iscsit_add_cmd_to_immediate_queue(cmd, conn, cmd->i_state);    Existing callers for iscsit_sequence_cmd() will silently   ignore commands with CMDSN_LOWER_THAN_EXP, so force this   return for CMDSN_MAXCMDSN_OVERRUN as well.. ", "struct iscsit_cmd *iscsit_allocate_cmd(struct iscsit_conn *conn, int state)": "iscsit_add_cmd_to_immediate_queue(cmd, cmd->conn, ISTATE_SEND_R2T);spin_lock_bh(&cmd->r2t_lock);return 0;}struct iscsi_r2t  iscsit_get_r2t_for_eos(struct iscsit_cmd  cmd,u32 offset,u32 length){struct iscsi_r2t  r2t;spin_lock_bh(&cmd->r2t_lock);list_for_each_entry(r2t, &cmd->cmd_r2t_list, r2t_list) {if ((r2t->offset <= offset) &&    (r2t->offset + r2t->xfer_len) >= (offset + length)) {spin_unlock_bh(&cmd->r2t_lock);return r2t;}}spin_unlock_bh(&cmd->r2t_lock);pr_err(\"Unable to locate R2T for Offset: %u, Length:\"\" %u\\n\", offset, length);return NULL;}struct iscsi_r2t  iscsit_get_r2t_from_list(struct iscsit_cmd  cmd){struct iscsi_r2t  r2t;spin_lock_bh(&cmd->r2t_lock);list_for_each_entry(r2t, &cmd->cmd_r2t_list, r2t_list) {if (!r2t->sent_r2t) {spin_unlock_bh(&cmd->r2t_lock);return r2t;}}spin_unlock_bh(&cmd->r2t_lock);pr_err(\"Unable to locate next R2T to send for ITT:\"\" 0x%08x.\\n\", cmd->init_task_tag);return NULL;}void iscsit_free_r2t(struct iscsi_r2t  r2t, struct iscsit_cmd  cmd){lockdep_assert_held(&cmd->r2t_lock);list_del(&r2t->r2t_list);kmem_cache_free(lio_r2t_cache, r2t);}void iscsit_free_r2ts_from_list(struct iscsit_cmd  cmd){struct iscsi_r2t  r2t,  r2t_tmp;spin_lock_bh(&cmd->r2t_lock);list_for_each_entry_safe(r2t, r2t_tmp, &cmd->cmd_r2t_list, r2t_list)iscsit_free_r2t(r2t, cmd);spin_unlock_bh(&cmd->r2t_lock);}static int iscsit_wait_for_tag(struct se_session  se_sess, int state, int  cpup){int tag = -1;DEFINE_SBQ_WAIT(wait);struct sbq_wait_state  ws;struct sbitmap_queue  sbq;if (state == TASK_RUNNING)return tag;sbq = &se_sess->sess_tag_pool;ws = &sbq->ws[0];for (;;) {sbitmap_prepare_to_wait(sbq, ws, &wait, state);if (signal_pending_state(state, current))break;tag = sbitmap_queue_get(sbq, cpup);if (tag >= 0)break;schedule();}sbitmap_finish_wait(sbq, ws, &wait);return tag;}    May be called from software interrupt (timer) context for allocating   iSCSI NopINs. ", "if (!(hdr->flags & ISCSI_FLAG_CMD_WRITE) &&    !(hdr->flags & ISCSI_FLAG_CMD_FINAL)) ": "iscsit_setup_scsi_cmd(struct iscsit_conn  conn, struct iscsit_cmd  cmd,  unsigned char  buf){int data_direction, payload_length;struct iscsi_ecdb_ahdr  ecdb_ahdr;struct iscsi_scsi_req  hdr;int iscsi_task_attr;unsigned char  cdb;int sam_task_attr;atomic_long_inc(&conn->sess->cmd_pdus);hdr= (struct iscsi_scsi_req  ) buf;payload_length= ntoh24(hdr->dlength);  FIXME; Add checks for AdditionalHeaderSegment ", "if (!cmd->immediate_data) ": "iscsit_process_scsi_cmd(struct iscsit_conn  conn, struct iscsit_cmd  cmd,    struct iscsi_scsi_req  hdr){int cmdsn_ret = 0;    Check the CmdSN against ExpCmdSNMaxCmdSN here if   the Immediate Bit is not set, and no Immediate   Data is attached.     A PDUCmdSN carrying Immediate Data can only   be processed after the DataCRC has passed.   If the DataCRC fails, the CmdSN MUST NOT   be acknowledged. (See below) ", "atomic_long_add(payload_length, &conn->sess->rx_data_octets);pr_debug(\"Got DataOut ITT: 0x%08x, TTT: 0x%08x,\"\" DataSN: 0x%08x, Offset: %u, Length: %u, CID: %hu\\n\",hdr->itt, hdr->ttt, hdr->datasn, ntohl(hdr->offset),payload_length, conn->cid);if (cmd->cmd_flags & ICF_GOT_LAST_DATAOUT) ": "iscsit_check_dataout_hdr(struct iscsit_conn  conn, void  buf,   struct iscsit_cmd  cmd, u32 payload_length,   bool  success){struct iscsi_data  hdr = buf;struct se_cmd  se_cmd;int rc;  iSCSI write ", "rc = iscsit_check_post_dataout(cmd, (unsigned char *)hdr, data_crc_failed);if ((rc == DATAOUT_NORMAL) || (rc == DATAOUT_WITHIN_COMMAND_RECOVERY))return 0;else if (rc == DATAOUT_SEND_R2T) ": "iscsit_check_dataout_payload(struct iscsit_cmd  cmd, struct iscsi_data  hdr,     bool data_crc_failed){struct iscsit_conn  conn = cmd->conn;int rc, ooo_cmdsn;    Increment post receive data and CRC values or perform   within-command recovery. ", "if (hdr->ttt == cpu_to_be32(0xFFFFFFFF)) ": "iscsit_setup_nop_out(struct iscsit_conn  conn, struct iscsit_cmd  cmd, struct iscsi_nopout  hdr){u32 payload_length = ntoh24(hdr->dlength);if (!(hdr->flags & ISCSI_FLAG_CMD_FINAL)) {pr_err(\"NopOUT Flag's, Left Most Bit not set, protocol error.\\n\");if (!cmd)return iscsit_add_reject(conn, ISCSI_REASON_PROTOCOL_ERROR, (unsigned char  )hdr);return iscsit_reject_cmd(cmd, ISCSI_REASON_PROTOCOL_ERROR, (unsigned char  )hdr);}if (hdr->itt == RESERVED_ITT && !(hdr->opcode & ISCSI_OP_IMMEDIATE)) {pr_err(\"NOPOUT ITT is reserved, but Immediate Bit is\"\" not set, protocol error.\\n\");if (!cmd)return iscsit_add_reject(conn, ISCSI_REASON_PROTOCOL_ERROR, (unsigned char  )hdr);return iscsit_reject_cmd(cmd, ISCSI_REASON_PROTOCOL_ERROR, (unsigned char  )hdr);}if (payload_length > conn->conn_ops->MaxXmitDataSegmentLength) {pr_err(\"NOPOUT Ping Data DataSegmentLength: %u is\"\" greater than MaxXmitDataSegmentLength: %u, protocol\"\" error.\\n\", payload_length,conn->conn_ops->MaxXmitDataSegmentLength);if (!cmd)return iscsit_add_reject(conn, ISCSI_REASON_PROTOCOL_ERROR, (unsigned char  )hdr);return iscsit_reject_cmd(cmd, ISCSI_REASON_PROTOCOL_ERROR, (unsigned char  )hdr);}pr_debug(\"Got NOPOUT Ping %s ITT: 0x%08x, TTT: 0x%08x,\"\" CmdSN: 0x%08x, ExpStatSN: 0x%08x, Length: %u\\n\",hdr->itt == RESERVED_ITT ? \"Response\" : \"Request\",hdr->itt, hdr->ttt, hdr->cmdsn, hdr->exp_statsn,payload_length);    This is not a response to a Unsolicited NopIN, which means   it can either be a NOPOUT ping request (with a valid ITT),   or a NOPOUT not requesting a NOPIN (with a reserved ITT).   Either way, make sure we allocate an struct iscsit_cmd, as both   can contain ping data. ", "if (hdr->itt != RESERVED_ITT) ": "iscsit_process_nop_out(struct iscsit_conn  conn, struct iscsit_cmd  cmd,   struct iscsi_nopout  hdr){struct iscsit_cmd  cmd_p = NULL;int cmdsn_ret = 0;    Initiator is expecting a NopIN ping reply.. ", "if (function != ISCSI_TM_FUNC_TASK_REASSIGN) ": "iscsit_handle_task_mgt_cmd(struct iscsit_conn  conn, struct iscsit_cmd  cmd,   unsigned char  buf){struct se_tmr_req  se_tmr;struct iscsi_tmr_req  tmr_req;struct iscsi_tm  hdr;int out_of_order_cmdsn = 0, ret;u8 function, tcm_function = TMR_UNKNOWN;hdr= (struct iscsi_tm  ) buf;hdr->flags &= ~ISCSI_FLAG_CMD_FINAL;function = hdr->flags;pr_debug(\"Got Task Management Request ITT: 0x%08x, CmdSN:\"\" 0x%08x, Function: 0x%02x, RefTaskTag: 0x%08x, RefCmdSN:\"\" 0x%08x, CID: %hu\\n\", hdr->itt, hdr->cmdsn, function,hdr->rtt, hdr->refcmdsn, conn->cid);if ((function != ISCSI_TM_FUNC_ABORT_TASK) &&    ((function != ISCSI_TM_FUNC_TASK_REASSIGN) &&     hdr->rtt != RESERVED_ITT)) {pr_err(\"RefTaskTag should be set to 0xFFFFFFFF.\\n\");hdr->rtt = RESERVED_ITT;}if ((function == ISCSI_TM_FUNC_TASK_REASSIGN) &&!(hdr->opcode & ISCSI_OP_IMMEDIATE)) {pr_err(\"Task Management Request TASK_REASSIGN not\"\" issued as immediate command, bad iSCSI Initiator\"\"implementation\\n\");return iscsit_add_reject_cmd(cmd,     ISCSI_REASON_PROTOCOL_ERROR, buf);}if ((function != ISCSI_TM_FUNC_ABORT_TASK) &&    be32_to_cpu(hdr->refcmdsn) != ISCSI_RESERVED_TAG)hdr->refcmdsn = cpu_to_be32(ISCSI_RESERVED_TAG);cmd->data_direction = DMA_NONE;cmd->tmr_req = kzalloc(sizeof( cmd->tmr_req), GFP_KERNEL);if (!cmd->tmr_req) {return iscsit_add_reject_cmd(cmd,     ISCSI_REASON_BOOKMARK_NO_RESOURCES,     buf);}__target_init_cmd(&cmd->se_cmd, &iscsi_ops,  conn->sess->se_sess, 0, DMA_NONE,  TCM_SIMPLE_TAG, cmd->sense_buffer + 2,  scsilun_to_int(&hdr->lun),  conn->cmd_cnt);target_get_sess_cmd(&cmd->se_cmd, true);    TASK_REASSIGN for ERL=2  connection stays inside of   LIO-Target $FABRIC_MOD ", "text_ptr = strchr(text_in, '=');BUG_ON(!text_ptr);if (!strncmp(\"=All\", text_ptr, 5)) ": "iscsit_process_text_cmd(struct iscsit_conn  conn, struct iscsit_cmd  cmd,struct iscsi_text  hdr){unsigned char  text_in = cmd->text_in_ptr,  text_ptr;int cmdsn_ret;if (!text_in) {cmd->targ_xfer_tag = be32_to_cpu(hdr->ttt);if (cmd->targ_xfer_tag == 0xFFFFFFFF) {pr_err(\"Unable to locate text_in buffer for sendtargets\"       \" discovery\\n\");goto reject;}goto empty_sendtargets;}if (strncmp(\"SendTargets=\", text_in, 12) != 0) {pr_err(\"Received Text Data that is not\"\" SendTargets, cannot continue.\\n\");goto reject;}  '=' confirmed in strncmp ", "if ((reason_code == ISCSI_LOGOUT_REASON_CLOSE_SESSION) ||   ((reason_code == ISCSI_LOGOUT_REASON_CLOSE_CONNECTION) &&    be16_to_cpu(hdr->cid) == conn->cid))logout_remove = 1;spin_lock_bh(&conn->cmd_lock);list_add_tail(&cmd->i_conn_node, &conn->conn_cmd_list);spin_unlock_bh(&conn->cmd_lock);if (reason_code != ISCSI_LOGOUT_REASON_RECOVERY)iscsit_ack_from_expstatsn(conn, be32_to_cpu(hdr->exp_statsn));/* * Immediate commands are executed, well, immediately. * Non-Immediate Logout Commands are executed in CmdSN order. ": "iscsit_handle_logout_cmd(struct iscsit_conn  conn, struct iscsit_cmd  cmd,unsigned char  buf){int cmdsn_ret, logout_remove = 0;u8 reason_code = 0;struct iscsi_logout  hdr;struct iscsi_tiqn  tiqn = iscsit_snmp_get_tiqn(conn);hdr= (struct iscsi_logout  ) buf;reason_code= (hdr->flags & 0x7f);if (tiqn) {spin_lock(&tiqn->logout_stats.lock);if (reason_code == ISCSI_LOGOUT_REASON_CLOSE_SESSION)tiqn->logout_stats.normal_logouts++;elsetiqn->logout_stats.abnormal_logouts++;spin_unlock(&tiqn->logout_stats.lock);}pr_debug(\"Got Logout Request ITT: 0x%08x CmdSN: 0x%08x\"\" ExpStatSN: 0x%08x Reason: 0x%02x CID: %hu on CID: %hu\\n\",hdr->itt, hdr->cmdsn, hdr->exp_statsn, reason_code,hdr->cid, conn->cid);if (conn->conn_state != TARG_CONN_STATE_LOGGED_IN) {pr_err(\"Received logout request on connection that\"\" is not in logged in state, ignoring request.\\n\");iscsit_free_cmd(cmd, false);return 0;}cmd->iscsi_opcode       = ISCSI_OP_LOGOUT;cmd->i_state            = ISTATE_SEND_LOGOUTRSP;cmd->immediate_cmd      = ((hdr->opcode & ISCSI_OP_IMMEDIATE) ? 1 : 0);conn->sess->init_task_tag = cmd->init_task_tag  = hdr->itt;cmd->targ_xfer_tag      = 0xFFFFFFFF;cmd->cmd_sn             = be32_to_cpu(hdr->cmdsn);cmd->exp_stat_sn        = be32_to_cpu(hdr->exp_statsn);cmd->logout_cid         = be16_to_cpu(hdr->cid);cmd->logout_reason      = reason_code;cmd->data_direction     = DMA_NONE;    We need to sleep in these cases (by returning 1) until the Logout   Response gets sent in the tx thread. ", "switch (hdr->flags & ISCSI_FLAG_SNACK_TYPE_MASK) ": "iscsit_handle_snack(struct iscsit_conn  conn,unsigned char  buf){struct iscsi_snack  hdr;hdr= (struct iscsi_snack  ) buf;hdr->flags&= ~ISCSI_FLAG_CMD_FINAL;pr_debug(\"Got ISCSI_INIT_SNACK, ITT: 0x%08x, ExpStatSN:\"\" 0x%08x, Type: 0x%02x, BegRun: 0x%08x, RunLength: 0x%08x,\"\" CID: %hu\\n\", hdr->itt, hdr->exp_statsn, hdr->flags,hdr->begrun, hdr->runlength, conn->cid);if (!conn->sess->sess_ops->ErrorRecoveryLevel) {pr_err(\"Initiator sent SNACK request while in\"\" ErrorRecoveryLevel=0.\\n\");return iscsit_add_reject(conn, ISCSI_REASON_PROTOCOL_ERROR, buf);}    SNACK_DATA and SNACK_R2T are both 0,  so check which function to   call from inside iscsi_send_recovery_datain_or_r2t(). ", "switch (cmd->logout_reason) ": "iscsit_build_logout_rsp(struct iscsit_cmd  cmd, struct iscsit_conn  conn,struct iscsi_logout_rsp  hdr){struct iscsit_conn  logout_conn = NULL;struct iscsi_conn_recovery  cr = NULL;struct iscsit_session  sess = conn->sess;    The actual shutting down of Sessions andor Connections   for CLOSESESSION and CLOSECONNECTION Logout Requests   is done in scsi_logout_post_handler(). ", "if (iscsit_global->discovery_tpg)iscsit_tpg_disable_portal_group(iscsit_global->discovery_tpg, 1);target_unregister_template(&iscsi_ops);out:kfree(iscsit_global);return -ENOMEM;}static void __exit iscsi_target_cleanup_module(void)": "iscsit_response_queue= iscsit_response_queue,.iscsit_queue_data_in= iscsit_queue_rsp,.iscsit_queue_status= iscsit_queue_rsp,.iscsit_aborted_task= iscsit_aborted_task,.iscsit_xmit_pdu= iscsit_xmit_pdu,.iscsit_get_rx_pdu= iscsit_get_rx_pdu,.iscsit_get_sup_prot_ops = iscsit_get_sup_prot_ops,};static int __init iscsi_target_init_module(void){int ret = 0, size;pr_debug(\"iSCSI-Target \"ISCSIT_VERSION\"\\n\");iscsit_global = kzalloc(sizeof( iscsit_global), GFP_KERNEL);if (!iscsit_global)return -1;spin_lock_init(&iscsit_global->ts_bitmap_lock);mutex_init(&auth_id_lock);idr_init(&tiqn_idr);ret = target_register_template(&iscsi_ops);if (ret)goto out;size = BITS_TO_LONGS(ISCSIT_BITMAP_BITS)   sizeof(long);iscsit_global->ts_bitmap = vzalloc(size);if (!iscsit_global->ts_bitmap)goto configfs_out;if (!zalloc_cpumask_var(&iscsit_global->allowed_cpumask, GFP_KERNEL)) {pr_err(\"Unable to allocate iscsit_global->allowed_cpumask\\n\");goto bitmap_out;}cpumask_setall(iscsit_global->allowed_cpumask);lio_qr_cache = kmem_cache_create(\"lio_qr_cache\",sizeof(struct iscsi_queue_req),__alignof__(struct iscsi_queue_req), 0, NULL);if (!lio_qr_cache) {pr_err(\"Unable to kmem_cache_create() for\"\" lio_qr_cache\\n\");goto cpumask_out;}lio_dr_cache = kmem_cache_create(\"lio_dr_cache\",sizeof(struct iscsi_datain_req),__alignof__(struct iscsi_datain_req), 0, NULL);if (!lio_dr_cache) {pr_err(\"Unable to kmem_cache_create() for\"\" lio_dr_cache\\n\");goto qr_out;}lio_ooo_cache = kmem_cache_create(\"lio_ooo_cache\",sizeof(struct iscsi_ooo_cmdsn),__alignof__(struct iscsi_ooo_cmdsn), 0, NULL);if (!lio_ooo_cache) {pr_err(\"Unable to kmem_cache_create() for\"\" lio_ooo_cache\\n\");goto dr_out;}lio_r2t_cache = kmem_cache_create(\"lio_r2t_cache\",sizeof(struct iscsi_r2t), __alignof__(struct iscsi_r2t),0, NULL);if (!lio_r2t_cache) {pr_err(\"Unable to kmem_cache_create() for\"\" lio_r2t_cache\\n\");goto ooo_out;}iscsit_register_transport(&iscsi_target_transport);if (iscsit_load_discovery_tpg() < 0)goto r2t_out;return ret;r2t_out:iscsit_unregister_transport(&iscsi_target_transport);kmem_cache_destroy(lio_r2t_cache);ooo_out:kmem_cache_destroy(lio_ooo_cache);dr_out:kmem_cache_destroy(lio_dr_cache);qr_out:kmem_cache_destroy(lio_qr_cache);cpumask_out:free_cpumask_var(iscsit_global->allowed_cpumask);bitmap_out:vfree(iscsit_global->ts_bitmap);configfs_out:  XXX: this probably wants it to be it's own unwind step.. ", "cmd->maxcmdsn_inc = 0;hdr->exp_cmdsn = cpu_to_be32(conn->sess->exp_cmd_sn);hdr->max_cmdsn = cpu_to_be32((u32) atomic_read(&conn->sess->max_cmd_sn));pr_debug(\"Built Text Response: ITT: 0x%08x, TTT: 0x%08x, StatSN: 0x%08x,\"\" Length: %u, CID: %hu F: %d C: %d\\n\", cmd->init_task_tag,cmd->targ_xfer_tag, cmd->stat_sn, text_length, conn->cid,!!(hdr->flags & ISCSI_FLAG_CMD_FINAL),!!(hdr->flags & ISCSI_FLAG_TEXT_CONTINUE));return text_length + padding;}EXPORT_SYMBOL(iscsit_build_text_rsp": "iscsit_build_text_rsp(struct iscsit_cmd  cmd, struct iscsit_conn  conn,      struct iscsi_text_rsp  hdr,      enum iscsit_transport_type network_transport){int text_length, padding;bool completed = true;text_length = iscsit_build_sendtargets_response(cmd, network_transport,cmd->read_data_done,&completed);if (text_length < 0)return text_length;if (completed) {hdr->flags = ISCSI_FLAG_CMD_FINAL;} else {hdr->flags = ISCSI_FLAG_TEXT_CONTINUE;cmd->read_data_done += text_length;if (cmd->targ_xfer_tag == 0xFFFFFFFF)cmd->targ_xfer_tag = session_get_next_ttt(conn->sess);}hdr->opcode = ISCSI_OP_TEXT_RSP;padding = ((-text_length) & 3);hton24(hdr->dlength, text_length);hdr->itt = cmd->init_task_tag;hdr->ttt = cpu_to_be32(cmd->targ_xfer_tag);cmd->stat_sn = conn->stat_sn++;hdr->statsn = cpu_to_be32(cmd->stat_sn);iscsit_increment_maxcmdsn(cmd, conn->sess);    Reset maxcmdsn_inc in multi-part text payload exchanges to   correctly increment MaxCmdSN for each response answering a   non immediate text request with a valid CmdSN. ", "iscsit_thread_reschedule(conn);/* * mode == 1 signals iscsi_target_tx_thread() usage. * mode == 0 signals iscsi_target_rx_thread() usage. ": "iscsit_thread_check_cpumask(struct iscsit_conn  conn,struct task_struct  p,int mode){    The TX and RX threads maybe call iscsit_thread_check_cpumask()   at the same time. The RX thread might be faster and return from   iscsit_thread_reschedule() with conn_rx_reset_cpumask set to 0.   Then the TX thread sets it back to 1.   The next time the RX thread loops, it sees conn_rx_reset_cpumask   set to 1 and calls set_cpus_allowed_ptr() again and set it to 0. ", "l_conn = iscsit_get_conn_from_cid(sess,cmd->logout_cid);if (!l_conn) ": "iscsit_logout_post_handler_diffcid() as to give enough   time for any non immediate command's CmdSN to be   acknowledged on the connection in question.     Here we simply make sure the CID is still around. ", "timeout = 100000;interrupt_data[0][0] = 1;while (interrupt_data[0][0] || pmu_state != idle) ": "pmu_request(&req, NULL, 2, PMU_SET_INTR_MASK, pmu_intr_mask);timeout =  100000;while (!req.complete) {if (--timeout < 0) {printk(KERN_ERR \"init_pmu: no response from PMU\\n\");return 0;}udelay(10);pmu_poll();}  ack all pending interrupts ", "req->data[0] = PMU_ADB_CMD;req->nbytes += 2;req->reply_expected = 1;req->reply_len = 0;ret = pmu_queue_request(req);break;}if (ret) ": "pmu_queue_request(req);break;case CUDA_PACKET:switch (req->data[1]) {case CUDA_GET_TIME:if (req->nbytes != 2)break;req->data[0] = PMU_READ_RTC;req->nbytes = 1;req->reply_len = 3;req->reply[0] = CUDA_PACKET;req->reply[1] = 0;req->reply[2] = CUDA_GET_TIME;ret = pmu_queue_request(req);break;case CUDA_SET_TIME:if (req->nbytes != 6)break;req->data[0] = PMU_SET_RTC;req->nbytes = 5;for (i = 1; i <= 4; ++i)req->data[i] = req->data[i+1];req->reply_len = 3;req->reply[0] = CUDA_PACKET;req->reply[1] = 0;req->reply[2] = CUDA_SET_TIME;ret = pmu_queue_request(req);break;}break;case ADB_PACKET:    if (!pmu_has_adb)    return -ENXIO;for (i = req->nbytes - 1; i > 1; --i)req->data[i+2] = req->data[i];req->data[3] = req->nbytes - 2;req->data[2] = pmu_adb_flags; req->data[1] = req->data[1];", "extern void low_sleep_handler(void);extern void enable_kernel_altivec(void);extern void enable_kernel_fp(void);#ifdef DEBUG_SLEEPint pmu_polled_request(struct adb_request *req);void pmu_blink(int n);#endif/* * This table indicates for each PMU opcode: * - the number of data bytes to be sent with the command, or -1 *   if a length byte should be sent, * - the number of response bytes which the PMU will return, or *   -1 if it will send a length byte. ": "pmu_poll_adb,.reset_bus    = pmu_adb_reset_bus,};#endif   CONFIG_ADB ", "if (pmu_kind == PMU_KEYLARGO_BASED) ": "pmu_wait_complete(&req);if (req.reply_len > 0)pmu_version = req.reply[0];  Read server mode setting ", "static unsigned long async_req_locks;#define NUM_IRQ_STATS 13static unsigned int pmu_irq_stats[NUM_IRQ_STATS];static struct proc_dir_entry *proc_pmu_root;static struct proc_dir_entry *proc_pmu_info;static struct proc_dir_entry *proc_pmu_irqstats;static struct proc_dir_entry *proc_pmu_options;static int option_server_mode;int pmu_battery_count;static int pmu_cur_battery;unsigned int pmu_power_flags = PMU_PWR_AC_PRESENT;struct pmu_battery_info pmu_batteries[PMU_MAX_BATTERIES];static int query_batt_timer = BATTERY_POLLING_COUNT;static struct adb_request batt_req;static struct proc_dir_entry *proc_pmu_batt[PMU_MAX_BATTERIES];int asleep;#ifdef CONFIG_ADBstatic int adb_dev_map;static int pmu_adb_flags;static int pmu_probe(void);static int pmu_init(void);static int pmu_send_request(struct adb_request *req, int sync);static int pmu_adb_autopoll(int devs);static int pmu_adb_reset_bus(void);#endif /* CONFIG_ADB ": "pmu_suspended;static DEFINE_SPINLOCK(pmu_lock);static u8 pmu_intr_mask;static int pmu_version;static int drop_interrupts;#if defined(CONFIG_SUSPEND) && defined(CONFIG_PPC32)static int option_lid_wakeup = 1;#endif   CONFIG_SUSPEND && CONFIG_PPC32 ", "static voidpmu_handle_data(unsigned char *data, int len)": "pmu_resume(void){unsigned long flags;if (pmu_state == uninitialized || pmu_suspended < 1)return;spin_lock_irqsave(&pmu_lock, flags);pmu_suspended--;if (pmu_suspended > 0) {spin_unlock_irqrestore(&pmu_lock, flags);return;}adb_int_pending = 1;if (gpio_irq >= 0)enable_irq(gpio_irq);out_8(&via1[IER], CB1_INT | IER_SET);spin_unlock_irqrestore(&pmu_lock, flags);pmu_poll();}  Interrupt data could be the result data from an ADB cmd ", "#define RTC_OFFSET2082844800time64_t pmu_get_time(void)": "pmu_enable_irled(int on){struct adb_request req;if (pmu_state == uninitialized)return ;if (pmu_kind == PMU_KEYLARGO_BASED)return ;pmu_request(&req, NULL, 2, PMU_POWER_CTRL, PMU_POW_IRLED |    (on ? PMU_POW_ON : PMU_POW_OFF));pmu_wait_complete(&req);}  Offset between Unix time (1970-based) and Mac time (1904-based) ", "static int init_pmu(void);static void pmu_start(void);static irqreturn_t via_pmu_interrupt(int irq, void *arg);static irqreturn_t gpio1_interrupt(int irq, void *arg);#ifdef CONFIG_PROC_FSstatic int pmu_info_proc_show(struct seq_file *m, void *v);static int pmu_irqstats_proc_show(struct seq_file *m, void *v);static int pmu_battery_proc_show(struct seq_file *m, void *v);#endifstatic void pmu_pass_intr(unsigned char *data, int len);static const struct proc_ops pmu_options_proc_ops;#ifdef CONFIG_ADBconst struct adb_driver via_pmu_driver = ": "pmu_power_flags = PMU_PWR_AC_PRESENT;struct pmu_battery_info pmu_batteries[PMU_MAX_BATTERIES];static int query_batt_timer = BATTERY_POLLING_COUNT;static struct adb_request batt_req;static struct proc_dir_entry  proc_pmu_batt[PMU_MAX_BATTERIES];int asleep;#ifdef CONFIG_ADBstatic int adb_dev_map;static int pmu_adb_flags;static int pmu_probe(void);static int pmu_init(void);static int pmu_send_request(struct adb_request  req, int sync);static int pmu_adb_autopoll(int devs);static int pmu_adb_reset_bus(void);#endif   CONFIG_ADB ", "if (!smu_irq_inited || !smu->db_irq)smu_spinwait_cmd(cmd);return 0;}EXPORT_SYMBOL(smu_queue_cmd": "smu_queue_cmd(struct smu_cmd  cmd){unsigned long flags;if (smu == NULL)return -ENODEV;if (cmd->data_len > SMU_MAX_DATA ||    cmd->reply_len > SMU_MAX_DATA)return -EINVAL;cmd->status = 1;spin_lock_irqsave(&smu->lock, flags);list_add_tail(&cmd->link, &smu->cmd_list);if (smu->cmd_cur == NULL)smu_start_cmd();spin_unlock_irqrestore(&smu->lock, flags);  Workaround for early calls when irq isn't available ", "#ifdef CONFIG_MACint __init find_via_cuda(void)": "cuda_poll(void);static int cuda_write(struct adb_request  req);int cuda_request(struct adb_request  req, void ( done)(struct adb_request  ), int nbytes, ...);#ifdef CONFIG_ADBstruct adb_driver via_cuda_driver = {.name         = \"CUDA\",.probe        = cuda_probe,.send_request = cuda_send_request,.autopoll     = cuda_adb_autopoll,.poll         = cuda_poll,.reset_bus    = cuda_reset_adb_bus,};#endif   CONFIG_ADB ", "int macio_register_driver(struct macio_driver *drv)": "macio_register_driver - Registers a new MacIO device driver   @drv: pointer to the driver definition structure ", "void macio_unregister_driver(struct macio_driver *drv)": "macio_unregister_driver - Unregisters a new MacIO device driver   @drv: pointer to the driver definition structure ", "static void macio_release_dev(struct device *dev)": "macio_dev_put(struct macio_dev  dev){if (dev)put_device(&dev->ofdev.dev);}static int macio_device_probe(struct device  dev){int error = -ENODEV;struct macio_driver  drv;struct macio_dev  macio_dev;const struct of_device_id  match;drv = to_macio_driver(dev->driver);macio_dev = to_macio_device(dev);if (!drv->probe)return error;macio_dev_get(macio_dev);match = of_match_device(drv->driver.of_match_table, dev);if (match)error = drv->probe(macio_dev, match);if (error)macio_dev_put(macio_dev);return error;}static void macio_device_remove(struct device  dev){struct macio_dev   macio_dev = to_macio_device(dev);struct macio_driver   drv = to_macio_driver(dev->driver);if (dev->driver && drv->remove)drv->remove(macio_dev);macio_dev_put(macio_dev);}static void macio_device_shutdown(struct device  dev){struct macio_dev   macio_dev = to_macio_device(dev);struct macio_driver   drv = to_macio_driver(dev->driver);if (dev->driver && drv->shutdown)drv->shutdown(macio_dev);}static int macio_device_suspend(struct device  dev, pm_message_t state){struct macio_dev   macio_dev = to_macio_device(dev);struct macio_driver   drv = to_macio_driver(dev->driver);if (dev->driver && drv->suspend)return drv->suspend(macio_dev, state);return 0;}static int macio_device_resume(struct device   dev){struct macio_dev   macio_dev = to_macio_device(dev);struct macio_driver   drv = to_macio_driver(dev->driver);if (dev->driver && drv->resume)return drv->resume(macio_dev);return 0;}static int macio_device_modalias(const struct device  dev, struct kobj_uevent_env  env){return of_device_uevent_modalias(dev, env);}extern const struct attribute_group  macio_dev_groups[];struct bus_type macio_bus_type = {       .name= \"macio\",       .match= macio_bus_match,       .uevent= macio_device_modalias,       .probe= macio_device_probe,       .remove= macio_device_remove,       .shutdown = macio_device_shutdown,       .suspend= macio_device_suspend,       .resume= macio_device_resume,       .dev_groups = macio_dev_groups,};static int __init macio_bus_driver_init(void){return bus_register(&macio_bus_type);}postcore_initcall(macio_bus_driver_init);     macio_release_dev - free a macio device structure when all users of it are   finished.   @dev: device that's been disconnected     Will be called only by the device core when all users of this macio device   are done. This currently means never as we don't hot remove any macio   device yet, though that will happen with mediabay based devices in a later   implementation. ", "int macio_request_resource(struct macio_dev *dev, int resource_no,   const char *name)": "macio_enable_devres(struct macio_dev  dev){struct macio_devres  dr;dr = devres_find(&dev->ofdev.dev, maciom_release, NULL, NULL);if (!dr) {dr = devres_alloc(maciom_release, sizeof( dr), GFP_KERNEL);if (!dr)return -ENOMEM;}return devres_get(&dev->ofdev.dev, dr, NULL, NULL) != NULL;}static struct macio_devres   find_macio_dr(struct macio_dev  dev){return devres_find(&dev->ofdev.dev, maciom_release, NULL, NULL);}    macio_request_resource - Request an MMIO resource   @dev: pointer to the device holding the resource  @resource_no: resource number to request  @name: resource name    Mark  memory region number @resource_no associated with MacIO  device @dev as being reserved by owner @name.  Do not access  any address inside the memory regions unless this call returns  successfully.    Returns 0 on success, or %EBUSY on error.  A warning  message is also printed on failure. ", "int macio_request_resources(struct macio_dev *dev, const char *name)": "macio_request_resources - Reserve all memory resources  @dev: MacIO device whose resources are to be reserved  @name: Name to be associated with resource.    Mark all memory regions associated with MacIO device @dev as  being reserved by owner @name.  Do not access any address inside  the memory regions unless this call returns successfully.    Returns 0 on success, or %EBUSY on error.  A warning  message is also printed on failure. ", "void macio_release_resources(struct macio_dev *dev)": "macio_release_resources - Release reserved memory resources  @dev: MacIO device whose resources were previously reserved ", "for (i = 1; i < 16; i++) ": "adb_request  req){        int i;        printk(\"adb reply (%d)\", req->reply_len);        for(i = 0; i < req->reply_len; i++)                printk(\" %x\", req->reply[i]);        printk(\"\\n\");}#endifstatic int adb_scan_bus(void){int i, highFree=0, noMovement;int devmask = 0;struct adb_request req;  assumes adb_handler[] is all zeroes at this point ", "static DEFINE_MUTEX(adb_handler_mutex);static DEFINE_RWLOCK(adb_handler_lock);#if 0static void printADBreply(struct adb_request *req)": "adb_unregister returns, we know that the old handler isn't being   called. ", "if (is_vmalloc_addr(cpu_addr)) ": "rproc_va_to_pa(void  cpu_addr){    Return physical address according to virtual address location   - in vmalloc: if region ioremapped or defined as dma_alloc_coherent   - in kernel: if region allocated in generic dma memory pool ", "void *rproc_da_to_va(struct rproc *rproc, u64 da, size_t len, bool *is_iomem)": "rproc_da_to_va() - lookup the kernel virtual address for a remoteproc address   @rproc: handle of a remote processor   @da: remoteproc device address to translate   @len: length of the memory region @da is pointing to   @is_iomem: optional pointer filled in to indicate if @da is iomapped memory     Some remote processors will ask us to allocate them physically contiguous   memory regions (which we call \"carveouts\"), and map them to specific   device addresses (which are hardcoded in the firmware). They may also have   dedicated memory regions internal to the processors, and use them either   exclusively or alongside carveouts.     They may then ask us to copy objects into specific device addresses (e.g.   codedata sections) or expose us certain symbols in other device address   (e.g. their trace buffer).     This function is a helper function with which we can go over the allocated   carveouts and translate specific device addresses to kernel virtual addresses   so we can access the referenced memory. This function also allows to perform   translations on the internal remoteproc memory regions through a platform   implementation specific da_to_va ops, if present.     Note: phys_to_virt(iommu_iova_to_phys(rproc->domain, da)) will work too,   but only on kernel direct mapped RAM memory. Instead, we're just using   here the output of the DMA API for the carveouts, which should be more   correct.     Return: a valid kernel address on success or NULL on failure ", "ret = idr_alloc(&rproc->notifyids, rvring, 0, 0, GFP_KERNEL);if (ret < 0) ": "rproc_add_carveout(rproc, mem);}    Assign an rproc-wide unique index for this vring   TODO: assign a notifyid for rvdev updates as well   TODO: support predefined notifyids (via resource table) ", "__printf(5, 6)struct rproc_mem_entry *rproc_of_resm_mem_entry_init(struct device *dev, u32 of_resm_idx, size_t len,     u32 da, const char *name, ...)": "rproc_of_resm_mem_entry_init() - allocate and initialize rproc_mem_entry struct   from a reserved memory phandle   @dev: pointer on device struct   @of_resm_idx: reserved memory phandle index in \"memory-region\"   @len: memory carveout length   @da: device address   @name: carveout name     This function allocates a rproc_mem_entry struct and fill it with parameters   provided by client.     Return: a valid pointer on success, or NULL on failure ", "int rproc_of_parse_firmware(struct device *dev, int index, const char **fw_name)": "rproc_of_parse_firmware() - parse and return the firmware-name   @dev: pointer on device struct representing a rproc   @index: index to use for the firmware-name retrieval   @fw_name: pointer to a character string, in which the firmware             name is returned on success and unmodified otherwise.     This is an OF helper function that parses a device's DT node for   the \"firmware-name\" property and returns the firmware name pointer   in @fw_name on success.     Return: 0 on success, or an appropriate failure. ", "void rproc_resource_cleanup(struct rproc *rproc)": "rproc_resource_cleanup() - clean up and free all acquired resources   @rproc: rproc handle     This function will free all resources acquired for @rproc, and it   is called whenever @rproc either shuts down or fails to boot. ", "if (rproc->state == RPROC_DETACHED)return rproc_boot(rproc);/* * We're initiating an asynchronous firmware loading, so we can * be built-in kernel code, without hanging the boot process. ": "rproc_boot(rproc);release_firmware(fw);}static int rproc_trigger_auto_boot(struct rproc  rproc){int ret;    Since the remote processor is in a detached state, it has already   been booted by another entity.  As such there is no point in waiting   for a firmware image to be loaded, we can simply initiate the process   of attaching to it immediately. ", "rproc->cached_table = kmemdup(rproc->table_ptr,      rproc->table_sz, GFP_KERNEL);if (!rproc->cached_table)return -ENOMEM;/* * Since the remote processor is being switched off the clean table * won't be needed.  Allocated in rproc_set_rsc_table(). ": "rproc_shutdown(). ", "rproc->cached_table = kmemdup(rproc->table_ptr,      rproc->table_sz, GFP_KERNEL);if (!rproc->cached_table)return -ENOMEM;/* * Use a copy of the resource table for the remainder of the * shutdown process. ": "rproc_detach(). ", "#ifdef CONFIG_OFstruct rproc *rproc_get_by_phandle(phandle phandle)": "rproc_put() to decrement it back once rproc isn't needed anymore.     Return: rproc handle on success, and NULL on failure ", "int rproc_set_firmware(struct rproc *rproc, const char *fw_name)": "rproc_set_firmware() - assign a new firmware   @rproc: rproc handle to which the new firmware is being assigned   @fw_name: new firmware name to be assigned     This function allows remoteproc drivers or clients to configure a custom   firmware name that is different from the default name used during remoteproc   registration. The function does not trigger a remote processor boot,   only sets the firmware name used for a subsequent boot. This function   should also be called only when the remote processor is offline.     This allows either the userspace to configure a different name through   sysfs or a kernel-level remoteproc or a remoteproc client driver to set   a specific firmware when it is controlling the boot and shutdown of the   remote processor.     Return: 0 on success or a negative value upon failure ", "int devm_rproc_add(struct device *dev, struct rproc *rproc)": "rproc_delete_debug_dir(rproc);device_del(dev);rproc_remove_cdev:rproc_char_device_remove(rproc);return ret;}EXPORT_SYMBOL(rproc_add);static void devm_rproc_remove(void  rproc){rproc_del(rproc);}     devm_rproc_add() - resource managed rproc_add()   @dev: the underlying device   @rproc: the remote processor handle to register     This function performs like rproc_add() but the registered rproc device will   automatically be removed on driver detach.     Return: 0 on success, negative errno on failure ", "static DEFINE_IDA(rproc_dev_index);static struct workqueue_struct *rproc_recovery_wq;static const char * const rproc_crash_names[] = ": "rproc_alloc_carveout(struct rproc  rproc,struct rproc_mem_entry  mem);static int rproc_release_carveout(struct rproc  rproc,  struct rproc_mem_entry  mem);  Unique indices for remoteproc devices ", "if (rproc->table_ptr) ": "rproc_free_vring(struct rproc_vring  rvring){struct rproc  rproc = rvring->rvdev->rproc;int idx = rvring - rvring->rvdev->vring;struct fw_rsc_vdev  rsc;idr_remove(&rproc->notifyids, rvring->notifyid);    At this point rproc_stop() has been called and the installed resource   table in the remote processor memory may no longer be accessible. As   such and as per rproc_stop(), rproc->table_ptr points to the cached   resource table (rproc->cached_table).  The cached resource table is   only available when a remote processor has been booted by the   remoteproc core, otherwise it is NULL.     Based on the above, reset the virtio device section in the cached   resource table only if there is one to work with. ", "struct rproc *devm_rproc_alloc(struct device *dev, const char *name,       const struct rproc_ops *ops,       const char *firmware, int len)": "devm_rproc_alloc() - resource managed rproc_alloc()   @dev: the underlying device   @name: name of this remote processor   @ops: platform-specific handlers (mainly startstop)   @firmware: name of firmware file to load, can be NULL   @len: length of private data needed by the rproc driver (in bytes)     This function performs like rproc_alloc() but the acquired rproc device will   automatically be released on driver detach.     Return: new rproc instance, or NULL on failure ", "void rproc_add_subdev(struct rproc *rproc, struct rproc_subdev *subdev)": "rproc_add_subdev() - add a subdevice to a remoteproc   @rproc: rproc handle to add the subdevice to   @subdev: subdev handle to register     Caller is responsible for populating optional subdevice function pointers. ", "void rproc_remove_subdev(struct rproc *rproc, struct rproc_subdev *subdev)": "rproc_remove_subdev() - remove a subdevice from a remoteproc   @rproc: rproc handle to remove the subdevice from   @subdev: subdev handle, previously registered with rproc_add_subdev() ", "struct rproc *rproc_get_by_child(struct device *dev)": "rproc_get_by_child() - acquire rproc handle of @dev's ancestor   @dev:child device to find ancestor of     Return: the ancestor rproc instance, or NULL if not found ", "return -ENOSYS;}static int rproc_enable_iommu(struct rproc *rproc)": "rproc_report_crash(rproc, RPROC_MMUFAULT);    Let the iommu core know we're not really handling this fault;   we just used it as a recovery trigger. ", "int rproc_coredump_add_segment(struct rproc *rproc, dma_addr_t da, size_t size)": "rproc_coredump_add_segment() - add segment of device memory to coredump   @rproc:handle of a remote processor   @da:device address   @size:size of segment     Add device memory to the list of segments to be included in a coredump for   the remoteproc.     Return: 0 on success, negative errno on error. ", "int rproc_coredump_add_custom_segment(struct rproc *rproc,      dma_addr_t da, size_t size,      void (*dumpfn)(struct rproc *rproc,     struct rproc_dump_segment *segment,     void *dest, size_t offset,     size_t size),      void *priv)": "rproc_coredump_add_custom_segment() - add custom coredump segment   @rproc:handle of a remote processor   @da:device address   @size:size of segment   @dumpfn:custom dump function called for each segment during coredump   @priv:private data     Add device memory to the list of segments to be included in the coredump   and associate the segment with the given custom dump function and private   data.     Return: 0 on success, negative errno on error. ", "int rproc_coredump_set_elf_info(struct rproc *rproc, u8 class, u16 machine)": "rproc_coredump_set_elf_info() - set coredump elf information   @rproc:handle of a remote processor   @class:elf class for coredump elf file   @machine:elf machine for coredump elf file     Set elf information which will be used for coredump elf file.     Return: 0 on success, negative errno on error. ", "void rproc_coredump_using_sections(struct rproc *rproc)": "rproc_coredump_using_sections() - perform coredump using section headers   @rproc:rproc handle     This function will generate an ELF header for the registered sections of   segments and create a devcoredump device associated with rproc. Based on   the coredump configuration this function will directly copy the segments   from device memory to userspace or copy segments from device memory to   a separate buffer, which can then be read by userspace.   The first approach avoids using extra vmalloc memory. But it will stall   recovery flow until dump is read by userspace. ", "struct st_slim_rproc *st_slim_rproc_alloc(struct platform_device *pdev,char *fw_name)": "st_slim_rproc_alloc() - allocate and initialise slim rproc   @pdev: Pointer to the platform_device struct   @fw_name: Name of firmware for rproc to use     Function for allocating and initialising a slim rproc for use by   device drivers whose IP is based around the SLIM core. It   obtains and enables any clocks required by the SLIM core and also   ioremaps the various IO.     Return: st_slim_rproc pointer or PTR_ERR() on error. ", "void st_slim_rproc_put(struct st_slim_rproc *slim_rproc)": "st_slim_rproc_put() - put slim rproc resources    @slim_rproc: Pointer to the st_slim_rproc struct       Function for calling respective _put() functions on slim_rproc resources.     ", "irqreturn_t rproc_vq_interrupt(struct rproc *rproc, int notifyid)": "rproc_vq_interrupt() - tell remoteproc that a virtqueue is interrupted   @rproc: handle to the remote processor   @notifyid: index of the signalled virtqueue (unique per this @rproc)     This function should be called by the platform-specific rproc driver,   when the remote processor signals that a specific virtqueue has pending   messages available.     Return: IRQ_NONE if no message was found in the @notifyid virtqueue,   and otherwise returns IRQ_HANDLED. ", "int rproc_elf_sanity_check(struct rproc *rproc, const struct firmware *fw)": "rproc_elf_sanity_check() - Sanity Check for ELF32ELF64 firmware image   @rproc: the remote processor handle   @fw: the ELF firmware image     Make sure this fw image is sane (ie a correct ELF32ELF64 file).     Return: 0 on success and -EINVAL upon any failure ", "u64 rproc_elf_get_boot_addr(struct rproc *rproc, const struct firmware *fw)": "rproc_elf_get_boot_addr() - Get rproc's boot address.   @rproc: the remote processor handle   @fw: the ELF firmware image     Note that the boot address is not a configurable property of all remote   processors. Some will always boot at a specific hard-coded address.     Return: entry point address of the ELF image   ", "int rproc_elf_load_segments(struct rproc *rproc, const struct firmware *fw)": "rproc_elf_load_segments() - load firmware segments to memory   @rproc: remote processor which will be booted using these fw segments   @fw: the ELF firmware image     This function loads the firmware segments to memory, where the remote   processor expects them.     Some remote processors will expect their code and data to be placed   in specific device addresses, and can't have them dynamically assigned.     We currently support only those kind of remote processors, and expect   the program header's paddr member to contain those addresses. We then go   through the physically contiguous \"carveout\" memory regions which we   allocated (and mapped) earlier on behalf of the remote processor,   and \"translate\" device address to kernel addresses, so we can copy the   segments where they are expected.     Currently we only support remote processors that required carveout   allocations and got them mapped onto their iommus. Some processors   might be different: they might not have iommus, and would prefer to   directly allocate memory for every segmentresource. This is not yet   supported, though.     Return: 0 on success and an appropriate error code otherwise ", "int rproc_elf_load_rsc_table(struct rproc *rproc, const struct firmware *fw)": "rproc_elf_load_rsc_table() - load the resource table   @rproc: the rproc handle   @fw: the ELF firmware image     This function finds the resource table inside the remote processor's   firmware, load it into the @cached_table and update @table_ptr.     Return: 0 on success, negative errno on failure. ", "struct resource_table *rproc_elf_find_loaded_rsc_table(struct rproc *rproc,       const struct firmware *fw)": "rproc_elf_find_loaded_rsc_table() - find the loaded resource table   @rproc: the rproc handle   @fw: the ELF firmware image     This function finds the location of the loaded resource table. Don't   call this function if the table wasn't loaded yet - it's a bug if you do.     Return: pointer to the resource table if it is found or NULL otherwise.   If the table wasn't loaded yet the result is unspecified. ", "void vringh_kiov_advance(struct vringh_kiov *iov, size_t len)": "vringh_kiov_advance - skip bytes from vring_kiov   @iov: an iov passed to vringh_getdesc_ () (updated as we consume)   @len: the maximum length to advance ", "int vringh_init_user(struct vringh *vrh, u64 features,     unsigned int num, bool weak_barriers,     vring_desc_t __user *desc,     vring_avail_t __user *avail,     vring_used_t __user *used)": "vringh_init_user - initialize a vringh for a userspace vring.   @vrh: the vringh to initialize.   @features: the feature bits for this ring.   @num: the number of elements.   @weak_barriers: true if we only need memory barriers, not IO.   @desc: the userspace descriptor pointer.   @avail: the userspace avail pointer.   @used: the userspace used pointer.     Returns an error if num is invalid: you should check pointers   yourself! ", "int vringh_getdesc_user(struct vringh *vrh,struct vringh_iov *riov,struct vringh_iov *wiov,bool (*getrange)(struct vringh *vrh, u64 addr, struct vringh_range *r),u16 *head)": "vringh_complete_user().     Returns 0 if there was no descriptor, 1 if there was, or -errno.     Note that on error return, you can tell the difference between an   invalid ring and a single invalid descriptor: in the former case,    head will be vrh->vring.num.  You may be able to ignore an invalid   descriptor, but there's not much you can do with an invalid ring.     Note that you can reuse riov and wiov with subsequent calls. Content is   overwritten and memory reallocated if more space is needed.   When you don't have to use riov and wiov anymore, you should clean up them   calling vringh_iov_cleanup() to release the memory, even on error! ", "ssize_t vringh_iov_pull_user(struct vringh_iov *riov, void *dst, size_t len)": "vringh_iov_pull_user - copy bytes from vring_iov.   @riov: the riov as passed to vringh_getdesc_user() (updated as we consume)   @dst: the place to copy.   @len: the maximum length to copy.     Returns the bytes copied <= len or a negative errno. ", "ssize_t vringh_iov_push_user(struct vringh_iov *wiov,     const void *src, size_t len)": "vringh_iov_push_user - copy bytes into vring_iov.   @wiov: the wiov as passed to vringh_getdesc_user() (updated as we consume)   @src: the place to copy from.   @len: the maximum length to copy.     Returns the bytes copied <= len or a negative errno. ", "void vringh_abandon_user(struct vringh *vrh, unsigned int num)": "vringh_abandon_user - we've decided not to handle the descriptor(s).   @vrh: the vring.   @num: the number of descriptors to put back (ie. num   vringh_get_user() to undo).     The next vringh_get_user() will return the old descriptor(s) again. ", "int vringh_complete_multi_user(struct vringh *vrh,       const struct vring_used_elem used[],       unsigned num_used)": "vringh_complete_multi_user - we've finished with many descriptors.   @vrh: the vring.   @used: the head, length pairs.   @num_used: the number of used elements.     You should check vringh_need_notify_user() after one or more calls   to this function. ", "bool vringh_notify_enable_user(struct vringh *vrh)": "vringh_notify_enable_user - we want to know if something changes.   @vrh: the vring.     This always enables notifications, but returns false if there are   now more buffers available in the vring. ", "void vringh_notify_disable_user(struct vringh *vrh)": "vringh_notify_disable_user - don't tell us if something changes.   @vrh: the vring.     This is our normal running state: we disable and then only enable when   we're going to sleep. ", "int vringh_complete_user(struct vringh *vrh, u16 head, u32 len)": "vringh_need_notify_user() after one or more calls   to this function. ", "int vringh_init_kern(struct vringh *vrh, u64 features,     unsigned int num, bool weak_barriers,     struct vring_desc *desc,     struct vring_avail *avail,     struct vring_used *used)": "vringh_init_kern - initialize a vringh for a kernelspace vring.   @vrh: the vringh to initialize.   @features: the feature bits for this ring.   @num: the number of elements.   @weak_barriers: true if we only need memory barriers, not IO.   @desc: the userspace descriptor pointer.   @avail: the userspace avail pointer.   @used: the userspace used pointer.     Returns an error if num is invalid. ", "int vringh_getdesc_kern(struct vringh *vrh,struct vringh_kiov *riov,struct vringh_kiov *wiov,u16 *head,gfp_t gfp)": "vringh_complete_kern().   @gfp: flags for allocating larger riovwiov.     Returns 0 if there was no descriptor, 1 if there was, or -errno.     Note that on error return, you can tell the difference between an   invalid ring and a single invalid descriptor: in the former case,    head will be vrh->vring.num.  You may be able to ignore an invalid   descriptor, but there's not much you can do with an invalid ring.     Note that you can reuse riov and wiov with subsequent calls. Content is   overwritten and memory reallocated if more space is needed.   When you don't have to use riov and wiov anymore, you should clean up them   calling vringh_kiov_cleanup() to release the memory, even on error! ", "ssize_t vringh_iov_pull_kern(struct vringh_kiov *riov, void *dst, size_t len)": "vringh_iov_pull_kern - copy bytes from vring_iov.   @riov: the riov as passed to vringh_getdesc_kern() (updated as we consume)   @dst: the place to copy.   @len: the maximum length to copy.     Returns the bytes copied <= len or a negative errno. ", "ssize_t vringh_iov_push_kern(struct vringh_kiov *wiov,     const void *src, size_t len)": "vringh_iov_push_kern - copy bytes into vring_iov.   @wiov: the wiov as passed to vringh_getdesc_kern() (updated as we consume)   @src: the place to copy from.   @len: the maximum length to copy.     Returns the bytes copied <= len or a negative errno. ", "void vringh_abandon_kern(struct vringh *vrh, unsigned int num)": "vringh_abandon_kern - we've decided not to handle the descriptor(s).   @vrh: the vring.   @num: the number of descriptors to put back (ie. num   vringh_get_kern() to undo).     The next vringh_get_kern() will return the old descriptor(s) again. ", "bool vringh_notify_enable_kern(struct vringh *vrh)": "vringh_notify_enable_kern - we want to know if something changes.   @vrh: the vring.     This always enables notifications, but returns false if there are   now more buffers available in the vring. ", "void vringh_notify_disable_kern(struct vringh *vrh)": "vringh_notify_disable_kern - don't tell us if something changes.   @vrh: the vring.     This is our normal running state: we disable and then only enable when   we're going to sleep. ", "int vringh_complete_kern(struct vringh *vrh, u16 head, u32 len)": "vringh_need_notify_kern() after one or more calls   to this function. ", "int vringh_init_iotlb(struct vringh *vrh, u64 features,      unsigned int num, bool weak_barriers,      struct vring_desc *desc,      struct vring_avail *avail,      struct vring_used *used)": "vringh_init_iotlb - initialize a vringh for a ring with IOTLB.   @vrh: the vringh to initialize.   @features: the feature bits for this ring.   @num: the number of elements.   @weak_barriers: true if we only need memory barriers, not IO.   @desc: the userspace descriptor pointer.   @avail: the userspace avail pointer.   @used: the userspace used pointer.     Returns an error if num is invalid. ", "int vringh_init_iotlb_va(struct vringh *vrh, u64 features, unsigned int num, bool weak_barriers, struct vring_desc *desc, struct vring_avail *avail, struct vring_used *used)": "vringh_init_iotlb_va - initialize a vringh for a ring with IOTLB containing                          user VA.   @vrh: the vringh to initialize.   @features: the feature bits for this ring.   @num: the number of elements.   @weak_barriers: true if we only need memory barriers, not IO.   @desc: the userspace descriptor pointer.   @avail: the userspace avail pointer.   @used: the userspace used pointer.     Returns an error if num is invalid. ", "void vringh_set_iotlb(struct vringh *vrh, struct vhost_iotlb *iotlb,      spinlock_t *iotlb_lock)": "vringh_set_iotlb - initialize a vringh for a ring with IOTLB.   @vrh: the vring   @iotlb: iotlb associated with this vring   @iotlb_lock: spinlock to synchronize the iotlb accesses ", "int vringh_getdesc_iotlb(struct vringh *vrh, struct vringh_kiov *riov, struct vringh_kiov *wiov, u16 *head, gfp_t gfp)": "vringh_complete_iotlb().   @gfp: flags for allocating larger riovwiov.     Returns 0 if there was no descriptor, 1 if there was, or -errno.     Note that on error return, you can tell the difference between an   invalid ring and a single invalid descriptor: in the former case,    head will be vrh->vring.num.  You may be able to ignore an invalid   descriptor, but there's not much you can do with an invalid ring.     Note that you can reuse riov and wiov with subsequent calls. Content is   overwritten and memory reallocated if more space is needed.   When you don't have to use riov and wiov anymore, you should clean up them   calling vringh_kiov_cleanup() to release the memory, even on error! ", "ssize_t vringh_iov_pull_iotlb(struct vringh *vrh,      struct vringh_kiov *riov,      void *dst, size_t len)": "vringh_iov_pull_iotlb - copy bytes from vring_iov.   @vrh: the vring.   @riov: the riov as passed to vringh_getdesc_iotlb() (updated as we consume)   @dst: the place to copy.   @len: the maximum length to copy.     Returns the bytes copied <= len or a negative errno. ", "ssize_t vringh_iov_push_iotlb(struct vringh *vrh,      struct vringh_kiov *wiov,      const void *src, size_t len)": "vringh_iov_push_iotlb - copy bytes into vring_iov.   @vrh: the vring.   @wiov: the wiov as passed to vringh_getdesc_iotlb() (updated as we consume)   @src: the place to copy from.   @len: the maximum length to copy.     Returns the bytes copied <= len or a negative errno. ", "void vringh_abandon_iotlb(struct vringh *vrh, unsigned int num)": "vringh_abandon_iotlb - we've decided not to handle the descriptor(s).   @vrh: the vring.   @num: the number of descriptors to put back (ie. num   vringh_get_iotlb() to undo).     The next vringh_get_iotlb() will return the old descriptor(s) again. ", "bool vringh_notify_enable_iotlb(struct vringh *vrh)": "vringh_notify_enable_iotlb - we want to know if something changes.   @vrh: the vring.     This always enables notifications, but returns false if there are   now more buffers available in the vring. ", "void vringh_notify_disable_iotlb(struct vringh *vrh)": "vringh_notify_disable_iotlb - don't tell us if something changes.   @vrh: the vring.     This is our normal running state: we disable and then only enable when   we're going to sleep. ", "int vringh_complete_iotlb(struct vringh *vrh, u16 head, u32 len)": "vringh_need_notify_iotlb() after one or more calls   to this function. ", "offset = offsetof(struct vhost_msg, iotlb) - sizeof(int);break;case VHOST_IOTLB_MSG_V2:if (vhost_backend_has_feature(dev->vqs[0],      VHOST_BACKEND_F_IOTLB_ASID)) ": "vhost_chr_write_iter(struct vhost_dev  dev,     struct iov_iter  from){struct vhost_iotlb_msg msg;size_t offset;int type, ret;u32 asid = 0;ret = copy_from_iter(&type, sizeof(type), from);if (ret != sizeof(type)) {ret = -EINVAL;goto done;}switch (type) {case VHOST_IOTLB_MSG:  There maybe a hole after type for V1 message type,   so skip it here. ", "cmd->param1 = CONFIGURATION_ZONE;cmd->param2 = cpu_to_le16(DEVICE_LOCK_ADDR);cmd->count = READ_COUNT;atmel_i2c_checksum(cmd);cmd->msecs = MAX_EXEC_TIME_READ;cmd->rxsize = READ_RSP_SIZE;}EXPORT_SYMBOL(atmel_i2c_init_read_cmd": "atmel_i2c_init_read_cmd(struct atmel_i2c_cmd  cmd){cmd->word_addr = COMMAND;cmd->opcode = OPCODE_READ;    Read the word from Configuration zone that contains the lock bytes   (UserExtra, Selector, LockValue, LockConfig). ", "cmd->param2 = cpu_to_le16(keyid);atmel_i2c_checksum(cmd);cmd->msecs = MAX_EXEC_TIME_GENKEY;cmd->rxsize = GENKEY_RSP_SIZE;}EXPORT_SYMBOL(atmel_i2c_init_genkey_cmd": "atmel_i2c_init_genkey_cmd(struct atmel_i2c_cmd  cmd, u16 keyid){cmd->word_addr = COMMAND;cmd->count = GENKEY_COUNT;cmd->opcode = OPCODE_GENKEY;cmd->param1 = GENKEY_MODE_PRIVATE;  a random private key will be generated and stored in slot keyID ", "cmd->param2 = cpu_to_le16(DATA_SLOT_2);/* * The device only supports NIST P256 ECC keys. The public key size will * always be the same. Use a macro for the key size to avoid unnecessary * computations. ": "atmel_i2c_init_ecdh_cmd(struct atmel_i2c_cmd  cmd,    struct scatterlist  pubkey){size_t copied;cmd->word_addr = COMMAND;cmd->count = ECDH_COUNT;cmd->opcode = OPCODE_ECDH;cmd->param1 = ECDH_PREFIX_MODE;  private key slot ", "int atmel_i2c_send_receive(struct i2c_client *client, struct atmel_i2c_cmd *cmd)": "atmel_i2c_send_receive() - send a command to the device and receive its                              response.   @client: i2c client device   @cmd   : structure used to communicate with the device     After the device receives a Wake token, a watchdog counter starts within the   device. After the watchdog timer expires, the device enters sleep mode   regardless of whether some IO transmission or command execution is in   progress. If a command is attempted when insufficient time remains prior to   watchdog timer execution, the device will return the watchdog timeout error   code without attempting to execute the command. There is no way to reset the   counter other than to put the device into sleep or idle mode and then   wake it up again. ", "i2c_priv->wake_token_sz = atmel_i2c_wake_token_sz(bus_clk_rate);memset(i2c_priv->wake_token, 0, sizeof(i2c_priv->wake_token));atomic_set(&i2c_priv->tfm_count, 0);i2c_set_clientdata(client, i2c_priv);return device_sanity_check(client);}EXPORT_SYMBOL(atmel_i2c_probe": "atmel_i2c_probe(struct i2c_client  client){struct atmel_i2c_client_priv  i2c_priv;struct device  dev = &client->dev;int ret;u32 bus_clk_rate;if (!i2c_check_functionality(client->adapter, I2C_FUNC_I2C)) {dev_err(dev, \"I2C_FUNC_I2C not supported\\n\");return -ENODEV;}bus_clk_rate = i2c_acpi_find_bus_speed(&client->adapter->dev);if (!bus_clk_rate) {ret = device_property_read_u32(&client->adapter->dev,       \"clock-frequency\", &bus_clk_rate);if (ret) {dev_err(dev, \"failed to read clock-frequency property\\n\");return ret;}}if (bus_clk_rate > 1000000L) {dev_err(dev, \"%u exceeds maximum supported clock frequency (1MHz)\\n\",bus_clk_rate);return -EINVAL;}i2c_priv = devm_kmalloc(dev, sizeof( i2c_priv), GFP_KERNEL);if (!i2c_priv)return -ENOMEM;i2c_priv->client = client;mutex_init(&i2c_priv->lock);    WAKE_TOKEN_MAX_SIZE was calculated for the maximum bus_clk_rate -   1MHz. The previous bus_clk_rate check ensures us that wake_token_sz   will always be smaller than or equal to WAKE_TOKEN_MAX_SIZE. ", "it_page = kmap_atomic(sg_page(it));if (unlikely(!it_page)) ": "caam_dump_sg(const char  prefix_str, int prefix_type,  int rowsize, int groupsize, struct scatterlist  sg,  size_t tlen, bool ascii){struct scatterlist  it;void  it_page;size_t len;void  buf;for (it = sg; it && tlen > 0 ; it = sg_next(it)) {    make sure the scatterlist's page   has a valid virtual memory mapping ", "if (status_src[ssrc].report_ssed)return status_src[ssrc].report_ssed(jrdev, status, error);if (error)dev_err(jrdev, \"%d: %s\\n\", ssrc, error);elsedev_err(jrdev, \"%d: unknown error source\\n\", ssrc);return -EINVAL;}EXPORT_SYMBOL(caam_strstatus": "caam_strstatus(struct device  jrdev, u32 status, bool qi_v2){static const struct stat_src {int ( report_ssed)(struct device  jrdev, const u32 status,   const char  error);const char  error;} status_src[16] = {{ NULL, \"No error\" },{ NULL, NULL },{ report_ccb_status, \"CCB\" },{ report_jump_status, \"Jump\" },{ report_deco_status, \"DECO\" },{ report_qi_status, \"Queue Manager Interface\" },{ report_jr_status, \"Job Ring\" },{ report_cond_code_status, \"Condition Code\" },{ NULL, NULL },{ NULL, NULL },{ NULL, NULL },{ NULL, NULL },{ NULL, NULL },{ NULL, NULL },{ NULL, NULL },{ NULL, NULL },};u32 ssrc = status >> JRSTA_SSRC_SHIFT;const char  error = status_src[ssrc].error;    If there is an error handling function, call it to report the error.   Otherwise print the error source name. ", "edesc = aead_edesc_alloc(req, false);if (IS_ERR(edesc))return PTR_ERR(edesc);caam_req->flc = &ctx->flc[DECRYPT];caam_req->flc_dma = ctx->flc_dma[DECRYPT];caam_req->cbk = aead_decrypt_done;caam_req->ctx = &req->base;caam_req->edesc = edesc;ret = dpaa2_caam_enqueue(ctx->dev, caam_req);if (ret != -EINPROGRESS &&    !(ret == -EBUSY && req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)) ": "dpaa2_caam_enqueue(ctx->dev, caam_req);if (ret != -EINPROGRESS &&    !(ret == -EBUSY && req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)) {aead_unmap(ctx->dev, edesc, req);qi_cache_free(edesc);}return ret;}static int aead_decrypt(struct aead_request  req){struct aead_edesc  edesc;struct crypto_aead  aead = crypto_aead_reqtfm(req);struct caam_ctx  ctx = crypto_aead_ctx_dma(aead);struct caam_request  caam_req = aead_request_ctx_dma(req);int ret;  allocate extended descriptor ", "void cnstr_shdsc_aead_null_encap(u32 * const desc, struct alginfo *adata, unsigned int icvsize, int era)": "cnstr_shdsc_aead_null_encap - IPSec ESP encapsulation shared descriptor                                 (non-protocol) with no (null) encryption.   @desc: pointer to buffer used for descriptor construction   @adata: pointer to authentication transform definitions.           A split key is required for SEC Era < 6; the size of the split key           is specified in this case. Valid algorithm values - one of           OP_ALG_ALGSEL_{MD5, SHA1, SHA224, SHA256, SHA384, SHA512} ANDed           with OP_ALG_AAI_HMAC_PRECOMP.   @icvsize: integrity check value (ICV) size (truncated or full)   @era: SEC Era ", "void cnstr_shdsc_aead_null_decap(u32 * const desc, struct alginfo *adata, unsigned int icvsize, int era)": "cnstr_shdsc_aead_null_decap - IPSec ESP decapsulation shared descriptor                                 (non-protocol) with no (null) decryption.   @desc: pointer to buffer used for descriptor construction   @adata: pointer to authentication transform definitions.           A split key is required for SEC Era < 6; the size of the split key           is specified in this case. Valid algorithm values - one of           OP_ALG_ALGSEL_{MD5, SHA1, SHA224, SHA256, SHA384, SHA512} ANDed           with OP_ALG_AAI_HMAC_PRECOMP.   @icvsize: integrity check value (ICV) size (truncated or full)   @era: SEC Era ", "void cnstr_shdsc_aead_encap(u32 * const desc, struct alginfo *cdata,    struct alginfo *adata, unsigned int ivsize,    unsigned int icvsize, const bool is_rfc3686,    u32 *nonce, const u32 ctx1_iv_off, const bool is_qi,    int era)": "cnstr_shdsc_aead_encap - IPSec ESP encapsulation shared descriptor                            (non-protocol).   @desc: pointer to buffer used for descriptor construction   @cdata: pointer to block cipher transform definitions           Valid algorithm values - one of OP_ALG_ALGSEL_{AES, DES, 3DES} ANDed           with OP_ALG_AAI_CBC or OP_ALG_AAI_CTR_MOD128.   @adata: pointer to authentication transform definitions.           A split key is required for SEC Era < 6; the size of the split key           is specified in this case. Valid algorithm values - one of           OP_ALG_ALGSEL_{MD5, SHA1, SHA224, SHA256, SHA384, SHA512} ANDed           with OP_ALG_AAI_HMAC_PRECOMP.   @ivsize: initialization vector size   @icvsize: integrity check value (ICV) size (truncated or full)   @is_rfc3686: true when ctr(aes) is wrapped by rfc3686 template   @nonce: pointer to rfc3686 nonce   @ctx1_iv_off: IV offset in CONTEXT1 register   @is_qi: true when called from caamqi   @era: SEC Era ", "void cnstr_shdsc_aead_decap(u32 * const desc, struct alginfo *cdata,    struct alginfo *adata, unsigned int ivsize,    unsigned int icvsize, const bool geniv,    const bool is_rfc3686, u32 *nonce,    const u32 ctx1_iv_off, const bool is_qi, int era)": "cnstr_shdsc_aead_decap - IPSec ESP decapsulation shared descriptor                            (non-protocol).   @desc: pointer to buffer used for descriptor construction   @cdata: pointer to block cipher transform definitions           Valid algorithm values - one of OP_ALG_ALGSEL_{AES, DES, 3DES} ANDed           with OP_ALG_AAI_CBC or OP_ALG_AAI_CTR_MOD128.   @adata: pointer to authentication transform definitions.           A split key is required for SEC Era < 6; the size of the split key           is specified in this case. Valid algorithm values - one of           OP_ALG_ALGSEL_{MD5, SHA1, SHA224, SHA256, SHA384, SHA512} ANDed           with OP_ALG_AAI_HMAC_PRECOMP.   @ivsize: initialization vector size   @icvsize: integrity check value (ICV) size (truncated or full)   @geniv: whether to generate Encrypted Chain IV   @is_rfc3686: true when ctr(aes) is wrapped by rfc3686 template   @nonce: pointer to rfc3686 nonce   @ctx1_iv_off: IV offset in CONTEXT1 register   @is_qi: true when called from caamqi   @era: SEC Era ", "void cnstr_shdsc_aead_givencap(u32 * const desc, struct alginfo *cdata,       struct alginfo *adata, unsigned int ivsize,       unsigned int icvsize, const bool is_rfc3686,       u32 *nonce, const u32 ctx1_iv_off,       const bool is_qi, int era)": "cnstr_shdsc_aead_givencap - IPSec ESP encapsulation shared descriptor                               (non-protocol) with HW-generated initialization                               vector.   @desc: pointer to buffer used for descriptor construction   @cdata: pointer to block cipher transform definitions           Valid algorithm values - one of OP_ALG_ALGSEL_{AES, DES, 3DES} ANDed           with OP_ALG_AAI_CBC or OP_ALG_AAI_CTR_MOD128.   @adata: pointer to authentication transform definitions.           A split key is required for SEC Era < 6; the size of the split key           is specified in this case. Valid algorithm values - one of           OP_ALG_ALGSEL_{MD5, SHA1, SHA224, SHA256, SHA384, SHA512} ANDed           with OP_ALG_AAI_HMAC_PRECOMP.   @ivsize: initialization vector size   @icvsize: integrity check value (ICV) size (truncated or full)   @is_rfc3686: true when ctr(aes) is wrapped by rfc3686 template   @nonce: pointer to rfc3686 nonce   @ctx1_iv_off: IV offset in CONTEXT1 register   @is_qi: true when called from caamqi   @era: SEC Era ", "void cnstr_shdsc_gcm_encap(u32 * const desc, struct alginfo *cdata,   unsigned int ivsize, unsigned int icvsize,   const bool is_qi)": "cnstr_shdsc_gcm_encap - gcm encapsulation shared descriptor   @desc: pointer to buffer used for descriptor construction   @cdata: pointer to block cipher transform definitions           Valid algorithm values - OP_ALG_ALGSEL_AES ANDed with OP_ALG_AAI_GCM.   @ivsize: initialization vector size   @icvsize: integrity check value (ICV) size (truncated or full)   @is_qi: true when called from caamqi ", "void cnstr_shdsc_gcm_decap(u32 * const desc, struct alginfo *cdata,   unsigned int ivsize, unsigned int icvsize,   const bool is_qi)": "cnstr_shdsc_gcm_decap - gcm decapsulation shared descriptor   @desc: pointer to buffer used for descriptor construction   @cdata: pointer to block cipher transform definitions           Valid algorithm values - OP_ALG_ALGSEL_AES ANDed with OP_ALG_AAI_GCM.   @ivsize: initialization vector size   @icvsize: integrity check value (ICV) size (truncated or full)   @is_qi: true when called from caamqi ", "void cnstr_shdsc_rfc4106_encap(u32 * const desc, struct alginfo *cdata,       unsigned int ivsize, unsigned int icvsize,       const bool is_qi)": "cnstr_shdsc_rfc4106_encap - IPSec ESP gcm encapsulation shared descriptor                               (non-protocol).   @desc: pointer to buffer used for descriptor construction   @cdata: pointer to block cipher transform definitions           Valid algorithm values - OP_ALG_ALGSEL_AES ANDed with OP_ALG_AAI_GCM.   @ivsize: initialization vector size   @icvsize: integrity check value (ICV) size (truncated or full)   @is_qi: true when called from caamqi     Input sequence: AAD | PTXT   Output sequence: AAD | CTXT | ICV   AAD length (assoclen), which includes the IV length, is available in Math3. ", "void cnstr_shdsc_rfc4106_decap(u32 * const desc, struct alginfo *cdata,       unsigned int ivsize, unsigned int icvsize,       const bool is_qi)": "cnstr_shdsc_rfc4106_decap - IPSec ESP gcm decapsulation shared descriptor                               (non-protocol).   @desc: pointer to buffer used for descriptor construction   @cdata: pointer to block cipher transform definitions           Valid algorithm values - OP_ALG_ALGSEL_AES ANDed with OP_ALG_AAI_GCM.   @ivsize: initialization vector size   @icvsize: integrity check value (ICV) size (truncated or full)   @is_qi: true when called from caamqi ", "void cnstr_shdsc_rfc4543_encap(u32 * const desc, struct alginfo *cdata,       unsigned int ivsize, unsigned int icvsize,       const bool is_qi)": "cnstr_shdsc_rfc4543_encap - IPSec ESP gmac encapsulation shared descriptor                               (non-protocol).   @desc: pointer to buffer used for descriptor construction   @cdata: pointer to block cipher transform definitions           Valid algorithm values - OP_ALG_ALGSEL_AES ANDed with OP_ALG_AAI_GCM.   @ivsize: initialization vector size   @icvsize: integrity check value (ICV) size (truncated or full)   @is_qi: true when called from caamqi ", "void cnstr_shdsc_rfc4543_decap(u32 * const desc, struct alginfo *cdata,       unsigned int ivsize, unsigned int icvsize,       const bool is_qi)": "cnstr_shdsc_rfc4543_decap - IPSec ESP gmac decapsulation shared descriptor                               (non-protocol).   @desc: pointer to buffer used for descriptor construction   @cdata: pointer to block cipher transform definitions           Valid algorithm values - OP_ALG_ALGSEL_AES ANDed with OP_ALG_AAI_GCM.   @ivsize: initialization vector size   @icvsize: integrity check value (ICV) size (truncated or full)   @is_qi: true when called from caamqi ", "void cnstr_shdsc_chachapoly(u32 * const desc, struct alginfo *cdata,    struct alginfo *adata, unsigned int ivsize,    unsigned int icvsize, const bool encap,    const bool is_qi)": "cnstr_shdsc_chachapoly - Chacha20 + Poly1305 generic AEAD (rfc7539) and                            IPsec ESP (rfc7634, a.k.a. rfc7539esp) shared                            descriptor (non-protocol).   @desc: pointer to buffer used for descriptor construction   @cdata: pointer to block cipher transform definitions           Valid algorithm values - OP_ALG_ALGSEL_CHACHA20 ANDed with           OP_ALG_AAI_AEAD.   @adata: pointer to authentication transform definitions           Valid algorithm values - OP_ALG_ALGSEL_POLY1305 ANDed with           OP_ALG_AAI_AEAD.   @ivsize: initialization vector size   @icvsize: integrity check value (ICV) size (truncated or full)   @encap: true if encapsulation, false if decapsulation   @is_qi: true when called from caamqi ", "void cnstr_shdsc_skcipher_encap(u32 * const desc, struct alginfo *cdata,unsigned int ivsize, const bool is_rfc3686,const u32 ctx1_iv_off)": "cnstr_shdsc_skcipher_encap - skcipher encapsulation shared descriptor   @desc: pointer to buffer used for descriptor construction   @cdata: pointer to block cipher transform definitions           Valid algorithm values - one of OP_ALG_ALGSEL_{AES, DES, 3DES} ANDed           with OP_ALG_AAI_CBC or OP_ALG_AAI_CTR_MOD128                                  - OP_ALG_ALGSEL_CHACHA20   @ivsize: initialization vector size   @is_rfc3686: true when ctr(aes) is wrapped by rfc3686 template   @ctx1_iv_off: IV offset in CONTEXT1 register ", "void cnstr_shdsc_skcipher_decap(u32 * const desc, struct alginfo *cdata,unsigned int ivsize, const bool is_rfc3686,const u32 ctx1_iv_off)": "cnstr_shdsc_skcipher_decap - skcipher decapsulation shared descriptor   @desc: pointer to buffer used for descriptor construction   @cdata: pointer to block cipher transform definitions           Valid algorithm values - one of OP_ALG_ALGSEL_{AES, DES, 3DES} ANDed           with OP_ALG_AAI_CBC or OP_ALG_AAI_CTR_MOD128                                  - OP_ALG_ALGSEL_CHACHA20   @ivsize: initialization vector size   @is_rfc3686: true when ctr(aes) is wrapped by rfc3686 template   @ctx1_iv_off: IV offset in CONTEXT1 register ", "void cnstr_shdsc_xts_skcipher_encap(u32 * const desc, struct alginfo *cdata)": "cnstr_shdsc_xts_skcipher_encap - xts skcipher encapsulation shared descriptor   @desc: pointer to buffer used for descriptor construction   @cdata: pointer to block cipher transform definitions           Valid algorithm values - OP_ALG_ALGSEL_AES ANDed with OP_ALG_AAI_XTS. ", "void cnstr_shdsc_xts_skcipher_decap(u32 * const desc, struct alginfo *cdata)": "cnstr_shdsc_xts_skcipher_decap - xts skcipher decapsulation shared descriptor   @desc: pointer to buffer used for descriptor construction   @cdata: pointer to block cipher transform definitions           Valid algorithm values - OP_ALG_ALGSEL_AES ANDed with OP_ALG_AAI_XTS. ", "append_operation(desc, (adata->algtype & OP_ALG_ALGSEL_MASK) | OP_ALG_AAI_HMAC | OP_TYPE_CLASS2_ALG | OP_ALG_DECRYPT | OP_ALG_AS_INIT);/* * do a FIFO_LOAD of zero, this will trigger the internal key expansion * into both pads inside MDHA ": "gen_split_key(struct device  jrdev, u8  key_out,  struct alginfo   const adata, const u8  key_in, u32 keylen,  int max_keylen){u32  desc;struct split_key_result result;dma_addr_t dma_addr;unsigned int local_max;int ret = -ENOMEM;adata->keylen = split_key_len(adata->algtype & OP_ALG_ALGSEL_MASK);adata->keylen_pad = split_key_pad_len(adata->algtype &      OP_ALG_ALGSEL_MASK);local_max = max(keylen, adata->keylen_pad);dev_dbg(jrdev, \"split keylen %d split keylen padded %d\\n\",adata->keylen, adata->keylen_pad);print_hex_dump_debug(\"ctx.key@\" __stringify(__LINE__)\": \",     DUMP_PREFIX_ADDRESS, 16, 4, key_in, keylen, 1);if (local_max > max_keylen)return -EINVAL;desc = kmalloc(CAAM_CMD_SZ   6 + CAAM_PTR_SZ   2, GFP_KERNEL);if (!desc) {dev_err(jrdev, \"unable to allocate key input memory\\n\");return ret;}memcpy(key_out, key_in, keylen);dma_addr = dma_map_single(jrdev, key_out, local_max, DMA_BIDIRECTIONAL);if (dma_mapping_error(jrdev, dma_addr)) {dev_err(jrdev, \"unable to map key memory\\n\");goto out_free;}init_job_desc(desc, 0);append_key(desc, dma_addr, keylen, CLASS_2 | KEY_DEST_CLASS_REG);  Sets MDHA up into an HMAC-INIT ", "void cnstr_shdsc_ahash(u32 * const desc, struct alginfo *adata, u32 state,       int digestsize, int ctx_len, bool import_ctx, int era)": "cnstr_shdsc_ahash - ahash shared descriptor   @desc: pointer to buffer used for descriptor construction   @adata: pointer to authentication transform definitions.           A split key is required for SEC Era < 6; the size of the split key           is specified in this case.           Valid algorithm values - one of OP_ALG_ALGSEL_{MD5, SHA1, SHA224,           SHA256, SHA384, SHA512}.   @state: algorithm state OP_ALG_AS_{INIT, FINALIZE, INITFINALIZE, UPDATE}   @digestsize: algorithm's digest size   @ctx_len: size of Context Register   @import_ctx: true if previous Context Register needs to be restored                must be true for ahash update and final                must be false for ahash first and digest   @era: SEC Era ", "void cnstr_shdsc_sk_hash(u32 * const desc, struct alginfo *adata, u32 state, int digestsize, int ctx_len)": "cnstr_shdsc_sk_hash - shared descriptor for symmetric key cipher-based                         hash algorithms   @desc: pointer to buffer used for descriptor construction   @adata: pointer to authentication transform definitions.   @state: algorithm state OP_ALG_AS_{INIT, FINALIZE, INITFINALIZE, UPDATE}   @digestsize: algorithm's digest size   @ctx_len: size of Context Register ", "old_fq = drv_ctx->req_fq;/* Create a new req FQ in parked state ": "caam_drv_ctx_update(struct caam_drv_ctx  drv_ctx, u32  sh_desc){int ret;u32 num_words;struct qman_fq  new_fq,  old_fq;struct device  qidev = drv_ctx->qidev;num_words = desc_len(sh_desc);if (num_words > MAX_SDLEN) {dev_err(qidev, \"Invalid descriptor len: %d words\\n\", num_words);return -EINVAL;}  Note down older req FQ ", "drv_ctx->prehdr[0] = cpu_to_caam32((1 << PREHDR_RSLS_SHIFT) |   num_words);drv_ctx->prehdr[1] = cpu_to_caam32(PREHDR_ABS);memcpy(drv_ctx->sh_desc, sh_desc, desc_bytes(sh_desc));size = sizeof(drv_ctx->prehdr) + sizeof(drv_ctx->sh_desc);hwdesc = dma_map_single(qidev, drv_ctx->prehdr, size,DMA_BIDIRECTIONAL);if (dma_mapping_error(qidev, hwdesc)) ": "caam_drv_ctx_init(struct device  qidev,       int  cpu,       u32  sh_desc){size_t size;u32 num_words;dma_addr_t hwdesc;struct caam_drv_ctx  drv_ctx;const cpumask_t  cpus = qman_affine_cpus();num_words = desc_len(sh_desc);if (num_words > MAX_SDLEN) {dev_err(qidev, \"Invalid descriptor len: %d words\\n\",num_words);return ERR_PTR(-EINVAL);}drv_ctx = kzalloc(sizeof( drv_ctx), GFP_ATOMIC);if (!drv_ctx)return ERR_PTR(-ENOMEM);    Initialise pre-header - set RSLS and SDLEN - and shared descriptor   and dma-map them. ", "if (kill_fq(drv_ctx->qidev, drv_ctx->req_fq))dev_err(drv_ctx->qidev, \"Crypto session req FQ kill failed\\n\");dma_unmap_single(drv_ctx->qidev, drv_ctx->context_a, sizeof(drv_ctx->sh_desc) + sizeof(drv_ctx->prehdr), DMA_BIDIRECTIONAL);kfree(drv_ctx);}EXPORT_SYMBOL(caam_drv_ctx_rel": "caam_drv_ctx_rel(struct caam_drv_ctx  drv_ctx){if (IS_ERR_OR_NULL(drv_ctx))return;  Remove request FQ ", "init_job_desc(desc, 0);append_key_as_imm(desc, info->key_mod, info->key_mod_len,  info->key_mod_len, CLASS_2 | KEY_DEST_CLASS_REG);append_seq_in_ptr_intlen(desc, dma_in, info->input_len, 0);append_seq_out_ptr_intlen(desc, dma_out, output_len, 0);append_operation(desc, op);print_hex_dump_debug(\"data@\"__stringify(__LINE__)\": \",     DUMP_PREFIX_ADDRESS, 16, 1, info->input,     info->input_len, false);print_hex_dump_debug(\"jobdesc@\"__stringify(__LINE__)\": \",     DUMP_PREFIX_ADDRESS, 16, 1, desc,     desc_bytes(desc), false);testres.err = 0;init_completion(&testres.completion);ret = caam_jr_enqueue(jrdev, desc, caam_blob_job_done, &testres);if (ret == -EINPROGRESS) ": "caam_process_blob(struct caam_blob_priv  priv,      struct caam_blob_info  info, bool encap){const struct caam_drv_private  ctrlpriv;struct caam_blob_job_result testres;struct device  jrdev = &priv->jrdev;dma_addr_t dma_in, dma_out;int op = OP_PCLID_BLOB;size_t output_len;u32  desc;u32 moo;int ret;if (info->key_mod_len > CAAM_BLOB_KEYMOD_LENGTH)return -EINVAL;if (encap) {op |= OP_TYPE_ENCAP_PROTOCOL;output_len = info->input_len + CAAM_BLOB_OVERHEAD;} else {op |= OP_TYPE_DECAP_PROTOCOL;output_len = info->input_len - CAAM_BLOB_OVERHEAD;}desc = kzalloc(CAAM_BLOB_DESC_BYTES_MAX, GFP_KERNEL);if (!desc)return -ENOMEM;dma_in = dma_map_single(jrdev, info->input, info->input_len,DMA_TO_DEVICE);if (dma_mapping_error(jrdev, dma_in)) {dev_err(jrdev, \"unable to map input DMA buffer\\n\");ret = -ENOMEM;goto out_free;}dma_out = dma_map_single(jrdev, info->output, output_len, DMA_FROM_DEVICE);if (dma_mapping_error(jrdev, dma_out)) {dev_err(jrdev, \"unable to map output DMA buffer\\n\");ret = -ENOMEM;goto out_unmap_in;}ctrlpriv = dev_get_drvdata(jrdev->parent);moo = FIELD_GET(CSTA_MOO, rd_reg32(&ctrlpriv->ctrl->perfmon.status));if (moo != CSTA_MOO_SECURE && moo != CSTA_MOO_TRUSTED)dev_warn(jrdev, \"using insecure test key, enable HAB to use unique device key!\\n\");    A data blob is encrypted using a blob key (BK); a random number.   The BK is used as an AES-CCM key. The initial block (B0) and the   initial counter (Ctr0) are generated automatically and stored in   Class 1 Context DWords 0+1+2+3. The random BK is stored in the   Class 1 Key Register. Operation Mode is set to AES-CCM. ", "jrdev = caam_jr_alloc();if (IS_ERR(jrdev)) ": "caam_blob_gen_init(void){struct caam_drv_private  ctrlpriv;struct device  jrdev;    caam_blob_gen_init() may expectedly fail with -ENODEV, e.g. when   CAAM driver didn't probe or when SoC lacks BLOB support. An   error would be harsh in this case, so we stick to info level. ", "struct device *caam_jr_alloc(void)": "caam_jr_alloc() - Alloc a job ring for someone to use as needed.     returns :  pointer to the newly allocated physical        JobR dev can be written to if successful.  ", "void caam_jr_free(struct device *rdev)": "caam_jr_free() - Free the Job Ring   @rdev:      points to the dev that identifies the Job ring to               be released.  ", "int caam_jr_enqueue(struct device *dev, u32 *desc,    void (*cbk)(struct device *dev, u32 *desc,u32 status, void *areq),    void *areq)": "caam_jr_enqueue() - Enqueue a job descriptor head. Returns -EINPROGRESS   if OK, -ENOSPC if the queue is full, -EIO if it cannot map the caller's   descriptor.   @dev:  struct device of the job ring to be used   @desc: points to a job descriptor that execute our request. All          descriptors (and all referenced data) must be in a DMAable          region, and all data references must be physical addresses          accessible to CAAM (i.e. within a PAMU window granted          to it).   @cbk:  pointer to a callback function to be invoked upon completion          of this request. This has the form:          callback(struct device  dev, u32  desc, u32 stat, void  arg)          where:          dev:     contains the job ring device that processed this                   response.          desc:    descriptor that initiated the request, same as                   \"desc\" being argued to caam_jr_enqueue().          status:  untranslated status received from CAAM. See the                   reference manual for a detailed description of                   error meaning, or see the JRSTA definitions in the                   register header file          areq:    optional pointer to an argument passed with the                   original request   @areq: optional pointer to a user argument for use at callback          time.  ", "bool rvt_cq_enter(struct rvt_cq *cq, struct ib_wc *entry, bool solicited)": "rvt_cq_enter - add a new entry to the completion queue   @cq: completion queue   @entry: work completion entry to add   @solicited: true if @entry is solicited     This may be called with qp->s_lock held.     Return: return true on success, else return   false if cq is full. ", "struct rvt_dev_info *rvt_alloc_device(size_t size, int nports)": "rvt_alloc_device - allocate rdi   @size: how big of a structure to allocate   @nports: number of ports to allocate array slots for     Use IB core device alloc to allocate space for the rdi which is assumed to be   inside of the ib_device. Any extra space that drivers require should be   included in size.     We also allocate a port array based on the number of ports.     Return: pointer to allocated rdi ", "void rvt_dealloc_device(struct rvt_dev_info *rdi)": "rvt_dealloc_device - deallocate rdi   @rdi: structure to free     Free a structure allocated with rvt_alloc_device() ", "int rvt_register_device(struct rvt_dev_info *rdi)": "rvt_register_device - register a driver   @rdi: main dev structure for all of rdmavt operations     It is up to drivers to allocate the rdi and fill in the appropriate   information.     Return: 0 on success otherwise an errno. ", "void rvt_unregister_device(struct rvt_dev_info *rdi)": "rvt_unregister_device - remove a driver   @rdi: rvt dev struct ", "int rvt_init_port(struct rvt_dev_info *rdi, struct rvt_ibport *port,  int port_index, u16 *pkey_table)": "rvt_init_port - init internal data for driver port   @rdi: rvt_dev_info struct   @port: rvt port   @port_index: 0 based index of ports, different from IB core port num   @pkey_table: pkey_table for @port     Keep track of a list of ports. No need to have a detach port.   They persist until the driver goes away.     Return: always 0 ", "static void rvt_remove_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp)": "rvt_error_qp(qp, IB_WC_LOC_PROT_ERR);check_lwqe:spin_unlock(&qp->s_lock);spin_unlock(&qp->s_hlock);spin_unlock_irq(&qp->r_lock);if (lastwqe) {struct ib_event ev;ev.device = qp->ibqp.device;ev.element.qp = &qp->ibqp;ev.event = IB_EVENT_QP_LAST_WQE_REACHED;qp->ibqp.event_handler(&ev, qp->ibqp.qp_context);}}     rvt_remove_qp - remove qp form table   @rdi: rvt dev struct   @qp: qp to remove     Remove the QP from the table so it can't be found asynchronously by   the receive routine. ", "static void rvt_insert_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp)": "rvt_get_rwqe_ptr(&qp->r_rq, tail)->wr_id;if (++tail >= qp->r_rq.size)tail = 0;rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.recv_cq), &wc, 1);}if (qp->ip)RDMA_WRITE_UAPI_ATOMIC(wq->tail, tail);elsekwq->tail = tail;spin_unlock(&qp->r_rq.kwq->c_lock);} else if (qp->ibqp.event_handler) {ret = 1;}bail:return ret;}EXPORT_SYMBOL(rvt_error_qp);    Put the QP into the hash table.   The hash table holds a reference to the QP. ", "void rvt_comm_est(struct rvt_qp *qp)": "rvt_comm_est - handle trap with QP established   @qp: the QP ", "unsigned long rvt_rnr_tbl_to_usec(u32 index)": "rvt_rnr_tbl_to_usec - return index into ib_rvt_rnr_table    @index - the index    return usec from an index into ib_rvt_rnr_table ", "void rvt_add_retry_timer_ext(struct rvt_qp *qp, u8 shift)": "rvt_add_retry_timer_ext - addstart a retry timer    @qp - the QP    @shift - timeout shift to wait for multiple packets    add a retry timer on the QP ", "void rvt_add_rnr_timer(struct rvt_qp *qp, u32 aeth)": "rvt_add_rnr_timer - addstart an rnr timer on the QP   @qp: the QP   @aeth: aeth of RNR timeout, simulated aeth for loopback ", "rdi->driver_f.stop_send_queue(qp);rvt_del_timers_sync(qp);/* Wait for things to stop ": "rvt_stop_rc_timers(qp);qp->s_flags &= ~(RVT_S_TIMER | RVT_S_ANY_WAIT);spin_unlock(&qp->s_lock);spin_unlock(&qp->s_hlock);spin_unlock_irq(&qp->r_lock);  Stop the send queue and the retry timer ", "rdi->driver_f.quiesce_qp(qp);/* take qp out the hash and wait for it to be unused ": "rvt_del_timers_sync(qp);  Wait for things to stop ", "priv = rdi->driver_f.qp_priv_alloc(rdi, qp);if (IS_ERR(priv)) ": "rvt_rc_rnr_retry;    Driver needs to set up it's private QP structure and do any   initialization that is needed. ", "struct rvt_qp_iter *rvt_qp_iter_init(struct rvt_dev_info *rdi,     u64 v,     void (*cb)(struct rvt_qp *qp, u64 v))": "rvt_qp_iter_init - initial for QP iteration   @rdi: rvt devinfo   @v: u64 value   @cb: user-defined callback     This returns an iterator suitable for iterating QPs   in the system.     The @cb is a user-defined callback and @v is a 64-bit   value passed to and relevant for processing in the   @cb.  An example use case would be to alter QP processing   based on criteria not part of the rvt_qp.     Use cases that require memory allocation to succeed   must preallocate appropriately.     Return: a pointer to an rvt_qp_iter or NULL ", "int rvt_qp_iter_next(struct rvt_qp_iter *iter)__must_hold(RCU)": "rvt_qp_iter_next - return the next QP in iter   @iter: the iterator     Fine grained QP iterator suitable for use   with debugfs seq_file mechanisms.     Updates iter->qp with the current QP when the return   value is 0.     Return: 0 - iter->qp is valid 1 - no more QPs ", "void rvt_qp_exit(struct rvt_dev_info *rdi)": "rvt_qp_iter(rdi, (u64)&qp_inuse, rvt_free_qp_cb);return qp_inuse;}     rvt_qp_exit - clean up qps on device exit   @rdi: rvt dev structure     Check for qp leaks and free resources. ", "void rvt_copy_sge(struct rvt_qp *qp, struct rvt_sge_state *ss,  void *data, u32 length,  bool release, bool copy_last)": "rvt_copy_sge - copy data to SGE memory   @qp: associated QP   @ss: the SGE state   @data: the data to copy   @length: the length of the data   @release: boolean to release MR   @copy_last: do a separate copy of the last 8 bytes ", "void rvt_ruc_loopback(struct rvt_qp *sqp)": "rvt_ruc_loopback - handle UC and RC loopback requests   @sqp: the sending QP     This is called from rvt_do_send() to forward a WQE addressed to the same HFI   Note that although we are single threaded due to the send engine, we still   have to protect against post_send().  We don't have to worry about   receive interrupts since this is a connected protocol and all packets   will pass through here. ", "__be32 rvt_compute_aeth(struct rvt_qp *qp)": "rvt_compute_aeth - compute the AETH (syndrome + MSN)   @qp: the queue pair to compute the AETH for     Returns the AETH. ", "void rvt_get_credit(struct rvt_qp *qp, u32 aeth)": "rvt_get_credit - flush the send work queue of a QP   @qp: the qp who's send work queue to flush   @aeth: the Acknowledge Extended Transport Header     The QP s_lock should be held. ", "u32 rvt_restart_sge(struct rvt_sge_state *ss, struct rvt_swqe *wqe, u32 len)": "rvt_restart_sge - rewind the sge state for a wqe   @ss: the sge state pointer   @wqe: the wqe to rewind   @len: the data length from the start of the wqe in bytes     Returns the remaining data length. ", "int rvt_check_ah(struct ib_device *ibdev, struct rdma_ah_attr *ah_attr)": "rvt_check_ah - validate the attributes of AH   @ibdev: the ib device   @ah_attr: the attributes of the AH     If driver supports a more detailed check_ah function call back to it   otherwise just check the basics.     Return: 0 on success ", "int rvt_fast_reg_mr(struct rvt_qp *qp, struct ib_mr *ibmr, u32 key,    int access)": "rvt_fast_reg_mr - fast register physical MR   @qp: the queue pair where the work request comes from   @ibmr: the memory region to be registered   @key: updated key for this memory region   @access: access flags for this memory region     Returns 0 on success. ", "int rvt_invalidate_rkey(struct rvt_qp *qp, u32 rkey)": "rvt_invalidate_rkey - invalidate an MR rkey   @qp: queue pair associated with the invalidate op   @rkey: rkey to invalidate     Returns 0 on success. ", "int rvt_lkey_ok(struct rvt_lkey_table *rkt, struct rvt_pd *pd,struct rvt_sge *isge, struct rvt_sge *last_sge,struct ib_sge *sge, int acc)": "rvt_lkey_ok - check IB SGE for validity and initialize   @rkt: table containing lkey to check SGE against   @pd: protection domain   @isge: outgoing internal SGE   @last_sge: last outgoing SGE written   @sge: SGE to check   @acc: access flags     Check the IB SGE for validity and initialize our internal version   of it.     Increments the reference count when a new sge is stored.     Return: 0 if compressed, 1 if added , otherwise returns -errno. ", "int rvt_rkey_ok(struct rvt_qp *qp, struct rvt_sge *sge,u32 len, u64 vaddr, u32 rkey, int acc)": "rvt_rkey_ok - check the IB virtual address, length, and RKEY   @qp: qp for validation   @sge: SGE state   @len: length of data   @vaddr: virtual address to place data   @rkey: rkey to check   @acc: access flags     Return: 1 if successful, otherwise 0.     increments the reference count upon success ", "struct rvt_mcast *rvt_mcast_find(struct rvt_ibport *ibp, union ib_gid *mgid, u16 lid)": "rvt_mcast_find - search the global table for the given multicast GIDLID   NOTE: It is valid to have 1 MLID with multiple MGIDs.  It is not valid   to have 1 MGID with multiple MLIDs.   @ibp: the IB port structure   @mgid: the multicast GID to search for   @lid: the multicast LID portion of the multicast address (host order)     The caller is responsible for decrementing the reference count if found.     Return: NULL if not found. ", "struct ib_ucontext *ucontext = smp_load_acquire(&ufile->ucontext);if (!srcu_dereference(ufile->device->ib_dev,      &ufile->device->disassociate_srcu))return ERR_PTR(-EIO);if (!ucontext)return ERR_PTR(-EINVAL);return ucontext;}EXPORT_SYMBOL(ib_uverbs_get_ucontext_file": "ib_uverbs_get_ucontext_file(struct ib_uverbs_file  ufile){    We do not hold the hw_destroy_rwsem lock for this flow, instead   srcu is used. It does not matter if someone races this with   get_context, we get NULL or valid ucontext. ", "int ib_process_cq_direct(struct ib_cq *cq, int budget)": "ib_process_cq_direct - process a CQ in caller context   @cq:CQ to process   @budget:number of CQEs to poll for     This function is used to process all outstanding CQ entries.   It does not offload CQ processing to a different context and does   not ask for completion interrupts from the HCA.   Using direct processing on CQ with non IB_POLL_DIRECT type may trigger   concurrent processing.     Note: do not pass -1 as %budget unless it is guaranteed that the number   of completions that will be processed is small. ", "struct ib_cq *__ib_alloc_cq(struct ib_device *dev, void *private, int nr_cqe,    int comp_vector, enum ib_poll_context poll_ctx,    const char *caller)": "__ib_alloc_cq - allocate a completion queue   @dev:device to allocate the CQ for   @private:driver private data, accessible from cq->cq_context   @nr_cqe:number of CQEs to allocate   @comp_vector:HCA completion vectors for this CQ   @poll_ctx:context to poll the CQ from.   @caller:module owner name.     This is the proper interface to allocate a CQ for in-kernel users. A   CQ allocated with this interface will automatically be polled from the   specified context. The ULP must use wr->wr_cqe instead of wr->wr_id   to use this CQ abstraction. ", "struct ib_cq *__ib_alloc_cq_any(struct ib_device *dev, void *private,int nr_cqe, enum ib_poll_context poll_ctx,const char *caller)": "__ib_alloc_cq_any - allocate a completion queue   @dev:device to allocate the CQ for   @private:driver private data, accessible from cq->cq_context   @nr_cqe:number of CQEs to allocate   @poll_ctx:context to poll the CQ from   @caller:module owner name     Attempt to spread ULP Completion Queues over each device's interrupt   vectors. A simple best-effort mechanism is used. ", "void ib_free_cq(struct ib_cq *cq)": "ib_free_cq - free a completion queue   @cq:completion queue to free. ", "struct ib_cq *ib_cq_pool_get(struct ib_device *dev, unsigned int nr_cqe,     int comp_vector_hint,     enum ib_poll_context poll_ctx)": "ib_cq_pool_get() - Find the least used completion queue that matches     a given cpu hint (or least used for wild card affinity) and fits     nr_cqe.   @dev: rdma device   @nr_cqe: number of needed cqe entries   @comp_vector_hint: completion vector hint (-1) for the driver to assign     a comp vector based on internal counter   @poll_ctx: cq polling context     Finds a cq that satisfies @comp_vector_hint and @nr_cqe requirements and   claim entries in it for us.  In case there is no available cq, allocate   a new cq with the requirements and add it to the device pool.   IB_POLL_DIRECT cannot be used for shared cqs so it is not a valid value   for @poll_ctx. ", "void ib_cq_pool_put(struct ib_cq *cq, unsigned int nr_cqe)": "ib_cq_pool_put - Return a CQ taken from a shared pool.   @cq: The CQ to return.   @nr_cqe: The max number of cqes that the user had requested. ", "int ib_cm_listen(struct ib_cm_id *cm_id, __be64 service_id)": "ib_cm_listen - Initiates listening on the specified service ID for     connection and service ID resolution requests.   @cm_id: Connection identifier associated with the listen request.   @service_id: Service identifier matched against incoming connection     and service ID resolution requests.  The service ID should be specified     network-byte order.  If set to IB_CM_ASSIGN_SERVICE_ID, the CM will     assign a service ID to the caller. ", "struct ib_cm_id *ib_cm_insert_listen(struct ib_device *device,     ib_cm_handler cm_handler,     __be64 service_id)": "ib_cm_insert_listen - Create a new listening ib_cm_id and listen on   the given service ID.     If there's an existing ID listening on that same device and service ID,   return it.     @device: Device associated with the cm_id.  All related communication will   be associated with the specified device.   @cm_handler: Callback invoked to notify the user of CM events.   @service_id: Service identifier matched against incoming connection     and service ID resolution requests.  The service ID should be specified     network-byte order.  If set to IB_CM_ASSIGN_SERVICE_ID, the CM will     assign a service ID to the caller.     Callers should call ib_destroy_cm_id when done with the listener ID. ", "cm_id_priv = container_of(cm_id, struct cm_id_private, id);spin_lock_irqsave(&cm_id_priv->lock, flags);if (cm_id->state != IB_CM_IDLE || WARN_ON(cm_id_priv->timewait_info)) ": "ib_send_cm_req(struct ib_cm_id  cm_id,   struct ib_cm_req_param  param){struct cm_av av = {}, alt_av = {};struct cm_id_private  cm_id_priv;struct ib_mad_send_buf  msg;struct cm_req_msg  req_msg;unsigned long flags;int ret;ret = cm_validate_req_param(param);if (ret)return ret;  Verify that we're not in timewait. ", "listen_cm_id_priv = cm_find_listen(cm_id_priv->id.device,cpu_to_be64(IBA_GET(CM_REQ_SERVICE_ID, req_msg)));if (!listen_cm_id_priv) ": "ib_send_cm_dreq(&cur_cm_id_priv->id, NULL, 0);cm_deref_id(cur_cm_id_priv);}return NULL;}  Find matching listen request. ", "spin_lock_irq(&cm_id_priv->lock);cm_finalize_id(cm_id_priv);/* Refcount belongs to the event, pairs with cm_process_work() ": "ib_send_cm_rej(&cm_id_priv->id, IB_CM_REJ_INVALID_GID,       NULL, 0, NULL, 0);elseib_send_cm_rej(&cm_id_priv->id, IB_CM_REJ_INVALID_GID,       &work->path[0].sgid,       sizeof(work->path[0].sgid),       NULL, 0);goto rejected;}if (cm_id_priv->av.ah_attr.type == RDMA_AH_ATTR_TYPE_IB)cm_id_priv->av.dlid_datapath =IBA_GET(CM_REQ_PRIMARY_LOCAL_PORT_LID, req_msg);if (cm_req_has_alt_path(req_msg)) {ret = cm_init_av_by_path(&work->path[1], NULL, &cm_id_priv->alt_av);if (ret) {ib_send_cm_rej(&cm_id_priv->id,       IB_CM_REJ_INVALID_ALT_GID,       &work->path[0].sgid,       sizeof(work->path[0].sgid), NULL, 0);goto rejected;}}cm_id_priv->id.cm_handler = listen_cm_id_priv->id.cm_handler;cm_id_priv->id.context = listen_cm_id_priv->id.context;cm_format_req_event(work, cm_id_priv, &listen_cm_id_priv->id);  Now MAD handlers can see the new ID ", "}spin_unlock_irq(&cm.lock);cm_id_priv->id.cm_handler = listen_cm_id_priv->id.cm_handler;cm_id_priv->id.context = listen_cm_id_priv->id.context;/* * A SIDR ID does not need to be in the xarray since it does not receive * mads, is not placed in the remote_id or remote_qpn rbtree, and does * not enter timewait. ": "ib_send_cm_sidr_rep(&cm_id_priv->id,    &(struct ib_cm_sidr_rep_param){    .status = IB_SIDR_UNSUPPORTED });goto out;   No match. ", "void ib_pack(const struct ib_field        *desc,     int                           desc_len,     void                         *structure,     void                         *buf)": "ib_pack.h>static u64 value_read(int offset, int size, void  structure){switch (size) {case 1: return                 (u8   ) (structure + offset);case 2: return be16_to_cpup((__be16  ) (structure + offset));case 4: return be32_to_cpup((__be32  ) (structure + offset));case 8: return be64_to_cpup((__be64  ) (structure + offset));default:pr_warn(\"Field size %d bits not handled\\n\", size   8);return 0;}}     ib_pack - Pack a structure into a buffer   @desc:Array of structure field descriptions   @desc_len:Number of entries in @desc   @structure:Structure to pack from   @buf:Buffer to pack into     ib_pack() packs a list of structure fields into a buffer,   controlled by the array of fields in @desc. ", "void ib_unpack(const struct ib_field        *desc,       int                           desc_len,       void                         *buf,       void                         *structure)": "ib_unpack - Unpack a buffer into a structure   @desc:Array of structure field descriptions   @desc_len:Number of entries in @desc   @buf:Buffer to unpack from   @structure:Structure to unpack into     ib_pack() unpacks a list of structure fields from a buffer,   controlled by the array of fields in @desc. ", "int rdma_restrack_count(struct ib_device *dev, enum rdma_restrack_type type)": "rdma_restrack_count() - the current usage of specific object   @dev:  IB device   @type: actual type of object to operate ", "void rdma_restrack_set_name(struct rdma_restrack_entry *res, const char *caller)": "rdma_restrack_set_name() - set the task for this resource   @res:  resource entry   @caller: kernel name, the current task will be used if the caller is NULL. ", "void rdma_restrack_parent_name(struct rdma_restrack_entry *dst,       const struct rdma_restrack_entry *parent)": "rdma_restrack_parent_name() - set the restrack name properties based   on parent restrack   @dst: destination resource entry   @parent: parent resource entry ", "void rdma_restrack_new(struct rdma_restrack_entry *res,       enum rdma_restrack_type type)": "rdma_restrack_new() - Initializes new restrack entry to allow _put() interface   to release memory in fully automatic way.   @res: Entry to initialize   @type: REstrack type ", "void rdma_restrack_add(struct rdma_restrack_entry *res)": "rdma_restrack_add() - add object to the reource tracking database   @res:  resource entry ", "struct rdma_restrack_entry *rdma_restrack_get_byid(struct ib_device *dev,       enum rdma_restrack_type type, u32 id)": "rdma_restrack_get_byid() - translate from ID to restrack object   @dev: IB device   @type: resource track type   @id: ID to take a look     Return: Pointer to restrack entry or -ENOENT in case of error. ", "void rdma_restrack_del(struct rdma_restrack_entry *res)": "rdma_restrack_del() - delete object from the reource tracking database   @res:  resource entry ", "sa_path_set_dmac_zero(dst);}EXPORT_SYMBOL(ib_copy_path_rec_from_user": "ib_copy_path_rec_from_user(struct sa_path_rec  dst,struct ib_user_path_rec  src){u32 slid, dlid;memset(dst, 0, sizeof( dst));if ((ib_is_opa_gid((union ib_gid  )src->sgid)) ||    (ib_is_opa_gid((union ib_gid  )src->dgid))) {dst->rec_type = SA_PATH_REC_TYPE_OPA;slid = opa_get_lid_from_gid((union ib_gid  )src->sgid);dlid = opa_get_lid_from_gid((union ib_gid  )src->dgid);} else {dst->rec_type = SA_PATH_REC_TYPE_IB;slid = ntohs(src->slid);dlid = ntohs(src->dlid);}memcpy(dst->dgid.raw, src->dgid, sizeof dst->dgid);memcpy(dst->sgid.raw, src->sgid, sizeof dst->sgid);sa_path_set_dlid(dst, dlid);sa_path_set_slid(dst, slid);sa_path_set_raw_traffic(dst, src->raw_traffic);dst->flow_label= src->flow_label;dst->hop_limit= src->hop_limit;dst->traffic_class= src->traffic_class;dst->reversible= src->reversible;dst->numb_path= src->numb_path;dst->pkey= src->pkey;dst->sl= src->sl;dst->mtu_selector= src->mtu_selector;dst->mtu= src->mtu;dst->rate_selector= src->rate_selector;dst->rate= src->rate;dst->packet_life_time= src->packet_life_time;dst->preference= src->preference;dst->packet_life_time_selector = src->packet_life_time_selector;  TODO: No need to set this ", "multicast = &member->multicast;queue_join(member);return multicast;err:ib_sa_client_put(client);kfree(member);return ERR_PTR(ret);}EXPORT_SYMBOL(ib_sa_join_multicast": "ib_sa_join_multicast(struct ib_sa_client  client,     struct ib_device  device, u32 port_num,     struct ib_sa_mcmember_rec  rec,     ib_sa_comp_mask comp_mask, gfp_t gfp_mask,     int ( callback)(int status,     struct ib_sa_multicast  multicast),     void  context){struct mcast_device  dev;struct mcast_member  member;struct ib_sa_multicast  multicast;int ret;dev = ib_get_client_data(device, &mcast_client);if (!dev)return ERR_PTR(-ENODEV);member = kmalloc(sizeof  member, gfp_mask);if (!member)return ERR_PTR(-ENOMEM);ib_sa_client_get(client);member->client = client;member->multicast.rec =  rec;member->multicast.comp_mask = comp_mask;member->multicast.callback = callback;member->multicast.context = context;init_completion(&member->comp);refcount_set(&member->refcount, 1);member->state = MCAST_JOINING;member->group = acquire_group(&dev->port[port_num - dev->start_port],      &rec->mgid, gfp_mask);if (!member->group) {ret = -ENOMEM;goto err;}    The user will get the multicast structure in their callback.  They   could then free the multicast structure before we can return from   this routine.  So we save the pointer to return before queuing   any callback. ", "static void process_join_error(struct mcast_group *group, int status)": "ib_sa_free_multicast(&member->multicast);spin_lock_irq(&group->lock);}group->rec.join_state = 0;out:group->state = MCAST_BUSY;spin_unlock_irq(&group->lock);}static void mcast_work_handler(struct work_struct  work){struct mcast_group  group;struct mcast_member  member;struct ib_sa_multicast  multicast;int status, ret;u8 join_state;group = container_of(work, typeof( group), work);retest:spin_lock_irq(&group->lock);while (!list_empty(&group->pending_list) ||       (group->state != MCAST_BUSY)) {if (group->state != MCAST_BUSY) {spin_unlock_irq(&group->lock);process_group_error(group);goto retest;}member = list_entry(group->pending_list.next,    struct mcast_member, list);multicast = &member->multicast;join_state = multicast->rec.join_state;refcount_inc(&member->refcount);if (join_state == (group->rec.join_state & join_state)) {status = cmp_rec(&group->rec, &multicast->rec, multicast->comp_mask);if (!status)join_group(group, member, join_state);elselist_del_init(&member->list);spin_unlock_irq(&group->lock);ret = multicast->callback(status, multicast);} else {spin_unlock_irq(&group->lock);status = send_join(group, member);if (!status) {deref_member(member);return;}ret = fail_join(group, member, status);}deref_member(member);if (ret)ib_sa_free_multicast(&member->multicast);spin_lock_irq(&group->lock);}join_state = get_leave_state(group);if (join_state) {group->rec.join_state &= ~join_state;spin_unlock_irq(&group->lock);if (send_leave(group, join_state))goto retest;} else {group->state = MCAST_IDLE;spin_unlock_irq(&group->lock);release_group(group);}}    Fail a join request if it is still active - at the head of the pending queue. ", "int ib_init_ah_from_mcmember(struct ib_device *device, u32 port_num,     struct ib_sa_mcmember_rec *rec,     struct net_device *ndev,     enum ib_gid_type gid_type,     struct rdma_ah_attr *ah_attr)": "ib_init_ah_from_mcmember - Initialize AH attribute from multicast   member record and gid of the device.   @device:RDMA device   @port_num:Port of the rdma device to consider   @rec:Multicast member record to use   @ndev:Optional netdevice, applicable only for RoCE   @gid_type:GID type to consider   @ah_attr:AH attribute to fillup on successful completion     ib_init_ah_from_mcmember() initializes AH attribute based on multicast   member record and other device properties. On success the caller is   responsible to call rdma_destroy_ah_attr on the ah_attr. Returns 0 on   success or appropriate error code.   ", "void uverbs_uobject_put(struct ib_uobject *uobject)": "uverbs_uobject_put   is called. When the reference count is decreased, the uobject is freed.   For example, this is used when attaching a completion channel to a CQ. ", "if (f->f_op != fd_type->fops || uobject->ufile != ufile) ": "uverbs_uobject_fd_release(), and the caller is expected to ensure   that release is never done while a call to lookup is possible. ", "struct ib_pd *__ib_alloc_pd(struct ib_device *device, unsigned int flags,const char *caller)": "__ib_alloc_pd - Allocates an unused protection domain.   @device: The device on which to allocate the protection domain.   @flags: protection domain flags   @caller: caller's build-time module name     A protection domain object provides an association between QPs, shared   receive queues, address handles, memory regions, and memory windows.     Every PD has a local_dma_lkey which can be used as the lkey value for local   memory operations. ", "int ib_dealloc_pd_user(struct ib_pd *pd, struct ib_udata *udata)": "ib_dealloc_pd_user - Deallocates a protection domain.   @pd: The protection domain to deallocate.   @udata: Valid user data or NULL for kernel object     It is an error to call this function while any resources in the pd still   exist.  The caller is responsible to synchronously destroy them and   guarantee no new allocations will happen. ", "void rdma_copy_ah_attr(struct rdma_ah_attr *dest,       const struct rdma_ah_attr *src)": "rdma_copy_ah_attr - Copy rdma ah attribute from source to destination.   @dest:       Pointer to destination ah_attr. Contents of the destination                pointer is assumed to be invalid and attribute are overwritten.   @src:        Pointer to source ah_attr. ", "void rdma_replace_ah_attr(struct rdma_ah_attr *old,  const struct rdma_ah_attr *new)": "rdma_replace_ah_attr - Replace valid ah_attr with new new one.   @old:        Pointer to existing ah_attr which needs to be replaced.                old is assumed to be valid or zero'd   @new:        Pointer to the new ah_attr.     rdma_replace_ah_attr() first releases any reference in the old ah_attr if   old the ah_attr is valid; after that it copies the new attribute and holds   the reference to the replaced ah_attr. ", "void rdma_move_ah_attr(struct rdma_ah_attr *dest, struct rdma_ah_attr *src)": "rdma_destroy_ah_attr(old); old =  new;if (old->grh.sgid_attr)rdma_hold_gid_attr(old->grh.sgid_attr);}EXPORT_SYMBOL(rdma_replace_ah_attr);     rdma_move_ah_attr - Move ah_attr pointed by source to destination.   @dest:       Pointer to destination ah_attr to copy to.                dest is assumed to be valid or zero'd   @src:        Pointer to the new ah_attr.     rdma_move_ah_attr() first releases any reference in the destination ah_attr   if it is valid. This also transfers ownership of internal references from   src to dest, making src invalid in the process. No new reference of the src   ah_attr is taken. ", "struct ib_ah *rdma_create_ah(struct ib_pd *pd, struct rdma_ah_attr *ah_attr,     u32 flags)": "rdma_create_ah(struct ib_pd  pd,     struct rdma_ah_attr  ah_attr,     u32 flags,     struct ib_udata  udata,     struct net_device  xmit_slave){struct rdma_ah_init_attr init_attr = {};struct ib_device  device = pd->device;struct ib_ah  ah;int ret;might_sleep_if(flags & RDMA_CREATE_AH_SLEEPABLE);if (!udata && !device->ops.create_ah)return ERR_PTR(-EOPNOTSUPP);ah = rdma_zalloc_drv_obj_gfp(device, ib_ah,(flags & RDMA_CREATE_AH_SLEEPABLE) ? GFP_KERNEL : GFP_ATOMIC);if (!ah)return ERR_PTR(-ENOMEM);ah->device = device;ah->pd = pd;ah->type = ah_attr->type;ah->sgid_attr = rdma_update_sgid_attr(ah_attr, NULL);init_attr.ah_attr = ah_attr;init_attr.flags = flags;init_attr.xmit_slave = xmit_slave;if (udata)ret = device->ops.create_user_ah(ah, &init_attr, udata);elseret = device->ops.create_ah(ah, &init_attr, NULL);if (ret) {if (ah->sgid_attr)rdma_put_gid_attr(ah->sgid_attr);kfree(ah);return ERR_PTR(ret);}atomic_inc(&pd->usecnt);return ah;}     rdma_create_ah - Creates an address handle for the   given address vector.   @pd: The protection domain associated with the address handle.   @ah_attr: The attributes of the address vector.   @flags: Create address handle flags (see enum rdma_create_ah_flags).     It returns 0 on success and returns appropriate error code on error.   The address handle is used to reference a local or global destination   in all UD QP post sends. ", "struct ib_ah *rdma_create_user_ah(struct ib_pd *pd,  struct rdma_ah_attr *ah_attr,  struct ib_udata *udata)": "rdma_create_user_ah - Creates an address handle for the   given address vector.   It resolves destination mac address for ah attribute of RoCE type.   @pd: The protection domain associated with the address handle.   @ah_attr: The attributes of the address vector.   @udata: pointer to user's input output buffer information need by           provider driver.     It returns 0 on success and returns appropriate error code on error.   The address handle is used to reference a local or global destination   in all UD QP post sends. ", "if (ip6h->version != 6)return (ip4h->version == 4) ? 4 : 0;/* version may be 6 or 4 because the first 20 bytes could be garbled ": "ib_get_rdma_header_version(const union rdma_network_hdr  hdr){const struct iphdr  ip4h = (struct iphdr  )&hdr->roce4grh;struct iphdr ip4h_checked;const struct ipv6hdr  ip6h = (struct ipv6hdr  )&hdr->ibgrh;  If it's IPv6, the version must be 6, otherwise, the first   20 bytes (before the IPv4 header) are garbled. ", "void rdma_move_grh_sgid_attr(struct rdma_ah_attr *attr, union ib_gid *dgid,     u32 flow_label, u8 hop_limit, u8 traffic_class,     const struct ib_gid_attr *sgid_attr)": "rdma_move_grh_sgid_attr(ah_attr,&sgid,flow_class & 0xFFFFF,hoplimit,(flow_class >> 20) & 0xFF,sgid_attr);ret = ib_resolve_unicast_gid_dmac(device, ah_attr);if (ret)rdma_destroy_ah_attr(ah_attr);return ret;} else {rdma_ah_set_dlid(ah_attr, wc->slid);rdma_ah_set_path_bits(ah_attr, wc->dlid_path_bits);if ((wc->wc_flags & IB_WC_GRH) == 0)return 0;if (dgid.global.interface_id !=cpu_to_be64(IB_SA_WELL_KNOWN_GUID)) {sgid_attr = rdma_find_gid_by_port(device, &dgid, IB_GID_TYPE_IB, port_num, NULL);} elsesgid_attr = rdma_get_gid_attr(device, port_num, 0);if (IS_ERR(sgid_attr))return PTR_ERR(sgid_attr);flow_class = be32_to_cpu(grh->version_tclass_flow);rdma_move_grh_sgid_attr(ah_attr,&sgid,flow_class & 0xFFFFF,hoplimit,(flow_class >> 20) & 0xFF,sgid_attr);return 0;}}EXPORT_SYMBOL(ib_init_ah_attr_from_wc);     rdma_move_grh_sgid_attr - Sets the sgid attribute of GRH, taking ownership   of the reference     @attr:Pointer to AH attribute structure   @dgid:Destination GID   @flow_label:Flow label   @hop_limit:Hop limit   @traffic_class: traffic class   @sgid_attr:Pointer to SGID attribute     This takes ownership of the sgid_attr reference. The caller must ensure   rdma_destroy_ah_attr() is called before destroying the rdma_ah_attr after   calling this function. ", "struct ib_srq *ib_create_srq_user(struct ib_pd *pd,  struct ib_srq_init_attr *srq_init_attr,  struct ib_usrq_object *uobject,  struct ib_udata *udata)": "ib_create_srq_user - Creates a SRQ associated with the specified protection     domain.   @pd: The protection domain associated with the SRQ.   @srq_init_attr: A list of initial attributes required to create the     SRQ.  If SRQ creation succeeds, then the attributes are updated to     the actual capabilities of the created SRQ.   @uobject: uobject pointer if this is not a kernel SRQ   @udata: udata pointer if this is not a kernel SRQ     srq_attr->max_wr and srq_attr->max_sge are read the determine the   requested size of the SRQ, and set to the actual values allocated   on return.  If ib_create_srq() succeeds, then max_wr and max_sge   will always be at least as large as the requested values. ", "struct ib_qp *ib_create_qp_user(struct ib_device *dev, struct ib_pd *pd,struct ib_qp_init_attr *attr,struct ib_udata *udata,struct ib_uqp_object *uobj, const char *caller)": "ib_create_qp_user - Creates a QP associated with the specified protection     domain.   @dev: IB device   @pd: The protection domain associated with the QP.   @attr: A list of initial attributes required to create the     QP.  If QP creation succeeds, then the attributes are updated to     the actual capabilities of the created QP.   @udata: User data   @uobj: uverbs obect   @caller: caller's build-time module name ", "if (qp_init_attr->cap.max_rdma_ctxs)rdma_rw_init_qp(device, qp_init_attr);qp = create_qp(device, pd, qp_init_attr, NULL, NULL, caller);if (IS_ERR(qp))return qp;ib_qp_usecnt_inc(qp);if (qp_init_attr->cap.max_rdma_ctxs) ": "ib_create_qp_kernel(struct ib_pd  pd,  struct ib_qp_init_attr  qp_init_attr,  const char  caller){struct ib_device  device = pd->device;struct ib_qp  qp;int ret;    If the callers is using the RDMA API calculate the resources   needed for the RDMA READWRITE operations.     Note that these callers need to pass in a port number. ", "int ib_modify_qp_with_udata(struct ib_qp *ib_qp, struct ib_qp_attr *attr,    int attr_mask, struct ib_udata *udata)": "ib_modify_qp_with_udata - Modifies the attributes for the specified QP.   @ib_qp: The QP to modify.   @attr: On input, specifies the QP attributes to modify.  On output,     the current values of selected QP attributes are returned.   @attr_mask: A bit-mask used to specify which attributes of the QP     are being modified.   @udata: pointer to user's input output buffer information     are being modified.   It returns 0 on success and returns appropriate error code on error. ", "qp->send_cq = attr->send_cq;qp->recv_cq = attr->recv_cq;ret = ib_create_qp_security(qp, dev);if (ret)goto err_security;rdma_restrack_add(&qp->res);return qp;err_security:qp->device->ops.destroy_qp(qp, udata ? &dummy : NULL);err_create:rdma_restrack_put(&qp->res);kfree(qp);return ERR_PTR(ret);}/** * ib_create_qp_user - Creates a QP associated with the specified protection *   domain. * @dev: IB device * @pd: The protection domain associated with the QP. * @attr: A list of initial attributes required to create the *   QP.  If QP creation succeeds, then the attributes are updated to *   the actual capabilities of the created QP. * @udata: User data * @uobj: uverbs obect * @caller: caller's build-time module name ": "ib_close_qp(qp);return ERR_PTR(err);}return qp;}static struct ib_qp  create_qp(struct ib_device  dev, struct ib_pd  pd,       struct ib_qp_init_attr  attr,       struct ib_udata  udata,       struct ib_uqp_object  uobj, const char  caller){struct ib_udata dummy = {};struct ib_qp  qp;int ret;if (!dev->ops.create_qp)return ERR_PTR(-EOPNOTSUPP);qp = rdma_zalloc_drv_obj_numa(dev, ib_qp);if (!qp)return ERR_PTR(-ENOMEM);qp->device = dev;qp->pd = pd;qp->uobject = uobj;qp->real_qp = qp;qp->qp_type = attr->qp_type;qp->rwq_ind_tbl = attr->rwq_ind_tbl;qp->srq = attr->srq;qp->event_handler = attr->event_handler;qp->port = attr->port_num;qp->qp_context = attr->qp_context;spin_lock_init(&qp->mr_lock);INIT_LIST_HEAD(&qp->rdma_mrs);INIT_LIST_HEAD(&qp->sig_mrs);qp->send_cq = attr->send_cq;qp->recv_cq = attr->recv_cq;rdma_restrack_new(&qp->res, RDMA_RESTRACK_QP);WARN_ONCE(!udata && !caller, \"Missing kernel QP owner\");rdma_restrack_set_name(&qp->res, udata ? NULL : caller);ret = dev->ops.create_qp(qp, attr, udata);if (ret)goto err_create;    TODO: The mlx4 internally overwrites send_cq and recv_cq.   Unfortunately, it is not an easy task to fix that driver. ", "struct ib_mr *ib_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,  u32 max_num_sg)": "ib_alloc_mr() - Allocates a memory region   @pd:            protection domain associated with the region   @mr_type:       memory region type   @max_num_sg:    maximum sg entries available for registration.     Notes:   Memory registeration pagesg lists must not exceed max_num_sg.   For mr_type IB_MR_TYPE_MEM_REG, the total length cannot exceed   max_num_sg   used_page_size.   ", "struct ib_mr *ib_alloc_mr_integrity(struct ib_pd *pd,    u32 max_num_data_sg,    u32 max_num_meta_sg)": "ib_alloc_mr_integrity() - Allocates an integrity memory region   @pd:                      protection domain associated with the region   @max_num_data_sg:         maximum data sg entries available for registration   @max_num_meta_sg:         maximum metadata sg entries available for                             registration     Notes:   Memory registration pagesg lists must not exceed max_num_sg,   also the integrity pagesg lists must not exceed max_num_meta_sg.   ", "struct ib_xrcd *ib_alloc_xrcd_user(struct ib_device *device,   struct inode *inode, struct ib_udata *udata)": "ib_alloc_xrcd_user - Allocates an XRC domain.   @device: The device on which to allocate the XRC domain.   @inode: inode to connect XRCD   @udata: Valid user data or NULL for kernel object ", "int ib_dealloc_xrcd_user(struct ib_xrcd *xrcd, struct ib_udata *udata)": "ib_dealloc_xrcd_user - Deallocates an XRC domain.   @xrcd: The XRC domain to deallocate.   @udata: Valid user data or NULL for kernel object ", "struct ib_wq *ib_create_wq(struct ib_pd *pd,   struct ib_wq_init_attr *wq_attr)": "ib_create_wq - Creates a WQ associated with the specified protection   domain.   @pd: The protection domain associated with the WQ.   @wq_attr: A list of initial attributes required to create the   WQ. If WQ creation succeeds, then the attributes are updated to   the actual capabilities of the created WQ.     wq_attr->max_wr and wq_attr->max_sge determine   the requested size of the WQ, and set to the actual values allocated   on return.   If ib_create_wq() succeeds, then max_wr and max_sge will always be   at least as large as the requested values. ", "int ib_destroy_wq_user(struct ib_wq *wq, struct ib_udata *udata)": "ib_destroy_wq_user - Destroys the specified user WQ.   @wq: The WQ to destroy.   @udata: Valid user data ", "int ib_map_mr_sg_pi(struct ib_mr *mr, struct scatterlist *data_sg,    int data_sg_nents, unsigned int *data_sg_offset,    struct scatterlist *meta_sg, int meta_sg_nents,    unsigned int *meta_sg_offset, unsigned int page_size)": "ib_map_mr_sg_pi() - Map the dma mapped SG lists for PI (protection       information) and set an appropriate memory region for registration.   @mr:             memory region   @data_sg:        dma mapped scatterlist for data   @data_sg_nents:  number of entries in data_sg   @data_sg_offset: offset in bytes into data_sg   @meta_sg:        dma mapped scatterlist for metadata   @meta_sg_nents:  number of entries in meta_sg   @meta_sg_offset: offset in bytes into meta_sg   @page_size:      page vector desired page size     Constraints:   - The MR must be allocated with type IB_MR_TYPE_INTEGRITY.     Return: 0 on success.     After this completes successfully, the  memory region   is ready for registration. ", "int ib_sg_to_pages(struct ib_mr *mr, struct scatterlist *sgl, int sg_nents,unsigned int *sg_offset_p, int (*set_page)(struct ib_mr *, u64))": "ib_sg_to_pages() - Convert the largest prefix of a sg list       to a page vector   @mr:            memory region   @sgl:           dma mapped scatterlist   @sg_nents:      number of entries in sg   @sg_offset_p:   ==== =======================================================                   IN   start offset in bytes into sg                   OUT  offset in bytes for element n of the sg of the first                        byte that has not been processed where n is the return                        value of this function.                   ==== =======================================================   @set_page:      driver page assignment function pointer     Core service helper for drivers to convert the largest   prefix of given sg list to a page vector. The sg list   prefix converted is the prefix that meet the requirements   of ib_map_mr_sg.     Returns the number of sg elements that were assigned to   a page vector. ", "static void __ib_drain_rq(struct ib_qp *qp)": "ib_drain_sq(struct ib_qp  qp){struct ib_cq  cq = qp->send_cq;struct ib_qp_attr attr = { .qp_state = IB_QPS_ERR };struct ib_drain_cqe sdrain;struct ib_rdma_wr swr = {.wr = {.next = NULL,{ .wr_cqe= &sdrain.cqe, },.opcode= IB_WR_RDMA_WRITE,},};int ret;ret = ib_modify_qp(qp, &attr, IB_QP_STATE);if (ret) {WARN_ONCE(ret, \"failed to drain send queue: %d\\n\", ret);return;}sdrain.cqe.done = ib_drain_qp_done;init_completion(&sdrain.done);ret = ib_post_send(qp, &swr.wr, NULL);if (ret) {WARN_ONCE(ret, \"failed to drain send queue: %d\\n\", ret);return;}if (cq->poll_ctx == IB_POLL_DIRECT)while (wait_for_completion_timeout(&sdrain.done, HZ  10) <= 0)ib_process_cq_direct(cq, -1);elsewait_for_completion(&sdrain.done);}    Post a WR and block until its completion is reaped for the RQ. ", "void ib_drain_sq(struct ib_qp *qp)": "ib_drain_rq(struct ib_qp  qp){struct ib_cq  cq = qp->recv_cq;struct ib_qp_attr attr = { .qp_state = IB_QPS_ERR };struct ib_drain_cqe rdrain;struct ib_recv_wr rwr = {};int ret;ret = ib_modify_qp(qp, &attr, IB_QP_STATE);if (ret) {WARN_ONCE(ret, \"failed to drain recv queue: %d\\n\", ret);return;}rwr.wr_cqe = &rdrain.cqe;rdrain.cqe.done = ib_drain_qp_done;init_completion(&rdrain.done);ret = ib_post_recv(qp, &rwr, NULL);if (ret) {WARN_ONCE(ret, \"failed to drain recv queue: %d\\n\", ret);return;}if (cq->poll_ctx == IB_POLL_DIRECT)while (wait_for_completion_timeout(&rdrain.done, HZ  10) <= 0)ib_process_cq_direct(cq, -1);elsewait_for_completion(&rdrain.done);}     ib_drain_sq() - Block until all SQ CQEs have been consumed by the     application.   @qp:            queue pair to drain     If the device has a provider-specific drain function, then   call that.  Otherwise call the generic drain function   __ib_drain_sq().     The caller must:     ensure there is room in the CQ and SQ for the drain work request and   completion.     allocate the CQ using ib_alloc_cq().     ensure that there are no other contexts that are posting WRs concurrently.   Otherwise the drain is not guaranteed. ", "static void __ib_drain_sq(struct ib_qp *qp)": "ib_drain_qp_done(struct ib_cq  cq, struct ib_wc  wc){struct ib_drain_cqe  cqe = container_of(wc->wr_cqe, struct ib_drain_cqe,cqe);complete(&cqe->done);}    Post a WR and block until its completion is reaped for the SQ. ", "biter->__pg_bit = __fls(pgsz);}EXPORT_SYMBOL(__rdma_block_iter_start": "__rdma_block_iter_start(struct ib_block_iter  biter,     struct scatterlist  sglist, unsigned int nents,     unsigned long pgsz){memset(biter, 0, sizeof(struct ib_block_iter));biter->__sg = sglist;biter->__sg_nents = nents;  Driver provides best block size to use ", "struct rdma_hw_stats *rdma_alloc_hw_stats_struct(const struct rdma_stat_desc *descs, int num_counters,unsigned long lifespan)": "rdma_alloc_hw_stats_struct - Helper function to allocate dynamic struct     for the drivers.   @descs: array of static descriptors   @num_counters: number of elements in array   @lifespan: milliseconds between updates ", "void rdma_free_hw_stats_struct(struct rdma_hw_stats *stats)": "rdma_free_hw_stats_struct - Helper function to release rdma_hw_stats   @stats: statistics to release ", "int ib_port_register_client_groups(struct ib_device *ibdev, u32 port_num,   const struct attribute_group **groups)": "ib_port_register_client_groups - Add an ib_client's attributes to the port     @ibdev: IB device to add counters   @port_num: valid port number   @groups: Group list of attributes     Do not use. Only for legacy sysfs compatibility. ", "int ib_ud_header_init(int     payload_bytes,      int    lrh_present,      int    eth_present,      int    vlan_present,      int    grh_present,      int    ip_version,      int    udp_present,      int    immediate_present,      struct ib_ud_header *header)": "ib_ud_header_init - Initialize UD header structure   @payload_bytes:Length of packet payload   @lrh_present: specify if LRH is present   @eth_present: specify if Eth header is present   @vlan_present: packet is tagged vlan   @grh_present: GRH flag (if non-zero, GRH will be included)   @ip_version: if non-zero, IP header, V4 or V6, will be included   @udp_present :if non-zero, UDP header will be included   @immediate_present: specify if immediate data is present   @header:Structure to initialize ", "int ib_ud_header_pack(struct ib_ud_header *header,      void                *buf)": "ib_ud_header_pack - Pack UD header struct into wire format   @header:UD header struct   @buf:Buffer to pack into     ib_ud_header_pack() packs the UD header structure @header into wire   format in the buffer @buf. ", "int ib_ud_header_unpack(void                *buf,struct ib_ud_header *header)": "ib_ud_header_unpack - Unpack UD header struct from wire format   @header:UD header struct   @buf:Buffer to pack into     ib_ud_header_pack() unpacks the UD header structure @header from wire   format in the buffer @buf. ", "unsigned long ib_umem_find_best_pgsz(struct ib_umem *umem,     unsigned long pgsz_bitmap,     unsigned long virt)": "ib_umem_release(struct ib_device  dev, struct ib_umem  umem, int dirty){bool make_dirty = umem->writable && dirty;struct scatterlist  sg;unsigned int i;if (dirty)ib_dma_unmap_sgtable_attrs(dev, &umem->sgt_append.sgt,   DMA_BIDIRECTIONAL, 0);for_each_sgtable_sg(&umem->sgt_append.sgt, sg, i)unpin_user_page_range_dirty_lock(sg_page(sg),DIV_ROUND_UP(sg->length, PAGE_SIZE), make_dirty);sg_free_append_table(&umem->sgt_append);}     ib_umem_find_best_pgsz - Find best HW page size to use for this MR     @umem: umem struct   @pgsz_bitmap: bitmap of HW supported page sizes   @virt: IOVA     This helper is intended for HW that support multiple page   sizes but can do only a single page size in an MR.     Returns 0 if the umem requires page sizes not supported by   the driver to be mapped. Drivers always supporting PAGE_SIZE   or smaller will never see a 0 result. ", "struct ib_umem *ib_umem_get(struct ib_device *device, unsigned long addr,    size_t size, int access)": "ib_umem_get - Pin and DMA map userspace memory.     @device: IB device to connect UMEM   @addr: userspace virtual address to start at   @size: length of region to pin   @access: IB_ACCESS_xxx flags for memory being pinned ", "index = -reason;if (index < ARRAY_SIZE(iwcm_rej_reason_strs) &&    iwcm_rej_reason_strs[index])return iwcm_rej_reason_strs[index];elsereturn \"unrecognized reason\";}EXPORT_SYMBOL(iwcm_reject_msg": "iwcm_reject_msg(int reason){size_t index;  iWARP uses negative errnos ", "wait_event(cm_id_priv->connect_wait,   !test_bit(IWCM_F_CONNECT_WAIT, &cm_id_priv->flags));spin_lock_irqsave(&cm_id_priv->lock, flags);switch (cm_id_priv->state) ": "iw_cm_disconnect(struct iw_cm_id  cm_id, int abrupt){struct iwcm_id_private  cm_id_priv;unsigned long flags;int ret = 0;struct ib_qp  qp = NULL;cm_id_priv = container_of(cm_id, struct iwcm_id_private, id);  Wait if we're currently in a connect or accept downcall ", "qp = cm_id->device->ops.iw_get_qp(cm_id->device, iw_param->qpn);if (!qp) ": "iw_cm_connect(struct iw_cm_id  cm_id, struct iw_cm_conn_param  iw_param){struct iwcm_id_private  cm_id_priv;int ret;unsigned long flags;struct ib_qp  qp = NULL;cm_id_priv = container_of(cm_id, struct iwcm_id_private, id);ret = alloc_work_entries(cm_id_priv, 4);if (ret)return ret;set_bit(IWCM_F_CONNECT_WAIT, &cm_id_priv->flags);spin_lock_irqsave(&cm_id_priv->lock, flags);if (cm_id_priv->state != IW_CM_STATE_IDLE) {ret = -EINVAL;goto err;}  Get the ib_qp given the QPN ", "int rdma_rw_ctx_init(struct rdma_rw_ctx *ctx, struct ib_qp *qp, u32 port_num,struct scatterlist *sg, u32 sg_cnt, u32 sg_offset,u64 remote_addr, u32 rkey, enum dma_data_direction dir)": "rdma_rw_ctx_init - initialize a RDMA READWRITE context   @ctx:context to initialize   @qp:queue pair to operate on   @port_num:port num to which the connection is bound   @sg:scatterlist to READWRITE fromto   @sg_cnt:number of entries in @sg   @sg_offset:current byte offset into @sg   @remote_addr:remote address to readwrite (relative to @rkey)   @rkey:remote key to operate on   @dir:%DMA_TO_DEVICE for RDMA WRITE, %DMA_FROM_DEVICE for RDMA READ     Returns the number of WQEs that will be needed on the workqueue if   successful, or a negative error code. ", "int rdma_rw_ctx_signature_init(struct rdma_rw_ctx *ctx, struct ib_qp *qp,u32 port_num, struct scatterlist *sg, u32 sg_cnt,struct scatterlist *prot_sg, u32 prot_sg_cnt,struct ib_sig_attrs *sig_attrs,u64 remote_addr, u32 rkey, enum dma_data_direction dir)": "rdma_rw_ctx_signature_init - initialize a RW context with signature offload   @ctx:context to initialize   @qp:queue pair to operate on   @port_num:port num to which the connection is bound   @sg:scatterlist to READWRITE fromto   @sg_cnt:number of entries in @sg   @prot_sg:scatterlist to READWRITE protection information fromto   @prot_sg_cnt: number of entries in @prot_sg   @sig_attrs:signature offloading algorithms   @remote_addr:remote address to readwrite (relative to @rkey)   @rkey:remote key to operate on   @dir:%DMA_TO_DEVICE for RDMA WRITE, %DMA_FROM_DEVICE for RDMA READ     Returns the number of WQEs that will be needed on the workqueue if   successful, or a negative error code. ", "struct ib_send_wr *rdma_rw_ctx_wrs(struct rdma_rw_ctx *ctx, struct ib_qp *qp,u32 port_num, struct ib_cqe *cqe, struct ib_send_wr *chain_wr)": "rdma_rw_ctx_wrs - return chain of WRs for a RDMA READ or WRITE operation   @ctx:context to operate on   @qp:queue pair to operate on   @port_num:port num to which the connection is bound   @cqe:completion queue entry for the last WR   @chain_wr:WR to append to the posted chain     Return the WR chain for the set of RDMA READWRITE operations described by   @ctx, as well as any memory registration operations needed.  If @chain_wr   is non-NULL the WR it points to will be appended to the chain of WRs posted.   If @chain_wr is not set @cqe must be set so that the caller gets a   completion notification. ", "int rdma_rw_ctx_post(struct rdma_rw_ctx *ctx, struct ib_qp *qp, u32 port_num,struct ib_cqe *cqe, struct ib_send_wr *chain_wr)": "rdma_rw_ctx_post - post a RDMA READ or RDMA WRITE operation   @ctx:context to operate on   @qp:queue pair to operate on   @port_num:port num to which the connection is bound   @cqe:completion queue entry for the last WR   @chain_wr:WR to append to the posted chain     Post the set of RDMA READWRITE operations described by @ctx, as well as   any memory registration operations needed.  If @chain_wr is non-NULL the   WR it points to will be appended to the chain of WRs posted.  If @chain_wr   is not set @cqe must be set so that the caller gets a completion   notification. ", "void rdma_rw_ctx_destroy(struct rdma_rw_ctx *ctx, struct ib_qp *qp, u32 port_num, struct scatterlist *sg, u32 sg_cnt, enum dma_data_direction dir)": "rdma_rw_ctx_destroy - release all resources allocated by rdma_rw_ctx_init   @ctx:context to release   @qp:queue pair to operate on   @port_num:port num to which the connection is bound   @sg:scatterlist that was used for the READWRITE   @sg_cnt:number of entries in @sg   @dir:%DMA_TO_DEVICE for RDMA WRITE, %DMA_FROM_DEVICE for RDMA READ ", "void rdma_rw_ctx_destroy_signature(struct rdma_rw_ctx *ctx, struct ib_qp *qp,u32 port_num, struct scatterlist *sg, u32 sg_cnt,struct scatterlist *prot_sg, u32 prot_sg_cnt,enum dma_data_direction dir)": "rdma_rw_ctx_destroy_signature - release all resources allocated by  rdma_rw_ctx_signature_init   @ctx:context to release   @qp:queue pair to operate on   @port_num:port num to which the connection is bound   @sg:scatterlist that was used for the READWRITE   @sg_cnt:number of entries in @sg   @prot_sg:scatterlist that was used for the READWRITE of the PI   @prot_sg_cnt: number of entries in @prot_sg   @dir:%DMA_TO_DEVICE for RDMA WRITE, %DMA_FROM_DEVICE for RDMA READ ", "unsigned int rdma_rw_mr_factor(struct ib_device *device, u32 port_num,       unsigned int maxpages)": "rdma_rw_mr_factor - return number of MRs required for a payload   @device:device handling the connection   @port_num:port num to which the connection is bound   @maxpages:maximum payload pages per rdma_rw_ctx     Returns the number of MRs the device requires to move @maxpayload   bytes. The returned value is used during transport creation to   compute max_rdma_ctxts and the size of the transport's Send and   Send Completion Queues. ", "bool rdma_dev_access_netns(const struct ib_device *dev, const struct net *net)": "rdma_dev_access_netns() - Return whether an rdma device can be accessed       from a specified net namespace or not.   @dev:Pointer to rdma device which needs to be checked   @net:Pointer to net namesapce for which access to be checked     When the rdma device is in shared mode, it ignores the net namespace.   When the rdma device is exclusive to a net namespace, rdma device net   namespace is checked against the specified one. ", "struct ib_device *ib_device_get_by_index(const struct net *net, u32 index)": "ib_device_put() to return the device reference count   when ib_device_get_by_index() returns valid device pointer. ", "struct ib_device *ib_device_get_by_name(const char *name,enum rdma_driver_id driver_id)": "ib_device_get_by_name(const char  name){struct ib_device  device;unsigned long index;xa_for_each (&devices, index, device)if (!strcmp(name, dev_name(&device->dev)))return device;return NULL;}     ib_device_get_by_name - Find an IB device by name   @name: The name to look for   @driver_id: The driver ID that must match (RDMA_DRIVER_UNKNOWN matches all)     Find and hold an ib_device by its name. The caller must call   ib_device_put() on the returned pointer. ", "struct ib_device *_ib_alloc_device(size_t size)": "ib_dealloc_device() must be used to free structures allocated with   ib_alloc_device(). ", "const struct ib_port_immutable*ib_port_immutable_read(struct ib_device *dev, unsigned int port)": "ib_port_immutable_read() - Read rdma port's immutable data   @dev: IB device   @port: port number whose immutable data to read. It starts with index 1 and          valid upto including rdma_end_port(). ", "mutex_lock(&device->compat_devs_mutex);cdev = xa_load(&device->compat_devs, rnet->id);if (cdev) ": "ib_register_device() to take the   compat_devs_mutex wins and gets to add the device. Others will wait   for completion here. ", "struct ib_port_data_rcu ": "ib_unregister_device(struct ib_device  device);static int ib_security_change(struct notifier_block  nb, unsigned long event,      void  lsm_data);static void ib_policy_change_task(struct work_struct  work);static DECLARE_WORK(ib_policy_change_work, ib_policy_change_task);static void __ibdev_printk(const char  level, const struct ib_device  ibdev,   struct va_format  vaf){if (ibdev && ibdev->dev.parent)dev_printk_emit(level[1] - '0',ibdev->dev.parent,\"%s %s %s: %pV\",dev_driver_string(ibdev->dev.parent),dev_name(ibdev->dev.parent),dev_name(&ibdev->dev),vaf);else if (ibdev)printk(\"%s%s: %pV\",       level, dev_name(&ibdev->dev), vaf);elseprintk(\"%s(NULL ib_device): %pV\", level, vaf);}void ibdev_printk(const char  level, const struct ib_device  ibdev,  const char  format, ...){struct va_format vaf;va_list args;va_start(args, format);vaf.fmt = format;vaf.va = &args;__ibdev_printk(level, ibdev, &vaf);va_end(args);}EXPORT_SYMBOL(ibdev_printk);#define define_ibdev_printk_level(func, level)                  \\void func(const struct ib_device  ibdev, const char  fmt, ...)  \\{                                                               \\struct va_format vaf;                                   \\va_list args;                                           \\\\va_start(args, fmt);                                    \\\\vaf.fmt = fmt;                                          \\vaf.va = &args;                                         \\\\__ibdev_printk(level, ibdev, &vaf);                     \\\\va_end(args);                                           \\}                                                               \\EXPORT_SYMBOL(func);define_ibdev_printk_level(ibdev_emerg, KERN_EMERG);define_ibdev_printk_level(ibdev_alert, KERN_ALERT);define_ibdev_printk_level(ibdev_crit, KERN_CRIT);define_ibdev_printk_level(ibdev_err, KERN_ERR);define_ibdev_printk_level(ibdev_warn, KERN_WARNING);define_ibdev_printk_level(ibdev_notice, KERN_NOTICE);define_ibdev_printk_level(ibdev_info, KERN_INFO);static struct notifier_block ibdev_lsm_nb = {.notifier_call = ib_security_change,};static int rdma_dev_change_netns(struct ib_device  device, struct net  cur_net, struct net  net);  Pointer to the RCU head at the start of the ib_port_data array ", "void ib_unregister_device_and_put(struct ib_device *ib_dev)": "ib_unregister_device_and_put - Unregister a device while holding a 'get'   @ib_dev: The device to unregister     This is the same as ib_unregister_device(), except it includes an internal   ib_device_put() that should match a 'get' obtained by the caller.     It is safe to call this routine concurrently from multiple threads while   holding the 'get'. When the function returns the device is fully   unregistered.     Drivers using this flow MUST use the driver_unregister callback to clean up   their resources associated with the device and dealloc it. ", "down_write(&devices_rwsem);if (xa_load(&devices, device->index) == device)xa_erase(&devices, device->index);up_write(&devices_rwsem);/* Expedite releasing netdev references ": "ib_unregister_driver() requires all devices to remain in the xarray   while their ops are callable. The last op we call is dealloc_driver   above.  This is needed to create a fence on op callbacks prior to   allowing the driver module to unload. ", "void ib_unregister_device_queued(struct ib_device *ib_dev)": "ib_unregister_device_queued - Unregister a device using a work queue   @ib_dev: The device to unregister     This schedules an asynchronous unregistration using a WQ for the device. A   driver should use this to avoid holding locks while doing unregistration,   such as holding the RTNL lock.     Drivers using this API must use ib_unregister_driver before module unload   to ensure that all scheduled unregistrations have completed. ", "int ib_register_client(struct ib_client *client)": "ib_register_client - Register an IB client   @client:Client to register     Upper level users of the IB drivers can use ib_register_client() to   register callbacks for IB device addition and removal.  When an IB   device is added, each registered client's add method will be called   (in the order the clients were registered), and when a device is   removed, each client's remove method will be called (in the reverse   order that clients were registered).  In addition, when   ib_register_client() is called, the client will receive an add   callback for all devices already registered. ", "void ib_unregister_client(struct ib_client *client)": "ib_unregister_client(client);return ret;}}up_read(&devices_rwsem);return 0;}EXPORT_SYMBOL(ib_register_client);     ib_unregister_client - Unregister an IB client   @client:Client to unregister     Upper level users use ib_unregister_client() to remove their client   registration.  When ib_unregister_client() is called, the client   will receive a remove callback for each IB device still registered.     This is a full fence, once it returns no client callbacks will be called,   or are running in another thread. ", "void ib_set_client_data(struct ib_device *device, struct ib_client *client,void *data)": "ib_set_client_data - Set IB client context   @device:Device to set context for   @client:Client to set context for   @data:Context to set     ib_set_client_data() sets client context data that can be retrieved with   ib_get_client_data(). This can only be called while the client is   registered to the device, once the ib_client remove() callback returns this   cannot be called. ", "void ib_register_event_handler(struct ib_event_handler *event_handler)": "ib_register_event_handler - Register an IB event handler   @event_handler:Handler to register     ib_register_event_handler() registers an event handler that will be   called back when asynchronous IB events occur (as defined in   chapter 11 of the InfiniBand Architecture Specification). This   callback occurs in workqueue context. ", "void ib_unregister_event_handler(struct ib_event_handler *event_handler)": "ib_unregister_event_handler - Unregister an event handler   @event_handler:Handler to unregister     Unregister an event handler registered with   ib_register_event_handler(). ", "int ib_query_port(struct ib_device *device,  u32 port_num,  struct ib_port_attr *port_attr)": "ib_query_port(struct ib_device  device,   u32 port_num,   struct ib_port_attr  port_attr){int err;memset(port_attr, 0, sizeof( port_attr));err = device->ops.query_port(device, port_num, port_attr);if (err || port_attr->subnet_prefix)return err;if (rdma_port_get_link_layer(device, port_num) !=    IB_LINK_LAYER_INFINIBAND)return 0;ib_get_cached_subnet_prefix(device, port_num,    &port_attr->subnet_prefix);return 0;}     ib_query_port - Query IB port attributes   @device:Device to query   @port_num:Port number to query   @port_attr:Port attributes     ib_query_port() returns the attributes of a port through the   @port_attr pointer. ", "int ib_device_set_netdev(struct ib_device *ib_dev, struct net_device *ndev, u32 port)": "ib_device_set_netdev - Associate the ib_dev with an underlying net_device   @ib_dev: Device to modify   @ndev: net_device to affiliate, may be NULL   @port: IB port the net_device is connected to     Drivers should use this to link the ib_device to a netdev so the netdev   shows up in interfaces like ib_enum_roce_netdev. Only one netdev may be   affiliated with any port.     The caller must ensure that the given ndev is not unregistered or   unregistering, and that either the ib_device is unregistered or   ib_device_set_netdev() is called with NULL when the ndev sends a   NETDEV_UNREGISTER event. ", "struct ib_device *ib_device_get_by_netdev(struct net_device *ndev,  enum rdma_driver_id driver_id)": "ib_device_get_by_netdev - Find an IB device associated with a netdev   @ndev: netdev to locate   @driver_id: The driver ID that must match (RDMA_DRIVER_UNKNOWN matches all)     Find and hold an ib_device that is associated with a netdev via   ib_device_set_netdev(). The caller must call ib_device_put() on the   returned pointer. ", "int ib_query_pkey(struct ib_device *device,  u32 port_num, u16 index, u16 *pkey)": "ib_query_pkey - Get P_Key table entry   @device:Device to query   @port_num:Port number to query   @index:P_Key table index to query   @pkey:Returned P_Key     ib_query_pkey() fetches the specified P_Key table entry. ", "int ib_modify_device(struct ib_device *device,     int device_modify_mask,     struct ib_device_modify *device_modify)": "ib_modify_device - Change IB device attributes   @device:Device to modify   @device_modify_mask:Mask of attributes to change   @device_modify:New attribute values     ib_modify_device() changes a device's attributes as specified by   the @device_modify_mask and @device_modify structure. ", "int ib_modify_port(struct ib_device *device,   u32 port_num, int port_modify_mask,   struct ib_port_modify *port_modify)": "ib_modify_port - Modifies the attributes for the specified port.   @device: The device to modify.   @port_num: The number of the port to modify.   @port_modify_mask: Mask used to specify which attributes of the port     to change.   @port_modify: New attribute values for the port.     ib_modify_port() changes a port's attributes as specified by the   @port_modify_mask and @port_modify structure. ", "int ib_find_gid(struct ib_device *device, union ib_gid *gid,u32 *port_num, u16 *index)": "ib_find_gid - Returns the port number and GID table index where     a specified GID value occurs. Its searches only for IB link layer.   @device: The device to query.   @gid: The GID value to search for.   @port_num: The port number of the device where the GID value was found.   @index: The index into the GID table where the GID was found.  This     parameter may be NULL. ", "int ib_find_pkey(struct ib_device *device, u32 port_num, u16 pkey, u16 *index)": "ib_find_pkey - Returns the PKey table index where a specified     PKey value occurs.   @device: The device to query.   @port_num: The port number of the device to search for the PKey.   @pkey: The PKey value to search for.   @index: The index into the PKey table where the PKey was found. ", "struct net_device *ib_get_net_dev_by_params(struct ib_device *dev,    u32 port,    u16 pkey,    const union ib_gid *gid,    const struct sockaddr *addr)": "ib_get_net_dev_by_params() - Return the appropriate net_dev   for a received CM request   @dev:An RDMA device on which the request has been received.   @port:Port number on the RDMA device.   @pkey:The Pkey the request came on.   @gid:A GID that the net_dev uses to communicate.   @addr:Contains the IP address that the request specified as its  destination.   ", "smp_store_release(&rdma_nl_types[index].cb_table, cb_table);}EXPORT_SYMBOL(rdma_nl_register": "rdma_nl_register(unsigned int index,      const struct rdma_nl_cbs cb_table[]){if (WARN_ON(!is_nl_msg_valid(index, 0)) ||    WARN_ON(READ_ONCE(rdma_nl_types[index].cb_table)))return;  Pairs with the READ_ONCE in is_nl_valid() ", "void rdma_roce_rescan_device(struct ib_device *ib_dev)": "rdma_roce_rescan_device - Rescan all of the network devices in the system   and add their gids, as needed, to the relevant RoCE devices.     @ib_dev:         the rdma device ", "bool rdma_is_zero_gid(const union ib_gid *gid)": "rdma_is_zero_gid - Check if given GID is zero or not.   @gid:GID to check   Returns true if given GID is zero, returns false otherwise. ", "const struct ib_gid_attr *rdma_find_gid_by_port(struct ib_device *ib_dev,      const union ib_gid *gid,      enum ib_gid_type gid_type,      u32 port, struct net_device *ndev)": "rdma_put_gid_attr() to release the reference. ", "int rdma_query_gid(struct ib_device *device, u32 port_num,   int index, union ib_gid *gid)": "rdma_query_gid - Read the GID content from the GID software cache   @device:Device to query the GID   @port_num:Port number of the device   @index:Index of the GID table entry to read   @gid:Pointer to GID where to store the entry's GID     rdma_query_gid() only reads the GID entry content for requested device,   port and index. It reads for IB, RoCE and iWarp link layers.  It doesn't   hold any reference to the GID table entry in the HCA or software cache.     Returns 0 on success or appropriate error code.   ", "void *rdma_read_gid_hw_context(const struct ib_gid_attr *attr)": "rdma_read_gid_hw_context - Read the HW GID context from GID attribute   @attr:Potinter to the GID attribute     rdma_read_gid_hw_context() reads the drivers GID HW context corresponding   to the SGID attr. Callers are required to already be holding the reference   to an existing GID entry.     Returns the HW GID context   ", "const struct ib_gid_attr *rdma_get_gid_attr(struct ib_device *device, u32 port_num, int index)": "rdma_get_gid_attr - Returns GID attributes for a port of a device   at a requested gid_index, if a valid GID entry exists.   @device:The device to query.   @port_num:The port number on the device where the GID value  is to be queried.   @index:Index of the GID table entry whose attributes are to                        be queried.     rdma_get_gid_attr() acquires reference count of gid attributes from the   cached GID table. Caller must invoke rdma_put_gid_attr() to release   reference to gid attribute regardless of link layer.     Returns pointer to valid gid attribute or ERR_PTR for the appropriate error   code. ", "ssize_t rdma_query_gid_table(struct ib_device *device,     struct ib_uverbs_gid_entry *entries,     size_t max_entries)": "rdma_query_gid_table - Reads GID table entries of all the ports of a device up to max_entries.   @device: The device to query.   @entries: Entries where GID entries are returned.   @max_entries: Maximum number of entries that can be returned.   Entries array must be allocated to hold max_entries number of entries.     Returns number of entries on success or appropriate error code. ", "void rdma_hold_gid_attr(const struct ib_gid_attr *attr)": "rdma_hold_gid_attr - Get reference to existing GID attribute     @attr:Pointer to the GID attribute whose reference  needs to be taken.     Increase the reference count to a GID attribute to keep it from being   freed. Callers are required to already be holding a reference to attribute.   ", "struct net_device *rdma_read_gid_attr_ndev_rcu(const struct ib_gid_attr *attr)": "rdma_read_gid_attr_ndev_rcu - Read GID attribute netdevice   which must be in UP state.     @attr:Pointer to the GID attribute     Returns pointer to netdevice if the netdevice was attached to GID and   netdevice is in UP state. Caller must hold RCU lock as this API   reads the netdev flags which can change while netdevice migrates to   different net namespace. Returns ERR_PTR with error code otherwise.   ", "int rdma_read_gid_l2_fields(const struct ib_gid_attr *attr,    u16 *vlan_id, u8 *smac)": "rdma_read_gid_l2_fields - Read the vlan ID and source MAC address       of a GID entry.     @attr:GID attribute pointer whose L2 fields to be read   @vlan_id:Pointer to vlan id to fill up if the GID entry has  vlan id. It is optional.   @smac:Pointer to smac to fill up for a GID entry. It is optional.     rdma_read_gid_l2_fields() returns 0 on success and returns vlan id   (if gid entry has vlan) and source MAC, or returns error. ", "[IB_GID_TYPE_IB]= \"IB/RoCE v1\",[IB_GID_TYPE_ROCE]= \"IB/RoCE v1\",[IB_GID_TYPE_ROCE_UDP_ENCAP]= \"RoCE v2\",};const char *ib_cache_gid_type_str(enum ib_gid_type gid_type)": "ib_dispatch_event_clients(&event);}static const char   const gid_type_str[] = {  IBRoCE v1 value is set for IB_GID_TYPE_IB and IB_GID_TYPE_ROCE for   user space compatibility reasons. ", "if (!is_ib)return 0;qp->qp_sec = kzalloc(sizeof(*qp->qp_sec), GFP_KERNEL);if (!qp->qp_sec)return -ENOMEM;qp->qp_sec->qp = qp;qp->qp_sec->dev = dev;mutex_init(&qp->qp_sec->mutex);INIT_LIST_HEAD(&qp->qp_sec->shared_qp_list);atomic_set(&qp->qp_sec->error_list_count, 0);init_completion(&qp->qp_sec->error_complete);ret = security_ib_alloc_security(&qp->qp_sec->security);if (ret) ": "ib_create_qp_security(qp, dev);if (ret)return ret;if (!qp->qp_sec)return 0;mutex_lock(&real_qp->qp_sec->mutex);ret = check_qp_port_pkey_settings(real_qp->qp_sec->ports_pkeys,  qp->qp_sec);if (ret)goto ret;if (qp != real_qp)list_add(&qp->qp_sec->shared_qp_list, &real_qp->qp_sec->shared_qp_list);ret:mutex_unlock(&real_qp->qp_sec->mutex);if (ret)destroy_qp_security(qp->qp_sec);return ret;}void ib_close_shared_qp_security(struct ib_qp_security  sec){struct ib_qp  real_qp = sec->qp->real_qp;mutex_lock(&real_qp->qp_sec->mutex);list_del(&sec->shared_qp_list);mutex_unlock(&real_qp->qp_sec->mutex);destroy_qp_security(sec);}int ib_create_qp_security(struct ib_qp  qp, struct ib_device  dev){unsigned int i;bool is_ib = false;int ret;rdma_for_each_port (dev, i) {is_ib = rdma_protocol_ib(dev, i);if (is_ib)break;}  If this isn't an IB device don't create the security context ", "void rdma_umap_priv_init(struct rdma_umap_priv *priv, struct vm_area_struct *vma, struct rdma_user_mmap_entry *entry)": "rdma_umap_priv_init() - Initialize the private data of a vma     @priv: The already allocated private data   @vma: The vm area struct that needs private data   @entry: entry into the mmap_xa that needs to be linked with         this vma     Each time we map IO memory into user space this keeps track of the   mapping. When the device is hot-unplugged we 'zap' the mmaps in user space   to point to the zero page and allow the hot unplug to proceed.     This is necessary for cases like PCI physical hot unplug as the actual BAR   memory may vanish after this and access to it from userspace could MCE.     RDMA drivers supporting disassociation must have their user space designed   to cope in some way with their IO pages going to the zero page.   ", "int rdma_user_mmap_io(struct ib_ucontext *ucontext, struct vm_area_struct *vma,      unsigned long pfn, unsigned long size, pgprot_t prot,      struct rdma_user_mmap_entry *entry)": "rdma_user_mmap_entry_get(), or NULL           if mmap_entry is not used by the driver     This is to be called by drivers as part of their mmap() functions if they   wish to send something like PCI-E BAR memory to userspace.     Return -EINVAL on wrong flags or size, -EAGAIN on failure to map. 0 on   success. ", "struct rdma_user_mmap_entry *rdma_user_mmap_entry_get_pgoff(struct ib_ucontext *ucontext,       unsigned long pgoff)": "rdma_user_mmap_entry_insert().  This function increases the refcnt of the   entry so that it won't be deleted from the xarray in the meantime.     Return an reference to an entry if exists or NULL if there is no   match. rdma_user_mmap_entry_put() must be called to put the reference. ", "void rdma_user_mmap_entry_remove(struct rdma_user_mmap_entry *entry)": "rdma_user_mmap_entry_remove() - Drop reference to entry and     mark it as unmmapable     @entry: the entry to insert into the mmap_xa     Drivers can call this to prevent userspace from creating more mappings for   entry, however existing mmaps continue to exist and ops->mmap_free() will   not be called until all user mmaps are destroyed. ", "int rdma_user_mmap_entry_insert_range(struct ib_ucontext *ucontext,      struct rdma_user_mmap_entry *entry,      size_t length, u32 min_pgoff,      u32 max_pgoff)": "rdma_user_mmap_entry_insert_range() - Insert an entry to the mmap_xa   in a given range.     @ucontext: associated user context.   @entry: the entry to insert into the mmap_xa   @length: length of the address that will be mmapped   @min_pgoff: minimum pgoff to be returned   @max_pgoff: maximum pgoff to be returned     This function should be called by drivers that use the rdma_user_mmap   interface for implementing their mmap syscall A database of mmap offsets is   handled in the core and helper functions are provided to insert entries   into the database and extract entries when the user calls mmap with the   given offset. The function allocates a unique page offset in a given range   that should be provided to user, the user will use the offset to retrieve   information such as address to be mapped and how.     Return: 0 on success and -ENOMEM on failure ", "start = ALIGN_DOWN(umem_dmabuf->umem.address, PAGE_SIZE);end = ALIGN(umem_dmabuf->umem.address + umem_dmabuf->umem.length,    PAGE_SIZE);for_each_sgtable_dma_sg(sgt, sg, i) ": "ib_umem_dmabuf_map_pages(struct ib_umem_dmabuf  umem_dmabuf){struct sg_table  sgt;struct scatterlist  sg;unsigned long start, end, cur = 0;unsigned int nmap = 0;long ret;int i;dma_resv_assert_held(umem_dmabuf->attach->dmabuf->resv);if (umem_dmabuf->sgt)goto wait_fence;sgt = dma_buf_map_attachment(umem_dmabuf->attach,     DMA_BIDIRECTIONAL);if (IS_ERR(sgt))return PTR_ERR(sgt);  modify the sg list in-place to match umem address and length ", "if (umem_dmabuf->first_sg) ": "ib_umem_dmabuf_unmap_pages(struct ib_umem_dmabuf  umem_dmabuf){dma_resv_assert_held(umem_dmabuf->attach->dmabuf->resv);if (!umem_dmabuf->sgt)return;  retore the original sg list ", "void rdma_copy_src_l2_addr(struct rdma_dev_addr *dev_addr,   const struct net_device *dev)": "rdma_copy_src_l2_addr - Copy netdevice source addresses   @dev_addr:Destination address pointer where to copy the addresses   @dev:Netdevice whose source addresses to copy     rdma_copy_src_l2_addr() copies source addresses from the specified netdevice.   This includes unicast address, broadcast address, device type and   interface index. ", "void rdma_addr_cancel(struct rdma_dev_addr *addr)": "rdma_addr_cancel - Cancel resolve ip request   @addr:Pointer to address structure given previously  during rdma_resolve_ip().   rdma_addr_cancel() is synchronous function which cancels any pending   request if there is any. ", "struct ib_umem_odp *ib_umem_odp_alloc_implicit(struct ib_device *device,       int access)": "ib_umem_odp_alloc_implicit - Allocate a parent implicit ODP umem     Implicit ODP umems do not have a VA range and do not have any page lists.   They exist only to hold the per_mm reference to help the driver create   children umems.     @device: IB device to create UMEM   @access: ib_reg_mr access flags ", "struct ib_umem_odp *ib_umem_odp_alloc_child(struct ib_umem_odp *root, unsigned long addr,size_t size,const struct mmu_interval_notifier_ops *ops)": "ib_umem_odp_alloc_child - Allocate a child ODP umem under an implicit                             parent ODP umem     @root: The parent umem enclosing the child. This must be allocated using          ib_alloc_implicit_odp_umem()   @addr: The starting userspace VA   @size: The length of the userspace VA   @ops: MMU interval ops, currently only @invalidate ", "struct ib_umem_odp *ib_umem_odp_get(struct ib_device *device,    unsigned long addr, size_t size, int access,    const struct mmu_interval_notifier_ops *ops)": "ib_umem_odp_get - Create a umem_odp for a userspace va     @device: IB device struct to get UMEM   @addr: userspace virtual address to start at   @size: length of region to pin   @access: IB_ACCESS_xxx flags for memory being pinned   @ops: MMU interval ops, currently only @invalidate     The driver should use when the access flags indicate ODP memory. It avoids   pinning, instead, stores the mm for future page fault handling in   conjunction with MMU notifiers. ", "if (!umem_odp->is_implicit_odp) ": "ib_umem_odp_release(struct ib_umem_odp  umem_odp){    Ensure that no more pages are mapped in the umem.     It is the driver's responsibility to ensure, before calling us,   that the hardware will not attempt to access the MR any more. ", "int ib_umem_odp_map_dma_and_lock(struct ib_umem_odp *umem_odp, u64 user_virt, u64 bcnt, u64 access_mask, bool fault)__acquires(&umem_odp->umem_mutex)": "ib_umem_odp_map_dma_and_lock - DMA map userspace memory in an ODP MR and lock it.     Maps the range passed in the argument to DMA addresses.   The DMA addresses of the mapped pages is updated in umem_odp->dma_list.   Upon success the ODP MR will be locked to let caller complete its device   page table update.     Returns the number of pages mapped in success, negative error code   for failure.   @umem_odp: the umem to map and pin   @user_virt: the address from which we need to map.   @bcnt: the minimal number of bytes to pin and map. The mapping might be          bigger due to alignment, and may also be smaller in case of an error          pinning or mapping a page. The actual pages mapped is returned in          the return value.   @access_mask: bit mask of the requested access permissions for the given                 range.   @fault: is faulting required for the given range ", "static int ib_umem_odp_map_dma_single_page(struct ib_umem_odp *umem_odp,unsigned int dma_index,struct page *page,u64 access_mask)": "ib_umem_odp_unmap_dma_pages(umem_odp, ib_umem_start(umem_odp),    ib_umem_end(umem_odp));mutex_unlock(&umem_odp->umem_mutex);mmu_interval_notifier_remove(&umem_odp->notifier);kvfree(umem_odp->dma_list);kvfree(umem_odp->pfn_list);}put_pid(umem_odp->tgid);kfree(umem_odp);}EXPORT_SYMBOL(ib_umem_odp_release);    Map for DMA and insert a single page into the on-demand paging page tables.     @umem: the umem to insert the page to.   @dma_index: index in the umem to add the dma to.   @page: the page struct to map and add.   @access_mask: access permissions needed for this page.     The function returns -EFAULT if the DMA mapping operation fails.   ", "void ib_sa_cancel_query(int id, struct ib_sa_query *query)": "ib_sa_cancel_query - try to cancel an SA query   @id:ID of query to cancel   @query:query pointer to cancel     Try to cancel an SA query.  If the id and query don't match up or   the query has already completed, nothing is done.  Otherwise the   query is canceled and will complete with a status of -EINTR. ", "int ib_init_ah_attr_from_path(struct ib_device *device, u32 port_num,      struct sa_path_rec *rec,      struct rdma_ah_attr *ah_attr,      const struct ib_gid_attr *gid_attr)": "ib_init_ah_attr_from_path - Initialize address handle attributes based on     an SA path record.   @device: Device associated ah attributes initialization.   @port_num: Port on the specified device.   @rec: path record entry to use for ah attributes initialization.   @ah_attr: address handle attributes to initialization from path record.   @gid_attr: SGID attribute to consider during initialization.     When ib_init_ah_attr_from_path() returns success,   (a) for IB link layer it optionally contains a reference to SGID attribute   when GRH is present for IB link layer.   (b) for RoCE link layer it contains a reference to SGID attribute.   User must invoke rdma_destroy_ah_attr() to release reference to SGID   attributes which are initialized using ib_init_ah_attr_from_path(). ", "int ib_sa_path_rec_get(struct ib_sa_client *client,       struct ib_device *device, u32 port_num,       struct sa_path_rec *rec,       ib_sa_comp_mask comp_mask,       unsigned long timeout_ms, gfp_t gfp_mask,       void (*callback)(int status,struct sa_path_rec *resp,unsigned int num_paths, void *context),       void *context,       struct ib_sa_query **sa_query)": "ib_sa_path_rec_get - Start a Path get query   @client:SA client   @device:device to send query on   @port_num: port number to send query on   @rec:Path Record to send in query   @comp_mask:component mask to send in query   @timeout_ms:time to wait for response   @gfp_mask:GFP mask to use for internal allocations   @callback:function called when query completes, times out or is   canceled   @context:opaque user context passed to callback   @sa_query:query context, used to cancel query     Send a Path Record Get query to the SA to look up a path.  The   callback function will be called when the query completes (or   fails); status is 0 for a successful response, -EINTR if the query   is canceled, -ETIMEDOUT is the query timed out, or -EIO if an error   occurred sending the query.  The resp parameter of the callback is   only valid if status is 0.     If the return value of ib_sa_path_rec_get() is negative, it is an   error code.  Otherwise it is a query ID that can be used to cancel   the query. ", "struct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,   u32 port_num,   enum ib_qp_type qp_type,   struct ib_mad_reg_req *mad_reg_req,   u8 rmpp_version,   ib_mad_send_handler send_handler,   ib_mad_recv_handler recv_handler,   void *context,   u32 registration_flags)": "ib_register_mad_agent - Register to sendreceive MADs     Context: Process context. ", "void ib_unregister_mad_agent(struct ib_mad_agent *mad_agent)": "ib_unregister_mad_agent - Unregisters a client from using MAD services     Context: Process context. ", "if (opa && base_version == OPA_MGMT_BASE_VERSION &&    data_len < mad_size - hdr_len)mad_send_wr->sg_list[1].length = data_len;elsemad_send_wr->sg_list[1].length = mad_size - hdr_len;mad_send_wr->sg_list[1].lkey = mad_agent->qp->pd->local_dma_lkey;mad_send_wr->mad_list.cqe.done = ib_mad_send_done;mad_send_wr->send_wr.wr.wr_cqe = &mad_send_wr->mad_list.cqe;mad_send_wr->send_wr.wr.sg_list = mad_send_wr->sg_list;mad_send_wr->send_wr.wr.num_sge = 2;mad_send_wr->send_wr.wr.opcode = IB_WR_SEND;mad_send_wr->send_wr.wr.send_flags = IB_SEND_SIGNALED;mad_send_wr->send_wr.remote_qpn = remote_qpn;mad_send_wr->send_wr.remote_qkey = IB_QP_SET_QKEY;mad_send_wr->send_wr.pkey_index = pkey_index;if (rmpp_active) ": "ib_create_send_mad(struct ib_mad_agent  mad_agent,   u32 remote_qpn, u16 pkey_index,   int rmpp_active, int hdr_len,   int data_len, gfp_t gfp_mask,   u8 base_version){struct ib_mad_agent_private  mad_agent_priv;struct ib_mad_send_wr_private  mad_send_wr;int pad, message_size, ret, size;void  buf;size_t mad_size;bool opa;mad_agent_priv = container_of(mad_agent, struct ib_mad_agent_private,      agent);opa = rdma_cap_opa_mad(mad_agent->device, mad_agent->port_num);if (opa && base_version == OPA_MGMT_BASE_VERSION)mad_size = sizeof(struct opa_mad);elsemad_size = sizeof(struct ib_mad);pad = get_pad_size(hdr_len, data_len, mad_size);message_size = hdr_len + data_len + pad;if (ib_mad_kernel_rmpp_agent(mad_agent)) {if (!rmpp_active && message_size > mad_size)return ERR_PTR(-EINVAL);} elseif (rmpp_active || message_size > mad_size)return ERR_PTR(-EINVAL);size = rmpp_active ? hdr_len : mad_size;buf = kzalloc(sizeof  mad_send_wr + size, gfp_mask);if (!buf)return ERR_PTR(-ENOMEM);mad_send_wr = buf + size;INIT_LIST_HEAD(&mad_send_wr->rmpp_list);mad_send_wr->send_buf.mad = buf;mad_send_wr->send_buf.hdr_len = hdr_len;mad_send_wr->send_buf.data_len = data_len;mad_send_wr->pad = pad;mad_send_wr->mad_agent_priv = mad_agent_priv;mad_send_wr->sg_list[0].length = hdr_len;mad_send_wr->sg_list[0].lkey = mad_agent->qp->pd->local_dma_lkey;  OPA MADs don't have to be the full 2048 bytes ", "if (qp_type == IB_QPT_SMI) ": "ib_is_mad_class_rmpp(mad_reg_req->mgmt_class)) {if (rmpp_version) {dev_dbg_ratelimited(&device->dev,\"%s: RMPP version for non-RMPP class 0x%x\\n\",__func__, mad_reg_req->mgmt_class);goto error1;}}  Make sure class supplied is consistent with QP type ", "int ib_post_send_mad(struct ib_mad_send_buf *send_buf,     struct ib_mad_send_buf **bad_send_buf)": "ib_post_send_mad - Posts MAD(s) to the send queue of the QP associated    with the registered client ", "void ib_free_recv_mad(struct ib_mad_recv_wc *mad_recv_wc)": "ib_free_recv_mad - Returns data buffers used to receive    a MAD to the access layer ", "__malloc void *_uverbs_alloc(struct uverbs_attr_bundle *bundle, size_t size,     gfp_t flags)": "_uverbs_alloc() - Quickly allocate memory for use with a bundle   @bundle: The bundle   @size: Number of bytes to allocate   @flags: Allocator flags     The bundle allocator is intended for allocations that are connected with   processing the system call related to the bundle. The allocated memory is   always freed once the system call completes, and cannot be freed any other   way.     This tries to use a small pool of pre-allocated memory for performance. ", "if (IS_ERR(attr)) ": "uverbs_get_flags64(u64  to, const struct uverbs_attr_bundle  attrs_bundle,       size_t idx, u64 allowed_bits){const struct uverbs_attr  attr;u64 flags;attr = uverbs_attr_get(attrs_bundle, idx);  Missing attribute means 0 flags ", "e->ptr_attr.data = uattr->data_s64;break;case UVERBS_ATTR_TYPE_IDRS_ARRAY:return uverbs_process_idrs_array(pbundle, attr_uapi, &e->objs_arr_attr, uattr, attr_bkey);default:return -EOPNOTSUPP;}return 0;}/* * We search the radix tree with the method prefix and now we want to fast * search the suffix bits to get a particular attribute pointer. It is not * totally clear to me if this breaks the radix tree encasulation or not, but * it uses the iter data to determine if the method iter points at the same * chunk that will store the attribute, if so it just derefs it directly. By * construction in most kernel configs the method and attrs will all fit in a * single radix chunk, so in most cases this will have no search. Other cases * this falls back to a full search. ": "_uverbs_get_const_signed() is the accessor ", "struct iw_cm_id *rdma_iw_cm_id(struct rdma_cm_id *id)": "rdma_iw_cm_id() - return the iw_cm_id pointer for this cm_id.   @id: Communication Identifier ", "struct rdma_cm_id *rdma_res_to_id(struct rdma_restrack_entry *res)": "rdma_res_to_id() - return the rdma_cm_id pointer for this restrack.   @res: rdma resource tracking entry pointer ", "qp_attr.qp_state = IB_QPS_INIT;ret = rdma_init_qp_attr(&id_priv->id, &qp_attr, &qp_attr_mask);if (ret)goto out;ret = ib_modify_qp(id_priv->id.qp, &qp_attr, qp_attr_mask);if (ret)goto out;qp_attr.qp_state = IB_QPS_RTR;ret = rdma_init_qp_attr(&id_priv->id, &qp_attr, &qp_attr_mask);if (ret)goto out;BUG_ON(id_priv->cma_dev->device != id_priv->id.device);if (conn_param)qp_attr.max_dest_rd_atomic = conn_param->responder_resources;ret = ib_modify_qp(id_priv->id.qp, &qp_attr, qp_attr_mask);out:mutex_unlock(&id_priv->qp_mutex);return ret;}static int cma_modify_qp_rts(struct rdma_id_private *id_priv,     struct rdma_conn_param *conn_param)": "rdma_init_qp_attr(&id_priv->id, &qp_attr, &qp_attr_mask);if (ret)return ret;ret = ib_modify_qp(qp, &qp_attr, qp_attr_mask);if (ret)return ret;qp_attr.qp_state = IB_QPS_RTR;ret = ib_modify_qp(qp, &qp_attr, IB_QP_STATE);if (ret)return ret;qp_attr.qp_state = IB_QPS_RTS;qp_attr.sq_psn = 0;ret = ib_modify_qp(qp, &qp_attr, IB_QP_STATE | IB_QP_SQ_PSN);return ret;}static int cma_init_conn_qp(struct rdma_id_private  id_priv, struct ib_qp  qp){struct ib_qp_attr qp_attr;int qp_attr_mask, ret;qp_attr.qp_state = IB_QPS_INIT;ret = rdma_init_qp_attr(&id_priv->id, &qp_attr, &qp_attr_mask);if (ret)return ret;return ib_modify_qp(qp, &qp_attr, qp_attr_mask);}int rdma_create_qp(struct rdma_cm_id  id, struct ib_pd  pd,   struct ib_qp_init_attr  qp_init_attr){struct rdma_id_private  id_priv;struct ib_qp  qp;int ret;id_priv = container_of(id, struct rdma_id_private, id);if (id->device != pd->device) {ret = -EINVAL;goto out_err;}qp_init_attr->port_num = id->port_num;qp = ib_create_qp(pd, qp_init_attr);if (IS_ERR(qp)) {ret = PTR_ERR(qp);goto out_err;}if (id->qp_type == IB_QPT_UD)ret = cma_init_ud_qp(id_priv, qp);elseret = cma_init_conn_qp(id_priv, qp);if (ret)goto out_destroy;id->qp = qp;id_priv->qp_num = qp->qp_num;id_priv->srq = (qp->srq != NULL);trace_cm_qp_create(id_priv, pd, qp_init_attr, 0);return 0;out_destroy:ib_destroy_qp(qp);out_err:trace_cm_qp_create(id_priv, pd, qp_init_attr, ret);return ret;}EXPORT_SYMBOL(rdma_create_qp);void rdma_destroy_qp(struct rdma_cm_id  id){struct rdma_id_private  id_priv;id_priv = container_of(id, struct rdma_id_private, id);trace_cm_qp_destroy(id_priv);mutex_lock(&id_priv->qp_mutex);ib_destroy_qp(id_priv->id.qp);id_priv->id.qp = NULL;mutex_unlock(&id_priv->qp_mutex);}EXPORT_SYMBOL(rdma_destroy_qp);static int cma_modify_qp_rtr(struct rdma_id_private  id_priv,     struct rdma_conn_param  conn_param){struct ib_qp_attr qp_attr;int qp_attr_mask, ret;mutex_lock(&id_priv->qp_mutex);if (!id_priv->id.qp) {ret = 0;goto out;}  Need to update QP attributes from default values. ", "rdma_addr_cancel(&id_priv->id.route.addr.dev_addr);break;case RDMA_CM_ROUTE_QUERY:cma_cancel_route(id_priv);break;case RDMA_CM_LISTEN:if (cma_any_addr(cma_src_addr(id_priv)) && !id_priv->cma_dev)cma_cancel_listens(id_priv);break;default:break;}}static void cma_release_port(struct rdma_id_private *id_priv)": "rdma_destroy_id(&dev_id_priv->id);mutex_lock(&lock);}}static void cma_cancel_listens(struct rdma_id_private  id_priv){mutex_lock(&lock);_cma_cancel_listens(id_priv);mutex_unlock(&lock);}static void cma_cancel_operation(struct rdma_id_private  id_priv, enum rdma_cm_state state){switch (state) {case RDMA_CM_ADDR_QUERY:    We can avoid doing the rdma_addr_cancel() based on state,   only RDMA_CM_ADDR_QUERY has a work that could still execute.   Notice that the addr_handler work could still be exiting   outside this state, however due to the interaction with the   handler_mutex the work is guaranteed not to touch id_priv   during exit. ", "int rdma_set_ack_timeout(struct rdma_cm_id *id, u8 timeout)": "rdma_accept(). It is applicable to primary   path only. The timeout will affect the local side of the QP, it is not   negotiated with remote side and zero disables the timer. In case it is   set before rdma_resolve_route, the value will also be used to determine   PacketLifeTime for RoCE.     Return: 0 for success ", "int rdma_set_min_rnr_timer(struct rdma_cm_id *id, u8 min_rnr_timer)": "rdma_set_min_rnr_timer() - Set the minimum RNR Retry timer of the        QP associated with a connection identifier.   @id: Communication identifier to associated with service type.   @min_rnr_timer: 5-bit value encoded as Table 45: \"Encoding for RNR NAK     Timer Field\" in the IBTA specification.     This function should be called before rdma_connect() on active   side, and on passive side before rdma_accept(). The timer value   will be associated with the local QP. When it receives a send it is   not read to handle, typically if the receive queue is empty, an RNR   Retry NAK is returned to the requester with the min_rnr_timer   encoded. The requester will then wait at least the time specified   in the NAK before retrying. The default is zero, which translates   to a minimum RNR Timer value of 655 ms.     Return: 0 for success ", "*to_destroy = dev_id_priv;dev_warn(&cma_dev->device->dev, \"RDMA CMA: %s, error %d\\n\", __func__, ret);return ret;}static int cma_listen_on_all(struct rdma_id_private *id_priv)": "rdma_listen(&dev_id_priv->id, id_priv->backlog);if (ret)goto err_listen;list_add_tail(&dev_id_priv->listen_item, &id_priv->listen_list);return 0;err_listen:  Caller must destroy this after releasing lock ", "if (id_priv->used_resolve_ip)rdma_addr_cancel(&id->route.addr.dev_addr);elseid_priv->used_resolve_ip = 1;ret = rdma_resolve_ip(cma_src_addr(id_priv), dst_addr,      &id->route.addr.dev_addr,      timeout_ms, addr_handler,      false, id_priv);}}if (ret)goto err;return 0;err:cma_comp_exch(id_priv, RDMA_CM_ADDR_QUERY, RDMA_CM_ADDR_BOUND);return ret;}EXPORT_SYMBOL(rdma_resolve_addr": "rdma_resolve_addr(struct rdma_cm_id  id, struct sockaddr  src_addr,      const struct sockaddr  dst_addr, unsigned long timeout_ms){struct rdma_id_private  id_priv =container_of(id, struct rdma_id_private, id);int ret;ret = resolve_prepare_src(id_priv, src_addr, dst_addr);if (ret)return ret;if (cma_any_addr(dst_addr)) {ret = cma_resolve_loopback(id_priv);} else {if (dst_addr->sa_family == AF_IB) {ret = cma_resolve_ib_addr(id_priv);} else {    The FSM can return back to RDMA_CM_ADDR_BOUND after   rdma_resolve_ip() is called, eg through the error   path in addr_handler(). If this happens the existing   request must be canceled before issuing a new one.   Since canceling a request is a bit slow and this   oddball path is rare, keep track once a request has   been issued. The track turns out to be a permanent   state since this is the only cancel as it is   immediately before rdma_resolve_ip(). ", "if (id_priv->reuseaddr) ": "rdma_bind_addr(id, (struct sockaddr  )&any_in);if (ret)return ret;if (WARN_ON(!cma_comp_exch(id_priv, RDMA_CM_ADDR_BOUND,   RDMA_CM_LISTEN)))return -EINVAL;}    Once the ID reaches RDMA_CM_LISTEN it is not allowed to be reusable   any more, and has to be unique in the bind list. ", "int rdma_connect_locked(struct rdma_cm_id *id,struct rdma_conn_param *conn_param)": "rdma_connect_locked - Initiate an active connection request.   @id: Connection identifier to connect.   @conn_param: Connection information used for connected QPs.     Same as rdma_connect() but can only be called from the   RDMA_CM_EVENT_ROUTE_RESOLVED handler callback. ", "int rdma_connect_ece(struct rdma_cm_id *id, struct rdma_conn_param *conn_param,     struct rdma_ucm_ece *ece)": "rdma_connect_ece - Initiate an active connection request with ECE data.   @id: Connection identifier to connect.   @conn_param: Connection information used for connected QPs.   @ece: ECE parameters     See rdma_connect() explanation. ", "trace_cm_disconnect(id_priv);if (ib_send_cm_dreq(id_priv->cm_id.ib, NULL, 0)) ": "rdma_disconnect(struct rdma_cm_id  id){struct rdma_id_private  id_priv;int ret;id_priv = container_of(id, struct rdma_id_private, id);if (!id_priv->cm_id.ib)return -EINVAL;if (rdma_cap_ib_cm(id->device, id->port_num)) {ret = cma_modify_qp_err(id_priv);if (ret)goto out;  Initiate or respond to a disconnect. ", "if (WARN_ON(id->qp))return -EINVAL;/* ULP is calling this wrong. ": "rdma_join_multicast(struct rdma_cm_id  id, struct sockaddr  addr,u8 join_state, void  context){struct rdma_id_private  id_priv =container_of(id, struct rdma_id_private, id);struct cma_multicast  mc;int ret;  Not supported for kernel QPs ", "struct rtrs_permit *rtrs_clt_get_permit(struct rtrs_clt_sess *clt,  enum rtrs_clt_con_type con_type,  enum wait_type can_wait)": "rtrs_clt_get_permit() - allocates permit for future RDMA operation   @clt:Current session   @con_type:Type of connection to use with the permit   @can_wait:Wait type     Description:      Allocates permit for the following RDMA operation.  Permit is used      to preallocate all resources and to propagate memory pressure      up earlier.     Context:      Can sleep if @wait == RTRS_PERMIT_WAIT ", "void rtrs_clt_put_permit(struct rtrs_clt_sess *clt, struct rtrs_permit *permit)": "rtrs_clt_put_permit() - puts allocated permit   @clt:Current session   @permit:Permit to be freed     Context:      Does not matter ", "mutex_lock(&clt->paths_ev_mutex);up = ++clt->paths_up;/* * Here it is safe to access paths num directly since up counter * is greater than MAX_PATHS_NUM only while rtrs_clt_open() is * in progress, thus paths removals are impossible. ": "rtrs_clt_open(), then each was disconnected   and the first one connected again.  That's why this nasty   game with counter value. ", "total_con = con_num + nr_poll_queues + 1;clt_path->s.con = kcalloc(total_con, sizeof(*clt_path->s.con),  GFP_KERNEL);if (!clt_path->s.con)goto err_free_path;clt_path->s.con_num = total_con;clt_path->s.irq_con_num = con_num + 1;clt_path->stats = kzalloc(sizeof(*clt_path->stats), GFP_KERNEL);if (!clt_path->stats)goto err_free_con;mutex_init(&clt_path->init_mutex);uuid_gen(&clt_path->s.uuid);memcpy(&clt_path->s.dst_addr, path->dst,       rdma_addr_size((struct sockaddr *)path->dst));/* * rdma_resolve_addr() passes src_addr to cma_bind_addr, which * checks the sa_family to be non-zero. If user passed src_addr=NULL * the sess->src_addr will contain only zeros, which is then fine. ": "rtrs_clt_close_work(struct work_struct  work);static void rtrs_clt_err_recovery_work(struct work_struct  work){struct rtrs_clt_path  clt_path;struct rtrs_clt_sess  clt;int delay_ms;clt_path = container_of(work, struct rtrs_clt_path, err_recovery_work);clt = clt_path->clt;delay_ms = clt->reconnect_delay_sec   1000;rtrs_clt_stop_and_destroy_conns(clt_path);queue_delayed_work(rtrs_wq, &clt_path->reconnect_dwork,   msecs_to_jiffies(delay_ms +    get_random_u32_below(RTRS_RECONNECT_SEED)));}static struct rtrs_clt_path  alloc_path(struct rtrs_clt_sess  clt,const struct rtrs_addr  path,size_t con_num, u32 nr_poll_queues){struct rtrs_clt_path  clt_path;int err = -ENOMEM;int cpu;size_t total_con;clt_path = kzalloc(sizeof( clt_path), GFP_KERNEL);if (!clt_path)goto err;    irqmode and poll   +1: Extra connection for user messages ", "int rtrs_clt_request(int dir, struct rtrs_clt_req_ops *ops,     struct rtrs_clt_sess *clt, struct rtrs_permit *permit,     const struct kvec *vec, size_t nr, size_t data_len,     struct scatterlist *sg, unsigned int sg_cnt)": "rtrs_clt_request() - Request data transfer tofrom server via RDMA.     @dir:READWRITE   @ops:callback function to be called as confirmation, and the pointer.   @clt:Session   @permit:Preallocated permit   @vec:Message that is sent to server together with the request.  Sum of len of all @vec elements limited to <= IO_MSG_SIZE.  Since the msg is copied internally it can be allocated on stack.   @nr:Number of elements in @vec.   @data_len:length of data sent tofrom server   @sg:Pages to be sentreceived tofrom server.   @sg_cnt:Number of elements in the @sg     Return:   0:Success   <0:Error     On dir=READ rtrs client will request a data transfer from Server to client.   The data that the server will respond with will be stored in @sg when   the user receives an %RTRS_CLT_RDMA_EV_RDMA_REQUEST_WRITE_COMPL event.   On dir=WRITE rtrs client will rdma write data in sg to server side. ", "int cnt = -1;struct rtrs_con *con;struct rtrs_clt_path *clt_path;struct path_it it;rcu_read_lock();for (path_it_init(&it, clt);     (clt_path = it.next_path(&it)) && it.i < it.clt->paths_num; it.i++) ": "rtrs_clt_rdma_cq_direct(struct rtrs_clt_sess  clt, unsigned int index){  If no path, return -1 for block layer not to try again ", "int rtrs_clt_query(struct rtrs_clt_sess *clt, struct rtrs_attrs *attr)": "rtrs_clt_query() - queries RTRS session attributes  @clt: session pointer  @attr: query results for session attributes.   Returns:      0 on success      -ECOMMno connection to the server ", "bool rtrs_srv_resp_rdma(struct rtrs_srv_op *id, int status)": "rtrs_srv_resp_rdma() - Finish an RDMA request     @id:Internal RTRS operation identifier   @status:Response Code sent to the other side for this operation.  0 = success, <=0 error   Context: any     Finish a RDMA operation. A message is sent to the client and the   corresponding memory areas will be released. ", "void rtrs_srv_set_sess_priv(struct rtrs_srv_sess *srv, void *priv)": "rtrs_srv_set_sess_priv() - Set private pointer in rtrs_srv.   @srv:Session pointer   @priv:The private pointer that is associated with the session. ", "int rtrs_srv_get_path_name(struct rtrs_srv_sess *srv, char *pathname,   size_t len)": "rtrs_srv_get_path_name() - Get rtrs_srv peer hostname.   @srv:Session   @pathname:Pathname buffer   @len:Length of sessname buffer ", "int rtrs_srv_get_queue_depth(struct rtrs_srv_sess *srv)": "rtrs_srv_get_queue_depth() - Get rtrs_srv qdepth.   @srv:Session ", "struct rtrs_srv_ctx *rtrs_srv_open(struct rtrs_srv_ops *ops, u16 port)": "rtrs_srv_open() - open RTRS server context   @ops:callback functions   @port:               port to listen on     Creates server context with specified callbacks.     Return a valid pointer on success otherwise PTR_ERR. ", "percpu_ref_kill(&srv_path->ids_inflight_ref);/* Wait for all completion ": "rtrs_srv_close_work(struct work_struct  work){struct rtrs_srv_path  srv_path;struct rtrs_srv_con  con;int i;srv_path = container_of(work, typeof( srv_path), close_work);rtrs_srv_destroy_path_files(srv_path);rtrs_srv_stop_hb(srv_path);for (i = 0; i < srv_path->s.con_num; i++) {if (!srv_path->s.con[i])continue;con = to_srv_con(srv_path->s.con[i]);rdma_disconnect(con->c.cm_id);ib_drain_qp(con->c.qp);}    Degrade ref count to the usual model with a single shared   atomic_t counter ", "int sockaddr_to_str(const struct sockaddr *addr, char *buf, size_t len)": "sockaddr_to_str() - convert sockaddr to a string.   @addr:the sockadddr structure to be converted.   @buf:string containing socket addr.   @len:string length.     The return value is the number of characters written into buf not   including the trailing '\\0'. If len is == 0 the function returns 0.. ", "int rtrs_addr_to_str(const struct rtrs_addr *addr, char *buf, size_t len)": "rtrs_addr_to_str() - convert rtrs_addr to a string \"src@dst\"   @addr:the rtrs_addr structure to be converted   @buf:string containing source and destination addr of a path  separated by '@' I.e. \"ip:1.1.1.1@ip:1.1.1.2\"  \"ip:1.1.1.1@ip:1.1.1.2\".   @len:string length     The return value is the number of characters written into buf not   including the trailing '\\0'. ", "int rtrs_addr_to_sockaddr(const char *str, size_t len, u16 port,  struct rtrs_addr *addr)": "rtrs_addr_to_sockaddr() - convert path string \"src,dst\" or \"src@dst\"   to sockaddreses   @str:string containing source and destination addr of a path  separated by ',' or '@' I.e. \"ip:1.1.1.1,ip:1.1.1.2\" or  \"ip:1.1.1.1@ip:1.1.1.2\". If str contains only one address it's  considered to be destination.   @len:string length   @port:Destination port number.   @addr:will be set to the sourcedestination address or to NULL  if str doesn't contain any source address.     Returns zero if conversion successful. Non-zero otherwise. ", "skb = __hci_cmd_sync(hdev, 0xfc2e, 0, NULL, HCI_INIT_TIMEOUT);if (IS_ERR(skb)) ": "btbcm_patchram(struct hci_dev  hdev, const struct firmware  fw){const struct hci_command_hdr  cmd;const u8  fw_ptr;size_t fw_size;struct sk_buff  skb;u16 opcode;int err = 0;  Start Download ", "void *vme_alloc_consistent(struct vme_resource *resource, size_t size,dma_addr_t *dma)": "vme_alloc_consistent - Allocate contiguous memory.   @resource: Pointer to VME resource.   @size: Size of allocation required.   @dma: Pointer to variable to store physical address of allocation.     Allocate a contiguous block of memory for use by the driver. This is used to   create the buffers for the slave windows.     Return: Virtual address of allocation on success, NULL on failure. ", "void vme_free_consistent(struct vme_resource *resource, size_t size,void *vaddr, dma_addr_t dma)": "vme_free_consistent - Free previously allocated memory.   @resource: Pointer to VME resource.   @size: Size of allocation to free.   @vaddr: Virtual address of allocation.   @dma: Physical address of allocation.     Free previously allocated block of contiguous memory. ", "size_t vme_get_size(struct vme_resource *resource)": "vme_master_get or vme_slave_get   depending on the type of window resource handed to it.     Return: Size of the window on success, zero on failure. ", "break;case VME_CRCSR:if (vme_base + size > VME_CRCSR_MAX)retval = -EFAULT;break;case VME_USER1:case VME_USER2:case VME_USER3:case VME_USER4:/* User Defined ": "vme_check_window(u32 aspace, unsigned long long vme_base,     unsigned long long size){int retval = 0;if (vme_base + size < size)return -EINVAL;switch (aspace) {case VME_A16:if (vme_base + size > VME_A16_MAX)retval = -EFAULT;break;case VME_A24:if (vme_base + size > VME_A24_MAX)retval = -EFAULT;break;case VME_A32:if (vme_base + size > VME_A32_MAX)retval = -EFAULT;break;case VME_A64:  The VME_A64_MAX limit is actually U64_MAX + 1 ", "struct vme_resource *vme_slave_request(struct vme_dev *vdev, u32 address,u32 cycle)": "vme_slave_request - Request a VME slave window resource.   @vdev: Pointer to VME device struct vme_dev assigned to driver instance.   @address: Required VME address space.   @cycle: Required VME data transfer cycle type.     Request use of a VME window resource capable of being set for the requested   address space and data transfer cycle.     Return: Pointer to VME resource on success, NULL on failure. ", "int vme_slave_set(struct vme_resource *resource, int enabled,unsigned long long vme_base, unsigned long long size,dma_addr_t buf_base, u32 aspace, u32 cycle)": "vme_slave_set - Set VME slave window configuration.   @resource: Pointer to VME slave resource.   @enabled: State to which the window should be configured.   @vme_base: Base address for the window.   @size: Size of the VME window.   @buf_base: Based address of buffer used to provide VME slave window storage.   @aspace: VME address space for the VME window.   @cycle: VME data transfer cycle type for the VME window.     Set configuration for provided VME slave window.     Return: Zero on success, -EINVAL if operation is not supported on this           device, if an invalid resource has been provided or invalid           attributes are provided. Hardware specific errors may also be           returned. ", "void vme_slave_free(struct vme_resource *resource)": "vme_slave_free - Free VME slave window   @resource: Pointer to VME slave resource.     Free the provided slave resource so that it may be reallocated. ", "struct vme_resource *vme_master_request(struct vme_dev *vdev, u32 address,u32 cycle, u32 dwidth)": "vme_master_request - Request a VME master window resource.   @vdev: Pointer to VME device struct vme_dev assigned to driver instance.   @address: Required VME address space.   @cycle: Required VME data transfer cycle type.   @dwidth: Required VME data transfer width.     Request use of a VME window resource capable of being set for the requested   address space, data transfer cycle and width.     Return: Pointer to VME resource on success, NULL on failure. ", "int vme_master_set(struct vme_resource *resource, int enabled,unsigned long long vme_base, unsigned long long size, u32 aspace,u32 cycle, u32 dwidth)": "vme_master_set - Set VME master window configuration.   @resource: Pointer to VME master resource.   @enabled: State to which the window should be configured.   @vme_base: Base address for the window.   @size: Size of the VME window.   @aspace: VME address space for the VME window.   @cycle: VME data transfer cycle type for the VME window.   @dwidth: VME data transfer width for the VME window.     Set configuration for provided VME master window.     Return: Zero on success, -EINVAL if operation is not supported on this           device, if an invalid resource has been provided or invalid           attributes are provided. Hardware specific errors may also be           returned. ", "ssize_t vme_master_read(struct vme_resource *resource, void *buf, size_t count,loff_t offset)": "vme_master_read - Read data from VME space into a buffer.   @resource: Pointer to VME master resource.   @buf: Pointer to buffer where data should be transferred.   @count: Number of bytes to transfer.   @offset: Offset into VME master window at which to start transfer.     Perform read of count bytes of data from location on VME bus which maps into   the VME master window at offset to buf.     Return: Number of bytes read, -EINVAL if resource is not a VME master           resource or read operation is not supported. -EFAULT returned if           invalid offset is provided. Hardware specific errors may also be           returned. ", "ssize_t vme_master_write(struct vme_resource *resource, void *buf,size_t count, loff_t offset)": "vme_master_write - Write data out to VME space from a buffer.   @resource: Pointer to VME master resource.   @buf: Pointer to buffer holding data to transfer.   @count: Number of bytes to transfer.   @offset: Offset into VME master window at which to start transfer.     Perform write of count bytes of data from buf to location on VME bus which   maps into the VME master window at offset.     Return: Number of bytes written, -EINVAL if resource is not a VME master           resource or write operation is not supported. -EFAULT returned if           invalid offset is provided. Hardware specific errors may also be           returned. ", "unsigned int vme_master_rmw(struct vme_resource *resource, unsigned int mask,unsigned int compare, unsigned int swap, loff_t offset)": "vme_master_rmw - Perform read-modify-write cycle.   @resource: Pointer to VME master resource.   @mask: Bits to be compared and swapped in operation.   @compare: Bits to be compared with data read from offset.   @swap: Bits to be swapped in data read from offset.   @offset: Offset into VME master window at which to perform operation.     Perform read-modify-write cycle on provided location:   - Location on VME bus is read.   - Bits selected by mask are compared with compare.   - Where a selected bit matches that in compare and are selected in swap,   the bit is swapped.   - Result written back to location on VME bus.     Return: Bytes written on success, -EINVAL if resource is not a VME master           resource or RMW operation is not supported. Hardware specific           errors may also be returned. ", "int vme_master_mmap(struct vme_resource *resource, struct vm_area_struct *vma)": "vme_master_mmap - Mmap region of VME master window.   @resource: Pointer to VME master resource.   @vma: Pointer to definition of user mapping.     Memory map a region of the VME master window into user space.     Return: Zero on success, -EINVAL if resource is not a VME master           resource or -EFAULT if map exceeds window size. Other generic mmap           errors may also be returned. ", "void vme_master_free(struct vme_resource *resource)": "vme_master_free - Free VME master window   @resource: Pointer to VME master resource.     Free the provided master resource so that it may be reallocated. ", "struct vme_resource *vme_dma_request(struct vme_dev *vdev, u32 route)": "vme_dma_request - Request a DMA controller.   @vdev: Pointer to VME device struct vme_dev assigned to driver instance.   @route: Required srcdestination combination.     Request a VME DMA controller with capability to perform transfers bewteen   requested sourcedestination combination.     Return: Pointer to VME DMA resource on success, NULL on failure. ", "struct vme_dma_list *vme_new_dma_list(struct vme_resource *resource)": "vme_dma_list_free().     Return: Pointer to new VME DMA list, NULL on allocation failure or invalid           VME DMA resource. ", "struct vme_dma_attr *vme_dma_pattern_attribute(u32 pattern, u32 type)": "vme_dma_free_attribute().     Return: Pointer to VME DMA attribute, NULL on failure. ", "struct vme_dma_attr *vme_dma_pci_attribute(dma_addr_t address)": "vme_dma_pci_attribute - Create \"PCI\" type VME DMA list attribute.   @address: PCI base address for DMA transfer.     Create VME DMA list attribute pointing to a location on PCI for DMA   transfers. It is the responsibility of the user to free used attributes   using vme_dma_free_attribute().     Return: Pointer to VME DMA attribute, NULL on failure. ", "struct vme_dma_attr *vme_dma_vme_attribute(unsigned long long address,u32 aspace, u32 cycle, u32 dwidth)": "vme_dma_vme_attribute - Create \"VME\" type VME DMA list attribute.   @address: VME base address for DMA transfer.   @aspace: VME address space to use for DMA transfer.   @cycle: VME bus cycle to use for DMA transfer.   @dwidth: VME data width to use for DMA transfer.     Create VME DMA list attribute pointing to a location on the VME bus for DMA   transfers. It is the responsibility of the user to free used attributes   using vme_dma_free_attribute().     Return: Pointer to VME DMA attribute, NULL on failure. ", "void vme_dma_free_attribute(struct vme_dma_attr *attributes)": "vme_dma_list_add() has returned. ", "int vme_dma_list_exec(struct vme_dma_list *list)": "vme_dma_list_exec - Queue a VME DMA list for execution.   @list: Pointer to VME list.     Queue the provided VME DMA list for execution. The call will return once the   list has been executed.     Return: Zero on success, -EINVAL if operation is not supported on this           device. Hardware specific errors also possible. ", "int vme_irq_request(struct vme_dev *vdev, int level, int statid,void (*callback)(int, int, void *),void *priv_data)": "vme_irq_request - Request a specific VME interrupt.   @vdev: Pointer to VME device struct vme_dev assigned to driver instance.   @level: Interrupt priority being requested.   @statid: Interrupt vector being requested.   @callback: Pointer to callback function called when VME interruptvector              received.   @priv_data: Generic pointer that will be passed to the callback function.     Request callback to be attached as a handler for VME interrupts with provided   level and statid.     Return: Zero on success, -EINVAL on invalid vme device, level or if the           function is not supported, -EBUSY if the levelstatid combination is           already in use. Hardware specific errors also possible. ", "void vme_irq_free(struct vme_dev *vdev, int level, int statid)": "vme_irq_free - Free a VME interrupt.   @vdev: Pointer to VME device struct vme_dev assigned to driver instance.   @level: Interrupt priority of interrupt being freed.   @statid: Interrupt vector of interrupt being freed.     Remove previously attached callback from VME interrupt priorityvector. ", "int vme_irq_generate(struct vme_dev *vdev, int level, int statid)": "vme_irq_generate - Generate VME interrupt.   @vdev: Pointer to VME device struct vme_dev assigned to driver instance.   @level: Interrupt priority at which to assert the interrupt.   @statid: Interrupt vector to associate with the interrupt.     Generate a VME interrupt of the provided level and with the provided   statid.     Return: Zero on success, -EINVAL on invalid vme device, level or if the           function is not supported. Hardware specific errors also possible. ", "struct vme_resource *vme_lm_request(struct vme_dev *vdev)": "vme_lm_request - Request a VME location monitor   @vdev: Pointer to VME device struct vme_dev assigned to driver instance.     Allocate a location monitor resource to the driver. A location monitor   allows the driver to monitor accesses to a contiguous number of   addresses on the VME bus.     Return: Pointer to a VME resource on success or NULL on failure. ", "int vme_lm_count(struct vme_resource *resource)": "vme_lm_count - Determine number of VME Addresses monitored   @resource: Pointer to VME location monitor resource.     The number of contiguous addresses monitored is hardware dependent.   Return the number of contiguous addresses monitored by the   location monitor.     Return: Count of addresses monitored or -EINVAL when provided with an     invalid location monitor resource. ", "int vme_lm_set(struct vme_resource *resource, unsigned long long lm_base,u32 aspace, u32 cycle)": "vme_lm_set - Configure location monitor   @resource: Pointer to VME location monitor resource.   @lm_base: Base address to monitor.   @aspace: VME address space to monitor.   @cycle: VME bus cycle type to monitor.     Set the base address, address space and cycle type of accesses to be   monitored by the location monitor.     Return: Zero on success, -EINVAL when provided with an invalid location     monitor resource or function is not supported. Hardware specific     errors may also be returned. ", "int vme_lm_get(struct vme_resource *resource, unsigned long long *lm_base,u32 *aspace, u32 *cycle)": "vme_lm_get - Retrieve location monitor settings   @resource: Pointer to VME location monitor resource.   @lm_base: Pointer used to output the base address monitored.   @aspace: Pointer used to output the address space monitored.   @cycle: Pointer used to output the VME bus cycle type monitored.     Retrieve the base address, address space and cycle type of accesses to   be monitored by the location monitor.     Return: Zero on success, -EINVAL when provided with an invalid location     monitor resource or function is not supported. Hardware specific     errors may also be returned. ", "int vme_lm_attach(struct vme_resource *resource, int monitor,void (*callback)(void *), void *data)": "vme_lm_attach - Provide callback for location monitor address   @resource: Pointer to VME location monitor resource.   @monitor: Offset to which callback should be attached.   @callback: Pointer to callback function called when triggered.   @data: Generic pointer that will be passed to the callback function.     Attach a callback to the specificed offset into the location monitors   monitored addresses. A generic pointer is provided to allow data to be   passed to the callback when called.     Return: Zero on success, -EINVAL when provided with an invalid location     monitor resource or function is not supported. Hardware specific     errors may also be returned. ", "int vme_lm_detach(struct vme_resource *resource, int monitor)": "vme_lm_detach - Remove callback for location monitor address   @resource: Pointer to VME location monitor resource.   @monitor: Offset to which callback should be removed.     Remove the callback associated with the specificed offset into the   location monitors monitored addresses.     Return: Zero on success, -EINVAL when provided with an invalid location     monitor resource or function is not supported. Hardware specific     errors may also be returned. ", "void vme_lm_free(struct vme_resource *resource)": "vme_lm_free - Free allocated VME location monitor   @resource: Pointer to VME location monitor resource.     Free allocation of a VME location monitor.     WARNING: This function currently expects that any callbacks that have            been attached to the location monitor have been removed.     Return: Zero on success, -EINVAL when provided with an invalid location     monitor resource. ", "int vme_slot_num(struct vme_dev *vdev)": "vme_slot_num - Retrieve slot ID   @vdev: Pointer to VME device struct vme_dev assigned to driver instance.     Retrieve the slot ID associated with the provided VME device.     Return: The slot ID on success, -EINVAL if VME bridge cannot be determined           or the function is not supported. Hardware specific errors may also           be returned. ", "static struct vme_bridge *find_bridge(struct vme_resource *resource)": "vme_bus_numbers;static LIST_HEAD(vme_bus_list);static DEFINE_MUTEX(vme_buses_lock);static int __init vme_init(void);static struct vme_dev  dev_to_vme_dev(struct device  dev){return container_of(dev, struct vme_dev, dev);}    Find the bridge that the resource is associated with. ", "err = __vme_register_driver_bus(drv, bridge, ndevs);if (err)break;}mutex_unlock(&vme_buses_lock);return err;}/** * vme_register_driver - Register a VME driver * @drv: Pointer to VME driver structure to register. * @ndevs: Maximum number of devices to allow to be enumerated. * * Register a VME device driver with the VME subsystem. * * Return: Zero on success, error value on registration failure. ": "vme_bus_type;dev_set_name(&vdev->dev, \"%s.%u-%u\", drv->name, bridge->num,vdev->num);err = device_register(&vdev->dev);if (err)goto err_reg;if (vdev->dev.platform_data) {list_add_tail(&vdev->drv_list, &drv->devices);list_add_tail(&vdev->bridge_list, &bridge->devices);} elsedevice_unregister(&vdev->dev);}return 0;err_reg:put_device(&vdev->dev);err_devalloc:list_for_each_entry_safe(vdev, tmp, &drv->devices, drv_list) {list_del(&vdev->drv_list);list_del(&vdev->bridge_list);device_unregister(&vdev->dev);}return err;}static int __vme_register_driver(struct vme_driver  drv, unsigned int ndevs){struct vme_bridge  bridge;int err = 0;mutex_lock(&vme_buses_lock);list_for_each_entry(bridge, &vme_bus_list, bus_list) {    This cannot cause trouble as we already have vme_buses_lock   and if the bridge is removed, it will have to go through   vme_unregister_bridge() to do it (which calls remove() on   the bridge which in turn tries to acquire vme_buses_lock and   will have to wait). ", "void vme_unregister_driver(struct vme_driver *drv)": "vme_unregister_driver - Unregister a VME driver   @drv: Pointer to VME driver structure to unregister.     Unregister a VME device driver from the VME subsystem. ", "ret = sscanf(w->sname, \"%s %d %s\", intf_name, &dai_id, dir);if (ret < 3) ": "gbaudio_module_update(struct gbaudio_codec_info  codec,  struct snd_soc_dapm_widget  w,  struct gbaudio_module_info  module, int enable){int dai_id, ret;char intf_name[NAME_SIZE], dir[NAME_SIZE];dev_dbg(module->dev, \"%s:Module update %s sequence\\n\", w->name,enable ? \"Enable\" : \"Disable\");if ((w->id != snd_soc_dapm_aif_in) && (w->id != snd_soc_dapm_aif_out)) {dev_dbg(codec->dev, \"No action required for %s\\n\", w->name);return 0;}  parse dai_id from AIF widget's stream_name ", "if (comp->card->instantiated) ": "gbaudio_register_module(struct gbaudio_module_info  module){int ret;struct snd_soc_component  comp;struct snd_card  card;struct gbaudio_jack  jack = NULL;if (!gbcodec) {dev_err(module->dev, \"GB Codec not yet probed\\n\");return -EAGAIN;}comp = gbcodec->component;card = comp->card->snd_card;down_write(&card->controls_rwsem);if (module->num_dais) {dev_err(gbcodec->dev,\"%d:DAIs not supported via gbcodec driver\\n\",module->num_dais);up_write(&card->controls_rwsem);return -EINVAL;}ret = gbaudio_init_jack(module, comp->card);if (ret) {up_write(&card->controls_rwsem);return ret;}if (module->dapm_widgets)snd_soc_dapm_new_controls(&comp->dapm, module->dapm_widgets,  module->num_dapm_widgets);if (module->controls)snd_soc_add_component_controls(comp, module->controls,       module->num_controls);if (module->dapm_routes)snd_soc_dapm_add_routes(&comp->dapm, module->dapm_routes,module->num_dapm_routes);  card already instantiated, create widgets here only ", "list_for_each_entry_safe(jack, n, &module->jack_list, list) ": "gbaudio_unregister_module(struct gbaudio_module_info  module){struct snd_soc_component  comp = gbcodec->component;struct snd_card  card = comp->card->snd_card;struct gbaudio_jack  jack,  n;int mask;dev_dbg(comp->dev, \"Unregister %s module\\n\", module->name);down_write(&card->controls_rwsem);mutex_lock(&gbcodec->lock);gbaudio_codec_cleanup(module);list_del(&module->list);dev_dbg(comp->dev, \"Process Unregister %s module\\n\", module->name);mutex_unlock(&gbcodec->lock);#ifdef CONFIG_SND_JACK  free jack devices for this module jack_list ", "state = kzalloc(sizeof(struct sp8870_state), GFP_KERNEL);if (state == NULL) goto error;/* setup the state ": "sp8870_attach(const struct sp8870_config  config,   struct i2c_adapter  i2c){struct sp8870_state  state = NULL;  allocate memory for the internal state ", "dev_set_drvdata(dev, indio_dev);chip->bus = *bus;if (name[4] == '3')chip->id = ID_ADT7316 + (name[6] - '6');else if (name[4] == '5')chip->id = ID_ADT7516 + (name[6] - '6');elsereturn -ENODEV;if (chip->id == ID_ADT7316 || chip->id == ID_ADT7516)chip->dac_bits = 12;else if (chip->id == ID_ADT7317 || chip->id == ID_ADT7517)chip->dac_bits = 10;elsechip->dac_bits = 8;chip->ldac_pin = devm_gpiod_get_optional(dev, \"adi,ldac\", GPIOD_OUT_LOW);if (IS_ERR(chip->ldac_pin)) ": "adt7316_probe(struct device  dev, struct adt7316_bus  bus,  const char  name){struct adt7316_chip_info  chip;struct iio_dev  indio_dev;int ret;indio_dev = devm_iio_device_alloc(dev, sizeof( chip));if (!indio_dev)return -ENOMEM;chip = iio_priv(indio_dev);  this is only used for device removal purposes ", "} while (!channel_map[ch]);/* this function can be called in two situations * 1- We have switched to ad-hoc mode and we are *    performing a complete syncro scan before conclude *    there are no interesting cell and to create a *    new one. In this case the link state is *    IEEE80211_NOLINK until we found an interesting cell. *    If so the ieee8021_new_net, called by the RX path *    will set the state to IEEE80211_LINKED, so we stop *    scanning * 2- We are linked and the root uses run iwlist scan. *    So we switch to IEEE80211_LINKED_SCANNING to remember *    that we are still logically linked (not interested in *    new network events, despite for updating the net list, *    but we are temporarily 'unlinked' as the driver shall *    not filter RX frames and the channel is changing. * So the only situation in witch are interested is to check * if the state become LINKED because of the #1 situation ": "ieee80211_softmac_scan_syncro(struct ieee80211_device  ieee){short ch = 0;u8 channel_map[MAX_CHANNEL_NUMBER + 1];memcpy(channel_map, GET_DOT11D_INFO(ieee)->channel_map, MAX_CHANNEL_NUMBER + 1);mutex_lock(&ieee->scan_mutex);while (1) {do {ch++;if (ch > MAX_CHANNEL_NUMBER)goto out;   scan completed ", "if (success) ": "rtllib_ps_tx_ack(struct rtllib_device  ieee, short success){unsigned long flags, flags2;spin_lock_irqsave(&ieee->lock, flags);if (ieee->sta_sleep == LPS_WAIT_NULL_DATA_SEND) {  Null frame with PS bit set ", "ieee80211_sta_wakeup(ieee, 0);/* update the tx status ": "ieee80211_softmac_xmit(struct ieee80211_txb  txb, struct ieee80211_device  ieee){unsigned int queue_index = txb->queue_index;unsigned long flags;int  i;struct cb_desc  tcb_desc = NULL;spin_lock_irqsave(&ieee->lock, flags);  called with 2nd parm 0, no tx mgmt lock required ", "static void ieee80211_resume_tx(struct ieee80211_device *ieee)": "ieee80211_stop_queue(ieee);#ifdef USB_TX_DRIVER_AGGREGATION_ENABLEskb_queue_tail(&ieee->skb_drv_aggQ[queue_index], txb->fragments[i]);#elseskb_queue_tail(&ieee->skb_waitQ[queue_index], txb->fragments[i]);#endif} else {ieee->softmac_data_hard_start_xmit(txb->fragments[i],   ieee->dev, ieee->rate);ieee->stats.tx_packets++;ieee->stats.tx_bytes += txb->fragments[i]->len;ieee->dev->trans_start = jiffies;}}ieee80211_txb_free(txb);exit:spin_unlock_irqrestore(&ieee->lock, flags);}EXPORT_SYMBOL(ieee80211_softmac_xmit);  called with ieee->lock acquired ", "void ieee80211_softmac_scan_syncro(struct ieee80211_device *ieee)": "ieee80211_get_beacon_(struct ieee80211_device  ieee);static void ieee80211_send_beacon(struct ieee80211_device  ieee){struct sk_buff  skb;if (!ieee->ieee_up)return;unsigned long flags;skb = ieee80211_get_beacon_(ieee);if (skb) {softmac_mgmt_xmit(skb, ieee);ieee->softmac_stats.tx_beacons++;dev_kfree_skb_any(skb);edit by thomas}ieee->beacon_timer.expires = jiffies +(MSECS( ieee->current_network.beacon_interval -5));spin_lock_irqsave(&ieee->beacon_lock,flags);if (ieee->beacon_txing && ieee->ieee_up) {if(!timer_pending(&ieee->beacon_timer))add_timer(&ieee->beacon_timer);mod_timer(&ieee->beacon_timer,  jiffies + msecs_to_jiffies(ieee->current_network.beacon_interval - 5));}spin_unlock_irqrestore(&ieee->beacon_lock,flags);}static void ieee80211_send_beacon_cb(struct timer_list  t){struct ieee80211_device  ieee =from_timer(ieee, t, beacon_timer);unsigned long flags;spin_lock_irqsave(&ieee->beacon_lock, flags);ieee80211_send_beacon(ieee);spin_unlock_irqrestore(&ieee->beacon_lock, flags);}static void ieee80211_send_probe(struct ieee80211_device  ieee){struct sk_buff  skb;skb = ieee80211_probe_req(ieee);if (skb) {softmac_mgmt_xmit(skb, ieee);ieee->softmac_stats.tx_probe_rq++;dev_kfree_skb_any(skb);edit by thomas}}static void ieee80211_send_probe_requests(struct ieee80211_device  ieee){if (ieee->active_scan && (ieee->softmac_features & IEEE_SOFTMAC_PROBERQ)) {ieee80211_send_probe(ieee);ieee80211_send_probe(ieee);}}  this performs syncro scan blocking the caller until all channels   in the allowed channel map has been checked. ", "if (ieee->state != IEEE80211_NOLINK)return;if ((ieee->iw_mode == IW_MODE_INFRA) && !(net->capability & WLAN_CAPABILITY_BSS))return;if ((ieee->iw_mode == IW_MODE_ADHOC) && !(net->capability & WLAN_CAPABILITY_IBSS))return;if (ieee->iw_mode == IW_MODE_INFRA || ieee->iw_mode == IW_MODE_ADHOC) ": "notify_wx_assoc_event(ieee);} else {printk(\"==================>silent reset associate\\n\");ieee->is_silent_reset = false;}if (ieee->data_hard_resume)ieee->data_hard_resume(ieee->dev);netif_carrier_on(ieee->dev);}static void ieee80211_associate_complete(struct ieee80211_device  ieee){int i;struct net_device  dev = ieee->dev;del_timer_sync(&ieee->associate_timer);ieee->state = IEEE80211_LINKED;ieee->UpdateHalRATRTableHandler(dev, ieee->dot11HTOperationalRateSet);schedule_work(&ieee->associate_complete_wq);}static void ieee80211_associate_procedure_wq(struct work_struct  work){struct ieee80211_device  ieee = container_of(work, struct ieee80211_device, associate_procedure_wq);ieee->sync_scan_hurryup = 1;mutex_lock(&ieee->wx_mutex);if (ieee->data_hard_stop)ieee->data_hard_stop(ieee->dev);ieee80211_stop_scan(ieee);printk(\"===>%s(), chan:%d\\n\", __func__, ieee->current_network.channel);ieee->set_chan(ieee->dev, ieee->current_network.channel);HTSetConnectBwMode(ieee, HT_CHANNEL_WIDTH_20, HT_EXTCHNL_OFFSET_NO_EXT);ieee->associate_seq = 1;ieee80211_associate_step1(ieee);mutex_unlock(&ieee->wx_mutex);}inline void ieee80211_softmac_new_net(struct ieee80211_device  ieee, struct ieee80211_network  net){u8 tmp_ssid[IW_ESSID_MAX_SIZE + 1];int tmp_ssid_len = 0;short apset, ssidset, ssidbroad, apmatch, ssidmatch;  we are interested in new only if we are not associated   and we are not associating  authenticating ", "for (i = 0; i < WEP_KEYS; i++) ": "libipw_wx_set_encode(struct libipw_device  ieee,    struct iw_request_info  info,    union iwreq_data  wrqu, char  keybuf){struct iw_point  erq = &(wrqu->encoding);struct net_device  dev = ieee->dev;struct libipw_security sec = {.flags = 0};int i, key, key_provided, len;struct lib80211_crypt_data   crypt;int host_crypto = ieee->host_encrypt || ieee->host_decrypt;LIBIPW_DEBUG_WX(\"SET_ENCODE\\n\");key = erq->flags & IW_ENCODE_INDEX;if (key) {if (key > WEP_KEYS)return -EINVAL;key--;key_provided = 1;} else {key_provided = 0;key = ieee->crypt_info.tx_keyidx;}LIBIPW_DEBUG_WX(\"Key: %d [%s]\\n\", key, key_provided ?   \"provided\" : \"default\");crypt = &ieee->crypt_info.crypt[key];if (erq->flags & IW_ENCODE_DISABLED) {if (key_provided &&  crypt) {LIBIPW_DEBUG_WX(\"Disabling encryption on key %d.\\n\",   key);lib80211_crypt_delayed_deinit(&ieee->crypt_info, crypt);} elseLIBIPW_DEBUG_WX(\"Disabling encryption.\\n\");  Check all the keys to see if any are still configured,   and if no key index was provided, de-init them all ", "if (idx != 0 && ext->alg != IW_ENCODE_ALG_WEP)return -EINVAL;if (ieee->iw_mode == IW_MODE_INFRA)crypt = &ieee->crypt[idx];elsereturn -EINVAL;}sec.flags |= SEC_ENABLED;// | SEC_ENCRYPT;if ((encoding->flags & IW_ENCODE_DISABLED) ||    ext->alg == IW_ENCODE_ALG_NONE) ": "ieee80211_wx_set_encode_ext(struct ieee80211_device  ieee,       struct iw_request_info  info,       union iwreq_data  wrqu, char  extra){int ret = 0;struct net_device  dev = ieee->dev;struct iw_point  encoding = &wrqu->encoding;struct iw_encode_ext  ext = (struct iw_encode_ext  )extra;int i, idx;int group_key = 0;const char  alg,  module;struct ieee80211_crypto_ops  ops;struct ieee80211_crypt_data   crypt;struct ieee80211_security sec = {.flags = 0,};idx = encoding->flags & IW_ENCODE_INDEX;if (idx) {if (idx < 1 || idx > WEP_KEYS)return -EINVAL;idx--;} elseidx = ieee->tx_keyidx;if (ext->ext_flags & IW_ENCODE_EXT_GROUP_KEY) {crypt = &ieee->crypt[idx];group_key = 1;} else {  some Cisco APs use idx>0 for unicast in dynamic WEP ", "break;case IW_AUTH_CIPHER_PAIRWISE:case IW_AUTH_CIPHER_GROUP:case IW_AUTH_KEY_MGMT:/* *                  * Host AP driver does not use these parameters and allows *                                   * wpa_supplicant to control them internally. *                                                    ": "ieee80211_wx_set_auth(struct ieee80211_device  ieee,       struct iw_request_info  info,       struct iw_param  data, char  extra){switch (data->flags & IW_AUTH_INDEX) {case IW_AUTH_WPA_VERSION:      need to support wpa2 here", "static struct ieee80211_frag_entry *ieee80211_frag_cache_find(struct ieee80211_device *ieee, unsigned int seq,  unsigned int frag, u8 tid, u8 *src, u8 *dst)": "ieee80211_rx_stats  rx_stats){struct rtl_80211_hdr_4addr  hdr = (struct rtl_80211_hdr_4addr  )skb->data;u16 fc = le16_to_cpu(hdr->frame_ctl);skb->dev = ieee->dev;skb_reset_mac_header(skb);skb_pull(skb, ieee80211_get_hdrlen(fc));skb->pkt_type = PACKET_OTHERHOST;skb->protocol = htons(ETH_P_80211_RAW);memset(skb->cb, 0, sizeof(skb->cb));netif_rx(skb);}  Called only as a tasklet (software IRQ) ", "if ((memcmp(hdr->addr1, ieee->dev->dev_addr, ETH_ALEN))) ": "ieee80211_rx_mgt(ieee, (struct rtl_80211_hdr_4addr  )skb->data, rx_stats);  if ((ieee->state == IEEE80211_LINKED) && (memcmp(hdr->addr3, ieee->current_network.bssid, ETH_ALEN))) ", "if (fwrq->e == 1) ": "rtllib_wx_set_freq(struct rtllib_device  ieee, struct iw_request_info  a,     union iwreq_data  wrqu, char  b){int ret;struct iw_freq  fwrq = &wrqu->freq;mutex_lock(&ieee->wx_mutex);if (ieee->iw_mode == IW_MODE_INFRA) {ret = 0;goto out;}  if setting by freq convert to channel ", "fwrq->m = ieee80211_wlan_frequencies[ieee->current_network.channel - 1] * 100000;fwrq->e = 1;/* fwrq->m = ieee->current_network.channel; ": "ieee80211_wx_get_freq(struct ieee80211_device  ieee,     struct iw_request_info  a,     union iwreq_data  wrqu, char  b){struct iw_freq  fwrq = &wrqu->freq;if (ieee->current_network.channel == 0)return -1;  NM 0.7.0 will not accept channel any more. ", "spin_lock_irqsave(&ieee->lock, flags);if (ieee->state != IEEE80211_LINKED &&ieee->state != IEEE80211_LINKED_SCANNING &&ieee->wap_set == 0)eth_zero_addr(wrqu->ap_addr.sa_data);elsememcpy(wrqu->ap_addr.sa_data,       ieee->current_network.bssid, ETH_ALEN);spin_unlock_irqrestore(&ieee->lock, flags);return 0;}EXPORT_SYMBOL(ieee80211_wx_get_wap": "ieee80211_wx_get_wap(struct ieee80211_device  ieee,    struct iw_request_info  info,    union iwreq_data  wrqu, char  extra){unsigned long flags;wrqu->ap_addr.sa_family = ARPHRD_ETHER;if (ieee->iw_mode == IW_MODE_MONITOR)return -1;  We want avoid to give to the user inconsistent infos", "struct sockaddr *temp = (struct sockaddr *)awrq;ieee->sync_scan_hurryup = 1;mutex_lock(&ieee->wx_mutex);/* use ifconfig hw ether ": "ieee80211_wx_set_wap(struct ieee80211_device  ieee, struct iw_request_info  info, union iwreq_data  awrq, char  extra){int ret = 0;unsigned long flags;short ifup = ieee->proto_started;   dev->flags & IFF_UP; ", "spin_lock_irqsave(&ieee->lock, flags);if (ieee->current_network.ssid[0] == '\\0' ||ieee->current_network.ssid_len == 0) ": "rtllib_wx_get_essid(struct rtllib_device  ieee, struct iw_request_info  a, union iwreq_data  wrqu, char  b){int len, ret = 0;unsigned long flags;if (ieee->iw_mode == IW_MODE_MONITOR)return -1;  We want avoid to give to the user inconsistent infos", "return 0;}EXPORT_SYMBOL(ieee80211_wx_set_rate": "ieee80211_wx_set_rate(struct ieee80211_device  ieee,     struct iw_request_info  info,     union iwreq_data  wrqu, char  extra){u32 target_rate = wrqu->bitrate.value;ieee->rate = target_rate  100000;  FIXME: we might want to limit rate also in management protocols. ", "wrqu->rts.disabled = (wrqu->rts.value == DEFAULT_RTS_THRESHOLD);return 0;}EXPORT_SYMBOL(ieee80211_wx_get_rts": "ieee80211_wx_get_rts(struct ieee80211_device  ieee,     struct iw_request_info  info,     union iwreq_data  wrqu, char  extra){wrqu->rts.value = ieee->rts;wrqu->rts.fixed = 0;  no auto select ", "return 0;}out:mutex_unlock(&ieee->wx_mutex);return ret;}EXPORT_SYMBOL(ieee80211_wx_set_scan": "ieee80211_wx_set_scan(struct ieee80211_device  ieee, struct iw_request_info  a,     union iwreq_data  wrqu, char  b){int ret = 0;mutex_lock(&ieee->wx_mutex);if (ieee->iw_mode == IW_MODE_MONITOR || !(ieee->proto_started)) {ret = -1;goto out;}if (ieee->state == IEEE80211_LINKED) {queue_work(ieee->wq, &ieee->wx_sync_scan_wq);  intentionally forget to up sem ", "spin_lock_irqsave(&ieee->lock, flags);if (wrqu->essid.flags && wrqu->essid.length) ": "rtllib_wx_set_essid(struct rtllib_device  ieee,struct iw_request_info  a,union iwreq_data  wrqu, char  extra){int ret = 0, len;short proto_started;unsigned long flags;rtllib_stop_scan_syncro(ieee);mutex_lock(&ieee->wx_mutex);proto_started = ieee->proto_started;len = min_t(__u16, wrqu->essid.length, IW_ESSID_MAX_SIZE);if (ieee->iw_mode == IW_MODE_MONITOR) {ret = -1;goto out;}if (proto_started)rtllib_stop_protocol(ieee, true);  this is just to be sure that the GET wx callback   has consistent infos. not needed otherwise ", "ieee->ps_timeout = wrqu->power.value / 1000;}if (wrqu->power.flags & IW_POWER_PERIOD) ": "ieee80211_wx_set_power(struct ieee80211_device  ieee, struct iw_request_info  info, union iwreq_data  wrqu, char  extra){int ret = 0;mutex_lock(&ieee->wx_mutex);if (wrqu->power.disabled) {ieee->ps = IEEE80211_PS_DISABLED;goto exit;}if (wrqu->power.flags & IW_POWER_TIMEOUT) {  ieee->ps_period = wrqu->power.value  1000; ", "/* goto exit; ": "ieee80211_wx_get_power(struct ieee80211_device  ieee, struct iw_request_info  info, union iwreq_data  wrqu, char  extra){mutex_lock(&ieee->wx_mutex);if (ieee->ps == IEEE80211_PS_DISABLED) {wrqu->power.disabled = 1;goto exit;}wrqu->power.disabled = 0;if ((wrqu->power.flags & IW_POWER_TYPE) == IW_POWER_TIMEOUT) {wrqu->power.flags = IW_POWER_TIMEOUT;wrqu->power.value = ieee->ps_timeout   1000;} else {  ret = -EOPNOTSUPP; ", "if (pNetwork->bssht.bdHTInfoLen != 0)pHTInfo->CurrentOpMode = pPeerHTInfo->OptMode;/* * <TODO: Config according to OBSS non-HT STA present!!> ": "HTUpdateSelfAndPeerSetting(struct ieee80211_device  ieee,struct ieee80211_network  pNetwork){PRT_HIGH_THROUGHPUT        pHTInfo = ieee->pHTInfo;struct ht_capability_ele        pPeerHTCap = (struct ht_capability_ele  )pNetwork->bssht.bdHTCapBuf;PHT_INFORMATION_ELEpPeerHTInfo = (PHT_INFORMATION_ELE)pNetwork->bssht.bdHTInfoBuf;if (pHTInfo->bCurrentHTSupport) {    Config current operation mode. ", "memset(dot11d_info->channel_map, 0, MAX_CHANNEL_NUMBER + 1);memset(dot11d_info->max_tx_pwr_dbm_list, 0xFF, MAX_CHANNEL_NUMBER + 1);/* Set new channel map ": "dot11d_reset(struct ieee80211_device  ieee){u32 i;struct rt_dot11d_info  dot11d_info = GET_DOT11D_INFO(ieee);  Clear old channel map ", "pTriple = (struct chnl_txpower_triple *)(pCoutryIe + 3);for (i = 0; i < NumTriples; i++) ": "dot11d_update_country_ie(struct ieee80211_device  dev, u8  pTaddr,    u16 CoutryIeLen, u8  pCoutryIe){struct rt_dot11d_info  dot11d_info = GET_DOT11D_INFO(dev);u8 i, j, NumTriples, MaxChnlNum;struct chnl_txpower_triple  pTriple;memset(dot11d_info->channel_map, 0, MAX_CHANNEL_NUMBER + 1);memset(dot11d_info->max_tx_pwr_dbm_list, 0xFF, MAX_CHANNEL_NUMBER + 1);MaxChnlNum = 0;NumTriples = (CoutryIeLen - 3)  3;   skip 3-byte country string. ", "dot11d_reset(dev);}break;case DOT11D_STATE_NONE:break;}}EXPORT_SYMBOL(dot11d_scan_complete": "dot11d_scan_complete(struct ieee80211_device  dev){struct rt_dot11d_info  dot11d_info = GET_DOT11D_INFO(dev);switch (dot11d_info->state) {case DOT11D_STATE_LEARNED:dot11d_info->state = DOT11D_STATE_DONE;break;case DOT11D_STATE_DONE:if (GET_CIE_WATCHDOG(dev) == 0) {  Reset country IE if previous one is gone. ", "for (i = 0; i < VCHIQ_INIT_RETRIES; i++) ": "vchiq_initialise(struct vchiq_instance   instance_out){struct vchiq_state  state;struct vchiq_instance  instance = NULL;int i, ret;    VideoCore may not be ready due to boot up timing.   It may never be ready if kernel and firmware are mismatched,so don't   block forever. ", "vchiq_shutdown_internal(state, instance);mutex_unlock(&state->mutex);vchiq_log_trace(vchiq_core_log_level, \"%s(%p): returning %d\", __func__, instance, status);free_bulk_waiter(instance);kfree(instance);return status;}EXPORT_SYMBOL(vchiq_shutdown": "vchiq_shutdown(struct vchiq_instance  instance){int status = 0;struct vchiq_state  state = instance->state;if (mutex_lock_killable(&state->mutex))return -EAGAIN;  Remove all services ", "#undef MODULE_PARAM_PREFIX#define MODULE_PARAM_PREFIX DEVICE_NAME \".\"#define KEEPALIVE_VER 1#define KEEPALIVE_VER_MIN KEEPALIVE_VER/* Run time control of log level, based on KERN_XXX level. ": "vchiq_connected.h\"#include \"vchiq_pagelist.h\"#define DEVICE_NAME \"vchiq\"#define TOTAL_SLOTS (VCHIQ_SLOT_ZERO_SLOTS + 2   32)#define MAX_FRAGMENTS (VCHIQ_NUM_CURRENT_BULKS   2)#define VCHIQ_PLATFORM_FRAGMENTS_OFFSET_IDX 0#define VCHIQ_PLATFORM_FRAGMENTS_COUNT_IDX  1#define BELL00x00#define BELL20x08#define ARM_DS_ACTIVEBIT(2)  Override the default prefix, which would be vchiq_arm (from the filename) ", "if (status != -EAGAIN)break;msleep(1);}return status;}EXPORT_SYMBOL(vchiq_bulk_transmit": "vchiq_bulk_transmit(struct vchiq_instance  instance, unsigned int handle, const void  data,    unsigned int size, void  userdata, enum vchiq_bulk_mode mode){int status;while (1) {switch (mode) {case VCHIQ_BULK_MODE_NOCALLBACK:case VCHIQ_BULK_MODE_CALLBACK:status = vchiq_bulk_transfer(instance, handle,     (void  )data, NULL,     size, userdata, mode,     VCHIQ_BULK_TRANSMIT);break;case VCHIQ_BULK_MODE_BLOCKING:status = vchiq_blocking_bulk_transfer(instance, handle, (void  )data, size,      VCHIQ_BULK_TRANSMIT);break;default:return -EINVAL;}    vchiq_ _bulk_transfer() may return -EAGAIN, so we need   to implement a retry mechanism since this function is   supposed to block until queued ", "if (status != -EAGAIN)break;msleep(1);}return status;}EXPORT_SYMBOL(vchiq_bulk_receive": "vchiq_bulk_receive(struct vchiq_instance  instance, unsigned int handle,       void  data, unsigned int size, void  userdata,       enum vchiq_bulk_mode mode){int status;while (1) {switch (mode) {case VCHIQ_BULK_MODE_NOCALLBACK:case VCHIQ_BULK_MODE_CALLBACK:status = vchiq_bulk_transfer(instance, handle, data, NULL,     size, userdata,     mode, VCHIQ_BULK_RECEIVE);break;case VCHIQ_BULK_MODE_BLOCKING:status = vchiq_blocking_bulk_transfer(instance, handle, (void  )data, size,      VCHIQ_BULK_RECEIVE);break;default:return -EINVAL;}    vchiq_ _bulk_transfer() may return -EAGAIN, so we need   to implement a retry mechanism since this function is   supposed to block until queued ", "status = vchiq_send_remote_use_active(state);if (!status)ack_cnt--;elseatomic_add(ack_cnt, &arm_state->ka_use_ack_count);}}out:vchiq_log_trace(vchiq_susp_log_level, \"%s exit %d\", __func__, ret);return ret;}intvchiq_release_internal(struct vchiq_state *state, struct vchiq_service *service)": "vchiq_release_service(instance, ka_handle);if (status) {vchiq_log_error(vchiq_susp_log_level,\"%s vchiq_release_service error %d\", __func__,status);}}}shutdown:vchiq_shutdown(instance);exit:return 0;}intvchiq_use_internal(struct vchiq_state  state, struct vchiq_service  service,   enum USE_TYPE_E use_type){struct vchiq_arm_state  arm_state = vchiq_platform_get_arm_state(state);int ret = 0;char entity[16];int  entity_uc;int local_uc;if (!arm_state) {ret = -EINVAL;goto out;}if (use_type == USE_TYPE_VCHIQ) {sprintf(entity, \"VCHIQ:   \");entity_uc = &arm_state->peer_use_count;} else if (service) {sprintf(entity, \"%c%c%c%c:%03d\",VCHIQ_FOURCC_AS_4CHARS(service->base.fourcc),service->client_id);entity_uc = &service->service_use_count;} else {vchiq_log_error(vchiq_susp_log_level, \"%s null service ptr\", __func__);ret = -EINVAL;goto out;}write_lock_bh(&arm_state->susp_res_lock);local_uc = ++arm_state->videocore_use_count;++( entity_uc);vchiq_log_trace(vchiq_susp_log_level, \"%s %s count %d, state count %d\", __func__, entity, entity_uc, local_uc);write_unlock_bh(&arm_state->susp_res_lock);if (!ret) {int status = 0;long ack_cnt = atomic_xchg(&arm_state->ka_use_ack_count, 0);while (ack_cnt && !status) {  Send the use notify to videocore ", "static voidpoll_services(struct vchiq_state *state)": "vchiq_close_service_internal(service, NO_CLOSE_RECVD))request_poll(state, service, VCHIQ_POLL_REMOVE);} else if (service_flags & BIT(VCHIQ_POLL_TERMINATE)) {vchiq_log_info(vchiq_core_log_level, \"%d: ps - terminate %d<->%d\",       state->id, service->localport, service->remoteport);if (vchiq_close_service_internal(service, NO_CLOSE_RECVD))request_poll(state, service, VCHIQ_POLL_TERMINATE);}if (service_flags & BIT(VCHIQ_POLL_TXNOTIFY))notify_bulks(service, &service->bulk_tx, RETRY_POLL);if (service_flags & BIT(VCHIQ_POLL_RXNOTIFY))notify_bulks(service, &service->bulk_rx, RETRY_POLL);vchiq_service_put(service);}}  Called by the slot handler thread ", "if (status != -EAGAIN)break;msleep(1);}return status;}EXPORT_SYMBOL(vchiq_queue_kernel_message": "vchiq_queue_kernel_message(struct vchiq_instance  instance, unsigned int handle, void  data,       unsigned int size){int status;while (1) {status = vchiq_queue_message(instance, handle, memcpy_copy_callback,     data, size);    vchiq_queue_message() may return -EAGAIN, so we need to   implement a retry mechanism since this function is supposed   to block until queued ", "static inline voidremote_event_create(wait_queue_head_t *wq, struct remote_event *event)": "vchiq_release_message(service->instance, service->handle, header);return status;}inline voidvchiq_set_conn_state(struct vchiq_state  state, enum vchiq_connstate newstate){enum vchiq_connstate oldstate = state->conn_state;vchiq_log_info(vchiq_core_log_level, \"%d: %s->%s\", state->id, conn_state_names[oldstate],       conn_state_names[newstate]);state->conn_state = newstate;vchiq_platform_conn_state_changed(state, oldstate, newstate);}  This initialises a single remote_event, and the associated wait_queue. ", "callback();} else ": "vchiq_add_connected_callback(void ( callback)(void)){connected_init();if (mutex_lock_killable(&g_connected_mutex))return;if (g_connected) {  We're already connected. Call the callback immediately. ", "int cvm_oct_free_work(void *work_queue_entry)": "cvm_oct_free_work- Free a work queue entry     @work_queue_entry: Work queue entry to free     Returns Zero on success, Negative on failure. ", "spin_lock_irqsave(&ieee->lock, flags);if (ieee->link_state != MAC80211_LINKED &&ieee->link_state != MAC80211_LINKED_SCANNING &&ieee->wap_set == 0)eth_zero_addr(wrqu->ap_addr.sa_data);elsememcpy(wrqu->ap_addr.sa_data,       ieee->current_network.bssid, ETH_ALEN);spin_unlock_irqrestore(&ieee->lock, flags);return 0;}EXPORT_SYMBOL(rtllib_wx_get_wap": "rtllib_wx_get_wap(struct rtllib_device  ieee,    struct iw_request_info  info,    union iwreq_data  wrqu, char  extra){unsigned long flags;wrqu->ap_addr.sa_family = ARPHRD_ETHER;if (ieee->iw_mode == IW_MODE_MONITOR)return -1;  We want avoid to give to the user inconsistent infos", "if (ieee->iw_mode == IW_MODE_MASTER) ": "rtllib_wx_set_wap(struct rtllib_device  ieee, struct iw_request_info  info, union iwreq_data  awrq, char  extra){int ret = 0;unsigned long flags;short ifup = ieee->proto_started;struct sockaddr  temp = (struct sockaddr  )awrq;rtllib_stop_scan_syncro(ieee);mutex_lock(&ieee->wx_mutex);  use ifconfig hw ether ", "wrqu->rts.disabled = (wrqu->rts.value == DEFAULT_RTS_THRESHOLD);return 0;}EXPORT_SYMBOL(rtllib_wx_get_rts": "rtllib_wx_get_rts(struct rtllib_device  ieee,     struct iw_request_info  info,     union iwreq_data  wrqu, char  extra){wrqu->rts.value = ieee->rts;wrqu->rts.fixed = 0;  no auto select ", "return 0;}out:return ret;}EXPORT_SYMBOL(rtllib_wx_set_scan": "rtllib_wx_set_scan(struct rtllib_device  ieee, struct iw_request_info  a,     union iwreq_data  wrqu, char  b){int ret = 0;if (ieee->iw_mode == IW_MODE_MONITOR || !(ieee->proto_started)) {ret = -1;goto out;}if (ieee->link_state == MAC80211_LINKED) {schedule_work(&ieee->wx_sync_scan_wq);  intentionally forget to up sem ", "}} while (!ieee->active_channel_map[ieee->current_network.channel]);if (ieee->scanning_continue == 0)goto out;ieee->set_chan(ieee->dev, ieee->current_network.channel);if (ieee->active_channel_map[ieee->current_network.channel] == 1)rtllib_send_probe_requests(ieee, 0);schedule_delayed_work(&ieee->softmac_scan_wq,      msecs_to_jiffies(RTLLIB_SOFTMAC_SCAN_TIME));mutex_unlock(&ieee->scan_mutex);return;out:if (IS_DOT11D_ENABLE(ieee))dot11d_scan_complete(ieee);ieee->current_network.channel = last_channel;out1:ieee->actscanning = false;ieee->scan_watch_dog = 0;ieee->scanning_continue = 0;mutex_unlock(&ieee->scan_mutex);}static void rtllib_beacons_start(struct rtllib_device *ieee)": "rtllib_act_scanning(ieee, true))return;mutex_lock(&ieee->scan_mutex);if (ieee->rf_power_state == rf_off) {netdev_info(ieee->dev,    \"======>%s():rf state is rf_off, return\\n\",    __func__);goto out1;}do {ieee->current_network.channel =(ieee->current_network.channel + 1) %MAX_CHANNEL_NUMBER;if (ieee->scan_watch_dog++ > MAX_CHANNEL_NUMBER) {if (!ieee->active_channel_map[ieee->current_network.channel])ieee->current_network.channel = 6;goto out;   no good chans ", "void rtllib_EnableNetMonitorMode(struct net_device *dev,bool bInitState)": "rtllib_get_beacon_(struct rtllib_device  ieee);static void rtllib_send_beacon(struct rtllib_device  ieee){struct sk_buff  skb;if (!ieee->ieee_up)return;skb = rtllib_get_beacon_(ieee);if (skb) {softmac_mgmt_xmit(skb, ieee);ieee->softmac_stats.tx_beacons++;}if (ieee->beacon_txing && ieee->ieee_up)mod_timer(&ieee->beacon_timer, jiffies +  (msecs_to_jiffies(ieee->current_network.beacon_interval - 5)));}static void rtllib_send_beacon_cb(struct timer_list  t){struct rtllib_device  ieee =from_timer(ieee, t, beacon_timer);unsigned long flags;spin_lock_irqsave(&ieee->beacon_lock, flags);rtllib_send_beacon(ieee);spin_unlock_irqrestore(&ieee->beacon_lock, flags);}  Enables network monitor mode, all rx packets will be received. ", "if (ieee->link_state != MAC80211_NOLINK)return;if ((ieee->iw_mode == IW_MODE_INFRA) && !(net->capability &    WLAN_CAPABILITY_ESS))return;if ((ieee->iw_mode == IW_MODE_ADHOC) && !(net->capability &     WLAN_CAPABILITY_IBSS))return;if ((ieee->iw_mode == IW_MODE_ADHOC) &&    (net->channel > ieee->ibss_maxjoin_chal))return;if (ieee->iw_mode == IW_MODE_INFRA || ieee->iw_mode == IW_MODE_ADHOC) ": "notify_wx_assoc_event(ieee);}netif_carrier_on(ieee->dev);ieee->is_roaming = false;if (rtllib_is_54g(&ieee->current_network) &&   (ieee->modulation & RTLLIB_OFDM_MODULATION)) {ieee->rate = 108;netdev_info(ieee->dev, \"Using G rates:%d\\n\", ieee->rate);} else {ieee->rate = 22;ieee->set_wireless_mode(ieee->dev, WIRELESS_MODE_B);netdev_info(ieee->dev, \"Using B rates:%d\\n\", ieee->rate);}if (ieee->ht_info->bCurrentHTSupport && ieee->ht_info->enable_ht) {netdev_info(ieee->dev, \"Successfully associated, ht enabled\\n\");HTOnAssocRsp(ieee);} else {netdev_info(ieee->dev,    \"Successfully associated, ht not enabled(%d, %d)\\n\",    ieee->ht_info->bCurrentHTSupport,    ieee->ht_info->enable_ht);memset(ieee->dot11ht_oper_rate_set, 0, 16);}ieee->link_detect_info.SlotNum = 2   (1 +       ieee->current_network.beacon_interval        500);if (ieee->link_detect_info.NumRecvBcnInPeriod == 0 ||    ieee->link_detect_info.NumRecvDataInPeriod == 0) {ieee->link_detect_info.NumRecvBcnInPeriod = 1;ieee->link_detect_info.NumRecvDataInPeriod = 1;}psc->LpsIdleCount = 0;ieee->link_change(ieee->dev);if (ieee->is_silent_reset) {netdev_info(ieee->dev, \"silent reset associate\\n\");ieee->is_silent_reset = false;}}static void rtllib_sta_send_associnfo(struct rtllib_device  ieee){}static void rtllib_associate_complete(struct rtllib_device  ieee){del_timer_sync(&ieee->associate_timer);ieee->link_state = MAC80211_LINKED;rtllib_sta_send_associnfo(ieee);schedule_work(&ieee->associate_complete_wq);}static void rtllib_associate_procedure_wq(void  data){struct rtllib_device  ieee = container_of_dwork_rsl(data,     struct rtllib_device,     associate_procedure_wq);rtllib_stop_scan_syncro(ieee);ieee->rtllib_ips_leave(ieee->dev);mutex_lock(&ieee->wx_mutex);rtllib_stop_scan(ieee);HTSetConnectBwMode(ieee, HT_CHANNEL_WIDTH_20, HT_EXTCHNL_OFFSET_NO_EXT);if (ieee->rf_power_state == rf_off) {ieee->rtllib_ips_leave_wq(ieee->dev);mutex_unlock(&ieee->wx_mutex);return;}ieee->associate_seq = 1;rtllib_associate_step1(ieee, ieee->current_network.bssid);mutex_unlock(&ieee->wx_mutex);}inline void rtllib_softmac_new_net(struct rtllib_device  ieee,   struct rtllib_network  net){u8 tmp_ssid[IW_ESSID_MAX_SIZE + 1];int tmp_ssid_len = 0;short apset, ssidset, ssidbroad, apmatch, ssidmatch;  we are interested in new only if we are not associated   and we are not associating  authenticating ", "static struct rtllib_frag_entry *rtllib_frag_cache_find(struct rtllib_device *ieee, unsigned int seq,  unsigned int frag, u8 tid, u8 *src, u8 *dst)": "rtllib_rx_mgt(struct rtllib_device  ieee, struct sk_buff  skb,  struct rtllib_rx_stats  stats);static inline void rtllib_monitor_rx(struct rtllib_device  ieee,     struct sk_buff  skb,     struct rtllib_rx_stats  rx_status,     size_t hdr_length){skb->dev = ieee->dev;skb_reset_mac_header(skb);skb_pull(skb, hdr_length);skb->pkt_type = PACKET_OTHERHOST;skb->protocol = htons(ETH_P_80211_RAW);memset(skb->cb, 0, sizeof(skb->cb));netif_rx(skb);}  Called only as a tasklet (software IRQ) ", "for (i = 0; i < NUM_WEP_KEYS; i++) ": "rtllib_wx_set_encode(struct rtllib_device  ieee, struct iw_request_info  info, union iwreq_data  wrqu, char  keybuf){struct iw_point  erq = &wrqu->encoding;struct net_device  dev = ieee->dev;struct rtllib_security sec = {.flags = 0};int i, key, key_provided, len;struct lib80211_crypt_data   crypt;key = erq->flags & IW_ENCODE_INDEX;if (key) {if (key > NUM_WEP_KEYS)return -EINVAL;key--;key_provided = 1;} else {key_provided = 0;key = ieee->crypt_info.tx_keyidx;}netdev_dbg(ieee->dev, \"Key: %d [%s]\\n\", key, key_provided ?   \"provided\" : \"default\");crypt = &ieee->crypt_info.crypt[key];if (erq->flags & IW_ENCODE_DISABLED) {if (key_provided &&  crypt) {netdev_dbg(ieee->dev,   \"Disabling encryption on key %d.\\n\", key);lib80211_crypt_delayed_deinit(&ieee->crypt_info, crypt);} else {netdev_dbg(ieee->dev, \"Disabling encryption.\\n\");}  Check all the keys to see if any are still configured,   and if no key index was provided, de-init them all ", "if (idx != 0 && ext->alg != IW_ENCODE_ALG_WEP)return -EINVAL;if (ieee->iw_mode == IW_MODE_INFRA)crypt = &ieee->crypt_info.crypt[idx];elsereturn -EINVAL;}sec.flags |= SEC_ENABLED;if ((encoding->flags & IW_ENCODE_DISABLED) ||    ext->alg == IW_ENCODE_ALG_NONE) ": "rtllib_wx_set_encode_ext(struct rtllib_device  ieee,     struct iw_request_info  info,     union iwreq_data  wrqu, char  extra){int ret = 0;struct net_device  dev = ieee->dev;struct iw_point  encoding = &wrqu->encoding;struct iw_encode_ext  ext = (struct iw_encode_ext  )extra;int i, idx;int group_key = 0;const char  alg,  module;struct lib80211_crypto_ops  ops;struct lib80211_crypt_data   crypt;struct rtllib_security sec = {.flags = 0,};idx = encoding->flags & IW_ENCODE_INDEX;if (idx) {if (idx < 1 || idx > NUM_WEP_KEYS)return -EINVAL;idx--;} else {idx = ieee->crypt_info.tx_keyidx;}if (ext->ext_flags & IW_ENCODE_EXT_GROUP_KEY) {crypt = &ieee->crypt_info.crypt[idx];group_key = 1;} else {  some Cisco APs use idx>0 for unicast in dynamic WEP ", "break;case IW_AUTH_TKIP_COUNTERMEASURES:ieee->tkip_countermeasures = data->value;break;case IW_AUTH_DROP_UNENCRYPTED:ieee->drop_unencrypted = data->value;break;case IW_AUTH_80211_AUTH_ALG:if (data->value & IW_AUTH_ALG_SHARED_KEY) ": "rtllib_wx_set_auth(struct rtllib_device  ieee,       struct iw_request_info  info,       struct iw_param  data, char  extra){switch (data->flags & IW_AUTH_INDEX) {case IW_AUTH_WPA_VERSION:break;case IW_AUTH_CIPHER_PAIRWISE:case IW_AUTH_CIPHER_GROUP:case IW_AUTH_KEY_MGMT:  Host AP driver does not use these parameters and allows   wpa_supplicant to control them internally. ", "ieee->fts = DEFAULT_FTS;ieee->scan_age = DEFAULT_MAX_SCAN_AGE;ieee->open_wep = 1;/* Default to enabling full open WEP with host based encrypt/decrypt ": "alloc_rtllib(int sizeof_priv){struct rtllib_device  ieee = NULL;struct net_device  dev;int i, err;pr_debug(\"rtllib: Initializing...\\n\");dev = alloc_etherdev(sizeof(struct rtllib_device) + sizeof_priv);if (!dev) {pr_err(\"Unable to allocate net_device.\\n\");return NULL;}ieee = (struct rtllib_device  )netdev_priv_rsl(dev);ieee->dev = dev;err = rtllib_networks_allocate(ieee);if (err) {pr_err(\"Unable to allocate beacon storage: %d\\n\", err);goto free_netdev;}rtllib_networks_initialize(ieee);  Default fragmentation threshold is maximum payload size ", ".duration_id = 0,.seq_ctl = 0,.qos_ctl = 0};int qos_activated = ieee->current_network.qos_data.active;u8 dest[ETH_ALEN];u8 src[ETH_ALEN];struct lib80211_crypt_data *crypt = NULL;struct cb_desc *tcb_desc;u8 bIsMulticast = false;u8 IsAmsdu = false;boolbdhcp = false;spin_lock_irqsave(&ieee->lock, flags);/* If there is no driver handler to take the TXB, don't bother * creating it... ": "rtllib_xmit_inter(struct sk_buff  skb, struct net_device  dev){struct rtllib_device  ieee = (struct rtllib_device  )     netdev_priv_rsl(dev);struct rtllib_txb  txb = NULL;struct rtllib_hdr_3addrqos  frag_hdr;int i, bytes_per_frag, nr_frags, bytes_last_frag, frag_size;unsigned long flags;struct net_device_stats  stats = &ieee->stats;int ether_type = 0, encrypt;int bytes, fc, qos_ctl = 0, hdr_len;struct sk_buff  skb_frag;struct rtllib_hdr_3addrqos header = {   Ensure zero initialized ", "int nvec_write_async(struct nvec_chip *nvec, const unsigned char *data,     short size)": "nvec_write_async - Asynchronously write a message to NVEC   @nvec: An nvec_chip instance   @data: The message data, starting with the request type   @size: The size of @data     Queue a single message to be transferred to the embedded controller   and return immediately.     Returns: 0 on success, a negative error code on failure. If a failure   occurred, the nvec driver may print an error. ", "int nvec_register_notifier(struct nvec_chip *nvec, struct notifier_block *nb,   unsigned int events)": "nvec_write_sync(). ", "int fbtft_write_spi_emulate_9(struct fbtft_par *par, void *buf, size_t len)": "fbtft_write_spi_emulate_9() - write SPI emulating 9-bit   @par: Driver data   @buf: Buffer to write   @len: Length of buffer (must be divisible by 8)     When 9-bit SPI is not available, this function can be used to emulate that.   par->extra must hold a transformation buffer used for transfer. ", "gpiod_set_value(par->gpio.wr, 1);/* Set data ": "fbtft_write_gpio16_wr(struct fbtft_par  par, void  buf, size_t len){u16 data;int i;#ifndef DO_NOT_OPTIMIZE_FBTFT_WRITE_GPIOstatic u16 prev_data;#endiffbtft_par_dbg_hex(DEBUG_WRITE, par, par->info->device, u8, buf, len,  \"%s(len=%zu): \", __func__, len);while (len) {data =  (u16  )buf;  Start writing by pulling down WR ", "pad = (len % 4) ? 4 - (len % 4) : 0;for (i = 0; i < pad; i++)*buf++ = 0x000;}va_start(args, len);*buf++ = (u8)va_arg(args, unsigned int);i = len - 1;while (i--) ": "fbtft_write_reg8_bus9(struct fbtft_par  par, int len, ...){va_list args;int i, ret;int pad = 0;u16  buf = (u16  )par->buf;if (unlikely(par->debug & DEBUG_WRITE_REGISTER)) {va_start(args, len);for (i = 0; i < len; i++) (((u8  )buf) + i) = (u8)va_arg(args, unsigned int);va_end(args);fbtft_par_dbg_hex(DEBUG_WRITE_REGISTER, par,  par->info->device, u8, buf, len, \"%s: \",  __func__);}if (len <= 0)return;if (par->spi && (par->spi->bits_per_word == 8)) {  we're emulating 9-bit, pad start of buffer with no-ops   (assuming here that zero is a no-op) ", "if (!par->txbuf.buf)return par->fbtftops.write(par, vmem16, len);/* buffered write ": "fbtft_write_vmem16_bus8(struct fbtft_par  par, size_t offset, size_t len){u16  vmem16;__be16  txbuf16 = par->txbuf.buf;size_t remain;size_t to_copy;size_t tx_array_size;int i;int ret = 0;size_t startbyte_size = 0;fbtft_par_dbg(DEBUG_WRITE_VMEM, par, \"%s(offset=%zu, len=%zu)\\n\",      __func__, offset, len);remain = len  2;vmem16 = (u16  )(par->info->screen_buffer + offset);gpiod_set_value(par->gpio.dc, 1);  non buffered write ", "return fbtft_write_buf_dc(par, vmem16, len, 1);}EXPORT_SYMBOL(fbtft_write_vmem16_bus16": "fbtft_write_vmem16_bus16(struct fbtft_par  par, size_t offset, size_t len){u16  vmem16;fbtft_par_dbg(DEBUG_WRITE_VMEM, par, \"%s(offset=%zu, len=%zu)\\n\",      __func__, offset, len);vmem16 = (u16  )(par->info->screen_buffer + offset);  no need for buffered write with 16-bit bus ", "bl_props.power = FB_BLANK_POWERDOWN;if (!gpiod_get_value(par->gpio.led[0]))par->polarity = true;bd = backlight_device_register(dev_driver_string(par->info->device),       par->info->device, par,       &fbtft_bl_ops, &bl_props);if (IS_ERR(bd)) ": "fbtft_register_backlight(struct fbtft_par  par){struct backlight_device  bd;struct backlight_properties bl_props = { 0, };if (!par->gpio.led[0]) {fbtft_par_dbg(DEBUG_BACKLIGHT, par,      \"%s(): led pin not set, exiting.\\n\", __func__);return;}bl_props.type = BACKLIGHT_RAW;  Assume backlight is off, get polarity from current state of pin ", "struct fb_info *fbtft_framebuffer_alloc(struct fbtft_display *display,struct device *dev,struct fbtft_platform_data *pdata)": "fbtft_framebuffer_alloc - creates a new frame buffer info structure     @display: pointer to structure describing the display   @dev: pointer to the device for this fb, this can be NULL   @pdata: platform data for the display in use     Creates a new frame buffer info structure.     Also creates and populates the following structures:     info->fbops     info->fbdefio     info->pseudo_palette     par->fbtftops     par->txbuf     Returns the new structure, or NULL if an error occurred.   ", "void fbtft_framebuffer_release(struct fb_info *info)": "fbtft_framebuffer_release - frees up all memory used by the framebuffer     @info: frame buffer info structure   ", "int fbtft_register_framebuffer(struct fb_info *fb_info)": "fbtft_register_framebuffer - registers a tft frame buffer device  @fb_info: frame buffer info structure      Sets SPI driverdata if needed    Requests needed gpios.    Initializes display    Updates display.  Registers a frame buffer device @fb_info.    Returns negative errno on error, or zero for success.   ", "int fbtft_unregister_framebuffer(struct fb_info *fb_info)": "fbtft_unregister_framebuffer - releases a tft frame buffer device  @fb_info: frame buffer info structure      Frees SPI driverdata if needed    Frees gpios.  Unregisters frame buffer device.   ", "static int fbtft_init_display_from_property(struct fbtft_par *par)": "fbtft_init_display_from_property() - Device Tree init_display() function   @par: Driver data     Return: 0 if successful, negative if error ", "int fbtft_probe_common(struct fbtft_display *display,       struct spi_device *sdev,       struct platform_device *pdev)": "fbtft_probe_common() - Generic device probe() helper function   @display: Display properties   @sdev: SPI device   @pdev: Platform device     Allocates, initializes and registers a framebuffer     Either @sdev or @pdev should be NULL     Return: 0 if successful, negative if error ", "void fbtft_remove_common(struct device *dev, struct fb_info *info)": "fbtft_remove_common() - Generic device remove() helper function   @dev: Device   @info: Framebuffer     Unregisters and releases the framebuffer ", "void of_iommu_get_resv_regions(struct device *dev, struct list_head *list)": "of_iommu_get_resv_regions - reserved region driver helper for device tree   @dev: device for which to get reserved regions   @list: reserved region list     IOMMU drivers can use this to implement their .get_resv_regions() callback   for memory regions attached to a device tree node. See the reserved-memory   device tree bindings on how to use these:       Documentationdevicetreebindingsreserved-memoryreserved-memory.txt ", "struct iommu_group *iommu_group_alloc(void)": "iommu_put_resv_regions(device->dev, &dev_resv_regions);if (ret)break;}mutex_unlock(&group->mutex);return ret;}EXPORT_SYMBOL_GPL(iommu_get_group_resv_regions);static ssize_t iommu_group_show_resv_regions(struct iommu_group  group,     char  buf){struct iommu_resv_region  region,  next;struct list_head group_resv_regions;int offset = 0;INIT_LIST_HEAD(&group_resv_regions);iommu_get_group_resv_regions(group, &group_resv_regions);list_for_each_entry_safe(region, next, &group_resv_regions, list) {offset += sysfs_emit_at(buf, offset, \"0x%016llx 0x%016llx %s\\n\",(long long)region->start,(long long)(region->start +    region->length - 1),iommu_group_resv_type_string[region->type]);kfree(region);}return offset;}static ssize_t iommu_group_show_type(struct iommu_group  group,     char  buf){char  type = \"unknown\";mutex_lock(&group->mutex);if (group->default_domain) {switch (group->default_domain->type) {case IOMMU_DOMAIN_BLOCKED:type = \"blocked\";break;case IOMMU_DOMAIN_IDENTITY:type = \"identity\";break;case IOMMU_DOMAIN_UNMANAGED:type = \"unmanaged\";break;case IOMMU_DOMAIN_DMA:type = \"DMA\";break;case IOMMU_DOMAIN_DMA_FQ:type = \"DMA-FQ\";break;}}mutex_unlock(&group->mutex);return sysfs_emit(buf, \"%s\\n\", type);}static IOMMU_GROUP_ATTR(name, S_IRUGO, iommu_group_show_name, NULL);static IOMMU_GROUP_ATTR(reserved_regions, 0444,iommu_group_show_resv_regions, NULL);static IOMMU_GROUP_ATTR(type, 0644, iommu_group_show_type,iommu_group_store_type);static void iommu_group_release(struct kobject  kobj){struct iommu_group  group = to_iommu_group(kobj);pr_debug(\"Releasing group %d\\n\", group->id);if (group->iommu_data_release)group->iommu_data_release(group->iommu_data);ida_free(&iommu_group_ida, group->id);if (group->default_domain)iommu_domain_free(group->default_domain);if (group->blocking_domain)iommu_domain_free(group->blocking_domain);kfree(group->name);kfree(group);}static const struct kobj_type iommu_group_ktype = {.sysfs_ops = &iommu_group_sysfs_ops,.release = iommu_group_release,};     iommu_group_alloc - Allocate a new group     This function is called by an iommu driver to allocate a new iommu   group.  The iommu group represents the minimum granularity of the iommu.   Upon successful return, the caller holds a reference to the supplied   group in order to hold the group until devices are added.  Use   iommu_group_put() to release this extra reference count, allowing the   group to be automatically reclaimed once it has no devices or external   references. ", "int iommu_get_msi_cookie(struct iommu_domain *domain, dma_addr_t base)": "iommu_get_msi_cookie - Acquire just MSI remapping resources   @domain: IOMMU domain to prepare   @base: Start address of IOVA region for MSI mappings     Users who manage their own IOVA allocation and do not want DMA API support,   but would still like to take advantage of automatic MSI remapping, can use   this to initialise their own domain appropriately. Users should reserve a   contiguous IOVA region, starting at @base, large enough to accommodate the   number of PAGE_SIZE mappings necessary to cover every MSI doorbell address   used by the devices attached to @domain. ", "void iommu_dma_get_resv_regions(struct device *dev, struct list_head *list)": "iommu_dma_get_resv_regions - Reserved region driver helper   @dev: Device from iommu_get_resv_regions()   @list: Reserved region list from iommu_get_resv_regions()     IOMMU drivers can use this to implement their .get_resv_regions callback   for general non-IOMMU-specific reservations. Currently, this covers GICv3   ITS region reservation on ACPI based ARM platforms that may require HW MSI   reservation. ", "if (cpu_feature_enabled(X86_FEATURE_LA57) &&    amd_iommu_gpt_level != PAGE_MODE_5_LEVEL)return false;/* * Since DTE[Mode]=0 is prohibited on SNP-enabled system * (i.e. EFR[SNPSup]=1), IOMMUv2 page table cannot be used without * setting up IOMMUv1 page table. ": "amd_iommu_v2_supported(void){  CPU page table size should match IOMMU guest page table size ", "ret = -EBUSY;if (pdom->dev_cnt > 0 || pdom->flags & PD_IOMMUV2_MASK)goto out;if (!pdom->gcr3_tbl)ret = domain_enable_v2(pdom, pasids);out:spin_unlock_irqrestore(&pdom->lock, flags);return ret;}EXPORT_SYMBOL(amd_iommu_domain_enable_v2": "amd_iommu_domain_enable_v2(struct iommu_domain  dom, int pasids){struct protection_domain  pdom = to_pdomain(dom);unsigned long flags;int ret;spin_lock_irqsave(&pdom->lock, flags);    Save us all sanity checks whether devices already in the   domain support IOMMUv2. Just force that the domain has no   devices attached when it is switched into IOMMUv2 mode. ", "static int device_flush_iotlb(struct iommu_dev_data *dev_data,      u64 address, size_t size)": "amd_iommu_flush_tlb_all(struct amd_iommu  iommu){u32 dom_id;u16 last_bdf = iommu->pci_seg->last_bdf;for (dom_id = 0; dom_id <= last_bdf; ++dom_id) {struct iommu_cmd cmd;build_inv_iommu_pages(&cmd, 0, CMD_INV_IOMMU_ALL_PAGES_ADDRESS,      dom_id, 1);iommu_queue_command(iommu, &cmd);}iommu_completion_wait(iommu);}static void amd_iommu_flush_tlb_domid(struct amd_iommu  iommu, u32 dom_id){struct iommu_cmd cmd;build_inv_iommu_pages(&cmd, 0, CMD_INV_IOMMU_ALL_PAGES_ADDRESS,      dom_id, 1);iommu_queue_command(iommu, &cmd);iommu_completion_wait(iommu);}static void amd_iommu_flush_all(struct amd_iommu  iommu){struct iommu_cmd cmd;build_inv_all(&cmd);iommu_queue_command(iommu, &cmd);iommu_completion_wait(iommu);}static void iommu_flush_irt(struct amd_iommu  iommu, u16 devid){struct iommu_cmd cmd;build_inv_irt(&cmd, devid);iommu_queue_command(iommu, &cmd);}static void amd_iommu_flush_irt_all(struct amd_iommu  iommu){u32 devid;u16 last_bdf = iommu->pci_seg->last_bdf;if (iommu->irtcachedis_enabled)return;for (devid = 0; devid <= last_bdf; devid++)iommu_flush_irt(iommu, devid);iommu_completion_wait(iommu);}void iommu_flush_all_caches(struct amd_iommu  iommu){if (iommu_feature(iommu, FEATURE_IA)) {amd_iommu_flush_all(iommu);} else {amd_iommu_flush_dte_all(iommu);amd_iommu_flush_irt_all(iommu);amd_iommu_flush_tlb_all(iommu);}}    Command send function for flushing on-device TLB ", "/* Drop reference taken in amd_iommu_bind_pasid ": "amd_iommu_bind_pasid ", "put_pasid_state(pasid_state);/* Clear the pasid state so that the pasid can be re-used ": "amd_iommu_unbind_pasid(struct pci_dev  pdev, u32 pasid){struct pasid_state  pasid_state;struct device_state  dev_state;u32 sbdf;might_sleep();if (!amd_iommu_v2_supported())return;sbdf = get_pci_sbdf_id(pdev);dev_state = get_device_state(sbdf);if (dev_state == NULL)return;if (pasid >= dev_state->max_pasids)goto out;pasid_state = get_pasid_state(dev_state, pasid);if (pasid_state == NULL)goto out;    Drop reference taken here. We are safe because we still hold   the reference taken in the amd_iommu_bind_pasid function. ", "if (cc_platform_has(CC_ATTR_MEM_ENCRYPT))return -ENODEV;if (!amd_iommu_v2_supported())return -ENODEV;if (pasids <= 0 || pasids > (PASID_MASK + 1))return -EINVAL;sbdf = get_pci_sbdf_id(pdev);dev_state = kzalloc(sizeof(*dev_state), GFP_KERNEL);if (dev_state == NULL)return -ENOMEM;spin_lock_init(&dev_state->lock);init_waitqueue_head(&dev_state->wq);dev_state->pdev  = pdev;dev_state->sbdf = sbdf;tmp = pasids;for (dev_state->pasid_levels = 0; (tmp - 1) & ~0x1ff; tmp >>= 9)dev_state->pasid_levels += 1;atomic_set(&dev_state->count, 1);dev_state->max_pasids = pasids;ret = -ENOMEM;dev_state->states = (void *)get_zeroed_page(GFP_KERNEL);if (dev_state->states == NULL)goto out_free_dev_state;dev_state->domain = iommu_domain_alloc(&pci_bus_type);if (dev_state->domain == NULL)goto out_free_states;/* See iommu_is_default_domain() ": "amd_iommu_init_device(struct pci_dev  pdev, int pasids){struct device_state  dev_state;struct iommu_group  group;unsigned long flags;int ret, tmp;u32 sbdf;might_sleep();    When memory encryption is active the device is likely not in a   direct-mapped domain. Forbid using IOMMUv2 functionality for now. ", "int dev_pm_opp_register_notifier(struct device *dev, struct notifier_block *nb)": "dev_pm_opp_register_notifier() - Register OPP notifier for the device   @dev:Device for which notifier needs to be registered   @nb:Notifier block to be registered     Return: 0 on success or a negative error value. ", "int dev_pm_opp_unregister_notifier(struct device *dev,   struct notifier_block *nb)": "dev_pm_opp_unregister_notifier() - Unregister OPP notifier for the device   @dev:Device for which notifier needs to be unregistered   @nb:Notifier block to be unregistered     Return: 0 on success or a negative error value. ", "job = mempool_alloc(&kc->job_pool, GFP_NOIO);mutex_init(&job->lock);/* * set up for the read. ": "dm_kcopyd_copy(struct dm_kcopyd_client  kc, struct dm_io_region  from,    unsigned int num_dests, struct dm_io_region  dests,    unsigned int flags, dm_kcopyd_notify_fn fn, void  context){struct kcopyd_job  job;int i;    Allocate an array of jobs consisting of one master job   followed by SPLIT_COUNT sub jobs. ", "wait_event(kc->destroyq, !atomic_read(&kc->nr_jobs));BUG_ON(!list_empty(&kc->callback_jobs));BUG_ON(!list_empty(&kc->complete_jobs));BUG_ON(!list_empty(&kc->io_jobs));BUG_ON(!list_empty(&kc->pages_jobs));destroy_workqueue(kc->kcopyd_wq);dm_io_client_destroy(kc->io_client);client_free_pages(kc);mempool_exit(&kc->job_pool);kfree(kc);}EXPORT_SYMBOL(dm_kcopyd_client_destroy": "dm_kcopyd_client_destroy(struct dm_kcopyd_client  kc){  Wait for completion of all jobs submitted by this client. ", "struct io ": "dm_io_client {mempool_t pool;struct bio_set bios;};    Aligning 'struct io' reduces the number of bits required to store   its address.  Refer to store_io_and_region_in_bio() below. ", "void dm_kobject_release(struct kobject *kobj)": "dm_kobject_release is called from this process    6. dm_kobject_release calls complete()    7. a reschedule happens before dm_kobject_release returns    8. dm_sysfs_exit continues, the dm device is unloaded, module reference       count is decremented    9. The user unloads the dm module   10. The other process that was rescheduled in step 7 continues to run,       it is now executing code in unloaded module, so it crashes     Note that if the process that takes the foreign reference to dm kobject   has a low priority and the system is sufficiently loaded with   higher-priority processes that prevent the low-priority process from   being scheduled long enough, this bug may really happen.     In order to fix this module unload race, we place the release method   into a helper code that is compiled directly into the kernel. ", "dev = MKDEV(major, minor);if (MAJOR(dev) != major || MINOR(dev) != minor)return -EOVERFLOW;} else ": "dm_get_device(struct dm_target  ti, const char  path, blk_mode_t mode,  struct dm_dev   result){int r;dev_t dev;unsigned int major, minor;char dummy;struct dm_dev_internal  dd;struct dm_table  t = ti->table;BUG_ON(!t);if (sscanf(path, \"%u:%u%c\", &major, &minor, &dummy) == 2) {  Extract the majorminor numbers ", "if (t->depth >= 2)kvfree(t->index[t->depth - 2]);/* free the targets ": "dm_put_device call missing for %s\",       dm_device_name(md), dd->dm_dev->name);dm_put_table_device(md, dd->dm_dev);kfree(dd);}}static void dm_table_destroy_crypto_profile(struct dm_table  t);void dm_table_destroy(struct dm_table  t){if (!t)return;  free the indexes ", "return;if (bitmap->mddev->bitmap_info.external)return;if (!bitmap->storage.sb_page) /* no superblock ": "md_bitmap_update_sb(struct bitmap  bitmap){bitmap_super_t  sb;if (!bitmap || !bitmap->mddev)   no bitmap for this array ", "for (i = 0; i < bitmap->storage.file_pages; i++) ": "md_bitmap_unplug(struct bitmap  bitmap){unsigned long i;int dirty, need_write;int writing = 0;if (!md_bitmap_enabled(bitmap))return;  look at each page to see if there are any set bits that need to be   flushed out to disk ", "prepare_to_wait(&bitmap->overflow_wait, &__wait,TASK_UNINTERRUPTIBLE);spin_unlock_irq(&bitmap->counts.lock);schedule();finish_wait(&bitmap->overflow_wait, &__wait);continue;}switch (*bmc) ": "md_bitmap_startwrite(struct bitmap  bitmap, sector_t offset, unsigned long sectors, int behind){if (!bitmap)return 0;if (behind) {int bw;atomic_inc(&bitmap->behind_writes);bw = atomic_read(&bitmap->behind_writes);if (bw > bitmap->behind_writes_used)bitmap->behind_writes_used = bw;pr_debug(\"inc write-behind count %d%lu\\n\", bw, bitmap->mddev->bitmap_info.max_write_behind);}while (sectors) {sector_t blocks;bitmap_counter_t  bmc;spin_lock_irq(&bitmap->counts.lock);bmc = md_bitmap_get_counter(&bitmap->counts, offset, &blocks, 1);if (!bmc) {spin_unlock_irq(&bitmap->counts.lock);return 0;}if (unlikely(COUNTER( bmc) == COUNTER_MAX)) {DEFINE_WAIT(__wait);  note that it is safe to do the prepare_to_wait   after the test as long as we do it before dropping   the spinlock. ", "int rv = 0;sector_t blocks1;*blocks = 0;while (*blocks < (PAGE_SIZE>>9)) ": "md_bitmap_start_sync(struct bitmap  bitmap, sector_t offset, sector_t  blocks, int degraded){  bitmap_start_sync must always report on multiples of whole   pages, otherwise resync (which is very PAGE_SIZE based) will   get confused.   So call __bitmap_start_sync repeatedly (if needed) until   At least PAGE_SIZE>>9 blocks are covered.   Return the 'or' of the result. ", "if (RESYNC(*bmc)) ": "md_bitmap_end_sync(struct bitmap  bitmap, sector_t offset, sector_t  blocks, int aborted){bitmap_counter_t  bmc;unsigned long flags;if (bitmap == NULL) { blocks = 1024;return;}spin_lock_irqsave(&bitmap->counts.lock, flags);bmc = md_bitmap_get_counter(&bitmap->counts, offset, blocks, 0);if (bmc == NULL)goto unlock;  locked ", "sector_t sector = 0;sector_t blocks;if (!bitmap)return;while (sector < bitmap->mddev->resync_max_sectors) ": "md_bitmap_close_sync(struct bitmap  bitmap){  Sync has finished, and any bitmap chunks that weren't synced   properly have been aborted.  It remains to us to clear the   RESYNC bit wherever it is still on ", "return;if (bitmap->sysfs_can_clear)sysfs_put(bitmap->sysfs_can_clear);if (mddev_is_clustered(bitmap->mddev) && bitmap->mddev->cluster_info &&bitmap->cluster_slot == md_cluster_ops->slot_number(bitmap->mddev))md_cluster_stop(bitmap->mddev);/* Shouldn't be needed - but just in case.... ": "md_bitmap_free(struct bitmap  bitmap){unsigned long k, pages;struct bitmap_page  bp;if (!bitmap)   there was no bitmap ", "if (bio->bi_opf & REQ_NOWAIT) ": "md_handle_request(struct mddev  mddev, struct bio  bio){check_suspended:if (is_suspended(mddev, bio)) {DEFINE_WAIT(__wait);  Bail out if REQ_NOWAIT is set for the bio ", "wait_event_lock_irq(mddev->sb_wait,    !mddev->flush_bio ||    ktime_before(req_start, mddev->prev_flush_start),    mddev->lock);/* new request after previous flush is completed ": "md_flush_request(struct mddev  mddev, struct bio  bio){ktime_t req_start = ktime_get_boottime();spin_lock_irq(&mddev->lock);  flush requests wait until ongoing flush completes,   hence coalescing all the pending requests. ", "if (!mddev->gendisk || blk_get_integrity(mddev->gendisk))return 0; /* shouldn't register, or already is ": "md_integrity_register(struct mddev  mddev){struct md_rdev  rdev,  reference = NULL;if (list_empty(&mddev->disks))return 0;   nothing to do ", "return 0;if (blk_integrity_compare(mddev->gendisk, rdev->bdev->bd_disk) != 0) ": "md_integrity_add_rdev(struct md_rdev  rdev, struct mddev  mddev){struct blk_integrity  bi_mddev;if (!mddev->gendisk)return 0;bi_mddev = blk_get_integrity(mddev->gendisk);if (!bi_mddev)   nothing to do ", "if (!does_sb_need_changing(mddev)) ": "md_update_sb(struct mddev  mddev, int force_change){struct md_rdev  rdev;int sync_req;int nospares = 0;int any_badblocks_changed = 0;int ret = -1;if (!md_is_rdwr(mddev)) {if (force_change)set_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);return;}repeat:if (mddev_is_clustered(mddev)) {if (test_and_clear_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags))force_change = 1;if (test_and_clear_bit(MD_SB_CHANGE_CLEAN, &mddev->sb_flags))nospares = 1;ret = md_cluster_ops->metadata_update_start(mddev);  Has someone else has updated the sb ", "#define MD_DEFAULT_MAX_CORRECTED_READ_ERRORS 20/* Default safemode delay: 200 msec ": "md_wakeup_thread_directly(struct md_thread __rcu  thread);    Default number of read corrections we'll attempt on an rdev   before ejecting it from the array. We divide the read error   count by 2 for every hour elapsed between read errors. ", "mddev->reshape_position = save_rp;set_bit(MD_RECOVERY_INTR, &mddev->recovery);md_reap_sync_thread(mddev);}mddev_unlock(mddev);}} else if (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))return -EBUSY;else if (cmd_match(page, \"resync\"))clear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);else if (cmd_match(page, \"recover\")) ": "md_check_recovery. ", "struct bio *bio;if (!page)return;if (test_bit(Faulty, &rdev->flags))return;bio = bio_alloc_bioset(rdev->meta_bdev ? rdev->meta_bdev : rdev->bdev,       1,       REQ_OP_WRITE | REQ_SYNC | REQ_PREFLUSH | REQ_FUA,       GFP_NOIO, &mddev->sync_set);atomic_inc(&rdev->nr_pending);bio->bi_iter.bi_sector = sector;__bio_add_page(bio, page, size, 0);bio->bi_private = rdev;bio->bi_end_io = super_written;if (test_bit(MD_FAILFAST_SUPPORTED, &mddev->flags) &&    test_bit(FailFast, &rdev->flags) &&    !test_bit(LastDev, &rdev->flags))bio->bi_opf |= MD_FAILFAST;atomic_inc(&mddev->pending_writes);submit_bio(bio);}int md_super_wait(struct mddev *mddev)": "md_error(mddev, rdev);if (!test_bit(Faulty, &rdev->flags)    && (bio->bi_opf & MD_FAILFAST)) {set_bit(MD_SB_NEED_REWRITE, &mddev->sb_flags);set_bit(LastDev, &rdev->flags);}} elseclear_bit(LastDev, &rdev->flags);bio_put(bio);rdev_dec_pending(rdev, mddev);if (atomic_dec_and_test(&mddev->pending_writes))wake_up(&mddev->sb_wait);}void md_super_write(struct mddev  mddev, struct md_rdev  rdev,   sector_t sector, int size, struct page  page){  write first size bytes of page to sector of rdev   Increment mddev->pending_writes before returning   and decrement it on completion, waking up sb_wait   if zero is reached.   If an error occurred, call md_error ", "atomic_sub(blocks, &mddev->recovery_active);wake_up(&mddev->recovery_wait);if (!ok) ": "md_done_sync(struct mddev  mddev, int blocks, int ok){  another \"blocks\" (512byte) blocks have been synced ", "if (!ro && mddev->ro == MD_RDONLY && mddev->pers) ": "md_write_start and which are not ready for writes yet. ", "void md_write_inc(struct mddev *mddev, struct bio *bi)": "md_write_end().   Unlike md_write_start(), it is safe to call md_write_inc() inside   a spinlocked region. ", "clear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);}mddev_unlock(mddev);}if (err)return err;sysfs_notify_dirent_safe(mddev->sysfs_degraded);} else ": "md_reap_sync_thread(mddev);}mddev_unlock(mddev);}} else if (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))return -EBUSY;else if (cmd_match(page, \"resync\"))clear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);else if (cmd_match(page, \"recover\")) {clear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);set_bit(MD_RECOVERY_RECOVER, &mddev->recovery);} else if (cmd_match(page, \"reshape\")) {int err;if (mddev->pers->start_reshape == NULL)return -EINVAL;err = mddev_lock(mddev);if (!err) {if (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery)) {err =  -EBUSY;} else if (mddev->reshape_position == MaxSector ||   mddev->pers->check_reshape == NULL ||   mddev->pers->check_reshape(mddev)) {clear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);err = mddev->pers->start_reshape(mddev);} else {    If reshape is still in progress, and   md_check_recovery() can continue to reshape,   don't restart reshape because data can be   corrupted for raid456. ", "struct md_rdev *rdev;rdev_for_each(rdev, mddev) ": "md_finish_reshape(struct mddev  mddev){  called be personality module when reshape completes. ", "rdev_for_each_rcu(iter, mddev) ": "md_reload_sb(struct mddev  mddev, int nr){struct md_rdev  rdev = NULL,  iter;int err;  Find the rdev ", "if (data->feat & F_RDACS_WONLY)data->rdac_cache[i] = data->max_pos / 2;}if (data->feat & F_CMD_INC)err = sysfs_create_group(&dev->kobj, &ad525x_group_commands);if (err) ": "ad_dpot_remove_files(struct device  dev,unsigned int features, unsigned int rdac){sysfs_remove_file(&dev->kobj,dpot_attrib_wipers[rdac]);if (features & F_CMD_EEP)sysfs_remove_file(&dev->kobj,dpot_attrib_eeprom[rdac]);if (features & F_CMD_TOL)sysfs_remove_file(&dev->kobj,dpot_attrib_tolerance[rdac]);if (features & F_CMD_OTP) {sysfs_remove_file(&dev->kobj,dpot_attrib_otp_en[rdac]);sysfs_remove_file(&dev->kobj,dpot_attrib_otp[rdac]);}}int ad_dpot_probe(struct device  dev,struct ad_dpot_bus_data  bdata, unsigned long devid,    const char  name){struct dpot_data  data;int i, err = 0;data = kzalloc(sizeof(struct dpot_data), GFP_KERNEL);if (!data) {err = -ENOMEM;goto exit;}dev_set_drvdata(dev, data);mutex_init(&data->update_lock);data->bdata =  bdata;data->devid = devid;data->max_pos = 1 << DPOT_MAX_POS(devid);data->rdac_mask = data->max_pos - 1;data->feat = DPOT_FEAT(devid);data->uid = DPOT_UID(devid);data->wipers = DPOT_WIPERS(devid);for (i = DPOT_RDAC0; i < MAX_RDACS; i++)if (data->wipers & (1 << i)) {err = ad_dpot_add_files(dev, data->feat, i);if (err)goto exit_remove_files;  power-up midscale ", "c2dev->access = c2dev->flash_access = 0;ops->access(c2dev, 0);dev_info(c2dev->dev, \"C2 port %s added\\n\", name);dev_info(c2dev->dev, \"%s flash has %d blocks x %d bytes \"\"(%d bytes total)\\n\",name, ops->blocks_num, ops->block_size,ops->blocks_num * ops->block_size);return c2dev;error_device_create:spin_lock_irq(&c2port_idr_lock);idr_remove(&c2port_idr, c2dev->id);spin_unlock_irq(&c2port_idr_lock);error_idr_alloc:kfree(c2dev);return ERR_PTR(ret);}EXPORT_SYMBOL(c2port_device_register": "c2port_device_register(char  name,struct c2port_ops  ops, void  devdata){struct c2port_device  c2dev;int ret;if (unlikely(!ops) || unlikely(!ops->access) || \\unlikely(!ops->c2d_dir) || unlikely(!ops->c2ck_set) || \\unlikely(!ops->c2d_get) || unlikely(!ops->c2d_set))return ERR_PTR(-EINVAL);c2dev = kzalloc(sizeof(struct c2port_device), GFP_KERNEL);if (unlikely(!c2dev))return ERR_PTR(-ENOMEM);idr_preload(GFP_KERNEL);spin_lock_irq(&c2port_idr_lock);ret = idr_alloc(&c2port_idr, c2dev, 0, 0, GFP_NOWAIT);spin_unlock_irq(&c2port_idr_lock);idr_preload_end();if (ret < 0)goto error_idr_alloc;c2dev->id = ret;bin_attr_flash_data.size = ops->blocks_num   ops->block_size;c2dev->dev = device_create(c2port_class, NULL, 0, c2dev,   \"c2port%d\", c2dev->id);if (IS_ERR(c2dev->dev)) {ret = PTR_ERR(c2dev->dev);goto error_device_create;}dev_set_drvdata(c2dev->dev, c2dev);strncpy(c2dev->name, name, C2PORT_NAME_LEN - 1);c2dev->ops = ops;mutex_init(&c2dev->mutex);  By default C2 port access is off ", "void __iomem *fwnode_iomap(struct fwnode_handle *fwnode, int index)": "fwnode_iomap - Maps the memory mapped IO for a given fwnode   @fwnode:Pointer to the firmware node   @index:Index of the IO range     Return: a pointer to the mapped memory. ", "int fwnode_irq_get(const struct fwnode_handle *fwnode, unsigned int index)": "fwnode_irq_get - Get IRQ directly from a fwnode   @fwnode:Pointer to the firmware node   @index:Zero-based index of the IRQ     Return: Linux IRQ number on success. Negative errno on failure. ", "int fwnode_irq_get_byname(const struct fwnode_handle *fwnode, const char *name)": "fwnode_irq_get_byname - Get IRQ from a fwnode using its name   @fwnode:Pointer to the firmware node   @name:IRQ name     Description:   Find a match to the string @name in the 'interrupt-names' string array   in _DSD for ACPI, or of_node for Device Tree. Then get the Linux IRQ   number of the IRQ resource corresponding to the index of the matched   string.     Return: Linux IRQ number on success, or negative errno otherwise. ", "if (fwnode_ep.id < endpoint ||    (best_ep && best_ep_id < fwnode_ep.id))continue;fwnode_handle_put(best_ep);best_ep = fwnode_handle_get(ep);best_ep_id = fwnode_ep.id;}return best_ep;}EXPORT_SYMBOL_GPL(fwnode_graph_get_endpoint_by_id);/** * fwnode_graph_get_endpoint_count - Count endpoints on a device node * @fwnode: The node related to a device * @flags: fwnode lookup flags * Count endpoints in a device node. * * If FWNODE_GRAPH_DEVICE_DISABLED flag is specified, also unconnected endpoints * and endpoints connected to disabled devices are counted. ": "fwnode_graph_parse_endpoint(ep, &fwnode_ep);if (ret < 0)continue;if (fwnode_ep.port != port)continue;if (fwnode_ep.id == endpoint)return ep;if (!endpoint_next)continue;    If the endpoint that has just been found is not the first   matching one and the ID of the one found previously is closer   to the requested endpoint ID, skip it. ", "int component_compare_of(struct device *dev, void *data)": "component_match_add_release()     A common compare function when compare_data is device of_node. e.g.   component_match_add_release(masterdev, &match, component_release_of,   component_compare_of, component_dev_of_node) ", "void component_match_add_release(struct device *parent,struct component_match **matchptr,void (*release)(struct device *, void *),int (*compare)(struct device *, void *), void *compare_data)": "component_match_add_typed(). ", "const char *dev_driver_string(const struct device *dev)": "dev_driver_string - Return a device's driver name, if at all possible   @dev: struct device to get the name of     Will return the device's driver's name if it is bound to a device.  If   the device is not bound to a driver, it will return the name of the bus   it is attached to.  If it is not attached to a bus either, an empty   string will be returned. ", "static void __fwnode_link_del(struct fwnode_link *link)": "function is called. ", "char *devm_kvasprintf(struct device *dev, gfp_t gfp, const char *fmt,      va_list ap)": "devm_kvasprintf - Allocate resource managed space and format a string       into that.   @dev: Device to allocate memory for   @gfp: the GFP mask used in the devm_kmalloc() call when         allocating memory   @fmt: The printf()-style format string   @ap: Arguments for the format string   RETURNS:   Pointer to allocated string on success, NULL on failure. ", "}ret = alloc_lookup_fw_priv(name, &fw_cache, &fw_priv, dbuf, size,   offset, opt_flags);/* * bind with 'priv' now to avoid warning in failure path * of requesting firmware. ": "request_firmware_prepare(struct firmware   firmware_p, const char  name,  struct device  device, void  dbuf, size_t size,  size_t offset, u32 opt_flags){struct firmware  firmware;struct fw_priv  fw_priv;int ret; firmware_p = firmware = kzalloc(sizeof( firmware), GFP_KERNEL);if (!firmware) {dev_err(device, \"%s: kmalloc(struct firmware) failed\\n\",__func__);return -ENOMEM;}if (firmware_request_builtin_buf(firmware, name, dbuf, size)) {dev_dbg(device, \"using built-in %s\\n\", name);return 0;   assigned ", "int firmware_request_cache(struct device *device, const char *name)": "request_firmware_nowait() with no uevent set.  ", "intrequest_partial_firmware_into_buf(const struct firmware **firmware_p,  const char *name, struct device *device,  void *buf, size_t size, size_t offset)": "request_partial_firmware_into_buf() - load partial firmware into a previously allocated buffer   @firmware_p: pointer to firmware image   @name: name of firmware file   @device: device for which firmware is being loaded and DMA region allocated   @buf: address of buffer to load firmware into   @size: size of buffer   @offset: offset into file to read     This function works pretty much like request_firmware_into_buf except   it allows a partial read of the file. ", "static void fw_abort_batch_reqs(struct firmware *fw)": "release_firmware().     Failed batched requests are possible as well, in such cases we just share   the struct fw_priv and won't release it until all requests are woken   and have gone through this same path. ", "if (capable(CAP_SYS_RAWIO))return true;/* Anybody who can open the device can do a read-safe command ": "scsi_cmd_allowed(unsigned char  cmd, bool open_for_write){  root can do any command. ", "static int ioctl_probe(struct Scsi_Host *host, void __user *buffer)": "scsi_ioctl.h>#include <scsisg.h>#include <scsiscsi_dbg.h>#include \"scsi_logging.h\"#define NORMAL_RETRIES5#define IOCTL_NORMAL_TIMEOUT(10   HZ)#define MAX_BUF PAGE_SIZE     ioctl_probe  --  return host identification   @host:host to identify   @buffer:userspace buffer for identification     Return an identifying string at @buffer, if @buffer is non-NULL, filling   to the length stored at   (int  ) @buffer. ", "for (k = 0; k < len; ++k) ": "__scsi_format_command(char  logbuf, size_t logbuf_len,     const unsigned char  cdb, size_t cdb_len){int len, k;size_t off;off = scsi_format_opcode_name(logbuf, logbuf_len, cdb);if (off >= logbuf_len)return off;len = scsi_command_size(cdb);if (cdb_len < len)len = cdb_len;  print out all bytes in cdb ", "if (cmd->cmd_len > 16) ": "scsi_print_command(struct scsi_cmnd  cmd){int k;char  logbuf;size_t off, logbuf_len;logbuf = scsi_log_reserve_buffer(&logbuf_len);if (!logbuf)return;off = sdev_format_header(logbuf, logbuf_len, scmd_name(cmd), scsi_cmd_to_rq(cmd)->tag);if (off >= logbuf_len)goto out_printk;off += scnprintf(logbuf + off, logbuf_len - off, \"CDB: \");if (WARN_ON(off >= logbuf_len))goto out_printk;off += scsi_format_opcode_name(logbuf + off, logbuf_len - off,       cmd->cmnd);if (off >= logbuf_len)goto out_printk;  print out all bytes in cdb ", "int scsi_change_queue_depth(struct scsi_device *sdev, int depth)": "scsi_change_queue_depth - change a device's queue depth   @sdev: SCSI Device in question   @depth: number of commands allowed to be queued to the driver     Sets the device queue depth and returns the new value. ", "int scsi_track_queue_full(struct scsi_device *sdev, int depth)": "scsi_track_queue_full - track QUEUE_FULL events to adjust queue depth   @sdev: SCSI Device in question   @depth: Current number of outstanding SCSI commands on this device,           not counting the one returned as QUEUE_FULL.     Description:This function will track successive QUEUE_FULL events on a   specific SCSI device to determine if and when there is a   need to adjust the queue depth on the device.     Returns:0 - No change needed, >0 - Adjust queue depth to this new depth,   -1 - Drop back to untagged operation using host->cmd_per_lun   as the untagged command depth     Lock Status:None held on entry     Notes:Low level drivers may call this at any time and we will do   \"The Right Thing.\"  We are interrupt context safe. ", "int scsi_report_opcode(struct scsi_device *sdev, unsigned char *buffer,       unsigned int len, unsigned char opcode,       unsigned short sa)": "scsi_report_opcode - Find out if a given command is supported   @sdev:scsi device to query   @buffer:scratch buffer (must be at least 20 bytes long)   @len:length of buffer   @opcode:opcode for the command to look up   @sa:service action for the command to look up     Uses the REPORT SUPPORTED OPERATION CODES to check support for the   command identified with @opcode and @sa. If the command does not   have a service action, @sa must be 0. Returns -EINVAL if RSOC fails,   0 if the command is not supported and 1 if the device claims to   support the command. ", "int scsi_device_get(struct scsi_device *sdev)": "scsi_device_get  -  get an additional reference to a scsi_device   @sdev:device to get a reference to     Description: Gets a reference to the scsi_device and increments the use count   of the underlying LLDD module.  You must hold host_lock of the   parent Scsi_Host or already have a reference when calling this.     This will fail if a device is deleted or cancelled, or when the LLD module   is in the process of being unloaded. ", "void scsi_device_put(struct scsi_device *sdev)": "scsi_device_put  -  release a reference to a scsi_device   @sdev:device to release a reference on.     Description: Release a reference to the scsi_device and decrements the use   count of the underlying LLDD module.  The device is freed once the last   user vanishes. ", "if (!scsi_device_get(next))break;next = NULL;list = list->next;}spin_unlock_irqrestore(shost->host_lock, flags);if (prev)scsi_device_put(prev);return next;}EXPORT_SYMBOL(__scsi_iterate_devices": "__scsi_iterate_devices(struct Scsi_Host  shost,   struct scsi_device  prev){struct list_head  list = (prev ? &prev->siblings : &shost->__devices);struct scsi_device  next = NULL;unsigned long flags;spin_lock_irqsave(shost->host_lock, flags);while (list->next != &shost->__devices) {next = list_entry(list->next, struct scsi_device, siblings);  skip devices that we can't get a reference to ", "void starget_for_each_device(struct scsi_target *starget, void *data,     void (*fn)(struct scsi_device *, void *))": "starget_for_each_device  -  helper to walk all devices of a target   @starget:target whose devices we want to iterate over.   @data:Opaque passed to each function call.   @fn:Function to call on each device     This traverses over each device of @starget.  The devices have   a reference that must be released by scsi_host_put when breaking   out of the loop. ", "void __starget_for_each_device(struct scsi_target *starget, void *data,       void (*fn)(struct scsi_device *, void *))": "__starget_for_each_device - helper to walk all devices of a target (UNLOCKED)   @starget:target whose devices we want to iterate over.   @data:parameter for callback @fn()   @fn:callback function that is invoked for each device     This traverses over each device of @starget.  It does _not_   take a reference on the scsi_device, so the whole loop must be   protected by shost->host_lock.     Note:  The only reason why drivers would want to use this is because   they need to access the device list in irq context.  Otherwise you   really want to use starget_for_each_device instead.  ", "struct scsi_device *__scsi_device_lookup_by_target(struct scsi_target *starget,   u64 lun)": "scsi_device_lookup_by_target - find a device given the target (UNLOCKED)   @starget:SCSI target pointer   @lun:SCSI Logical Unit Number     Description: Looks up the scsi_device with the specified @lun for a given   @starget.  The returned scsi_device does not have an additional   reference.  You must hold the host's host_lock over this call and   any access to the returned scsi_device. A scsi_device in state   SDEV_DEL is skipped.     Note:  The only reason why drivers should use this is because   they need to access the device list in irq context.  Otherwise you   really want to use scsi_device_lookup_by_target instead.  ", "int scsi_block_when_processing_errors(struct scsi_device *sdev)": "scsi_block_when_processing_errors - Prevent cmds from being queued.   @sdev:Device on which we are performing recovery.     Description:       We block until the host is out of error recovery, and then check to       see whether the host or the device is offline.     Return value:       0 when dev was taken offline by error recovery. 1 OK to proceed. ", "void scsi_eh_prep_cmnd(struct scsi_cmnd *scmd, struct scsi_eh_save *ses,unsigned char *cmnd, int cmnd_size, unsigned sense_bytes)": "scsi_eh_prep_cmnd  - Save a scsi command info as part of error recovery   @scmd:       SCSI command structure to hijack   @ses:        structure to save restore information   @cmnd:       CDB to send. Can be NULL if no new cmnd is needed   @cmnd_size:  size in bytes of @cmnd (must be <= MAX_COMMAND_SIZE)   @sense_bytes: size of sense data to copy. or 0 (if != 0 @cmnd is ignored)     This function is used to save a scsi command information before re-execution   as part of the error recovery process.  If @sense_bytes is 0 the command   sent must be one that does not transfer any data.  If @sense_bytes != 0   @cmnd is ignored and this functions sets up a REQUEST_SENSE command   and cmnd buffers to read @sense_bytes into @scmd->sense_buffer. ", "void scsi_eh_restore_cmnd(struct scsi_cmnd* scmd, struct scsi_eh_save *ses)": "scsi_eh_restore_cmnd  - Restore a scsi command info as part of error recovery   @scmd:       SCSI command structure to restore   @ses:        saved information from a coresponding call to scsi_eh_prep_cmnd     Undo any damage done by above scsi_eh_prep_cmnd(). ", "void scsi_eh_finish_cmd(struct scsi_cmnd *scmd, struct list_head *done_q)": "scsi_eh_finish_cmd - Handle a cmd that eh is finished with.   @scmd:Original SCSI cmd that eh has finished.   @done_q:Queue for processed commands.     Notes:      We don't want to use the normal command completion while we are are      still handling errors - it may cause other commands to be queued,      and that would disturb what we are doing.  Thus we really want to      keep a list of pending commands for final completion, and once we      are ready to leave error handling we handle completion for real. ", "void scsi_eh_flush_done_q(struct list_head *done_q)": "scsi_eh_flush_done_q - finish processed commands or retry them.   @done_q:list_head of processed commands. ", "static enum scsi_disposition scsi_try_bus_reset(struct scsi_cmnd *scmd)": "scsi_report_bus_reset(host, scmd_channel(scmd));spin_unlock_irqrestore(host->host_lock, flags);}return rtn;}     scsi_try_bus_reset - ask host to perform a bus reset   @scmd:SCSI cmd to send bus reset. ", "static enum scsi_disposition scsi_try_target_reset(struct scsi_cmnd *scmd)": "scsi_report_device_reset(struct scsi_device  sdev, void  data){sdev->was_reset = 1;sdev->expecting_cc_ua = 1;}     scsi_try_target_reset - Ask host to perform a target reset   @scmd:SCSI cmd used to send a target reset     Notes:      There is no timeout for this operation.  if this operation is      unreliable for a given host, then the host itself needs to put a      timer on it, and set the host back to a consistent state prior to      returning. ", "scsi_report_sense(sdev, &sshdr);if (scsi_sense_is_deferred(&sshdr))return NEEDS_RETRY;if (sdev->handler && sdev->handler->check_sense) ": "scsi_command_normalize_sense(scmd, &sshdr))return FAILED;  no valid sense data ", "bool scsi_get_sense_info_fld(const u8 *sense_buffer, int sb_len,     u64 *info_out)": "scsi_get_sense_info_fld - get information field from sense data (either fixed or descriptor format)   @sense_buffer:byte array of sense data   @sb_len:number of valid bytes in sense_buffer   @info_out:pointer to 64 integer where 8 or 4 byte information  field will be placed if found.     Return value:  true if information field found, false if not found. ", "void srp_start_tl_fail_timers(struct srp_rport *rport)": "srp_start_tl_fail_timers(struct srp_rport  rport){struct Scsi_Host  shost = rport_to_shost(rport);int delay, fast_io_fail_tmo, dev_loss_tmo;lockdep_assert_held(&rport->mutex);delay = rport->reconnect_delay;fast_io_fail_tmo = rport->fast_io_fail_tmo;dev_loss_tmo = rport->dev_loss_tmo;pr_debug(\"%s current state: %d\\n\", dev_name(&shost->shost_gendev), rport->state);if (rport->state == SRP_RPORT_LOST)return;if (delay > 0)queue_delayed_work(system_long_wq, &rport->reconnect_work,   1UL   delay   HZ);if ((fast_io_fail_tmo >= 0 || dev_loss_tmo >= 0) &&    srp_rport_set_state(rport, SRP_RPORT_BLOCKED) == 0) {pr_debug(\"%s new state: %d\\n\", dev_name(&shost->shost_gendev), rport->state);scsi_block_targets(shost, &shost->shost_gendev);if (fast_io_fail_tmo >= 0)queue_delayed_work(system_long_wq,   &rport->fast_io_fail_work,   1UL   fast_io_fail_tmo   HZ);if (dev_loss_tmo >= 0)queue_delayed_work(system_long_wq,   &rport->dev_loss_work,   1UL   dev_loss_tmo   HZ);}}     srp_start_tl_fail_timers() - start the transport layer failure timers   @rport: SRP target port.     Start the transport layer fast IO failure and device loss timers. Do not   modify a timer that was already started. ", "static void __rport_fail_io_fast(struct srp_rport *rport)": "srp_reconnect_rport(rport);if (res != 0) {shost_printk(KERN_ERR, shost,     \"reconnect attempt %d failed (%d)\\n\",     ++rport->failed_reconnects, res);delay = rport->reconnect_delay  min(100, max(1, rport->failed_reconnects - 10));if (delay > 0)queue_delayed_work(system_long_wq,   &rport->reconnect_work, delay   HZ);}}    scsi_block_targets() must have been called before this function is   called to guarantee that no .queuecommand() calls are in progress. ", "enum scsi_timeout_action srp_timed_out(struct scsi_cmnd *scmd)": "srp_timed_out() - SRP transport intercept of the SCSI timeout EH   @scmd: SCSI command.     If a timeout occurs while an rport is in the blocked state, ask the SCSI   EH to continue waiting (SCSI_EH_RESET_TIMER). Otherwise let the SCSI core   handle the timeout (SCSI_EH_NOT_HANDLED).     Note: This function is called from soft-IRQ context and with the request   queue lock held. ", "void srp_rport_get(struct srp_rport *rport)": "srp_rport_get() - increment rport reference count   @rport: SRP target port. ", "void srp_rport_put(struct srp_rport *rport)": "srp_rport_put() - decrement rport reference count   @rport: SRP target port. ", "int scsi_dma_map(struct scsi_cmnd *cmd)": "scsi_dma_map - perform DMA mapping against command's sg lists   @cmd:scsi command     Returns the number of sg lists actually used, zero if the sg lists   is NULL, or -ENOMEM if the mapping failed. ", "void scsi_dma_unmap(struct scsi_cmnd *cmd)": "scsi_dma_unmap - unmap command's sg lists mapped by scsi_dma_map   @cmd:scsi command ", "u32fc_get_event_number(void)": "fc_get_event_number - Obtain the next sequential FC event number     Notes:     We could have inlined this, but it would have required fc_event_seq to     be exposed. For now, live with the subroutine call.     Atomic used to avoid lockunlock... ", "voidfc_host_post_fc_event(struct Scsi_Host *shost, u32 event_number,enum fc_host_event_code event_code,u32 data_len, char *data_buf, u64 vendor_id)": "fc_host_post_fc_event - routine to do the work of posting an event                        on an fc_host.   @shost:host the event occurred on   @event_number:fc event number obtained from get_fc_event_number()   @event_code:fc_host event being posted   @data_len:amount, in bytes, of event data   @data_buf:pointer to event data   @vendor_id:          value for Vendor id     Notes:  This routine assumes no locks are held on entry. ", "voidfc_host_post_event(struct Scsi_Host *shost, u32 event_number,enum fc_host_event_code event_code, u32 event_data)": "fc_host_post_event - called to post an even on an fc_host.   @shost:host the event occurred on   @event_number:fc event number obtained from get_fc_event_number()   @event_code:fc_host event being posted   @event_data:32bits of data for the event being posted     Notes:  This routine assumes no locks are held on entry. ", "voidfc_host_post_vendor_event(struct Scsi_Host *shost, u32 event_number,u32 data_len, char * data_buf, u64 vendor_id)": "fc_host_post_vendor_event - called to post a vendor unique event                        on an fc_host   @shost:host the event occurred on   @event_number:fc event number obtained from get_fc_event_number()   @data_len:amount, in bytes, of vendor unique data   @data_buf:pointer to vendor unique data   @vendor_id:          Vendor id     Notes:  This routine assumes no locks are held on entry. ", "struct fc_rport *fc_find_rport_by_wwpn(struct Scsi_Host *shost, u64 wwpn)": "fc_find_rport_by_wwpn - find the fc_rport pointer for a given wwpn   @shost:host the fc_rport is associated with   @wwpn:wwpn of the fc_rport device     Notes:  This routine assumes no locks are held on entry. ", "voidfc_host_fpin_rcv(struct Scsi_Host *shost, u32 fpin_len, char *fpin_buf,u8 event_acknowledge)": "fc_host_fpin_rcv - routine to process a received FPIN.   @shost:host the FPIN was received on   @fpin_len:length of FPIN payload, in bytes   @fpin_buf:pointer to FPIN payload   @event_acknowledge:1, if LLDD handles this event.   Notes:  This routine assumes no locks are held on entry. ", "enum scsi_timeout_action fc_eh_timed_out(struct scsi_cmnd *scmd)": "fc_eh_timed_out - FC Transport IO timeout intercept handler   @scmd:The SCSI command which timed out     This routine protects against error handlers getting invoked while a   rport is in a blocked state, typically due to a temporarily loss of   connectivity. If the error handlers are allowed to proceed, requests   to abort io, reset the target, etc will likely fail as there is no way   to communicate with the device to perform the requested function. These   failures may result in the midlayer taking the device offline, requiring   manual intervention to restore operation.     This routine, called whenever an io times out, validates the state of   the underlying rport. If the rport is blocked, it returns   EH_RESET_TIMER, which will continue to reschedule the timeout.   Eventually, either the device will return, or devloss_tmo will fire,   and when the timeout then fires, it will be handled normally.   If the rport is not blocked, normal error handling continues.     Notes:  This routine assumes no locks are held on entry. ", "i->t.create_work_queue = 1;i->t.user_scan = fc_user_scan;/* * Setup SCSI Target Attributes. ": "fc_attach_transport(struct fc_function_template  ft){int count;struct fc_internal  i = kzalloc(sizeof(struct fc_internal),GFP_KERNEL);if (unlikely(!i))return NULL;i->t.target_attrs.ac.attrs = &i->starget_attrs[0];i->t.target_attrs.ac.class = &fc_transport_class.class;i->t.target_attrs.ac.match = fc_target_match;i->t.target_size = sizeof(struct fc_starget_attrs);transport_container_register(&i->t.target_attrs);i->t.host_attrs.ac.attrs = &i->host_attrs[0];i->t.host_attrs.ac.class = &fc_host_class.class;i->t.host_attrs.ac.match = fc_host_match;i->t.host_size = sizeof(struct fc_host_attrs);if (ft->get_fc_host_stats)i->t.host_attrs.statistics = &fc_statistics_group;transport_container_register(&i->t.host_attrs);i->rport_attr_cont.ac.attrs = &i->rport_attrs[0];i->rport_attr_cont.ac.class = &fc_rport_class.class;i->rport_attr_cont.ac.match = fc_rport_match;i->rport_attr_cont.statistics = &fc_rport_statistics_group;transport_container_register(&i->rport_attr_cont);i->vport_attr_cont.ac.attrs = &i->vport_attrs[0];i->vport_attr_cont.ac.class = &fc_vport_class.class;i->vport_attr_cont.ac.match = fc_vport_match;transport_container_register(&i->vport_attr_cont);i->f = ft;  Transport uses the shost workq for scsi scanning ", "voidfc_remove_host(struct Scsi_Host *shost)": "fc_remove_host - called to terminate any fc_transport-related elements for a scsi host.   @shost:Which &Scsi_Host     This routine is expected to be called immediately preceding the   a driver's call to scsi_remove_host().     WARNING: A driver utilizing the fc_transport, which fails to call     this routine prior to scsi_remove_host(), will leave dangling     objects in sysclassfc_remote_ports. Access to any of these     objects can result in a system crash !!!     Notes:  This routine assumes no locks are held on entry. ", "struct fc_rport *fc_remote_port_add(struct Scsi_Host *shost, int channel,struct fc_rport_identifiers  *ids)": "fc_remote_port_add - notify fc transport of the existence of a remote FC port.   @shost:scsi host the remote port is connected to.   @channel:Channel on shost port connected to.   @ids:The world wide names, fc address, and FC4 port  roles for the remote port.     The LLDD calls this routine to notify the transport of the existence   of a remote port. The LLDD provides the unique identifiers (wwpn,wwn)   of the port, it's FC address (port_id), and the FC4 roles that are   active for the port.     For ports that are FCP targets (aka scsi targets), the FC transport   maintains consistent target id bindings on behalf of the LLDD.   A consistent target id binding is an assignment of a target id to   a remote port identifier, which persists while the scsi host is   attached. The remote port can disappear, then later reappear, and   it's target id assignment remains the same. This allows for shifts   in FC addressing (if binding by wwpn or wwnn) with no apparent   changes to the scsi subsystem which is based on scsi host number and   target id values.  Bindings are only valid during the attachment of   the scsi host. If the host detaches, then later re-attaches, target   id bindings may change.     This routine is responsible for returning a remote port structure.   The routine will search the list of remote ports it maintains   internally on behalf of consistent target id mappings. If found, the   remote port structure will be reused. Otherwise, a new remote port   structure will be allocated.     Whenever a remote port is allocated, a new fc_remote_port class   device is created.     Should not be called from interrupt context.     Notes:  This routine assumes no locks are held on entry. ", "voidfc_remote_port_delete(struct fc_rport  *rport)": "fc_remote_port_delete - notifies the fc transport that a remote port is no longer in existence.   @rport:The remote port that no longer exists     The LLDD calls this routine to notify the transport that a remote   port is no longer part of the topology. Note: Although a port   may no longer be part of the topology, it may persist in the remote   ports displayed by the fc_host. We do this under 2 conditions:     1) If the port was a scsi target, we delay its deletion by \"blocking\" it.      This allows the port to temporarily disappear, then reappear without      disrupting the SCSI device tree attached to it. During the \"blocked\"      period the port will still exist.     2) If the port was a scsi target and disappears for longer than we      expect, we'll delete the port and the tear down the SCSI device tree      attached to it. However, we want to semi-persist the target id assigned      to that port if it eventually does exist. The port structure will      remain (although with minimal information) so that the target id      bindings also remain.     If the remote port is not an FCP Target, it will be fully torn down   and deallocated, including the fc_remote_port class device.     If the remote port is an FCP Target, the port will be placed in a   temporary blocked state. From the LLDD's perspective, the rport no   longer exists. From the SCSI midlayer's perspective, the SCSI target   exists, but all sdevs on it are blocked from further IO. The following   is then expected.       If the remote port does not return (signaled by a LLDD call to     fc_remote_port_add()) within the dev_loss_tmo timeout, then the     scsi target is removed - killing all outstanding io and removing the     scsi devices attached to it. The port structure will be marked Not     Present and be partially cleared, leaving only enough information to     recognize the remote port relative to the scsi target id binding if     it later appears.  The port will remain as long as there is a valid     binding (e.g. until the user changes the binding type or unloads the     scsi host with the binding).       If the remote port returns within the dev_loss_tmo value (and matches     according to the target id binding type), the port structure will be     reused. If it is no longer a SCSI target, the target will be torn     down. If it continues to be a SCSI target, then the target will be     unblocked (allowing io to be resumed), and a scan will be activated     to ensure that all luns are detected.     Called from normal process context only - cannot be called from interrupt.     Notes:  This routine assumes no locks are held on entry. ", "rport = fc_remote_port_create(shost, channel, ids);return rport;}EXPORT_SYMBOL(fc_remote_port_add);/** * fc_remote_port_delete - notifies the fc transport that a remote port is no longer in existence. * @rport:The remote port that no longer exists * * The LLDD calls this routine to notify the transport that a remote * port is no longer part of the topology. Note: Although a port * may no longer be part of the topology, it may persist in the remote * ports displayed by the fc_host. We do this under 2 conditions: * * 1) If the port was a scsi target, we delay its deletion by \"blocking\" it. *    This allows the port to temporarily disappear, then reappear without *    disrupting the SCSI device tree attached to it. During the \"blocked\" *    period the port will still exist. * * 2) If the port was a scsi target and disappears for longer than we *    expect, we'll delete the port and the tear down the SCSI device tree *    attached to it. However, we want to semi-persist the target id assigned *    to that port if it eventually does exist. The port structure will *    remain (although with minimal information) so that the target id *    bindings also remain. * * If the remote port is not an FCP Target, it will be fully torn down * and deallocated, including the fc_remote_port class device. * * If the remote port is an FCP Target, the port will be placed in a * temporary blocked state. From the LLDD's perspective, the rport no * longer exists. From the SCSI midlayer's perspective, the SCSI target * exists, but all sdevs on it are blocked from further I/O. The following * is then expected. * *   If the remote port does not return (signaled by a LLDD call to *   fc_remote_port_add()) within the dev_loss_tmo timeout, then the *   scsi target is removed - killing all outstanding i/o and removing the *   scsi devices attached to it. The port structure will be marked Not *   Present and be partially cleared, leaving only enough information to *   recognize the remote port relative to the scsi target id binding if *   it later appears.  The port will remain as long as there is a valid *   binding (e.g. until the user changes the binding type or unloads the *   scsi host with the binding). * *   If the remote port returns within the dev_loss_tmo value (and matches *   according to the target id binding type), the port structure will be *   reused. If it is no longer a SCSI target, the target will be torn *   down. If it continues to be a SCSI target, then the target will be *   unblocked (allowing i/o to be resumed), and a scan will be activated *   to ensure that all luns are detected. * * Called from normal process context only - cannot be called from interrupt. * * Notes: *This routine assumes no locks are held on entry. ": "fc_remote_port_rolechg(rport, ids->roles);return rport;}}spin_unlock_irqrestore(shost->host_lock, flags);  No consistent binding found - create new remote port entry ", "int fc_block_rport(struct fc_rport *rport)": "fc_block_rport() - Block SCSI eh thread for blocked fc_rport.   @rport: Remote port that scsi_eh is trying to recover.     This routine can be called from a FC LLD scsi_eh callback. It   blocks the scsi_eh thread until the fc_rport leaves the   FC_PORTSTATE_BLOCKED, or the fast_io_fail_tmo fires. This is   necessary to avoid the scsi_eh failing recovery actions for blocked   rports which would lead to offlined SCSI devices.     Returns: 0 if the fc_rport left the state FC_PORTSTATE_BLOCKED.      FAST_IO_FAIL if the fast_io_fail_tmo fired, this should be      passed back to scsi_eh. ", "int fc_block_scsi_eh(struct scsi_cmnd *cmnd)": "fc_block_scsi_eh - Block SCSI eh thread for blocked fc_rport   @cmnd: SCSI command that scsi_eh is trying to recover     This routine can be called from a FC LLD scsi_eh callback. It   blocks the scsi_eh thread until the fc_rport leaves the   FC_PORTSTATE_BLOCKED, or the fast_io_fail_tmo fires. This is   necessary to avoid the scsi_eh failing recovery actions for blocked   rports which would lead to offlined SCSI devices.     Returns: 0 if the fc_rport left the state FC_PORTSTATE_BLOCKED.      FAST_IO_FAIL if the fast_io_fail_tmo fired, this should be      passed back to scsi_eh. ", "struct fc_vport *fc_vport_create(struct Scsi_Host *shost, int channel,struct fc_vport_identifiers *ids)": "fc_vport_create - Admin App or LLDD requests creation of a vport   @shost:scsi host the virtual port is connected to.   @channel:channel on shost port connected to.   @ids:The world wide names, FC4 port roles, etc for                the virtual port.     Notes:  This routine assumes no locks are held on entry. ", "kfree(vport);}static int scsi_is_fc_vport(const struct device *dev)": "fc_vport_terminate(vport);return stat ? stat : count;}static FC_DEVICE_ATTR(host, vport_delete, S_IWUSR, NULL,store_fc_host_vport_delete);static int fc_host_match(struct attribute_container  cont,  struct device  dev){struct Scsi_Host  shost;struct fc_internal  i;if (!scsi_is_host_device(dev))return 0;shost = dev_to_shost(dev);if (!shost->transportt  || shost->transportt->host_attrs.ac.class    != &fc_host_class.class)return 0;i = to_fc_internal(shost->transportt);return &i->t.host_attrs.ac == cont;}static int fc_target_match(struct attribute_container  cont,    struct device  dev){struct Scsi_Host  shost;struct fc_internal  i;if (!scsi_is_target_device(dev))return 0;shost = dev_to_shost(dev->parent);if (!shost->transportt  || shost->transportt->host_attrs.ac.class    != &fc_host_class.class)return 0;i = to_fc_internal(shost->transportt);return &i->t.target_attrs.ac == cont;}static void fc_rport_dev_release(struct device  dev){struct fc_rport  rport = dev_to_rport(dev);put_device(dev->parent);kfree(rport);}int scsi_is_fc_rport(const struct device  dev){return dev->release == fc_rport_dev_release;}EXPORT_SYMBOL(scsi_is_fc_rport);static int fc_rport_match(struct attribute_container  cont,    struct device  dev){struct Scsi_Host  shost;struct fc_internal  i;if (!scsi_is_fc_rport(dev))return 0;shost = dev_to_shost(dev->parent);if (!shost->transportt  || shost->transportt->host_attrs.ac.class    != &fc_host_class.class)return 0;i = to_fc_internal(shost->transportt);return &i->rport_attr_cont.ac == cont;}static void fc_vport_dev_release(struct device  dev){struct fc_vport  vport = dev_to_vport(dev);put_device(dev->parent);  release kobj parent ", "static int period_to_str(char *buf, int period)": "spi_dv_device(to_scsi_device(dev));return 1;}static ssize_tstore_spi_revalidate(struct device  dev, struct device_attribute  attr,     const char  buf, size_t count){struct scsi_target  starget = transport_class_to_starget(dev);device_for_each_child(&starget->dev, NULL, child_iter);return count;}static DEVICE_ATTR(revalidate, S_IWUSR, NULL, store_spi_revalidate);  Translate the period into ns according to the current spec   for SDTRPPR messages ", "voidspi_schedule_dv_device(struct scsi_device *sdev)": "spi_schedule_dv_device - schedule domain validation to occur on the device  @sdev:The device to validate    Identical to spi_dv_device() above, except that the DV will be  scheduled to occur in a workqueue later.  All memory allocations  are atomic, so may be called from any context including those holding  SCSI locks. ", "void spi_display_xfer_agreement(struct scsi_target *starget)": "spi_display_xfer_agreement - Print the current target transfer agreement   @starget: The target for which to display the agreement     Each SPI port is required to maintain a transfer agreement for each   other port on the bus.  This function prints a one-line summary of   the current agreement; more detailed information is available in sysfs. ", "} else if (msg[0] & 0x80) ": "spi_print_msg(const unsigned char  msg){int len = 1, i;if (msg[0] == EXTENDED_MESSAGE) {len = 2 + msg[1];if (len == 2)len += 256;if (msg[2] < ARRAY_SIZE(extended_msgs))printk (\"%s \", extended_msgs[msg[2]]); else printk (\"Extended Message, reserved code (0x%02x) \",(int) msg[2]);switch (msg[2]) {case EXTENDED_MODIFY_DATA_POINTER:print_ptr(msg, 3, \"pointer\");break;case EXTENDED_SDTR:print_nego(msg, 3, 4, 0);break;case EXTENDED_WDTR:print_nego(msg, 0, 0, 3);break;case EXTENDED_PPR:print_nego(msg, 3, 5, 6);break;case EXTENDED_MODIFY_BIDI_DATA_PTR:print_ptr(msg, 3, \"out\");print_ptr(msg, 7, \"in\");break;default:for (i = 2; i < len; ++i) printk(\"%02x \", msg[i]);}  Identify ", "void sas_remove_children(struct device *dev)": "scsi_is_sas_port(dev))sas_port_delete(dev_to_sas_port(dev));else if (pass == 1 && scsi_is_sas_phy(dev))sas_phy_delete(dev_to_phy(dev));return 0;}     sas_remove_children  -  tear down a devices SAS data structures   @dev:device belonging to the sas object     Removes all SAS PHYs and remote PHYs for a given object ", "void sas_remove_host(struct Scsi_Host *shost)": "sas_remove_host  -  tear down a Scsi_Host's SAS data structures   @shost:Scsi Host that is torn down     Removes all SAS PHYs and remote PHYs for a given Scsi_Host and remove the   Scsi_Host as well.     Note: Do not call scsi_remove_host() on the Scsi_Host any more, as it is   already removed. ", "u64 sas_get_address(struct scsi_device *sdev)": "sas_get_address - return the SAS address of the device   @sdev: scsi device     Returns the SAS address of the scsi device ", "struct sas_phy *sas_phy_alloc(struct device *parent, int number)": "sas_phy_alloc  -  allocates and initialize a SAS PHY structure   @parent:Parent device   @number:Phy index     Allocates an SAS PHY structure.  It will be added in the device tree   below the device specified by @parent, which has to be either a Scsi_Host   or sas_rphy.     Returns:  SAS PHY allocated or %NULL if the allocation failed. ", "int sas_phy_add(struct sas_phy *phy)": "sas_phy_add  -  add a SAS PHY to the device hierarchy   @phy:The PHY to be added     Publishes a SAS PHY to the rest of the system. ", "void sas_phy_free(struct sas_phy *phy)": "sas_phy_free  -  free a SAS PHY   @phy:SAS PHY to free     Frees the specified SAS PHY.     Note:     This function must only be called on a PHY that has not     successfully been added using sas_phy_add(). ", "struct sas_port *sas_port_alloc(struct device *parent, int port_id)": "sas_port_alloc - allocate and initialize a SAS port structure     @parent:parent device   @port_id:port number     Allocates a SAS port structure.  It will be added to the device tree   below the device specified by @parent which must be either a Scsi_Host   or a sas_expander_device.     Returns %NULL on error ", "struct sas_port *sas_port_alloc_num(struct device *parent)": "sas_port_alloc_num - allocate and initialize a SAS port structure     @parent:parent device     Allocates a SAS port structure and a number to go with it.  This   interface is really for adapters where the port number has no   meansing, so the sas class should manage them.  It will be added to   the device tree below the device specified by @parent which must be   either a Scsi_Host or a sas_expander_device.     Returns %NULL on error ", "int sas_port_add(struct sas_port *port)": "sas_port_add - add a SAS port to the device hierarchy   @port:port to be added     publishes a port to the rest of the system ", "void sas_port_free(struct sas_port *port)": "sas_port_free  -  free a SAS PORT   @port:SAS PORT to free     Frees the specified SAS PORT.     Note:     This function must only be called on a PORT that has not     successfully been added using sas_port_add(). ", "struct sas_phy *sas_port_get_phy(struct sas_port *port)": "sas_port_get_phy - try to take a reference on a port member   @port: port to check ", "void sas_port_add_phy(struct sas_port *port, struct sas_phy *phy)": "sas_port_add_phy - add another phy to a port to form a wide port   @port:port to add the phy to   @phy:phy to add     When a port is initially created, it is empty (has no phys).  All   ports must have at least one phy to operated, and all wide ports   must have at least two.  The current code makes no difference   between ports and wide ports, but the only object that can be   connected to a remote device is a port, so ports must be formed on   all devices with phys if they're connected to anything. ", "void sas_port_delete_phy(struct sas_port *port, struct sas_phy *phy)": "sas_port_delete_phy - remove a phy from a port or wide port   @port:port to remove the phy from   @phy:phy to remove     This operation is used for tearing down ports again.  It must be   done to every port or wide port before calling sas_port_delete. ", "static void sas_rphy_initialize(struct sas_rphy *rphy)": "scsi_is_sas_rphy(dev))return 0;shost = dev_to_shost(dev->parent->parent);if (!shost->transportt)return 0;if (shost->transportt->host_attrs.ac.class !=&sas_host_class.class)return 0;i = to_sas_internal(shost->transportt);return &i->rphy_attr_cont.ac == cont;}static int sas_end_dev_match(struct attribute_container  cont,     struct device  dev){struct Scsi_Host  shost;struct sas_internal  i;struct sas_rphy  rphy;if (!scsi_is_sas_rphy(dev))return 0;shost = dev_to_shost(dev->parent->parent);rphy = dev_to_rphy(dev);if (!shost->transportt)return 0;if (shost->transportt->host_attrs.ac.class !=&sas_host_class.class)return 0;i = to_sas_internal(shost->transportt);return &i->end_dev_attr_cont.ac == cont &&rphy->identify.device_type == SAS_END_DEVICE;}static int sas_expander_match(struct attribute_container  cont,      struct device  dev){struct Scsi_Host  shost;struct sas_internal  i;struct sas_rphy  rphy;if (!scsi_is_sas_rphy(dev))return 0;shost = dev_to_shost(dev->parent->parent);rphy = dev_to_rphy(dev);if (!shost->transportt)return 0;if (shost->transportt->host_attrs.ac.class !=&sas_host_class.class)return 0;i = to_sas_internal(shost->transportt);return &i->expander_attr_cont.ac == cont &&(rphy->identify.device_type == SAS_EDGE_EXPANDER_DEVICE || rphy->identify.device_type == SAS_FANOUT_EXPANDER_DEVICE);}static void sas_expander_release(struct device  dev){struct sas_rphy  rphy = dev_to_rphy(dev);struct sas_expander_device  edev = rphy_to_expander_device(rphy);put_device(dev->parent);kfree(edev);}static void sas_end_device_release(struct device  dev){struct sas_rphy  rphy = dev_to_rphy(dev);struct sas_end_device  edev = rphy_to_end_device(rphy);put_device(dev->parent);kfree(edev);}     sas_rphy_initialize - common rphy initialization   @rphy:rphy to initialise     Used by both sas_end_device_alloc() and sas_expander_alloc() to   initialise the common rphy component of each. ", "int sas_rphy_add(struct sas_rphy *rphy)": "sas_rphy_add  -  add a SAS remote PHY to the device hierarchy   @rphy:The remote PHY to be added     Publishes a SAS remote PHY to the rest of the system. ", "void sas_rphy_free(struct sas_rphy *rphy)": "sas_rphy_remove()'d) ", "int scsi_is_sas_port(const struct device *dev)": "sas_rphy_delete(port->rphy);port->rphy = NULL;}mutex_lock(&port->phy_list_mutex);list_for_each_entry_safe(phy, tmp_phy, &port->phy_list, port_siblings) {sas_port_delete_link(port, phy);list_del_init(&phy->port_siblings);}mutex_unlock(&port->phy_list_mutex);if (port->is_backlink) {struct device  parent = port->dev.parent;sysfs_remove_link(&port->dev.kobj, dev_name(parent));port->is_backlink = 0;}transport_remove_device(dev);device_del(dev);transport_destroy_device(dev);put_device(dev);}EXPORT_SYMBOL(sas_port_delete);     scsi_is_sas_port -  check if a struct device represents a SAS port   @dev:device to check     Returns:  %1 if the device represents a SAS Port, %0 else ", "void sas_rphy_unlink(struct sas_rphy *rphy)": "sas_rphy_unlink  -  unlink SAS remote PHY   @rphy:SAS remote phy to unlink from its parent port     Removes port reference to an rphy ", "struct scsi_transport_template *sas_attach_transport(struct sas_function_template *ft)": "sas_attach_transport  -  instantiate SAS transport template   @ft:SAS transport class function template ", "void sas_release_transport(struct scsi_transport_template *t)": "sas_release_transport  -  release SAS transport template instance   @t:transport template instance ", "ssleep(esp_bus_reset_settle);err = scsi_add_host(esp->host, esp->dev);if (err)return err;instance++;scsi_scan_host(esp->host);return 0;}EXPORT_SYMBOL(scsi_esp_register": "scsi_esp_register(struct esp  esp){static int instance;int err;if (!esp->num_tags)esp->num_tags = ESP_DEFAULT_TAGS;esp->host->transportt = esp_transport_template;esp->host->max_lun = ESP_MAX_LUN;esp->host->cmd_per_lun = 2;esp->host->unique_id = instance;esp_set_clock_params(esp);esp_get_revision(esp);esp_init_swstate(esp);esp_bootup_reset(esp);dev_printk(KERN_INFO, esp->dev, \"esp%u: regs[%1p:%1p] irq[%u]\\n\",   esp->host->unique_id, esp->regs, esp->dma_regs,   esp->host->irq);dev_printk(KERN_INFO, esp->dev,   \"esp%u: is a %s, %u MHz (ccf=%u), SCSI ID %u\\n\",   esp->host->unique_id, esp_chip_names[esp->rev],   esp->cfreq  1000000, esp->cfact, esp->scsi_id);  Let the SCSI bus reset settle. ", "void scsi_remove_host(struct Scsi_Host *shost)": "scsi_remove_host - remove a scsi host   @shost:a pointer to a scsi host to remove  ", "int scsi_add_host_with_dma(struct Scsi_Host *shost, struct device *dev,   struct device *dma_dev)": "scsi_add_host_with_dma - add a scsi host with dma device   @shost:scsi host pointer to add   @dev:a struct device of type scsi class   @dma_dev:dma device for the host     Note: You rarely need to worry about this unless you're in a   virtualised host environments, so use the simpler scsi_add_host()   function instead.     Return value:    0 on success  != 0 for error  ", "kfree(dev_name(&shost->shost_dev));}kfree(shost->shost_data);ida_free(&host_index_ida, shost->host_no);if (shost->shost_state != SHOST_CREATED)put_device(parent);kfree(shost);}static struct device_type scsi_host_type = ": "scsi_host_put() have been called but neither   scsi_host_add() nor scsi_remove_host() has been called.   This avoids that the memory allocated for the shost_dev   name is leaked. ", "struct Scsi_Host *scsi_host_lookup(unsigned short hostnum)": "scsi_host_get() took. The put_device() below dropped  the reference from class_find_device().  ", "int scsi_host_busy(struct Scsi_Host *shost)": "scsi_host_busy - Return the host busy counter   @shost:Pointer to Scsi_Host to inc.  ", "}hostdata->input_Q = NULL;hostdata->selecting = NULL;hostdata->connected = NULL;hostdata->disconnected_Q = NULL;hostdata->state = S_UNCONNECTED;hostdata->dma = D_DMA_OFF;hostdata->incoming_ptr = 0;hostdata->outgoing_len = 0;reset_wd33c93(instance);SCpnt->result = DID_RESET << 16;enable_irq(instance->irq);spin_unlock_irq(instance->host_lock);return SUCCESS;}intwd33c93_abort(struct scsi_cmnd * cmd)": "wd33c93_host_reset(struct scsi_cmnd   SCpnt){struct Scsi_Host  instance;struct WD33C93_hostdata  hostdata;int i;instance = SCpnt->device->host;spin_lock_irq(instance->host_lock);hostdata = (struct WD33C93_hostdata  ) instance->hostdata;printk(\"scsi%d: reset. \", instance->host_no);disable_irq(instance->irq);hostdata->dma_stop(instance, NULL, 0);for (i = 0; i < 8; i++) {hostdata->busy[i] = 0;hostdata->sync_xfer[i] =calc_sync_xfer(DEFAULT_SX_PER  4, DEFAULT_SX_OFF,0, hostdata->sx_table);hostdata->sync_stat[i] = SS_UNSET;  using default sync values ", "#ifdef PROC_STATISTICShostdata->cmd_cnt[i] = 0;hostdata->disc_allowed_cnt[i] = 0;hostdata->disc_done_cnt[i] = 0;#endif}hostdata->input_Q = NULL;hostdata->selecting = NULL;hostdata->connected = NULL;hostdata->disconnected_Q = NULL;hostdata->state = S_UNCONNECTED;hostdata->dma = D_DMA_OFF;hostdata->level2 = L2_BASIC;hostdata->disconnect = DIS_ADAPTIVE;hostdata->args = DEBUG_DEFAULTS;hostdata->incoming_ptr = 0;hostdata->outgoing_len = 0;hostdata->default_sx_per = DEFAULT_SX_PER;hostdata->no_dma = 0;/* default is DMA enabled ": "wd33c93_init(struct Scsi_Host  instance, const wd33c93_regs regs,     dma_setup_t setup, dma_stop_t stop, int clock_freq){struct WD33C93_hostdata  hostdata;int i;int flags;int val;char buf[32];if (!done_setup && setup_strings)wd33c93_setup(setup_strings);hostdata = (struct WD33C93_hostdata  ) instance->hostdata;hostdata->regs = regs;hostdata->clock_freq = set_clk_freq(clock_freq, &i);calc_sx_table(i, hostdata->sx_table);hostdata->dma_setup = setup;hostdata->dma_stop = stop;hostdata->dma_bounce_buffer = NULL;hostdata->dma_bounce_len = 0;for (i = 0; i < 8; i++) {hostdata->busy[i] = 0;hostdata->sync_xfer[i] =calc_sync_xfer(DEFAULT_SX_PER  4, DEFAULT_SX_OFF,0, hostdata->sx_table);hostdata->sync_stat[i] = SS_UNSET;  using default sync values ", "tmp = (struct scsi_cmnd *) hostdata->input_Q;prev = NULL;while (tmp) ": "wd33c93_abort(struct scsi_cmnd   cmd){struct Scsi_Host  instance;struct WD33C93_hostdata  hostdata;wd33c93_regs regs;struct scsi_cmnd  tmp,  prev;disable_irq(cmd->device->host->irq);instance = cmd->device->host;hostdata = (struct WD33C93_hostdata  ) instance->hostdata;regs = hostdata->regs;    Case 1 : If the command hasn't been issued yet, we simply remove it       from the input_Q. ", "cmd->host_scribble = NULL;cmd->result = 0;/* We use the Scsi_Pointer structure that's included with each command * as a scratchpad (as it's intended to be used!). The handy thing about * the SCp.xxx fields is that they're always associated with a given * cmd, and are preserved across disconnect-reselect. This means we * can pretty much ignore SAVE_POINTERS and RESTORE_POINTERS messages * if we keep all the critical pointers and counters in SCp: *  - SCp.ptr is the pointer into the RAM buffer *  - SCp.this_residual is the size of that buffer *  - SCp.buffer points to the current scatter-gather buffer *  - SCp.buffers_residual tells us how many S.G. buffers there are *  - SCp.have_data_in is not used *  - SCp.sent_command is not used *  - SCp.phase records this command's SRCID_ER bit setting ": "wd33c93_queuecommand_lck(struct scsi_cmnd  cmd){struct scsi_pointer  scsi_pointer = WD33C93_scsi_pointer(cmd);struct WD33C93_hostdata  hostdata;struct scsi_cmnd  tmp;hostdata = (struct WD33C93_hostdata  ) cmd->device->host->hostdata;DB(DB_QUEUE_COMMAND,   printk(\"Q-%d-%02x( \", cmd->device->id, cmd->cmnd[0]))  Set up a few fields in the scsi_cmnd structure for our own use:    - host_scribble is the pointer to the next cmd in the input queue    - result is what you'd expect ", "static voidwd33c93_execute(struct Scsi_Host *instance)": "wd33c93_intr itself, which means that a wd33c93 interrupt   cannot occur while we are in here. ", "return 0;}EXPORT_SYMBOL(wd33c93_host_reset);EXPORT_SYMBOL(wd33c93_init);EXPORT_SYMBOL(wd33c93_abort);EXPORT_SYMBOL(wd33c93_queuecommand);EXPORT_SYMBOL(wd33c93_intr);EXPORT_SYMBOL(wd33c93_show_info": "wd33c93_show_info(struct seq_file  m, struct Scsi_Host  instance){#ifdef PROC_INTERFACEstruct WD33C93_hostdata  hd;struct scsi_cmnd  cmd;int x;hd = (struct WD33C93_hostdata  ) instance->hostdata;spin_lock_irq(&hd->lock);if (hd->proc & PR_VERSION)seq_printf(m, \"\\nVersion %s - %s.\",WD33C93_VERSION, WD33C93_DATE);if (hd->proc & PR_INFO) {seq_printf(m, \"\\nclock_freq=%02x no_sync=%02x no_dma=%d\"\" dma_mode=%02x fast=%d\",hd->clock_freq, hd->no_sync, hd->no_dma, hd->dma_mode, hd->fast);seq_puts(m, \"\\nsync_xfer[] =       \");for (x = 0; x < 7; x++)seq_printf(m, \"\\t%02x\", hd->sync_xfer[x]);seq_puts(m, \"\\nsync_stat[] =       \");for (x = 0; x < 7; x++)seq_printf(m, \"\\t%02x\", hd->sync_stat[x]);}#ifdef PROC_STATISTICSif (hd->proc & PR_STATISTICS) {seq_puts(m, \"\\ncommands issued:    \");for (x = 0; x < 7; x++)seq_printf(m, \"\\t%ld\", hd->cmd_cnt[x]);seq_puts(m, \"\\ndisconnects allowed:\");for (x = 0; x < 7; x++)seq_printf(m, \"\\t%ld\", hd->disc_allowed_cnt[x]);seq_puts(m, \"\\ndisconnects done:   \");for (x = 0; x < 7; x++)seq_printf(m, \"\\t%ld\", hd->disc_done_cnt[x]);seq_printf(m,\"\\ninterrupts: %ld, DATA_PHASE ints: %ld DMA, %ld PIO\",hd->int_cnt, hd->dma_cnt, hd->pio_cnt);}#endifif (hd->proc & PR_CONNECTED) {seq_puts(m, \"\\nconnected:     \");if (hd->connected) {cmd = (struct scsi_cmnd  ) hd->connected;seq_printf(m, \" %d:%llu(%02x)\",cmd->device->id, cmd->device->lun, cmd->cmnd[0]);}}if (hd->proc & PR_INPUTQ) {seq_puts(m, \"\\ninput_Q:       \");cmd = (struct scsi_cmnd  ) hd->input_Q;while (cmd) {seq_printf(m, \" %d:%llu(%02x)\",cmd->device->id, cmd->device->lun, cmd->cmnd[0]);cmd = (struct scsi_cmnd  ) cmd->host_scribble;}}if (hd->proc & PR_DISCQ) {seq_puts(m, \"\\ndisconnected_Q:\");cmd = (struct scsi_cmnd  ) hd->disconnected_Q;while (cmd) {seq_printf(m, \" %d:%llu(%02x)\",cmd->device->id, cmd->device->lun, cmd->cmnd[0]);cmd = (struct scsi_cmnd  ) cmd->host_scribble;}}seq_putc(m, '\\n');spin_unlock_irq(&hd->lock);#endif  PROC_INTERFACE ", "buf[len] = '\\0';for (bp = buf; *bp; ) ": "wd33c93_write_info(struct Scsi_Host  instance, char  buf, int len){#ifdef PROC_INTERFACEchar  bp;struct WD33C93_hostdata  hd;int x;hd = (struct WD33C93_hostdata  ) instance->hostdata;  We accept the following   keywords (same format as command-line, but arguments are not optional):      debug      disconnect      period      resync      proc      nodma      level2      burst      fast      nosync ", "int scsi_dev_info_list_add_keyed(int compatible, char *vendor, char *model, char *strflags, blist_flags_t flags, enum scsi_devinfo_key key)": "scsi_dev_info_list_add_keyed(compatible, vendor, model,    strflags, flags,    SCSI_DEVINFO_GLOBAL);}     scsi_dev_info_list_add_keyed - add one dev_info list entry.   @compatible: if true, null terminate short strings.  Otherwise space pad.   @vendor:vendor string   @model:model (product) string   @strflags:integer string   @flags:if strflags NULL, use this flag value   @key:specify list to use     Description:  Create and add one dev_info entry for @vendor, @model,  @strflags or @flag in list specified by @key. If @compatible,  add to the tail of the list, do not space pad, and set  devinfo->compatible. The scsi_static_device_list entries are  added with @compatible 1 and @clfags NULL.     Returns: 0 OK, -error on failure.  ", "int scsi_dev_info_list_del_keyed(char *vendor, char *model, enum scsi_devinfo_key key)": "scsi_dev_info_list_del_keyed - remove one dev_info list entry.   @vendor:vendor string   @model:model (product) string   @key:specify list to use     Description:  Remove and destroy one dev_info entry for @vendor, @model  in list specified by @key.     Returns: 0 OK, -error on failure.  ", "blist_flags_t scsi_get_device_flags_keyed(struct scsi_device *sdev,const unsigned char *vendor,const unsigned char *model,enum scsi_devinfo_key key)": "scsi_get_device_flags_keyed(sdev, vendor, model,   SCSI_DEVINFO_GLOBAL);}     scsi_get_device_flags_keyed - get device specific flags from the dynamic device list   @sdev:       &scsi_device to get flags for   @vendor:vendor name   @model:model name   @key:list to look up     Description:       Search the scsi_dev_info_list specified by @key for an entry       matching @vendor and @model, if found, return the matching       flags value, else return the host or global default settings.       Called during scan time.  ", "int scsi_dev_info_add_list(enum scsi_devinfo_key key, const char *name)": "scsi_dev_info_remove_list(SCSI_DEVINFO_GLOBAL);}     scsi_dev_info_add_list - add a new devinfo list   @key:key of the list to add   @name:Name of the list to add (for procscsidevice_info)     Adds the requested list, returns zero on success, -EEXIST if the   key is already registered to a list, or other error on failure. ", "int scsi_execute_cmd(struct scsi_device *sdev, const unsigned char *cmd,     blk_opf_t opf, void *buffer, unsigned int bufflen,     int timeout, int retries,     const struct scsi_exec_args *args)": "scsi_execute_cmd - insert request and wait for the result   @sdev:scsi_device   @cmd:scsi command   @opf:block layer request cmd_flags   @buffer:data buffer   @bufflen:len of buffer   @timeout:request timeout in HZ   @retries:number of times to retry request   @args:Optional args. See struct definition for field descriptions     Returns the scsi_cmnd result field if a command was executed, or a negative   Linux error code if we didn't get that far. ", "blk_status_t scsi_alloc_sgtables(struct scsi_cmnd *cmd)": "scsi_alloc_sgtables - Allocate and initialize data and integrity scatterlists   @cmd: SCSI command data structure to initialize.     Initializes @cmd->sdb and also @cmd->prot_sdb if data integrity is enabled   for @cmd.     Returns:     BLK_STS_OK       - on success     BLK_STS_RESOURCE - if the failure is retryable     BLK_STS_IOERR    - if the failure is fatal ", "static unsigned int scsi_mq_inline_sgl_size(struct Scsi_Host *shost)": "scsi_done(cmd);return 0;}  Size in bytes of the sg-list stored in the scsi-mq command-private data. ", "void scsi_block_requests(struct Scsi_Host *shost)": "scsi_unblock_requests(). ", "intscsi_mode_sense(struct scsi_device *sdev, int dbd, int modepage, int subpage,  unsigned char *buffer, int len, int timeout, int retries,  struct scsi_mode_data *data, struct scsi_sense_hdr *sshdr)": "scsi_mode_sense - issue a mode sense, falling back from 10 to six bytes if necessary.  @sdev:SCSI device to be queried  @dbd:set to prevent mode sense from returning block descriptors  @modepage: mode page being requested  @subpage: sub-page of the mode page being requested  @buffer: request buffer (may not be smaller than eight bytes)  @len:length of request buffer.  @timeout: command timeout  @retries: number of retries before failing  @data: returns a structure abstracting the mode header data  @sshdr: place to put sense data (or NULL if no sense to be collected).  must be SCSI_SENSE_BUFFERSIZE big.    Returns zero if successful, or a negative error number on failure ", "intscsi_test_unit_ready(struct scsi_device *sdev, int timeout, int retries,     struct scsi_sense_hdr *sshdr)": "scsi_test_unit_ready - test if unit is ready  @sdev:scsi device to change the state of.  @timeout: command timeout  @retries: number of retries before failing  @sshdr: outpout pointer for decoded sense information.    Returns zero if unsuccessful or an error if TUR failed.  For  removable media, UNIT_ATTENTION sets ->changed flag.  ", "intscsi_device_set_state(struct scsi_device *sdev, enum scsi_device_state state)": "scsi_device_set_state - Take the given device through the device state model.  @sdev:scsi device to change the state of.  @state:state to change to.    Returns zero if successful or an error if the requested  transition is illegal. ", "intscsi_device_quiesce(struct scsi_device *sdev)": "scsi_device_quiesce - Block all commands except power management.  @sdev:scsi device to quiesce.    This works by trying to transition to the SDEV_QUIESCE state  (which must be a legal transition).  When the device is in this  state, only power management requests will be accepted, all others will  be deferred.    Must be called with user context, may sleep.    Returns zero if unsuccessful or an error if not. ", "void scsi_device_resume(struct scsi_device *sdev)": "scsi_device_resume - Restart user issued commands to a quiesced device.  @sdev:scsi device to resume.    Moves the device from quiesced back to running and restarts the  queues.    Must be called with user context, may sleep. ", "void *scsi_kmap_atomic_sg(struct scatterlist *sgl, int sg_count,  size_t *offset, size_t *len)": "scsi_kmap_atomic_sg - find and atomically map an sg-elemnt   @sgl:scatter-gather list   @sg_count:number of segments in sg   @offset:offset in bytes into sg, on return offset into the mapped area   @len:bytes to map, on return number of bytes mapped     Returns virtual address of the start of the mapped page ", "void scsi_kunmap_atomic_sg(void *virt)": "scsi_kunmap_atomic_sg - atomically unmap a virtual address, previously mapped with scsi_kmap_atomic_sg   @virt:virtual address to be unmapped ", "int scsi_vpd_lun_id(struct scsi_device *sdev, char *id, size_t id_len)": "scsi_vpd_lun_id - return a unique device identification   @sdev: SCSI device   @id:   buffer for the identification   @id_len:  length of the buffer     Copies a unique device identification into @id based   on the information in the VPD page 0x83 of the device.   The string will be formatted as a SCSI name string.     Returns the length of the identification or error on failure.   If the identifier is longer than the supplied buffer the actual   identifier length is returned and the buffer is not zero-padded. ", "int scsi_vpd_tpg_id(struct scsi_device *sdev, int *rel_id)": "scsi_vpd_tpg_id - return a target port group identifier   @sdev: SCSI device     Returns the Target Port Group identifier from the information   froom VPD page 0x83 of the device.     Returns the identifier or error on failure. ", "const char *scsi_device_type(unsigned type)": "scsi_device_types[] = {\"Direct-Access    \",\"Sequential-Access\",\"Printer          \",\"Processor        \",\"WORM             \",\"CD-ROM           \",\"Scanner          \",\"Optical Device   \",\"Medium Changer   \",\"Communications   \",\"ASC IT8          \",\"ASC IT8          \",\"RAID             \",\"Enclosure        \",\"Direct-Access-RBC\",\"Optical card     \",\"Bridge controller\",\"Object storage   \",\"AutomationDrive \",\"Security Manager \",\"Direct-Access-ZBC\",};     scsi_device_type - Return 17-char string indicating device type.   @type: type number to look up ", "u64 scsilun_to_int(struct scsi_lun *scsilun)": "scsilun_to_int - convert a scsi_lun to an int   @scsilun:struct scsi_lun to be converted.     Description:       Convert @scsilun from a struct scsi_lun to a four-byte host byte-ordered       integer, and return the result. The caller must check for       truncation before using this function.     Notes:       For a description of the LUN format, post SCSI-3 see the SCSI       Architecture Model, for SCSI-3 see the SCSI Controller Commands.         Given a struct scsi_lun of: d2 04 0b 03 00 00 00 00, this function       returns the integer: 0x0b03d204         This encoding will return a standard integer LUN for LUNs smaller       than 256, which typically use a single level LUN structure with       addressing method 0. ", "void int_to_scsilun(u64 lun, struct scsi_lun *scsilun)": "int_to_scsilun - reverts an int into a scsi_lun   @lun:        integer to be reverted   @scsilun:struct scsi_lun to be set.     Description:       Reverts the functionality of the scsilun_to_int, which packed       an 8-byte lun value into an int. This routine unpacks the int       back into the lun value.     Notes:       Given an integer : 0x0b03d204, this function returns a       struct scsi_lun of: d2 04 0b 03 00 00 00 00   ", "bool scsi_normalize_sense(const u8 *sense_buffer, int sb_len,  struct scsi_sense_hdr *sshdr)": "scsi_normalize_sense - normalize main elements from either fixed or  descriptor sense data format into a common format.     @sense_buffer:byte array containing sense data returned by device   @sb_len:number of valid bytes in sense_buffer   @sshdr:pointer to instance of structure that common  elements are written to.     Notes:  The \"main elements\" from sense data are: response_code, sense_key,  asc, ascq and additional_length (only for descriptor format).    Typically this function can be called after a device has  responded to a SCSI command with the CHECK_CONDITION status.     Return value:  true if valid sense data information found, else false; ", "const u8 * scsi_sense_desc_find(const u8 * sense_buffer, int sb_len,int desc_type)": "scsi_sense_desc_find - search for a given descriptor type indescriptor sense data format.   @sense_buffer:byte array of descriptor format sense data   @sb_len:number of valid bytes in sense_buffer   @desc_type:value of descriptor type to find  (e.g. 0 -> information)     Notes:  only valid when sense data is in descriptor format     Return value:  pointer to start of (first) descriptor if found else NULL ", "void scsi_build_sense_buffer(int desc, u8 *buf, u8 key, u8 asc, u8 ascq)": "scsi_build_sense_buffer - build sense data in a buffer   @desc:Sense format (non-zero == descriptor format,                0 == fixed format)   @buf:Where to build sense data   @key:Sense key   @asc:Additional sense code   @ascq:Additional sense code qualifier    ", "int scsi_set_sense_information(u8 *buf, int buf_len, u64 info)": "scsi_set_sense_information - set the information field in a  formatted sense data buffer   @buf:Where to build sense data   @buf_len:    buffer length   @info:64-bit information value to be set     Return value:  0 on success or -EINVAL for invalid sense buffer length  ", "int scsi_set_sense_field_pointer(u8 *buf, int buf_len, u16 fp, u8 bp, bool cd)": "scsi_set_sense_field_pointer - set the field pointer sense key  specific information in a formatted sense data buffer   @buf:Where to build sense data   @buf_len:    buffer length   @fp:field pointer to be set   @bp:bit pointer to be set   @cd:commanddata bit     Return value:  0 on success or -EINVAL for invalid sense buffer length ", "int qlogicfas408_get_chip_type(int qbase, int int_type)": "qlogicfas408_info(struct Scsi_Host  host){struct qlogicfas408_priv  priv = get_priv_by_host(host);return priv->qinfo;}   Get type of chip ", "while (priv->qlcmd != NULL) ": "qlogicfas408_queuecommand_lck(struct scsi_cmnd  cmd){void ( done)(struct scsi_cmnd  ) = scsi_done;struct qlogicfas408_priv  priv = get_priv_by_cmd(cmd);set_host_byte(cmd, DID_OK);set_status_byte(cmd, SAM_STAT_GOOD);if (scmd_id(cmd) == priv->qinitid) {set_host_byte(cmd, DID_BAD_TARGET);done(cmd);return 0;}  wait for the last command's interrupt to finish ", "int qlogicfas408_host_reset(struct scsi_cmnd *cmd)": "qlogicfas408_abort(struct scsi_cmnd  cmd){struct qlogicfas408_priv  priv = get_priv_by_cmd(cmd);priv->qabort = 1;ql_zap(priv);return SUCCESS;}   Reset SCSI bus  FIXME: This function is invoked with cmd = NULL directly by  the PCMCIA qlogic_stub code. This wants fixing ", "const char *qlogicfas408_info(struct Scsi_Host *host)": "qlogicfas408_host_reset(struct scsi_cmnd  cmd){struct qlogicfas408_priv  priv = get_priv_by_cmd(cmd);unsigned long flags;priv->qabort = 2;spin_lock_irqsave(cmd->device->host->host_lock, flags);ql_zap(priv);spin_unlock_irqrestore(cmd->device->host->host_lock, flags);return SUCCESS;}   Return info string ", "ip[0] = 0x40;ip[1] = 0x20;ip[2] = (unsigned long) capacity / (ip[0] * ip[1]);if (ip[2] > 1024) ": "qlogicfas408_biosparam(struct scsi_device  disk, struct block_device  dev,   sector_t capacity, int ip[]){  This should mimic the DOS Qlogic driver's behavior exactly ", "static int qlogicfas408_queuecommand_lck(struct scsi_cmnd *cmd)": "qlogicfas408_ihandl(int irq, void  dev_id){unsigned long flags;struct Scsi_Host  host = dev_id;spin_lock_irqsave(host->host_lock, flags);ql_ihandl(dev_id);spin_unlock_irqrestore(host->host_lock, flags);return IRQ_HANDLED;}   Queued command ", "void qlogicfas408_setup(int qbase, int id, int int_type)": "qlogicfas408_get_chip_type(int qbase, int int_type){REG1;return inb(qbase + 0xe) & 0xf8;}   Perform initialization tasks ", "REG0;outb(0x40 | qlcfg8 | id, qbase + 8);/* (ini) bus id, disable scsi rst ": "qlogicfas408_setup(int qbase, int id, int int_type){outb(1, qbase + 8);  set for PIO pseudo DMA ", "void qlogicfas408_disable_ints(struct qlogicfas408_priv *priv)": "qlogicfas408_detect(int qbase, int int_type){REG1;return (((inb(qbase + 0xe) ^ inb(qbase + 0xe)) == 7) &&((inb(qbase + 0xe) ^ inb(qbase + 0xe)) == 7));}   Disable interrupts ", "}/* *Init and exit functions ": "qlogicfas408_disable_ints(struct qlogicfas408_priv  priv){int qbase = priv->qbase;int int_type = priv->int_type;REG1;outb(0, qbase + 0xb);  disable ints ", "unsigned char *scsi_bios_ptable(struct block_device *dev)": "scsi_bios_ptable - Read PC partition table out of first sector of device.   @dev: from this device     Description: Reads the first sector from the device and returns %0x42 bytes                starting at offset %0x1be.   Returns: partition table in kmalloc(GFP_KERNEL) memory, or NULL on error. ", "bool scsi_partsize(struct block_device *bdev, sector_t capacity, int geom[3])": "scsi_partsize - Parse cylindersheadssectors from PC partition table   @bdev: block device to parse   @capacity: size of the disk in sectors   @geom: output in form of [hds, cylinders, sectors]     Determine the BIOS mappinggeometry used to create the partition   table, storing the results in @geom.     Returns: %false on failure, %true on success. ", "logical_end = get_unaligned_le32(&largest->start_sect)    + get_unaligned_le32(&largest->nr_sects);/* This is for >1023 cylinders ": "scsicam_bios_param : partition %d has system \\n\",       i);#endifcyl = p->cyl + ((p->sector & 0xc0) << 2);if (cyl > largest_cyl) {largest_cyl = cyl;largest = p;}}}if (largest) {end_cyl = largest->end_cyl + ((largest->end_sector & 0xc0) << 2);end_head = largest->end_head;end_sector = largest->end_sector & 0x3f;if (end_head + 1 == 0 || end_sector == 0)goto out_free_buf;#ifdef DEBUGprintk(\"scsicam_bios_param : end at h = %d, c = %d, s = %d\\n\",       end_head, end_cyl, end_sector);#endifphysical_end = end_cyl   (end_head + 1)   end_sector +    end_head   end_sector + end_sector;  This is the actual _sector_ number at the end ", "device_remove_file(dev, attr);scsi_remove_device(sdev);if (kn)sysfs_unbreak_active_protection(kn);scsi_device_put(sdev);return count;};static DEVICE_ATTR(delete, S_IWUSR, NULL, sdev_store_delete);static ssize_tstore_state_field(struct device *dev, struct device_attribute *attr,  const char *buf, size_t count)": "scsi_remove_device().   device_remove_file() handles concurrent removal calls by   serializing these and by ignoring the second and later removal   attempts.  Concurrent calls of scsi_remove_device() are   serialized. The second and later calls of scsi_remove_device() are   ignored because the first call of that function changes the device   state into SDEV_DEL. ", "if (sdev->channel != starget->channel ||    sdev->id != starget->id)continue;if (sdev->sdev_state == SDEV_DEL ||    sdev->sdev_state == SDEV_CANCEL ||    !get_device(&sdev->sdev_gendev))continue;spin_unlock_irqrestore(shost->host_lock, flags);scsi_remove_device(sdev);put_device(&sdev->sdev_gendev);spin_lock_irqsave(shost->host_lock, flags);goto restart;}spin_unlock_irqrestore(shost->host_lock, flags);}/** * scsi_remove_target - try to remove a target and all its devices * @dev: generic starget or parent of generic stargets to be removed * * Note: This is slightly racy.  It is possible that if the user * requests the addition of another device then the target won't be * removed. ": "scsi_remove_target(struct scsi_target  starget){struct Scsi_Host  shost = dev_to_shost(starget->dev.parent);unsigned long flags;struct scsi_device  sdev;spin_lock_irqsave(shost->host_lock, flags); restart:list_for_each_entry(sdev, &shost->__devices, siblings) {    We cannot call scsi_device_get() here, as   we might've been called from rmmod() causing   scsi_device_get() to fail the module_is_live()   check. ", "void scsi_sanitize_inquiry_string(unsigned char *s, int len)": "scsi_sanitize_inquiry_string - remove non-graphical chars from an                                  INQUIRY result string   @s: INQUIRY result string to sanitize   @len: length of the string     Description:  The SCSI spec says that INQUIRY vendor, product, and revision  strings must consist entirely of graphic ASCII characters,  padded on the right with spaces.  Since not all devices obey  this rule, we will replace non-graphic or non-ASCII characters  with spaces.  Exception: a NUL character is interpreted as a  string terminator, so all the following characters are set to  spaces.  ", "scsi_target_reap(starget);put_device(&starget->dev);return sdev;}EXPORT_SYMBOL(__scsi_add_device": "__scsi_add_device(struct Scsi_Host  shost, uint channel,      uint id, u64 lun, void  hostdata){struct scsi_device  sdev = ERR_PTR(-ENODEV);struct device  parent = &shost->shost_gendev;struct scsi_target  starget;if (strncmp(scsi_scan_type, \"none\", 4) == 0)return ERR_PTR(-ENODEV);starget = scsi_alloc_target(parent, channel, id);if (!starget)return ERR_PTR(-ENOMEM);scsi_autopm_get_target(starget);mutex_lock(&shost->scan_mutex);if (!shost->async_scan)scsi_complete_async_scans();if (scsi_host_scan_allowed(shost) && scsi_autopm_get_host(shost) == 0) {scsi_probe_and_add_lun(starget, lun, NULL, &sdev,       SCSI_SCAN_RESCAN, hostdata);scsi_autopm_put_host(shost);}mutex_unlock(&shost->scan_mutex);scsi_autopm_put_target(starget);    paired with scsi_alloc_target().  Target will be destroyed unless   scsi_probe_and_add_lun made an underlying device visible ", "scsi_target_reap(starget);put_device(&starget->dev);return sdev;}EXPORT_SYMBOL(__scsi_add_device);int scsi_add_device(struct Scsi_Host *host, uint channel,    uint target, u64 lun)": "scsi_add_device(struct Scsi_Host  shost, uint channel,      uint id, u64 lun, void  hostdata){struct scsi_device  sdev = ERR_PTR(-ENODEV);struct device  parent = &shost->shost_gendev;struct scsi_target  starget;if (strncmp(scsi_scan_type, \"none\", 4) == 0)return ERR_PTR(-ENODEV);starget = scsi_alloc_target(parent, channel, id);if (!starget)return ERR_PTR(-ENOMEM);scsi_autopm_get_target(starget);mutex_lock(&shost->scan_mutex);if (!shost->async_scan)scsi_complete_async_scans();if (scsi_host_scan_allowed(shost) && scsi_autopm_get_host(shost) == 0) {scsi_probe_and_add_lun(starget, lun, NULL, &sdev,       SCSI_SCAN_RESCAN, hostdata);scsi_autopm_put_host(shost);}mutex_unlock(&shost->scan_mutex);scsi_autopm_put_target(starget);    paired with scsi_alloc_target().  Target will be destroyed unless   scsi_probe_and_add_lun made an underlying device visible ", "return;starget = scsi_alloc_target(parent, channel, id);if (!starget)return;scsi_autopm_get_target(starget);if (lun != SCAN_WILD_CARD) ": "scsi_scan_target(struct device  parent, unsigned int channel,unsigned int id, u64 lun, enum scsi_scan_mode rescan){struct Scsi_Host  shost = dev_to_shost(parent);blist_flags_t bflags = 0;int res;struct scsi_target  starget;if (shost->this_id == id)    Don't scan the host adapter ", "if (sdev->sdev_state == SDEV_DEL)continue;/* If device is already visible, skip adding it to sysfs ": "scsi_scan_host_selected(struct Scsi_Host  shost, unsigned int channel,    unsigned int id, u64 lun,    enum scsi_scan_mode rescan){SCSI_LOG_SCAN_BUS(3, shost_printk (KERN_INFO, shost,\"%s: <%u:%u:%llu>\\n\",__func__, channel, id, lun));if (((channel != SCAN_WILD_CARD) && (channel > shost->max_channel)) ||    ((id != SCAN_WILD_CARD) && (id >= shost->max_id)) ||    ((lun != SCAN_WILD_CARD) && (lun >= shost->max_lun)))return -EINVAL;mutex_lock(&shost->scan_mutex);if (!shost->async_scan)scsi_complete_async_scans();if (scsi_host_scan_allowed(shost) && scsi_autopm_get_host(shost) == 0) {if (channel == SCAN_WILD_CARD)for (channel = 0; channel <= shost->max_channel;     channel++)scsi_scan_channel(shost, channel, id, lun,  rescan);elsescsi_scan_channel(shost, channel, id, lun, rescan);scsi_autopm_put_host(shost);}mutex_unlock(&shost->scan_mutex);return 0;}static void scsi_sysfs_add_devices(struct Scsi_Host  shost){struct scsi_device  sdev;shost_for_each_device(sdev, shost) {  target removed before the device could be added ", "tpnt->queuecommand = NCR_700_queuecommand;tpnt->eh_abort_handler = NCR_700_abort;tpnt->eh_host_reset_handler = NCR_700_host_reset;tpnt->can_queue = NCR_700_COMMAND_SLOTS_PER_HOST;tpnt->sg_tablesize = NCR_700_SG_SEGMENTS;tpnt->cmd_per_lun = NCR_700_CMD_PER_LUN;tpnt->slave_configure = NCR_700_slave_configure;tpnt->slave_destroy = NCR_700_slave_destroy;tpnt->slave_alloc = NCR_700_slave_alloc;tpnt->change_queue_depth = NCR_700_change_queue_depth;if(tpnt->name == NULL)tpnt->name = \"53c700\";if(tpnt->proc_name == NULL)tpnt->proc_name = \"53c700\";host = scsi_host_alloc(tpnt, 4);if (!host)return NULL;memset(hostdata->slots, 0, sizeof(struct NCR_700_command_slot)       * NCR_700_COMMAND_SLOTS_PER_HOST);for (j = 0; j < NCR_700_COMMAND_SLOTS_PER_HOST; j++) ": "NCR_700_detect(struct scsi_host_template  tpnt,       struct NCR_700_Host_Parameters  hostdata, struct device  dev){dma_addr_t pScript, pSlots;__u8  memory;__u32  script;struct Scsi_Host  host;static int banner = 0;int j;if (tpnt->sdev_groups == NULL)tpnt->sdev_groups = NCR_700_dev_groups;memory = dma_alloc_coherent(dev, TOTAL_MEM_SIZE, &pScript, GFP_KERNEL);if (!memory) {hostdata->noncoherent = 1;memory = dma_alloc_noncoherent(dev, TOTAL_MEM_SIZE, &pScript, DMA_BIDIRECTIONAL, GFP_KERNEL);}if (!memory) {printk(KERN_ERR \"53c700: Failed to allocate memory for driver, detaching\\n\");return NULL;}script = (__u32  )memory;hostdata->msgin = memory + MSGIN_OFFSET;hostdata->msgout = memory + MSGOUT_OFFSET;hostdata->status = memory + STATUS_OFFSET;hostdata->slots = (struct NCR_700_command_slot  )(memory + SLOTS_OFFSET);hostdata->dev = dev;pSlots = pScript + SLOTS_OFFSET;  Fill in the missing routines from the host template ", "static inline intNCR_700_data_residual (struct Scsi_Host *host) ": "NCR_700_release(struct Scsi_Host  host){struct NCR_700_Host_Parameters  hostdata = (struct NCR_700_Host_Parameters  )host->hostdata[0];if (hostdata->noncoherent)dma_free_noncoherent(hostdata->dev, TOTAL_MEM_SIZE,hostdata->script, hostdata->pScript,DMA_BIDIRECTIONAL);elsedma_free_coherent(hostdata->dev, TOTAL_MEM_SIZE,  hostdata->script, hostdata->pScript);return 1;}static inline __u8NCR_700_identify(int can_disconnect, __u8 lun){return IDENTIFY_BASE |((can_disconnect) ? 0x40 : 0) |(lun & NCR_700_LUN_MASK);}    Function : static int data_residual (Scsi_Host  host)     Purpose : return residual data count of what's in the chip.  If you   really want to know what this function is doing, it's almost a   direct transcription of the algorithm described in the 53c710   guide, except that the DBC and DFIFO registers are only 6 bits   wide on a 53c700.     Inputs : host - SCSI host ", "spin_lock_irqsave(host->host_lock, flags);if((istat = NCR_700_readb(host, ISTAT_REG))      & (SCSI_INT_PENDING | DMA_INT_PENDING)) ": "NCR_700_intr(int irq, void  dev_id){struct Scsi_Host  host = (struct Scsi_Host  )dev_id;struct NCR_700_Host_Parameters  hostdata =(struct NCR_700_Host_Parameters  )host->hostdata[0];__u8 istat;__u32 resume_offset = 0;__u8 pun = 0xff, lun = 0xff;unsigned long flags;int handled = 0;  Use the host lock to serialise access to the 53c700   hardware.  Note: In future, we may need to take the queue   lock to enter the done routines.  When that happens, we   need to ensure that for this driver, the host lock and the   queue lock point to the same thing. ", "static void fas216_init_chip(FAS216_Info *info)": "fas216_init_chip - Initialise FAS216 state after reset   @info: state structure for interface     Initialise FAS216 state after reset ", "int fas216_add(struct Scsi_Host *host, struct device *dev)": "fas216_add - initialise FASNCRAMD SCSI ic.   @host: a driver-specific filled-out structure   @dev: parent device     Initialise FASNCRAMD SCSI ic.   Returns: 0 on success ", "static int fas216_queue_command_internal(struct scsi_cmnd *SCpnt, void (*done)(struct scsi_cmnd *))": "fas216_queue_command_internal - queue a command for the adapter to process   @SCpnt: Command to queue   @done: done function to call once command is complete     Queue a command for adapter to process.   Returns: 0 on success, else error.   Notes: io_request_lock is held, interrupts are disabled. ", "static void fas216_internal_done(struct scsi_cmnd *SCpnt)": "fas216_noqueue_command   @SCpnt: Command to wake     Trigger restart of a waiting thread in fas216_command ", "irqreturn_t fas216_intr(FAS216_Info *info)": "fas216_intr - handle interrupts to progress a command   @info: interface to service     Handle interrupts from the interface to progress a command ", "void fas216_release(struct Scsi_Host *host)": "fas216_release - release all resources for FASNCRAMD SCSI ic.   @host: a driver-specific filled-out structure     release all resources and put everything to bed for FASNCRAMD SCSI ic. ", "int fas216_eh_abort(struct scsi_cmnd *SCpnt)": "fas216_eh_abort - abort this command   @SCpnt: command to abort     Abort this command.   Returns: FAILED if unable to abort   Notes: io_request_lock is taken, and irqs are disabled ", "int fas216_eh_device_reset(struct scsi_cmnd *SCpnt)": "fas216_eh_device_reset - Reset the device associated with this command   @SCpnt: command specifing device to reset     Reset the device associated with this command.   Returns: FAILED if unable to reset.   Notes: We won't be re-entered, so we'll only have one device   reset on the go at one time. ", "int fas216_eh_bus_reset(struct scsi_cmnd *SCpnt)": "fas216_eh_bus_reset - Reset the bus associated with the command   @SCpnt: command specifing bus to reset     Reset the bus associated with the command.   Returns: FAILED if unable to reset.   Notes: Further commands are blocked. ", "int fas216_eh_host_reset(struct scsi_cmnd *SCpnt)": "fas216_eh_host_reset - Reset the host associated with this command   @SCpnt: command specifing host to reset     Reset the host associated with this command.   Returns: FAILED if unable to reset.   Notes: io_request_lock is taken, and irqs are disabled ", "void msgqueue_initialise(MsgQueue_t *msgq)": "msgqueue_initialise(MsgQueue_t  msgq)   Purpose : initialise a message queue   Params  : msgq - queue to initialise ", "void msgqueue_free(MsgQueue_t *msgq)": "msgqueue_free(MsgQueue_t  msgq)   Purpose : free a queue   Params  : msgq - queue to free ", "int msgqueue_msglength(MsgQueue_t *msgq)": "msgqueue_msglength(MsgQueue_t  msgq)   Purpose : calculate the total length of all messages on the message queue   Params  : msgq - queue to examine   Returns : number of bytes of messages in queue ", "struct message *msgqueue_getmsg(MsgQueue_t *msgq, int msgno)": "msgqueue_getmsg(MsgQueue_t  msgq, int msgno)   Purpose : return a message   Params  : msgq   - queue to obtain message from     : msgno  - message number   Returns : pointer to message string, or NULL ", "int msgqueue_addmsg(MsgQueue_t *msgq, int length, ...)": "msgqueue_addmsg(MsgQueue_t  msgq, int length, ...)   Purpose : add a message onto a message queue   Params  : msgq   - queue to add message on       length - length of message       ...    - message bytes   Returns : != 0 if successful ", "void msgqueue_flush(MsgQueue_t *msgq)": "msgqueue_flush(MsgQueue_t  msgq)   Purpose : flush all messages from message queue   Params  : msgq - queue to flush ", "int queue_initialise (Queue_t *queue)": "queue_initialise (Queue_t  queue)   Purpose : initialise a queue   Params  : queue - queue to initialise ", "void queue_free (Queue_t *queue)": "queue_free (Queue_t  queue)   Purpose : free a queue   Params  : queue - queue to free ", "int __queue_add(Queue_t *queue, struct scsi_cmnd *SCpnt, int head)": "__queue_add(Queue_t  queue, struct scsi_cmnd  SCpnt, int head)   Purpose : Add a new command onto a queue, adding REQUEST_SENSE to head.   Params  : queue - destination queue       SCpnt - command to add       head  - add command to head of queue   Returns : 0 on error, !0 on success ", "#include <linux/module.h>#include <linux/blkdev.h>#include <linux/kernel.h>#include <linux/string.h>#include <linux/slab.h>#include <linux/spinlock.h>#include <linux/list.h>#include <linux/init.h>#include <scsi/scsi.h>#include <scsi/scsi_cmnd.h>#include <scsi/scsi_device.h>#include <scsi/scsi_eh.h>#include <scsi/scsi_tcq.h>#define DEBUGtypedef struct queue_entry ": "queue_remove_exclude  not updating internal linked list properly  (was causing commands to go missing).     30-Aug-2000 RMKUse Linux list handling and spinlocks ", "struct scsi_cmnd *queue_remove_tgtluntag(Queue_t *queue, int target, int lun, int tag)": "queue_remove_tgtluntag (queue, target, lun, tag)   Purpose : remove a SCSI command from the queue for a specified targetluntag   Params  : queue  - queue to remove command from       target - target that we want       lun    - lun on device       tag    - tag on device   Returns : struct scsi_cmnd if successful, or NULL if no command satisfies requirements ", "int queue_remove_cmd(Queue_t *queue, struct scsi_cmnd *SCpnt)": "queue_remove_cmd(Queue_t  queue, struct scsi_cmnd  SCpnt)   Purpose : remove a specific command from the queues   Params  : queue - queue to look in       SCpnt - command to find   Returns : 0 if not found ", "void queue_remove_all_target(Queue_t *queue, int target)": "queue_remove_all_target(queue, target)   Purpose : remove all SCSI commands from the queue for a specified target   Params  : queue  - queue to remove command from             target - target device id   Returns : nothing ", "int queue_probetgtlun (Queue_t *queue, int target, int lun)": "queue_probetgtlun (queue, target, lun)   Purpose : check to see if we have a command in the queue for the specified       targetlun.   Params  : queue  - queue to look in       target - target we want to probe       lun    - lun on target   Returns : 0 if not found, != 0 if found ", "void fcoe_ctlr_init(struct fcoe_ctlr *fip, enum fip_mode mode)": "fcoe_ctlr_init() - Initialize the FCoE Controller instance   @fip: The FCoE controller to initialize   @mode: FIP mode to set ", "void fcoe_ctlr_destroy(struct fcoe_ctlr *fip)": "fcoe_ctlr_destroy() - Disable and tear down a FCoE controller   @fip: The FCoE controller to tear down     This is called by FCoE drivers before freeing the &fcoe_ctlr.     The receive handler will have been deleted before this to guarantee   that no more recv_work will be scheduled.     The timer routine will simply return once we set FIP_ST_DISABLED.   This guarantees that no further timeouts or work will be scheduled. ", "void fcoe_ctlr_link_up(struct fcoe_ctlr *fip)": "fcoe_ctlr_link_up() - Start FCoE controller   @fip: The FCoE controller to start     Called from the LLD when the network link is ready. ", "int fcoe_ctlr_link_down(struct fcoe_ctlr *fip)": "fcoe_ctlr_link_down() - Stop a FCoE controller   @fip: The FCoE controller to be stopped     Returns non-zero if the link was up and now isn't.     Called from the LLD when the network link is not ready.   There may be multiple calls while the link is down. ", "int fcoe_ctlr_els_send(struct fcoe_ctlr *fip, struct fc_lport *lport,       struct sk_buff *skb)": "fcoe_ctlr_els_send() - Send an ELS frame encapsulated by FIP if appropriate.   @fip:FCoE controller.   @lport:libfc fc_lport to send from   @skb:FCoE ELS frame including FC header but no FCoE headers.     Returns a non-zero error code if the frame should not be sent.   Returns zero if the caller should send the frame with FCoE encapsulation.     The caller must check that the length is a multiple of 4.   The SKB must have enough headroom (28 bytes) and tailroom (8 bytes).   The the skb must also be an fc_frame.     This is called from the lower-level driver with spinlocks held,   so we must not take a mutex here. ", "static void fcoe_ctlr_set_state(struct fcoe_ctlr *fip, enum fip_state state)": "fcoe_ctlr_recv_work(struct work_struct  );static int fcoe_ctlr_flogi_retry(struct fcoe_ctlr  );static void fcoe_ctlr_vn_start(struct fcoe_ctlr  );static int fcoe_ctlr_vn_recv(struct fcoe_ctlr  , struct sk_buff  );static void fcoe_ctlr_vn_timeout(struct fcoe_ctlr  );static int fcoe_ctlr_vn_lookup(struct fcoe_ctlr  , u32, u8  );static int fcoe_ctlr_vlan_recv(struct fcoe_ctlr  , struct sk_buff  );static u8 fcoe_all_fcfs[ETH_ALEN] = FIP_ALL_FCF_MACS;static u8 fcoe_all_enode[ETH_ALEN] = FIP_ALL_ENODE_MACS;static u8 fcoe_all_vn2vn[ETH_ALEN] = FIP_ALL_VN2VN_MACS;static u8 fcoe_all_p2p[ETH_ALEN] = FIP_ALL_P2P_MACS;static const char   const fcoe_ctlr_states[] = {[FIP_ST_DISABLED] =\"DISABLED\",[FIP_ST_LINK_WAIT] =\"LINK_WAIT\",[FIP_ST_AUTO] =\"AUTO\",[FIP_ST_NON_FIP] =\"NON_FIP\",[FIP_ST_ENABLED] =\"ENABLED\",[FIP_ST_VNMP_START] =\"VNMP_START\",[FIP_ST_VNMP_PROBE1] =\"VNMP_PROBE1\",[FIP_ST_VNMP_PROBE2] =\"VNMP_PROBE2\",[FIP_ST_VNMP_CLAIM] =\"VNMP_CLAIM\",[FIP_ST_VNMP_UP] =\"VNMP_UP\",};static const char  fcoe_ctlr_state(enum fip_state state){const char  cp = \"unknown\";if (state < ARRAY_SIZE(fcoe_ctlr_states))cp = fcoe_ctlr_states[state];if (!cp)cp = \"unknown\";return cp;}     fcoe_ctlr_set_state() - Set and do debug printing for the new FIP state.   @fip: The FCoE controller   @state: The new state ", "int fcoe_ctlr_recv_flogi(struct fcoe_ctlr *fip, struct fc_lport *lport, struct fc_frame *fp)": "fcoe_ctlr_recv_flogi() - Snoop pre-FIP receipt of FLOGI response   @fip: The FCoE controller   @lport: The local port   @fp: The FC frame to snoop     Snoop potential response to FLOGI or even incoming FLOGI.     The caller has checked that we are waiting for login as indicated   by fip->flogi_oxid != FC_XID_UNKNOWN.     The caller is responsible for freeing the frame.   Fill in the granted_mac address.     Return non-zero if the frame should not be delivered to libfc. ", "int fcoe_transport_attach(struct fcoe_transport *ft)": "fcoe_transport_attach - Attaches an FCoE transport   @ft: The fcoe transport to be attached     Returns : 0 for success ", "int fcoe_transport_detach(struct fcoe_transport *ft)": "fcoe_transport_detach - Detaches an FCoE transport   @ft: The fcoe transport to be attached     Returns : 0 for success ", "bp = (const u8 *) fr_hdr(fp);crc = ~crc32(~0, bp, len);error = crc ^ fr_crc(fp);return error;}EXPORT_SYMBOL(fc_frame_crc_check": "fc_frame_crc_check(struct fc_frame  fp){u32 crc;u32 error;const u8  bp;unsigned int len;WARN_ON(!fc_frame_is_linear(fp));fr_flags(fp) &= ~FCPHF_CRC_UNCHECKED;len = (fr_len(fp) + 3) & ~3;  round up length to include fill ", "skb_trim(fp_skb(fp), payload_len + sizeof(struct fc_frame_header));}return fp;}EXPORT_SYMBOL(fc_frame_alloc_fill": "fc_frame_alloc_fill(struct fc_lport  lp, size_t payload_len){struct fc_frame  fp;size_t fill;fill = payload_len % 4;if (fill != 0)fill = 4 - fill;fp = _fc_frame_alloc(payload_len + fill);if (fp) {memset((char  ) fr_hdr(fp) + payload_len, 0, fill);  trim is OK, we just allocated it so there are no fragments ", "struct fc_rport_priv *fc_rport_lookup(const struct fc_lport *lport,      u32 port_id)": "fc_rport_lookup() - Lookup a remote port by port_id   @lport:   The local port to lookup the remote port on   @port_id: The remote port ID to look up     The reference count of the fc_rport_priv structure is   increased by one. ", "struct fc_rport_priv *fc_rport_create(struct fc_lport *lport, u32 port_id)": "fc_rport_create() - Create a new remote port   @lport: The local port this remote port will be associated with   @port_id:   The identifiers for the new remote port     The remote port will start in the INIT state. ", "void fc_rport_destroy(struct kref *kref)": "fc_rport_destroy);return rdata;}if (lport->rport_priv_size > 0)rport_priv_size = lport->rport_priv_size;rdata = kzalloc(rport_priv_size, GFP_KERNEL);if (!rdata)return NULL;rdata->ids.node_name = -1;rdata->ids.port_name = -1;rdata->ids.port_id = port_id;rdata->ids.roles = FC_RPORT_ROLE_UNKNOWN;kref_init(&rdata->kref);mutex_init(&rdata->rp_mutex);rdata->local_port = lport;rdata->rp_state = RPORT_ST_INIT;rdata->event = RPORT_EV_NONE;rdata->flags = FC_RP_FLAGS_REC_SUPPORTED;rdata->e_d_tov = lport->e_d_tov;rdata->r_a_tov = lport->r_a_tov;rdata->maxframe_size = FC_MIN_MAX_PAYLOAD;INIT_DELAYED_WORK(&rdata->retry_work, fc_rport_timeout);INIT_WORK(&rdata->event_work, fc_rport_work);if (port_id != FC_FID_DIR_SERV) {rdata->lld_event_callback = lport->tt.rport_event_callback;list_add_rcu(&rdata->peers, &lport->disc.rports);}return rdata;}EXPORT_SYMBOL(fc_rport_create);     fc_rport_destroy() - Free a remote port after last reference is released   @kref: The remote port's kref ", "void fc_set_rport_loss_tmo(struct fc_rport *rport, u32 timeout)": "fc_set_rport_loss_tmo() - Set the remote port loss timeout   @rport:   The remote port that gets a new timeout value   @timeout: The new timeout value (in seconds) ", "int fc_rport_login(struct fc_rport_priv *rdata)": "fc_rport_login() - Start the remote port login state machine   @rdata: The remote port to be logged in to     Initiates the RP state machine. It is called from the LP module.   This function will issue the following commands to the N_Port   identified by the FC ID provided.     - PLOGI   - PRLI   - RTV     Locking Note: Called without the rport lock held. This   function will hold the rport lock, call an _enter_    function and then unlock the rport.     This indicates the intent to be logged into the remote port.   If it appears we are already logged in, ADISC is used to verify   the setup. ", "lport->tt.exch_mgr_reset(lport, 0, port_id);lport->tt.exch_mgr_reset(lport, port_id, 0);if (rport) ": "fc_rport_logoff(rdata);kref_put(&rdata->kref, fc_rport_destroy);return;}mutex_lock(&rdata->rp_mutex);if (rdata->rport)FC_RPORT_DBG(rdata, \"rport already allocated\\n\");rdata->rport = rport;rport->maxframe_size = rdata->maxframe_size;rport->supported_classes = rdata->supported_classes;rpriv = rport->dd_data;rpriv->local_port = lport;rpriv->rp_state = rdata->rp_state;rpriv->flags = rdata->flags;rpriv->e_d_tov = rdata->e_d_tov;rpriv->r_a_tov = rdata->r_a_tov;mutex_unlock(&rdata->rp_mutex);if (rport_ops && rport_ops->event_callback) {FC_RPORT_DBG(rdata, \"callback ev %d\\n\", event);rport_ops->event_callback(lport, rdata, event);}if (rdata->lld_event_callback) {FC_RPORT_DBG(rdata, \"lld callback ev %d\\n\", event);rdata->lld_event_callback(lport, rdata, event);}kref_put(&rdata->kref, fc_rport_destroy);break;case RPORT_EV_FAILED:case RPORT_EV_LOGO:case RPORT_EV_STOP:if (rdata->prli_count) {mutex_lock(&fc_prov_mutex);for (type = 1; type < FC_FC4_PROV_SIZE; type++) {prov = fc_passive_prov[type];if (prov && prov->prlo)prov->prlo(rdata);}mutex_unlock(&fc_prov_mutex);}port_id = rdata->ids.port_id;mutex_unlock(&rdata->rp_mutex);if (rport_ops && rport_ops->event_callback) {FC_RPORT_DBG(rdata, \"callback ev %d\\n\", event);rport_ops->event_callback(lport, rdata, event);}if (rdata->lld_event_callback) {FC_RPORT_DBG(rdata, \"lld callback ev %d\\n\", event);rdata->lld_event_callback(lport, rdata, event);}if (cancel_delayed_work_sync(&rdata->retry_work))kref_put(&rdata->kref, fc_rport_destroy);    Reset any outstanding exchanges before freeing rport. ", "void fc_rport_recv_req(struct fc_lport *lport, struct fc_frame *fp)": "fc_rport_recv_req() - Handler for requests   @lport: The local port that received the request   @fp:   The request frame     Reference counting: does not modify kref ", "void fc_rport_flush_queue(void)": "fc_rport_flush_queue() - Flush the rport_event_queue ", "void fc_rport_terminate_io(struct fc_rport *rport)": "fc_rport_terminate_io() - Stop all outstanding IO on a remote port   @rport: The remote port whose IO should be terminated ", "void fc_disc_config(struct fc_lport *lport, void *priv)": "fc_disc_config() - Configure the discovery layer for a local port   @lport: The local port that needs the discovery layer to be configured   @priv: Private data structre for users of the discovery layer ", "void fc_disc_init(struct fc_lport *lport)": "fc_disc_init() - Initialize the discovery layer for a local port   @lport: The local port that needs the discovery layer to be initialized ", "struct fc_lport *libfc_vport_create(struct fc_vport *vport, int privsize)": "libfc_vport_create() - Create a new NPIV vport instance   @vport: fc_vport structure from scsi_transport_fc   @privsize: driver private data size to allocate along with the Scsi_Host ", "struct fc_lport *fc_vport_id_lookup(struct fc_lport *n_port, u32 port_id)": "fc_vport_id_lookup() - find NPIV lport that matches a given fabric ID   @n_port: Top level N_Port which may have multiple NPIV VN_Ports   @port_id: Fabric ID to find a match for     Returns: matching lport pointer or NULL if there is no match ", "static void __fc_vport_setlink(struct fc_lport *n_port,       struct fc_lport *vn_port)": "fc_vport_setlink() - update link and status on a VN_Port   @n_port: parent N_Port   @vn_port: VN_Port to update     Locking: must be called with both the N_Port and VN_Port lp_mutex held ", "EXPORT_SYMBOL(fc_cpu_mask": "fc_cpu_mask;  cpu mask for possible cpus ", "if (fr_max_payload(fp))sp->cnt += DIV_ROUND_UP((fr_len(fp) - sizeof(*fh)),fr_max_payload(fp));elsesp->cnt++;/* * Send the frame. ": "fc_seq_send_locked(struct fc_lport  lport, struct fc_seq  sp,      struct fc_frame  fp){struct fc_exch  ep;struct fc_frame_header  fh = fc_frame_header_get(fp);int error = -ENXIO;u32 f_ctl;u8 fh_type = fh->fh_type;ep = fc_seq_exch(sp);if (ep->esb_stat & (ESB_ST_COMPLETE | ESB_ST_ABNORMAL)) {fc_frame_free(fp);goto out;}WARN_ON(!(ep->esb_stat & ESB_ST_SEQ_INIT));f_ctl = ntoh24(fh->fh_f_ctl);fc_exch_setup_hdr(ep, fp, f_ctl);fr_encaps(fp) = ep->encaps;    update sequence count if this frame is carrying   multiple FC frames when sequence offload is enabled   by LLD. ", "static struct fc_seq *fc_seq_start_next_locked(struct fc_seq *sp)": "fc_seq_start_next_locked() - Allocate a new sequence on the same  exchange as the supplied sequence   @sp: The sequenceexchange to get a new sequence for ", "/* * Locking notes: * * The EM code run in a per-CPU worker thread. * * To protect against concurrency between a worker thread code and timers, * sequence allocation and deallocation must be locked. *  - exchange refcnt can be done atomicly without locks. *  - sequence allocation must be locked by exch lock. *  - If the EM pool lock and ex_lock must be taken at the same time, then the *    EM pool lock must be taken before the ex_lock. ": "fc_exch_done() when done      with exchange and sequence tuple.  RX-inferred completion.      When we receive the next sequence on the same exchange, we can      retire the previous sequence ID.  (XXX not implemented).  Timeout.      R_A_TOV frees the sequence ID.  If we're waiting for ACK,      E_D_TOV causes abort and calls upper layer response handler      with FC_EX_TIMEOUT error.  Receive RJT      XXX defer.  Send ABTS      On timeout.     The following events may occur on recipient sequences:    Receive      Allocate sequence for first frame received.      Hold during receive handler.      Release when final frame received.      Keep status of last N of these for the ELS RES command.  XXX TBD.  Receive ABTS      Deallocate sequence  Send RJT      Deallocate     For now, we neglect conditions where only part of a sequence was   received or transmitted, or where out-of-order receipt is detected. ", "struct fc_seq *fc_seq_assign(struct fc_lport *lport, struct fc_frame *fp)": "fc_seq_release(). ", "void fc_exch_mgr_reset(struct fc_lport *lport, u32 sid, u32 did)": "fc_exch_mgr_reset() - Reset all EMs of a local port   @lport: The local port whose EMs are to be reset   @sid:   The source ID   @did:   The destination ID     Reset all EMs associated with a given local port. Release all   sequences and exchanges. If sid is non-zero then reset only the   exchanges sent from the local port's FID. If did is non-zero then   reset only exchanges destined for the local port's FID. ", "struct fc_seq *fc_exch_seq_send(struct fc_lport *lport,struct fc_frame *fp,void (*resp)(struct fc_seq *,     struct fc_frame *fp,     void *arg),void (*destructor)(struct fc_seq *, void *),void *arg, u32 timer_msec)": "fc_exch_seq_send() - Send a frame using a new exchange and sequence   @lport:The local port to send the frame on   @fp:The frame to be sent   @resp:The response handler for this request   @destructor: The destructor for the exchange   @arg:The argument to be passed to the response handler   @timer_msec: The timeout period for the exchange     The exchange response handler is set in this routine to resp()   function pointer. It can be called in two scenarios: if a timeout   occurs or if a response frame is received for the exchange. The   fc_frame pointer in response handler will also indicate timeout   as error using IS_ERR related macros.     The exchange destructor handler is also set in this routine.   The destructor handler is invoked by EM layer when exchange   is about to free, this can be used by caller to free its   resources along with exchange free.     The arg is passed back to resp and destructor handler.     The timeout value (in msec) for an exchange is set if non zero   timer_msec argument is specified. The timer is canceled when   it fires or when the exchange is done. The exchange timeout handler   is registered by EM layer.     The frame pointer with some of the header's fields must be   filled before calling this routine, those fields are:     - routing control   - FC port did   - FC port sid   - FC header type   - frame control   - parameter or relative offset ", "void fc_exch_update_stats(struct fc_lport *lport)": "fc_exch_update_stats() - update exches stats to lport   @lport: The local port to update exchange manager stats ", "struct fc_exch_mgr_anchor *fc_exch_mgr_add(struct fc_lport *lport,   struct fc_exch_mgr *mp,   bool (*match)(struct fc_frame *))": "fc_exch_mgr_add() - Add an exchange manager to a local port's list of EMs   @lport: The local port to add the exchange manager to   @mp:   The exchange manager to be added to the local port   @match: The match routine that indicates when this EM should be used ", "void fc_exch_mgr_del(struct fc_exch_mgr_anchor *ema)": "fc_exch_mgr_del() - Delete an EM from a local port's list   @ema: The exchange manager anchor identifying the EM to be deleted ", "int fc_exch_mgr_list_clone(struct fc_lport *src, struct fc_lport *dst)": "fc_exch_mgr_list_clone() - Share all exchange manager objects   @src: Source lport to clone exchange managers from   @dst: New lport that takes references to all the exchange managers ", "struct fc_exch_mgr *fc_exch_mgr_alloc(struct fc_lport *lport,      enum fc_class class,      u16 min_xid, u16 max_xid,      bool (*match)(struct fc_frame *))": "fc_exch_mgr_alloc() - Allocate an exchange manager   @lport:   The local port that the new EM will be associated with   @class:   The default FC class for new exchanges   @min_xid: The minimum XID for exchanges from the new EM   @max_xid: The maximum XID for exchanges from the new EM   @match:   The match routine for the new EM ", "void fc_exch_mgr_free(struct fc_lport *lport)": "fc_exch_mgr_free() - Free all exchange managers on a local port   @lport: The local port whose EMs are to be freed ", "static void fc_exch_recv_abts(struct fc_exch *ep, struct fc_frame *rx_fp)": "fc_exch_recv_abts() - Handle an incoming ABTS   @ep:   The exchange the abort was on   @rx_fp: The ABTS frame     This would be for target mode usually, but could be due to lost   FCP transfer ready, confirm or RRQ. We always handle this as an   exchange abort, ignoring the parameter. ", "int fc_exch_init(struct fc_lport *lport)": "fc_exch_init() - Initialize the exchange layer for a local port   @lport: The local port to initialize the exchange layer for ", "void fc_fill_hdr(struct fc_frame *fp, const struct fc_frame *in_fp, enum fc_rctl r_ctl, u32 f_ctl, u16 seq_cnt, u32 parm_offset)": "fc_fill_hdr() -  fill FC header fields based on request   @fp: reply frame containing header to be filled in   @in_fp: request frame containing header to use in filling in reply   @r_ctl: R_CTL value for header   @f_ctl: F_CTL value for header, with 0 pad   @seq_cnt: sequence count for the header, ignored if frame has a sequence   @parm_offset: parameter  offset value ", "void fc_fill_reply_hdr(struct fc_frame *fp, const struct fc_frame *in_fp,       enum fc_rctl r_ctl, u32 parm_offset)": "fc_fill_reply_hdr() -  fill FC reply header fields based on request   @fp: reply frame containing header to be filled in   @in_fp: request frame containing header to use in filling in reply   @r_ctl: R_CTL value for reply   @parm_offset: parameter  offset value ", "int fc_fc4_register_provider(enum fc_fh_type type, struct fc4_prov *prov)": "fc_fc4_register_provider() - register FC-4 upper-level provider.   @type: FC-4 type, such as FC_TYPE_FCP   @prov: structure describing provider including ops vector.     Returns 0 on success, negative error otherwise. ", "void fc_fc4_deregister_provider(enum fc_fh_type type, struct fc4_prov *prov)": "fc_fc4_deregister_provider() - deregister FC-4 upper-level provider.   @type: FC-4 type, such as FC_TYPE_FCP   @prov: structure describing provider including ops vector. ", "void fc_get_host_port_state(struct Scsi_Host *shost)": "fc_get_host_port_state() - Return the port state of the given Scsi_Host   @shost:  The SCSI host whose port state is to be determined ", "void fc_get_host_speed(struct Scsi_Host *shost)": "fc_get_host_speed() - Return the speed of the given Scsi_Host   @shost: The SCSI host whose port speed is to be determined ", "struct fc_host_statistics *fc_get_host_stats(struct Scsi_Host *shost)": "fc_get_host_stats() - Return the Scsi_Host's statistics   @shost: The SCSI host whose statistics are to be returned ", "int fc_fabric_login(struct fc_lport *lport)": "fc_fabric_login() - Start the lport state machine   @lport: The local port that should log into the fabric     Locking Note: This function should not be called   with the lport lock held. ", "void __fc_linkup(struct fc_lport *lport)": "fc_linkup() - Handler for transport linkup events   @lport: The lport whose link is up ", "void __fc_linkdown(struct fc_lport *lport)": "fc_linkdown() - Handler for transport linkdown events   @lport: The lport whose link is down ", "int fc_fabric_logoff(struct fc_lport *lport)": "fc_fabric_logoff() - Logout of the fabric   @lport: The local port to logoff the fabric     Return value:  0 for success, -1 for failure ", "int fc_lport_destroy(struct fc_lport *lport)": "fc_lport_destroy() - Unregister a fc_lport   @lport: The local port to unregister     Note:   exit routine for fc_lport instance   clean-up all the allocated memory   and free up other system resources.   ", "int fc_set_mfs(struct fc_lport *lport, u32 mfs)": "fc_set_mfs() - Set the maximum frame size for a local port   @lport: The local port to set the MFS for   @mfs:   The new MFS ", "void fc_lport_set_local_id(struct fc_lport *lport, u32 port_id)": "fc_lport_set_local_id() - set the local port Port ID for point-to-multipoint   @lport: The local port which will have its Port ID set.   @port_id: The new port ID.     Called by the lower-level driver when transport sets the local port_id.   This is used in VN_port to VN_port mode for FCoE, and causes FLOGI and   discovery to be skipped. ", "static void fc_lport_recv_rlir_req(struct fc_lport *lport, struct fc_frame *fp)": "fc_lport_recv_rlir_req() - Handle received Registered Link Incident Report.   @lport: Fibre Channel local port receiving the RLIR   @fp:   The RLIR request frame ", "int fc_lport_reset(struct fc_lport *lport)": "fc_lport_reset() - Reset a local port   @lport: The local port which should be reset     Locking Note: This functions should not be called with the   lport lock held. ", "void fc_lport_logo_resp(struct fc_seq *sp, struct fc_frame *fp,void *lp_arg)": "fc_lport_logo_resp() - Handle response to LOGO request   @sp:    The sequence that the LOGO was on   @fp:    The LOGO frame   @lp_arg: The lport port that received the LOGO request     Locking Note: This function will be called without the lport lock   held, but it will lock, call an _enter_  function or fc_lport_error()   and then unlock the lport. ", "void fc_lport_flogi_resp(struct fc_seq *sp, struct fc_frame *fp, void *lp_arg)": "fc_lport_flogi_resp() - Handle response to FLOGI request   @sp:    The sequence that the FLOGI was on   @fp:    The FLOGI response frame   @lp_arg: The lport port that received the FLOGI response     Locking Note: This function will be called without the lport lock   held, but it will lock, call an _enter_  function or fc_lport_error()   and then unlock the lport. ", "int fc_lport_config(struct fc_lport *lport)": "fc_lport_config() - Configure a fc_lport   @lport: The local port to be configured ", "int fc_lport_init(struct fc_lport *lport)": "fc_lport_init() - Initialize the lport layer for a local port   @lport: The local port to initialize the exchange layer for ", "int fc_lport_bsg_request(struct bsg_job *job)": "fc_lport_bsg_request() - The common entry point for sending      FC Passthrough requests   @job: The BSG passthrough job ", "int fc_queuecommand(struct Scsi_Host *shost, struct scsi_cmnd *sc_cmd)": "fc_queuecommand() - The queuecommand function of the SCSI template   @shost: The Scsi_Host that the command was issued to   @sc_cmd:   The scsi_cmnd to be executed     This is the io strategy routine, called by the SCSI layer. ", "int fc_eh_abort(struct scsi_cmnd *sc_cmd)": "fc_eh_abort() - Abort a command   @sc_cmd: The SCSI command to abort     From SCSI host template.   Send an ABTS to the target device and wait for the response. ", "int fc_eh_device_reset(struct scsi_cmnd *sc_cmd)": "fc_eh_device_reset() - Reset a single LUN   @sc_cmd: The SCSI command which identifies the device whose      LUN is to be reset     Set from SCSI host template. ", "int fc_eh_host_reset(struct scsi_cmnd *sc_cmd)": "fc_eh_host_reset() - Reset a Scsi_Host.   @sc_cmd: The SCSI command that identifies the SCSI host to be reset ", "int fc_slave_alloc(struct scsi_device *sdev)": "fc_slave_alloc() - Configure the queue depth of a Scsi_Host   @sdev: The SCSI device that identifies the SCSI host     Configures queue depth based on host's cmd_per_len. If not set   then we use the libfc default. ", "void fc_fcp_destroy(struct fc_lport *lport)": "fc_fcp_destroy() - Tear down the FCP layer for a given local port   @lport: The local port that no longer needs the FCP layer ", "int fc_fcp_init(struct fc_lport *lport)": "fc_fcp_init() - Initialize the FCP layer for a local port   @lport: The local port to initialize the exchange layer for ", "struct fc_seq *fc_elsct_send(struct fc_lport *lport, u32 did,     struct fc_frame *fp, unsigned int op,     void (*resp)(struct fc_seq *,  struct fc_frame *,  void *),     void *arg, u32 timer_msec)": "fc_elsct_send() - Send an ELS or CT frame   @lport:The local port to send the frame on   @did:The destination ID for the frame   @fp:The frame to be sent   @op:The operational code   @resp:The callback routine when the response is received   @arg:The argument to pass to the response callback routine   @timer_msec: The timeout period for the frame (in msecs) ", "int fc_elsct_init(struct fc_lport *lport)": "fc_elsct_init() - Initialize the ELSCT layer   @lport: The local port to initialize the ELSCT layer for ", "for (i = 0; i < ha->num_phys; i++) ": "sas_prep_resume_ha(struct sas_ha_struct  ha){int i;set_bit(SAS_HA_REGISTERED, &ha->state);set_bit(SAS_HA_RESUMING, &ha->state);  clear out any stale link eventsdata from the suspension path ", "i = phys_suspended(ha);if (i)dev_info(ha->dev, \"waiting up to 25 seconds for %d phy%s to resume\\n\", i, i > 1 ? \"s\" : \"\");wait_event_timeout(ha->eh_wait_q, phys_suspended(ha) == 0, tmo);for (i = 0; i < ha->num_phys; i++) ": "sas_resume_ha(struct sas_ha_struct  ha, bool drain){const unsigned long tmo = msecs_to_jiffies(25000);int i;  deform ports on phys that did not resume   at this point we may be racing the phy coming back (as posted   by the lldd).  So we post the event and once we are in the   libsas context check that the phy remains suspended before   tearing it down. ", "mutex_lock(&ha->drain_mutex);__sas_drain_work(ha);mutex_unlock(&ha->drain_mutex);}EXPORT_SYMBOL(sas_suspend_ha": "sas_suspend_ha(struct sas_ha_struct  ha){int i;sas_disable_events(ha);scsi_block_requests(ha->core.shost);for (i = 0; i < ha->num_phys; i++) {struct asd_sas_port  port = ha->sas_port[i];sas_discover_event(port, DISCE_SUSPEND);}  flush suspend events while unregistered ", "static struct kmem_cache *qla_tgt_mgmt_cmd_cachep;struct kmem_cache *qla_tgt_plogi_cachep;static mempool_t *qla_tgt_mgmt_cmd_mempool;static struct workqueue_struct *qla_tgt_wq;static DEFINE_MUTEX(qla_tgt_mutex);static LIST_HEAD(qla_tgt_glist);static const char *prot_op_str(u32 prot_op)": "qlt_unreg_sess(struct fc_port  sess);static void qlt_24xx_handle_abts(struct scsi_qla_host  ,struct abts_recv_from_24xx  );static void qlt_send_busy(struct qla_qpair  , struct atio_from_isp  ,    uint16_t);static int qlt_check_reserve_free_req(struct qla_qpair  qpair, uint32_t);static inline uint32_t qlt_make_handle(struct qla_qpair  );    Global Variables ", "mutex_lock(&vha->vha_tgt.tgt_mutex);tgt->tgt_stop = 1;qlt_clear_tgt_db(tgt);mutex_unlock(&vha->vha_tgt.tgt_mutex);mutex_unlock(&qla_tgt_mutex);ql_dbg(ql_dbg_tgt_mgt, vha, 0xf009,    \"Waiting for sess works (tgt %p)\", tgt);spin_lock_irqsave(&tgt->sess_work_lock, flags);do ": "qlt_stop_phase1(struct qla_tgt  tgt){struct scsi_qla_host  vha = tgt->vha;struct qla_hw_data  ha = tgt->ha;unsigned long flags;mutex_lock(&ha->optrom_mutex);mutex_lock(&qla_tgt_mutex);if (tgt->tgt_stop || tgt->tgt_stopped) {ql_dbg(ql_dbg_tgt_mgt, vha, 0xf04e,    \"Already in tgt->tgt_stop or tgt_stopped state\\n\");mutex_unlock(&qla_tgt_mutex);mutex_unlock(&ha->optrom_mutex);return -EPERM;}ql_dbg(ql_dbg_tgt_mgt, vha, 0xe003, \"Stopping target for host %ld(%p)\\n\",    vha->host_no, vha);    Mutex needed to sync with qla_tgt_fc_port_[added,deleted].   Lock is needed, because we still can get an incoming packet. ", "ql_dbg(ql_dbg_async, vha, 0xe100,\"RESET-TMR online/active/old-count/new-count = %d/%d/%d/%d.\\n\",vha->flags.online, qla2x00_reset_active(vha),mcmd->reset_count, qpair->chip_reset);ha->tgt.tgt_ops->free_mcmd(mcmd);spin_unlock_irqrestore(qpair->qp_lock_ptr, flags);return;}if (mcmd->flags == QLA24XX_MGMT_SEND_NACK) ": "qlt_xmit_tm_rsp(struct qla_tgt_mgmt_cmd  mcmd){struct scsi_qla_host  vha = mcmd->sess->vha;struct qla_hw_data  ha = vha->hw;unsigned long flags;struct qla_qpair  qpair = mcmd->qpair;bool free_mcmd = true;ql_dbg(ql_dbg_tgt_mgt, vha, 0xf013,    \"TM response mcmd (%p) status %#x state %#x\",    mcmd, mcmd->fc_tm_rsp, mcmd->flags);spin_lock_irqsave(qpair->qp_lock_ptr, flags);if (!vha->flags.online || mcmd->reset_count != qpair->chip_reset) {    Either the port is not online or this request was from   previous life, just abort the processing. ", "cmd->state = QLA_TGT_STATE_PROCESSED;ql_dbg_qp(ql_dbg_async, qpair, 0xe101,\"RESET-RSP online/active/old-count/new-count = %d/%d/%d/%d.\\n\",vha->flags.online, qla2x00_reset_active(vha),cmd->reset_count, qpair->chip_reset);res = 0;goto out_unmap_unlock;}/* Does F/W have an IOCBs for this request ": "qlt_xmit_response(struct qla_tgt_cmd  cmd, int xmit_type,uint8_t scsi_status){struct scsi_qla_host  vha = cmd->vha;struct qla_qpair  qpair = cmd->qpair;struct ctio7_to_24xx  pkt;struct qla_tgt_prm prm;uint32_t full_req_cnt = 0;unsigned long flags = 0;int res;if (!qpair->fw_started || (cmd->reset_count != qpair->chip_reset) ||    (cmd->sess && cmd->sess->deleted)) {cmd->state = QLA_TGT_STATE_PROCESSED;return 0;}ql_dbg_qp(ql_dbg_tgt, qpair, 0xe018,    \"is_send_status=%d, cmd->bufflen=%d, cmd->sg_cnt=%d, cmd->dma_data_direction=%d se_cmd[%p] qp %d\\n\",    (xmit_type & QLA_TGT_XMIT_STATUS) ?    1 : 0, cmd->bufflen, cmd->sg_cnt, cmd->dma_data_direction,    &cmd->se_cmd, qpair->id);res = qlt_pre_xmit_response(cmd, &prm, xmit_type, scsi_status,    &full_req_cnt);if (unlikely(res != 0)) {return res;}spin_lock_irqsave(qpair->qp_lock_ptr, flags);if (xmit_type == QLA_TGT_XMIT_STATUS)qpair->tgt_counters.core_qla_snd_status++;elseqpair->tgt_counters.core_qla_que_buf++;if (!qpair->fw_started || cmd->reset_count != qpair->chip_reset) {    Either the port is not online or this request was from   previous life, just abort the processing. ", "cmd->aborted = 1;cmd->write_data_transferred = 0;cmd->state = QLA_TGT_STATE_DATA_IN;vha->hw->tgt.tgt_ops->handle_data(cmd);ql_dbg_qp(ql_dbg_async, qpair, 0xe102,\"RESET-XFR online/active/old-count/new-count = %d/%d/%d/%d.\\n\",vha->flags.online, qla2x00_reset_active(vha),cmd->reset_count, qpair->chip_reset);return 0;}/* Calculate number of entries and segments required ": "qlt_rdy_to_xfer(struct qla_tgt_cmd  cmd){struct ctio7_to_24xx  pkt;struct scsi_qla_host  vha = cmd->vha;struct qla_tgt  tgt = cmd->tgt;struct qla_tgt_prm prm;unsigned long flags = 0;int res = 0;struct qla_qpair  qpair = cmd->qpair;memset(&prm, 0, sizeof(prm));prm.cmd = cmd;prm.tgt = tgt;prm.sg = NULL;prm.req_cnt = 1;if (!qpair->fw_started || (cmd->reset_count != qpair->chip_reset) ||    (cmd->sess && cmd->sess->deleted)) {    Either the port is not online or this request was from   previous life, just abort the processing. ", "ql_dbg(ql_dbg_tgt_mgt, vha, 0xf016,    \"multiple abort. %p transport_state %x, t_state %x, \"    \"se_cmd_flags %x\\n\", cmd, cmd->se_cmd.transport_state,    cmd->se_cmd.t_state, cmd->se_cmd.se_cmd_flags);return -EIO;}cmd->aborted = 1;cmd->trc_flags |= TRC_ABORT;spin_unlock_irqrestore(&cmd->cmd_lock, flags);qlt_send_term_exchange(cmd->qpair, cmd, &cmd->atio, 0, 1);return 0;}EXPORT_SYMBOL(qlt_abort_cmd": "qlt_abort_cmd(struct qla_tgt_cmd  cmd){struct qla_tgt  tgt = cmd->tgt;struct scsi_qla_host  vha = tgt->vha;struct se_cmd  se_cmd = &cmd->se_cmd;unsigned long flags;ql_dbg(ql_dbg_tgt_mgt, vha, 0xf014,    \"qla_target(%d): terminating exchange for aborted cmd=%p \"    \"(se_cmd=%p, tag=%llu)\", vha->vp_idx, cmd, &cmd->se_cmd,    se_cmd->tag);spin_lock_irqsave(&cmd->cmd_lock, flags);if (cmd->aborted) {if (cmd->sg_mapped)qlt_unmap_sg(vha, cmd);spin_unlock_irqrestore(&cmd->cmd_lock, flags);    It's normal to see 2 calls in this path:    1) XFER Rdy completion + CMD_T_ABORT    2) TCM TMR - drain_state_list ", "ql_dbg(ql_dbg_tgt_mgt, vha, 0xf016,    \"multiple abort. %p transport_state %x, t_state %x, \"    \"se_cmd_flags %x\\n\", cmd, cmd->se_cmd.transport_state,    cmd->se_cmd.t_state, cmd->se_cmd.se_cmd_flags);return -EIO;}cmd->aborted = 1;cmd->trc_flags |= TRC_ABORT;spin_unlock_irqrestore(&cmd->cmd_lock, flags);qlt_send_term_exchange(cmd->qpair, cmd, &cmd->atio, 0, 1);return 0;}EXPORT_SYMBOL(qlt_abort_cmd);void qlt_free_cmd(struct qla_tgt_cmd *cmd)": "qlt_free_cmd(cmd);vha->hw->tgt.num_qfull_cmds_alloc--;}}vha->hw->tgt.num_qfull_cmds_dropped = 0;}static void qlt_chk_exch_leak_thresh_hold(struct scsi_qla_host  vha){uint32_t total_leaked;total_leaked = vha->hw->tgt.num_qfull_cmds_dropped;if (vha->hw->tgt.leak_exchg_thresh_hold &&    (total_leaked > vha->hw->tgt.leak_exchg_thresh_hold)) {ql_dbg(ql_dbg_tgt, vha, 0xe079,    \"Chip reset due to exchange starvation: %d%d.\\n\",    total_leaked, vha->hw->cur_fw_xcb_count);if (IS_P3P_TYPE(vha->hw))set_bit(FCOE_CTX_RESET_NEEDED, &vha->dpc_flags);elseset_bit(ISP_ABORT_NEEDED, &vha->dpc_flags);qla2xxx_wake_dpc(vha);}}int qlt_abort_cmd(struct qla_tgt_cmd  cmd){struct qla_tgt  tgt = cmd->tgt;struct scsi_qla_host  vha = tgt->vha;struct se_cmd  se_cmd = &cmd->se_cmd;unsigned long flags;ql_dbg(ql_dbg_tgt_mgt, vha, 0xf014,    \"qla_target(%d): terminating exchange for aborted cmd=%p \"    \"(se_cmd=%p, tag=%llu)\", vha->vp_idx, cmd, &cmd->se_cmd,    se_cmd->tag);spin_lock_irqsave(&cmd->cmd_lock, flags);if (cmd->aborted) {if (cmd->sg_mapped)qlt_unmap_sg(vha, cmd);spin_unlock_irqrestore(&cmd->cmd_lock, flags);    It's normal to see 2 calls in this path:    1) XFER Rdy completion + CMD_T_ABORT    2) TCM TMR - drain_state_list ", "int qlt_lport_register(void *target_lport_ptr, u64 phys_wwpn,       u64 npiv_wwpn, u64 npiv_wwnn,       int (*callback)(struct scsi_qla_host *, void *, u64, u64))": "qlt_lport_register - register lport with external module     @target_lport_ptr: pointer for tcm_qla2xxx specific lport data   @phys_wwpn: physical port WWPN   @npiv_wwpn: NPIV WWPN   @npiv_wwnn: NPIV WWNN   @callback:  lport initialization callback for tcm_qla2xxx code ", "void qlt_lport_deregister(struct scsi_qla_host *vha)": "qlt_lport_deregister - Degister lport     @vha:  Registered scsi_qla_host pointer ", "i = SDMMC_GET_HDATA_WIDTH(mci_readl(host, HCON));if (!i) ": "dw_mci_probe(struct dw_mci  host){const struct dw_mci_drv_data  drv_data = host->drv_data;int width, i, ret = 0;u32 fifo_size;if (!host->pdata) {host->pdata = dw_mci_parse_dt(host);if (IS_ERR(host->pdata))return dev_err_probe(host->dev, PTR_ERR(host->pdata),     \"platform data not available\\n\");}host->biu_clk = devm_clk_get(host->dev, \"biu\");if (IS_ERR(host->biu_clk)) {dev_dbg(host->dev, \"biu clock not available\\n\");} else {ret = clk_prepare_enable(host->biu_clk);if (ret) {dev_err(host->dev, \"failed to enable biu clock\\n\");return ret;}}host->ciu_clk = devm_clk_get(host->dev, \"ciu\");if (IS_ERR(host->ciu_clk)) {dev_dbg(host->dev, \"ciu clock not available\\n\");host->bus_hz = host->pdata->bus_hz;} else {ret = clk_prepare_enable(host->ciu_clk);if (ret) {dev_err(host->dev, \"failed to enable ciu clock\\n\");goto err_clk_biu;}if (host->pdata->bus_hz) {ret = clk_set_rate(host->ciu_clk, host->pdata->bus_hz);if (ret)dev_warn(host->dev, \"Unable to set bus rate to %uHz\\n\", host->pdata->bus_hz);}host->bus_hz = clk_get_rate(host->ciu_clk);}if (!host->bus_hz) {dev_err(host->dev,\"Platform data must supply bus speed\\n\");ret = -ENODEV;goto err_clk_ciu;}if (host->pdata->rstc) {reset_control_assert(host->pdata->rstc);usleep_range(10, 50);reset_control_deassert(host->pdata->rstc);}if (drv_data && drv_data->init) {ret = drv_data->init(host);if (ret) {dev_err(host->dev,\"implementation specific init failed\\n\");goto err_clk_ciu;}}timer_setup(&host->cmd11_timer, dw_mci_cmd11_timer, 0);timer_setup(&host->cto_timer, dw_mci_cto_timer, 0);timer_setup(&host->dto_timer, dw_mci_dto_timer, 0);spin_lock_init(&host->lock);spin_lock_init(&host->irq_lock);INIT_LIST_HEAD(&host->queue);dw_mci_init_fault(host);    Get the host data width - this assumes that HCON has been set with   the correct values. ", "/* disable clock to CIU ": "dw_mci_remove(struct dw_mci  host){dev_dbg(host->dev, \"remove slot\\n\");if (host->slot)dw_mci_cleanup_slot(host->slot);mci_writel(host, RINTSTS, 0xFFFFFFFF);mci_writel(host, INTMASK, 0);   disable all mmc interrupt first ", "mci_writel(host, FIFOTH, host->fifoth_val);host->prev_blksz = 0;/* Put in max timeout ": "dw_mci_runtime_resume(struct device  dev){int ret = 0;struct dw_mci  host = dev_get_drvdata(dev);if (host->slot &&    (mmc_can_gpio_cd(host->slot->mmc) ||     !mmc_card_is_removable(host->slot->mmc))) {ret = clk_prepare_enable(host->biu_clk);if (ret)return ret;}ret = clk_prepare_enable(host->ciu_clk);if (ret)goto err;if (!dw_mci_ctrl_reset(host, SDMMC_CTRL_ALL_RESET_FLAGS)) {clk_disable_unprepare(host->ciu_clk);ret = -ENODEV;goto err;}if (host->use_dma && host->dma_ops->init)host->dma_ops->init(host);    Restore the initial value at FIFOTH register   And Invalidate the prev_blksz with zero ", "return 0;}EXPORT_SYMBOL(cqhci_resume": "cqhci_resume(struct mmc_host  mmc){  Re-enable is done upon first request ", "comp_status = cqhci_readl(cq_host, CQHCI_TCN);cqhci_writel(cq_host, comp_status, CQHCI_TCN);pr_debug(\"%s: cqhci: TCN: 0x%08lx\\n\", mmc_hostname(mmc), comp_status);spin_lock(&cq_host->lock);for_each_set_bit(tag, &comp_status, cq_host->num_slots) ": "cqhci_irq(struct mmc_host  mmc, u32 intmask, int cmd_error,      int data_error){u32 status;unsigned long tag = 0, comp_status;struct cqhci_host  cq_host = mmc->cqe_private;status = cqhci_readl(cq_host, CQHCI_IS);cqhci_writel(cq_host, status, CQHCI_IS);pr_debug(\"%s: cqhci: IRQ status: 0x%08x\\n\", mmc_hostname(mmc), status);if ((status & (CQHCI_IS_RED | CQHCI_IS_GCE | CQHCI_IS_ICCE)) ||    cmd_error || data_error) {if (status & CQHCI_IS_RED)mmc_debugfs_err_stats_inc(mmc, MMC_ERR_CMDQ_RED);if (status & CQHCI_IS_GCE)mmc_debugfs_err_stats_inc(mmc, MMC_ERR_CMDQ_GCE);if (status & CQHCI_IS_ICCE)mmc_debugfs_err_stats_inc(mmc, MMC_ERR_CMDQ_ICCE);cqhci_error_irq(mmc, status, cmd_error, data_error);}if (status & CQHCI_IS_TCC) {  read TCN and complete the request ", "cqhci_memres = platform_get_resource_byname(pdev, IORESOURCE_MEM,   \"cqhci\");if (!cqhci_memres) ": "cqhci_pltfm_init(struct platform_device  pdev){struct cqhci_host  cq_host;struct resource  cqhci_memres = NULL;  check and setup CMDQ interface ", "void mmc_request_done(struct mmc_host *host, struct mmc_request *mrq)": "mmc_request_done - finish processing an MMC request  @host: MMC host which completed request  @mrq: MMC request which request    MMC drivers should call this function when they have completed  their processing of a request. ", "err = mmc_retune(host);if (err) ": "mmc_start_request(struct mmc_host  host, struct mmc_request  mrq){int err;  Assumes host controller has been runtime resumed by mmc_claim_host ", "if (mrq->done)mrq->done(mrq);}EXPORT_SYMBOL(mmc_request_done);static void __mmc_start_request(struct mmc_host *host, struct mmc_request *mrq)": "mmc_wait_for_req_done(). ", "int mmc_cqe_start_req(struct mmc_host *host, struct mmc_request *mrq)": "mmc_cqe_start_req - Start a CQE request.   @host: MMC host to start the request   @mrq: request to start     Start the request, re-tuning if needed and it is possible. Returns an error   code if the request fails to start or -EBUSY if CQE is busy. ", "void mmc_cqe_request_done(struct mmc_host *host, struct mmc_request *mrq)": "mmc_cqe_request_done - CQE has finished processing an MMC request  @host: MMC host which completed request  @mrq: MMC request which completed    CQE drivers should call this function when they have completed  their processing of a request. ", "void mmc_cqe_post_req(struct mmc_host *host, struct mmc_request *mrq)": "mmc_cqe_post_req - CQE post process of a completed MMC request  @host: MMC host  @mrq: MMC request to be processed ", "int mmc_cqe_recovery(struct mmc_host *host)": "mmc_cqe_recovery - Recover from CQE errors.   @host: MMC host to recover     Recovery consists of stopping CQE, stopping eMMC, discarding the queue   in eMMC, and discarding the queue in CQE. CQE must call   mmc_cqe_request_done() on all requests. An error is returned if the eMMC   fails to discard its queue. ", "bool mmc_is_req_done(struct mmc_host *host, struct mmc_request *mrq)": "mmc_is_req_done - Determine if a 'cap_cmd_during_tfr' request is done  @host: MMC host  @mrq: MMC request    mmc_is_req_done() is used with requests that have  mrq->cap_cmd_during_tfr = true. mmc_is_req_done() must be called after  starting a request and before waiting for it to complete. That is,  either in between calls to mmc_start_req(), or after mmc_wait_for_req()  and before mmc_wait_for_req_done(). If it is called at other times the  result is not meaningful. ", "cmd.flags        = MMC_RSP_R1B | MMC_CMD_AC;cmd.flags       &= ~MMC_RSP_CRC; /* Ignore CRC ": "mmc_wait_for_cmd(host, &cmd, 0);memset(&cmd, 0, sizeof(cmd));cmd.opcode       = MMC_CMDQ_TASK_MGMT;cmd.arg          = 1;   Discard entire queue ", "void mmc_set_data_timeout(struct mmc_data *data, const struct mmc_card *card)": "mmc_set_data_timeout - set the timeout for a data command  @data: data phase for command  @card: the MMC card associated with the data transfer    Computes the data timeout parameters according to the  correct algorithm given the card type. ", "int __mmc_claim_host(struct mmc_host *host, struct mmc_ctx *ctx,     atomic_t *abort)": "__mmc_claim_host - exclusively claim a host  @host: mmc host to claim  @ctx: context that claims the host or NULL in which case the default  context will be used  @abort: whether or not the operation should be aborted    Claim a host for a set of operations.  If @abort is non null and  dereference a non-zero value then this will return prematurely with  that non-zero value without acquiring the lock.  Returns zero  with the lock held otherwise. ", "void mmc_release_host(struct mmc_host *host)": "mmc_release_host - release a host  @host: mmc host to release    Release a MMC host, allowing others to claim the host  for their operations. ", "if (cd_irq && !(host->caps & MMC_CAP_NEEDS_POLL))__pm_wakeup_event(host->ws, 5000);host->detect_change = 1;mmc_schedule_delayed_work(&host->detect, delay);}/** *mmc_detect_change - process change of state on a MMC socket *@host: host which changed state. *@delay: optional delay to wait before detection (jiffies) * *MMC drivers should call this when they detect a card has been *inserted or removed. The MMC layer will confirm that any *present card is still functional, and initialize any newly *inserted. ": "mmc_detect_change(struct mmc_host  host, unsigned long delay, bool cd_irq){    Prevent system sleep for 5s to allow user space to consume the   corresponding uevent. This is especially useful, when CD irq is used   as a system wakeup, but doesn't hurt in other cases. ", "if (arg == MMC_TRIM_ARG)erase_timeout = card->ext_csd.trim_timeout;elseerase_timeout = card->ext_csd.hc_erase_timeout;} else ": "mmc_erase_timeout(struct mmc_card  card,          unsigned int arg, unsigned int qty){unsigned int erase_timeout;if (arg == MMC_DISCARD_ARG ||    (arg == MMC_TRIM_ARG && card->ext_csd.rev >= 6)) {erase_timeout = card->ext_csd.trim_timeout;} else if (card->ext_csd.erase_group_def & 1) {  High Capacity Erase Group Size uses HC timeouts ", "if (card->ext_csd.feature_support & MMC_DISCARD_FEATURE)return 1;return 0;}EXPORT_SYMBOL(mmc_can_discard": "mmc_can_discard(struct mmc_card  card){    As there's no way to detect the discard support bit at v4.5   use the sw feature support filed. ", "if (mmc_card_mmc(card) && !(card->ext_csd.erase_group_def & 1))return card->pref_erase;max_discard = mmc_do_calc_max_discard(card, MMC_ERASE_ARG);if (mmc_can_trim(card)) ": "mmc_calc_max_discard(struct mmc_card  card){struct mmc_host  host = card->host;unsigned int max_discard, max_trim;    Without erase_group_def set, MMC erase timeout depends on clock   frequence which can change.  In that case, the best choice is   just the preferred erase size. ", "int mmc_hw_reset(struct mmc_card *card)": "mmc_hw_reset_for_init(struct mmc_host  host){mmc_pwrseq_reset(host);if (!(host->caps & MMC_CAP_HW_RESET) || !host->ops->card_hw_reset)return;host->ops->card_hw_reset(host);}     mmc_hw_reset - reset the card in hardware   @card: card to be reset     Hard reset the card. This function is only for upper layers, like the   block layer or card drivers. You cannot use it in host drivers (struct   mmc_card might be gone then).     Return: 0 on success, -errno on failure ", "if (!ret && host->ops->get_cd && !host->ops->get_cd(host)) ": "mmc_detect_card_removed(struct mmc_host  host){int ret;if (!host->card || mmc_card_removed(host->card))return 1;ret = host->bus_ops->alive(host);    Card detect status and alive check may be out of sync if card is   removed slowly, when card detect switch changes while cardslot   pads are still contacted in hardware (refer to \"SD Card Mechanical   Addendum, Appendix C: Card Detection Switch\"). So reschedule a   detect work 200ms later for this case. ", "if (card->ext_csd.rev < 3 ||    !mmc_card_mmc(card) ||    !mmc_card_is_blockaddr(card) ||     mmc_card_is_removable(card->host))return -ENOENT;/* * eMMC storage has two special boot partitions in addition to the * main one.  NVIDIA's bootloader linearizes eMMC boot0->boot1->main * accesses, this means that the partition table addresses are shifted * by the size of boot partitions.  In accordance with the eMMC * specification, the boot partition size is calculated as follows: * *boot partition size = 128K byte x BOOT_SIZE_MULT * * Calculate number of sectors occupied by the both boot partitions. ": "mmc_card_alternative_gpt_sector(struct mmc_card  card, sector_t  gpt_sector){unsigned int boot_sectors_num;if ((!(card->host->caps2 & MMC_CAP2_ALT_GPT_TEGRA)))return -EOPNOTSUPP;  filter out unrelated cards ", "if (ctx->cd_irq >= 0)irq = ctx->cd_irq;else if (!(host->caps & MMC_CAP_NEEDS_POLL))irq = gpiod_to_irq(ctx->cd_gpio);if (irq >= 0) ": "mmc_gpiod_request_cd_irq(struct mmc_host  host){struct mmc_gpio  ctx = host->slot.handler_priv;int irq = -EINVAL;int ret;if (host->slot.cd_irq >= 0 || !ctx || !ctx->cd_gpio)return;    Do not use IRQ if the platform prefers to poll, e.g., because that   IRQ number is already used by another unit and cannot be shared. ", "int mmc_gpiod_request_ro(struct mmc_host *host, const char *con_id, unsigned int idx, unsigned int debounce)": "mmc_gpiod_request_ro - request a gpio descriptor for write protection   @host: mmc host   @con_id: function within the GPIO consumer   @idx: index of the GPIO to obtain in the consumer   @debounce: debounce time in microseconds     Returns zero on success, else an error. ", "void mmc_run_bkops(struct mmc_card *card)": "mmc_run_bkops - Run BKOPS for supported cards  @card: MMC card to run BKOPS for    Run background operations synchronously for cards having manual BKOPS  enabled and in case it reports urgent BKOPS level.", "int mmc_register_driver(struct mmc_driver *drv)": "mmc_register_driver - register a media driver  @drv: MMC media driver ", "void mmc_unregister_driver(struct mmc_driver *drv)": "mmc_unregister_driver - unregister a media driver  @drv: MMC media driver ", "void mmc_retune_disable(struct mmc_host *host)": "mmc_retune_release(host);}}EXPORT_SYMBOL(mmc_retune_unpause);     mmc_retune_disable() - exit a transfer mode that requires retuning   @host: host which should not retune anymore     It is not meant for temporarily preventing retuning! ", "int mmc_of_parse_voltage(struct mmc_host *host, u32 *mask)": "mmc_of_parse_voltage - return mask of supported voltages   @host: host whose properties should be parsed.   @mask: mask of voltages available for MMCSDSDIO     Parse the \"voltage-ranges\" property, returning zero if it is not   found, negative errno if the voltage-range specification is invalid,   or one if the voltage-range is specified and successfully parsed. ", "struct mmc_host *mmc_alloc_host(int extra, struct device *dev)": "mmc_alloc_host - initialise the per-host structure.  @extra: sizeof private data structure  @dev: pointer to host device model structure    Initialise the per-host structure. ", "int mmc_add_host(struct mmc_host *host)": "mmc_free_host( (struct mmc_host   )res);}struct mmc_host  devm_mmc_alloc_host(struct device  dev, int extra){struct mmc_host   dr,  host;dr = devres_alloc(devm_mmc_host_release, sizeof( dr), GFP_KERNEL);if (!dr)return NULL;host = mmc_alloc_host(extra, dev);if (!host) {devres_free(dr);return NULL;} dr = host;devres_add(dev, dr);return host;}EXPORT_SYMBOL(devm_mmc_alloc_host);static int mmc_validate_host_caps(struct mmc_host  host){struct device  dev = host->parent;u32 caps = host->caps, caps2 = host->caps2;if (caps & MMC_CAP_SDIO_IRQ && !host->ops->enable_sdio_irq) {dev_warn(dev, \"missing ->enable_sdio_irq() ops\\n\");return -EINVAL;}if (caps2 & (MMC_CAP2_HS400_ES | MMC_CAP2_HS400) &&    !(caps & MMC_CAP_8_BIT_DATA) && !(caps2 & MMC_CAP2_NO_MMC)) {dev_warn(dev, \"drop HS400 support since no 8-bit bus\\n\");host->caps2 = caps2 & ~MMC_CAP2_HS400_ES & ~MMC_CAP2_HS400;}return 0;}    mmc_add_host - initialise host hardware  @host: mmc host    Register the host with the driver model. The host must be  prepared to start servicing requests before this function  completes. ", "void mmc_remove_host(struct mmc_host *host)": "mmc_remove_host - remove host hardware  @host: mmc host    Unregister and remove all cards associated with this host,  and power down the MMC bus. No new requests will be issued  after this function has returned. ", "int fw_csr_string(const u32 *directory, int key, char *buf, size_t size)": "fw_csr_string() - reads a string from the configuration ROM   @directory:e.g. root directory or unit directory   @key:the key of the preceding directory entry   @buf:where to put the string   @size:size of @buf, in bytes     The string is taken from a minimal ASCII text descriptor leaf after   the immediate entry with @key.  The string is zero-terminated.   An overlong string is silently truncated such that it and the   zero byte fit into @size.     Returns strlen(buf) or a negative error code. ", "smp_rmb();return device->card->driver->enable_phys_dma(device->card,     device->node_id,     generation);}EXPORT_SYMBOL(fw_device_enable_phys_dma": "fw_device_enable_phys_dma(struct fw_device  device){int generation = device->generation;  device->node_id, accessed below, must not be older than generation ", "if (card->driver->cancel_packet(card, &transaction->packet) == 0)return 0;/* * If the request packet has already been sent, we need to see * if the transaction is still pending and remove it in that case. ": "fw_cancel_transaction(struct fw_card  card,  struct fw_transaction  transaction){u32 tstamp;    Cancel the packet transmission if it's still queued.  That   will call the packet transmission callback which cancels   the transaction. ", "void __fw_send_request(struct fw_card *card, struct fw_transaction *t, int tcode,int destination_id, int generation, int speed, unsigned long long offset,void *payload, size_t length, union fw_transaction_callback callback,bool with_tstamp, void *callback_data)": "fw_run_transaction() in a context that can sleep.     In case of lock requests, specify one of the firewire-core specific %TCODE_   constants instead of %TCODE_LOCK_REQUEST in @tcode.     Make sure that the value in @destination_id is not older than the one in   @generation.  Otherwise the request is in danger to be sent to a wrong node.     In case of asynchronous stream packets i.e. %TCODE_STREAM_DATA, the caller   needs to synthesize @destination_id with fw_stream_packet_destination_id().   It will contain tag, channel, and sy data instead of a node ID then.     The payload buffer at @data is going to be DMA-mapped except in case of   @length <= 8 or of local (loopback) requests.  Hence make sure that the   buffer complies with the restrictions of the streaming DMA mapping API.   @payload must not be freed before the @callback is called.     In case of request types without payload, @data is NULL and @length is 0.     After the transaction is completed successfully or unsuccessfully, the   @callback will be called.  Among its parameters is the response code which   is either one of the rcodes per IEEE 1394 or, in case of internal errors,   the firewire-core specific %RCODE_SEND_ERROR.  The other firewire-core   specific rcodes (%RCODE_CANCELLED, %RCODE_BUSY, %RCODE_GENERATION,   %RCODE_NO_ACK) denote transaction timeout, busy responder, stale request   generation, or missing ACK respectively.     Note some timing corner cases:  fw_send_request() may complete much earlier   than when the request packet actually hits the wire.  On the other hand,   transaction completion and hence execution of @callback may happen even   before fw_send_request() returns. ", "int fw_core_add_address_handler(struct fw_address_handler *handler,const struct fw_address_region *region)": "fw_core_add_address_handler() - register for incoming requests   @handler:callback   @region:region in the IEEE 1212 node space address range     region->start, ->end, and handler->length have to be quadlet-aligned.     When a request is received that falls within the specified address range,   the specified callback is invoked.  The parameters passed to the callback   give the details of the particular request.     To be called in process context.   Return value:  0 on success, non-zero otherwise.     The start offset of the handler's address region is determined by   fw_core_add_address_handler() and is returned in handler->offset.     Address allocations are exclusive, except for the FCP registers. ", "void fw_core_remove_address_handler(struct fw_address_handler *handler)": "fw_core_remove_address_handler() - unregister an address handler   @handler: callback     To be called in process context.     When fw_core_remove_address_handler() returns, @handler->callback() is   guaranteed to not run on any CPU anymore. ", "void fw_send_response(struct fw_card *card,      struct fw_request *request, int rcode)": "fw_send_response: - send response packet for asynchronous transaction.   @card:interface to send the response at.   @request:firewire request data for the transaction.   @rcode:response code to send.     Submit a response packet into the asynchronous response transmission queue. The @request   is going to be released when the transmission successfully finishes later. ", "int fw_get_request_speed(struct fw_request *request)": "fw_get_request_speed() - returns speed at which the @request was received   @request: firewire request data ", "return;}offset = ((u64)HEADER_GET_OFFSET_HIGH(p->header[1]) << 32) |p->header[2];if (!is_in_fcp_region(offset, request->length))handle_exclusive_region_request(card, p, request, offset);elsehandle_fcp_region_request(card, p, request, offset);}EXPORT_SYMBOL(fw_core_handle_request": "fw_core_handle_request(struct fw_card  card, struct fw_packet  p){struct fw_request  request;unsigned long long offset;if (p->ack != ACK_PENDING && p->ack != ACK_COMPLETE)return;if (TCODE_IS_LINK_INTERNAL(HEADER_GET_TCODE(p->header[0]))) {fw_cdev_handle_phy_packet(card, p);return;}request = allocate_request(card, p);if (request == NULL) {  FIXME: send statically allocated busy packet. ", "switch (tcode) ": "fw_core_handle_response(struct fw_card  card, struct fw_packet  p){struct fw_transaction  t = NULL,  iter;unsigned long flags;u32  data;size_t data_length;int tcode, tlabel, source, rcode;tcode= HEADER_GET_TCODE(p->header[0]);tlabel= HEADER_GET_TLABEL(p->header[0]);source= HEADER_GET_SOURCE(p->header[1]);rcode= HEADER_GET_RCODE(p->header[1]);spin_lock_irqsave(&card->lock, flags);list_for_each_entry(iter, &card->transaction_list, link) {if (iter->node_id == source && iter->tlabel == tlabel) {if (!try_cancel_split_timeout(iter)) {spin_unlock_irqrestore(&card->lock, flags);goto timed_out;}list_del_init(&iter->link);card->tlabel_mask &= ~(1ULL << iter->tlabel);t = iter;break;}}spin_unlock_irqrestore(&card->lock, flags);if (!t) { timed_out:fw_notice(card, \"unsolicited response (source %x, tlabel %x)\\n\",  source, tlabel);return;}    FIXME: sanity check packet, is length correct, does tcodes   and addresses match. ", "const char *fw_rcode_string(int rcode)": "fw_rcode_string - convert a firewire result code to an error description   @rcode: the result code ", "i = 0;while (i < desc->length)i += (desc->data[i] >> 16) + 1;if (i != desc->length)return -EINVAL;mutex_lock(&card_mutex);if (config_rom_length + required_space(desc) > 256) ": "fw_core_add_descriptor(struct fw_descriptor  desc){size_t i;int ret;    Check descriptor is valid; the length of all blocks in the   descriptor has to add up to exactly the length of the   block. ", "card->br_short = short_reset;/* Use an arbitrary short delay to combine multiple reset requests. ": "fw_schedule_bus_reset(struct fw_card  card, bool delayed, bool short_reset){  We don't try hard to sort out requests of long vs. short resets. ", "dummy_driver.free_iso_context= card->driver->free_iso_context;dummy_driver.stop_iso= card->driver->stop_iso;card->driver = &dummy_driver;spin_lock_irqsave(&card->lock, flags);fw_destroy_nodes(card);spin_unlock_irqrestore(&card->lock, flags);/* Wait for all users, especially device workqueue jobs, to finish. ": "fw_core_remove_card(struct fw_card  card){struct fw_card_driver dummy_driver = dummy_driver_template;unsigned long flags;card->driver->update_phy_reg(card, 4,     PHY_LINK_ACTIVE | PHY_CONTENDER, 0);fw_schedule_bus_reset(card, false, true);mutex_lock(&card_mutex);list_del_init(&card->link);mutex_unlock(&card_mutex);  Switch off most of the card driver interface. ", "if (!is_next_generation(generation, card->generation) &&    card->local_node != NULL) ": "fw_core_handle_bus_reset(struct fw_card  card, int node_id, int generation,      int self_id_count, u32  self_ids, bool bm_abdicate){struct fw_node  local_node;unsigned long flags;spin_lock_irqsave(&card->lock, flags);    If the selfID buffer is not the immediate successor of the   previously processed one, we cannot reliably compare the   old and new topologies. ", "void fw_iso_resource_manage(struct fw_card *card, int generation,    u64 channels_mask, int *channel, int *bandwidth,    bool allocate)": "fw_iso_resource_manage() - Allocate or deallocate a channel andor bandwidth   @card: card interface for this action   @generation: bus generation   @channels_mask: bitmask for channel allocation   @channel: pointer for returning channel allocation result   @bandwidth: pointer for returning bandwidth allocation result   @allocate: whether to allocate (true) or deallocate (false)     In parameters: card, generation, channels_mask, bandwidth, allocate   Out parameters: channel, bandwidth     This function blocks (sleeps) during communication with the IRM.     Allocates or deallocates at most one channel out of channels_mask.   channels_mask is a bitfield with MSB for channel 63 and LSB for channel 0.   (Note, the IRM's CHANNELS_AVAILABLE is a big-endian bitfield with MSB for   channel 0 and LSB for channel 63.)   Allocates or deallocates as many bandwidth allocation units as specified.     Returns channel < 0 if no channel was allocated or deallocated.   Returns bandwidth = 0 if no bandwidth was allocated or deallocated.     If generation is stale, deallocations succeed but allocations fail with   channel = -EAGAIN.     If channel allocation fails, no bandwidth will be allocated either.   If bandwidth allocation fails, no channel will be allocated either.   But deallocations of channel and bandwidth are tried independently   of each other's success. ", "void devfreq_get_freq_range(struct devfreq *devfreq,    unsigned long *min_freq,    unsigned long *max_freq)": "devfreq_get_freq_range() - Get the current freq range   @devfreq:the devfreq instance   @min_freq:the min frequency   @max_freq:the max frequency     This takes into consideration all constraints. ", "int devfreq_update_status(struct devfreq *devfreq, unsigned long freq)": "devfreq_update_status() - Update statistics of devfreq behavior   @devfreq:the devfreq instance   @freq:the update target frequency ", "int devfreq_update_target(struct devfreq *devfreq, unsigned long freq)": "update_devfreq() and devfreq governors. ", "void devfreq_monitor_start(struct devfreq *devfreq)": "devfreq_monitor_start() - Start load monitoring of devfreq instance   @devfreq:the devfreq instance.     Helper function for starting devfreq device load monitoring. By   default delayed work based monitoring is supported. Function   to be called from governor in response to DEVFREQ_GOV_START   event when device is added to devfreq framework. ", "void devfreq_monitor_stop(struct devfreq *devfreq)": "devfreq_monitor_stop() - Stop load monitoring of a devfreq instance   @devfreq:the devfreq instance.     Helper function to stop devfreq device load monitoring. Function   to be called from governor in response to DEVFREQ_GOV_STOP   event when device is removed from devfreq framework. ", "void devfreq_monitor_suspend(struct devfreq *devfreq)": "devfreq_monitor_suspend() - Suspend load monitoring of a devfreq instance   @devfreq:the devfreq instance.     Helper function to suspend devfreq device load monitoring. Function   to be called from governor in response to DEVFREQ_GOV_SUSPEND   event or when polling interval is set to zero.     Note: Though this function is same as devfreq_monitor_stop(),   intentionally kept separate to provide hooks for collecting   transition statistics. ", "void devfreq_monitor_resume(struct devfreq *devfreq)": "devfreq_monitor_resume() - Resume load monitoring of a devfreq instance   @devfreq:    the devfreq instance.     Helper function to resume devfreq device load monitoring. Function   to be called from governor in response to DEVFREQ_GOV_RESUME   event or when polling interval is set to non-zero. ", "void devfreq_update_interval(struct devfreq *devfreq, unsigned int *delay)": "devfreq_update_interval() - Update device devfreq monitoring interval   @devfreq:    the devfreq instance.   @delay:      new polling interval to be set.     Helper function to set new load monitoring polling interval. Function   to be called from governor in response to DEVFREQ_GOV_UPDATE_INTERVAL event. ", "static struct devfreq_governor *try_then_request_governor(const char *name)": "devfreq_add_device) are built as modules.   devfreq_list_lock should be held by the caller. Returns the matched   governor's pointer or an error pointer. ", "int devfreq_remove_device(struct devfreq *devfreq)": "devfreq_remove_device(devfreq);devfreq = NULL;err_dev:kfree(devfreq);err_out:return ERR_PTR(err);}EXPORT_SYMBOL(devfreq_add_device);     devfreq_remove_device() - Remove devfreq feature from a device.   @devfreq:the devfreq instance to be removed     The opposite of devfreq_add_device(). ", "struct devfreq *devm_devfreq_add_device(struct device *dev,struct devfreq_dev_profile *profile,const char *governor_name,void *data)": "devm_devfreq_add_device() - Resource-managed devfreq_add_device()   @dev:the device to add devfreq feature.   @profile:device-specific profile to run devfreq.   @governor_name:name of the policy to choose frequency.   @data: devfreq driver pass to governors, governor should not change it.     This function manages automatically the memory of devfreq device using device   resource management and simplify the free operation for memory of devfreq   device. ", "void devm_devfreq_remove_device(struct device *dev, struct devfreq *devfreq)": "devm_devfreq_remove_device() - Resource-managed devfreq_remove_device()   @dev:the device from which to remove devfreq feature.   @devfreq:the devfreq instance to be removed ", "int devfreq_suspend_device(struct devfreq *devfreq)": "devfreq_suspend_device() - Suspend devfreq of a device.   @devfreq: the devfreq instance to be suspended     This function is intended to be called by the pm callbacks   (e.g., runtime_suspend, suspend) of the device driver that   holds the devfreq. ", "int devfreq_resume_device(struct devfreq *devfreq)": "devfreq_resume_device() - Resume devfreq of a device.   @devfreq: the devfreq instance to be resumed     This function is intended to be called by the pm callbacks   (e.g., runtime_resume, resume) of the device driver that   holds the devfreq. ", "int devfreq_add_governor(struct devfreq_governor *governor)": "devfreq_add_governor() - Add devfreq governor   @governor:the devfreq governor to be added ", "int devm_devfreq_add_governor(struct device *dev,      struct devfreq_governor *governor)": "devfreq_remove_governor(void  governor){WARN_ON(devfreq_remove_governor(governor));}     devm_devfreq_add_governor() - Add devfreq governor   @dev:device which adds devfreq governor   @governor:the devfreq governor to be added     This is a resource-managed variant of devfreq_add_governor(). ", "struct dev_pm_opp *devfreq_recommended_opp(struct device *dev,   unsigned long *freq,   u32 flags)": "devfreq_recommended_opp() - Helper function to get proper OPP for the       freq value given to target callback.   @dev:The devfreq user device. (parent of devfreq)   @freq:The frequency given to target function   @flags:Flags handed from devfreq framework.     The callers are required to call dev_pm_opp_put() for the returned OPP after   use. ", "int devfreq_register_opp_notifier(struct device *dev, struct devfreq *devfreq)": "devfreq_register_opp_notifier() - Helper function to get devfreq notified       for any changes in the OPP availability       changes   @dev:The devfreq user device. (parent of devfreq)   @devfreq:The devfreq object. ", "int devfreq_unregister_opp_notifier(struct device *dev, struct devfreq *devfreq)": "devfreq_unregister_opp_notifier() - Helper function to stop getting devfreq         notified for any changes in the OPP         availability changes anymore.   @dev:The devfreq user device. (parent of devfreq)   @devfreq:The devfreq object.     At exit() callback of devfreq_dev_profile, this must be included if   devfreq_recommended_opp is used. ", "int devm_devfreq_register_opp_notifier(struct device *dev,       struct devfreq *devfreq)": "devm_devfreq_register_opp_notifier() - Resource-managed    devfreq_register_opp_notifier()   @dev:The devfreq user device. (parent of devfreq)   @devfreq:The devfreq object. ", "void devm_devfreq_unregister_opp_notifier(struct device *dev, struct devfreq *devfreq)": "devm_devfreq_unregister_opp_notifier() - Resource-managed      devfreq_unregister_opp_notifier()   @dev:The devfreq user device. (parent of devfreq)   @devfreq:The devfreq object. ", "int devfreq_register_notifier(struct devfreq *devfreq,      struct notifier_block *nb,      unsigned int list)": "devfreq_register_notifier() - Register a driver with devfreq   @devfreq:The devfreq object.   @nb:The notifier block to register.   @list:DEVFREQ_TRANSITION_NOTIFIER. ", "int devfreq_unregister_notifier(struct devfreq *devfreq,struct notifier_block *nb,unsigned int list)": "devfreq_unregister_notifier() - Unregister a driver with devfreq   @devfreq:The devfreq object.   @nb:The notifier block to be unregistered.   @list:DEVFREQ_TRANSITION_NOTIFIER. ", "int devm_devfreq_register_notifier(struct device *dev,struct devfreq *devfreq,struct notifier_block *nb,unsigned int list)": "devm_devfreq_register_notifier()  - Resource-managed devfreq_register_notifier()   @dev:The devfreq user device. (parent of devfreq)   @devfreq:The devfreq object.   @nb:The notifier block to be unregistered.   @list:DEVFREQ_TRANSITION_NOTIFIER. ", "void devm_devfreq_unregister_notifier(struct device *dev,      struct devfreq *devfreq,      struct notifier_block *nb,      unsigned int list)": "devm_devfreq_unregister_notifier()  - Resource-managed devfreq_unregister_notifier()   @dev:The devfreq user device. (parent of devfreq)   @devfreq:The devfreq object.   @nb:The notifier block to be unregistered.   @list:DEVFREQ_TRANSITION_NOTIFIER. ", "int rio_query_mport(struct rio_mport *port,    struct rio_mport_attr *mport_attr)": "rio_query_mport - Query mport device attributes   @port: mport device to query   @mport_attr: mport attributes data structure     Returns attributes of specified mport through the   pointer to attributes data structure. ", "int intel_scu_ipc_dev_ioread8(struct intel_scu_ipc_dev *scu, u16 addr, u8 *data)": "intel_scu_ipc_dev_ioread8() - Read a byte via the SCU   @scu: Optional SCU IPC instance   @addr: Register on SCU   @data: Return pointer for read byte     Read a single register. Returns %0 on success or an error code. All   locking between SCU accesses is handled for the caller.     This function may sleep. ", "int intel_scu_ipc_dev_iowrite8(struct intel_scu_ipc_dev *scu, u16 addr, u8 data)": "intel_scu_ipc_dev_iowrite8() - Write a byte via the SCU   @scu: Optional SCU IPC instance   @addr: Register on SCU   @data: Byte to write     Write a single register. Returns %0 on success or an error code. All   locking between SCU accesses is handled for the caller.     This function may sleep. ", "int intel_scu_ipc_dev_readv(struct intel_scu_ipc_dev *scu, u16 *addr, u8 *data,    size_t len)": "intel_scu_ipc_dev_readv() - Read a set of registers   @scu: Optional SCU IPC instance   @addr: Register list   @data: Bytes to return   @len: Length of array     Read registers. Returns %0 on success or an error code. All locking   between SCU accesses is handled for the caller.     The largest array length permitted by the hardware is 5 items.     This function may sleep. ", "int intel_scu_ipc_dev_writev(struct intel_scu_ipc_dev *scu, u16 *addr, u8 *data,     size_t len)": "intel_scu_ipc_dev_writev() - Write a set of registers   @scu: Optional SCU IPC instance   @addr: Register list   @data: Bytes to write   @len: Length of array     Write registers. Returns %0 on success or an error code. All locking   between SCU accesses is handled for the caller.     The largest array length permitted by the hardware is 5 items.     This function may sleep. ", "int intel_scu_ipc_dev_update(struct intel_scu_ipc_dev *scu, u16 addr, u8 data,     u8 mask)": "intel_scu_ipc_dev_update() - Update a register   @scu: Optional SCU IPC instance   @addr: Register address   @data: Bits to update   @mask: Mask of bits to update     Read-modify-write power control unit register. The first data argument   must be register value and second is mask value mask is a bitmap that   indicates which bits to update. %0 = masked. Don't modify this bit, %1 =   modify this bit. returns %0 on success or an error code.     This function may sleep. Locking between SCU accesses is handled   for the caller. ", "int intel_scu_ipc_dev_simple_command(struct intel_scu_ipc_dev *scu, int cmd,     int sub)": "intel_scu_ipc_dev_simple_command() - Send a simple command   @scu: Optional SCU IPC instance   @cmd: Command   @sub: Sub type     Issue a simple command to the SCU. Do not use this interface if you must   then access data as any data values may be overwritten by another SCU   access by the time this function returns.     This function may sleep. Locking for SCU accesses is handled for the   caller. ", "int intel_scu_ipc_dev_command_with_size(struct intel_scu_ipc_dev *scu, int cmd,int sub, const void *in, size_t inlen,size_t size, void *out, size_t outlen)": "intel_scu_ipc_dev_command_with_size() - Command with data   @scu: Optional SCU IPC instance   @cmd: Command   @sub: Sub type   @in: Input data   @inlen: Input length in bytes   @size: Input size written to the IPC command register in whatever    units (dword, byte) the particular firmware requires. Normally    should be the same as @inlen.   @out: Output data   @outlen: Output length in bytes     Issue a command to the SCU which involves data transfers. Do the   data copies under the lock but leave it for the caller to interpret. ", "void wmi_driver_unregister(struct wmi_driver *driver)": "wmi_driver_unregister() - Unregister a WMI driver   @driver: WMI driver to unregister     Unregisters a WMI driver from the WMI bus. ", "int dcdbas_smi_request(struct smi_cmd *smi_cmd)": "dcdbas_smi_request: generate SMI request     Called with smi_data_lock. ", "int intel_punit_ipc_simple_command(int cmd, int para1, int para2)": "intel_punit_ipc_simple_command() - Simple IPC command   @cmd:IPC command code.   @para1:First 8bit parameter, set 0 if not used.   @para2:Second 8bit parameter, set 0 if not used.     Send a IPC command to P-Unit when there is no data transaction     Return:IPC error code or 0 on success. ", "int cros_ec_prepare_tx(struct cros_ec_device *ec_dev,       struct cros_ec_command *msg)": "cros_ec_prepare_tx() - Prepare an outgoing message in the output buffer.   @ec_dev: Device to register.   @msg: Message to write.     This is used by all ChromeOS EC drivers to prepare the outgoing message   according to different protocol versions.     Return: number of prepared bytes on success or negative error code. ", "int cros_ec_check_result(struct cros_ec_device *ec_dev, struct cros_ec_command *msg)": "cros_ec_check_result() - Check ec_msg->result.   @ec_dev: EC device.   @msg: Message to check.     This is used by ChromeOS EC drivers to check the ec_msg->result for   EC_RES_IN_PROGRESS and to warn about them.     The function should not check for furthermore error codes.  Otherwise,   it would break the ABI.     Return: -EAGAIN if ec_msg->result == EC_RES_IN_PROGRESS.  Otherwise, 0. ", "int cros_ec_query_all(struct cros_ec_device *ec_dev)": "cros_ec_query_all() -  Query the protocol version supported by the           ChromeOS EC.   @ec_dev: Device to register.     Return: 0 on success or negative error code. ", "int cros_ec_cmd_xfer(struct cros_ec_device *ec_dev, struct cros_ec_command *msg)": "cros_ec_cmd_xfer_status() instead since   that function implements the conversion.     Return:   >0 - EC command was executed successfully. The return value is the number        of bytes returned by the EC (excluding the header).   =0 - EC communication was successful. EC command execution results are        reported in msg->result. The result will be EC_RES_SUCCESS if the        command was executed successfully or report an EC command execution        error.   <0 - EC communication error. Return value is the Linux error code. ", "int cros_ec_get_next_event(struct cros_ec_device *ec_dev,   bool *wake_event,   bool *has_more_events)": "cros_ec_get_next_event() - Fetch next event from the ChromeOS EC.   @ec_dev: Device to fetch event from.   @wake_event: Pointer to a bool set to true upon return if the event might be                treated as a wake event. Ignored if null.   @has_more_events: Pointer to bool set to true if more than one event is                pending.                Some EC will set this flag to indicate cros_ec_get_next_event()                can be called multiple times in a row.                It is an optimization to prevent issuing a EC command for                nothing or wait for another interrupt from the EC to process                the next message.                Ignored if null.     Return: negative error code on errors; 0 for no data; or else number of   bytes received (i.e., an event was retrieved successfully). Event types are   written out to @ec_dev->event_data.event_type on success. ", "static int cros_ec_get_host_event_wake_mask(struct cros_ec_device *ec_dev, uint32_t *mask)": "cros_ec_get_host_event_wake_mask     Get the mask of host events that cause wake from suspend.     @ec_dev: EC device to call   @msg: message structure to use   @mask: result when function returns 0.     LOCKING:   the caller has ec_dev->lock mutex, or the caller knows there is   no other command in progress. ", "irqreturn_t cros_ec_irq_thread(int irq, void *data)": "cros_ec_irq_thread() - bottom half part of the interrupt handler   @irq: IRQ id   @data: (ec_dev) Device with events to process.     Return: Interrupt handled. ", "int cros_ec_register(struct cros_ec_device *ec_dev)": "cros_ec_register() - Register a new ChromeOS EC, using the provided info.   @ec_dev: Device to register.     Before calling this, allocate a pointer to a new device and then fill   in all the fields up to the --private-- marker.     Return: 0 on success or negative error code. ", "void cros_ec_unregister(struct cros_ec_device *ec_dev)": "cros_ec_unregister() - Remove a ChromeOS EC.   @ec_dev: Device to unregister.     Call this to deregister a ChromeOS EC, then clean up any private data.     Return: 0 on success or negative error code. ", "int cros_ec_suspend(struct cros_ec_device *ec_dev)": "cros_ec_suspend() - Handle a suspend operation for the ChromeOS EC device.   @ec_dev: Device to suspend.     This can be called by drivers to handle a suspend event.     Return: 0 on success or negative error code. ", "int cros_ec_resume(struct cros_ec_device *ec_dev)": "cros_ec_resume() - Handle a resume operation for the ChromeOS EC device.   @ec_dev: Device to resume.     This can be called by drivers to handle a resume event.     Return: 0 on success or negative error code. ", "u8 cros_ec_lpc_io_bytes_mec(enum cros_ec_lpc_mec_io_type io_type,    unsigned int offset, unsigned int length,    u8 *buf)": "cros_ec_lpc_io_bytes_mec() - Read  write bytes to MEC EMI port.     @io_type: MEC_IO_READ or MEC_IO_WRITE, depending on request   @offset:  Base read  write address   @length:  Number of bytes to read  write   @buf:     Destination  source buffer     Return: 8-bit checksum of all bytes read  written ", "#include <linux/export.h>#include <linux/rtc.h>static const unsigned char rtc_days_in_month[] = ": "rtc_time64_to_tm) ", "time64_t rtc_tm_to_time64(struct rtc_time *tm)": "rtc_tm_to_time64 - Converts rtc_time to time64_t.   Convert Gregorian date to seconds since 01-01-1970 00:00:00. ", "if ((ctrl4b & RTC_CTRL_4B_WIE) && (ctrl4a & RTC_CTRL_4A_WF)) ": "ds1685_rtc_poweroff(pdev);}    Check for a wake-up interrupt.  With Vcc applied, this is   essentially a second alarm interrupt, except it takes into   account the 'date' register in bank1 in addition to the   standard three alarm registers. ", "int omap_rtc_power_off_program(struct device *dev)": "omap_rtc_power_off_program: Set the pmic power off sequence. The RTC   generates pmic_pwr_enable control, which can be used to control an external   PMIC. ", "voidmptscsih_flush_running_cmds(MPT_SCSI_HOST *hd)": "mptscsih_flush_running_cmds - For each command found, search  Scsi_Host instance taskQ and reply to OS.  Called only if recovering from a FW reload.  @hd: Pointer to a SCSI HOST structure    Returns: None.    Must be called while new IOs are being queued. ", "/* *mptscsih_getFreeChainBuffer - Function to get a free chain *from the MPT_SCSI_HOST FreeChainQ. *@ioc: Pointer to MPT_ADAPTER structure *@req_idx: Index of the SCSI IO request frame. (output) * *return SUCCESS or FAILED ": "mptscsih_ioc_reset(MPT_ADAPTER  ioc, int post_reset);intmptscsih_event_process(MPT_ADAPTER  ioc, EventNotificationReply_t  pEvReply);voidmptscsih_taskmgmt_response_code(MPT_ADAPTER  ioc, u8 response_code);static intmptscsih_get_completion_code(MPT_ADAPTER  ioc,MPT_FRAME_HDR  req, MPT_FRAME_HDR  reply);intmptscsih_scandv_complete(MPT_ADAPTER  ioc, MPT_FRAME_HDR  mf, MPT_FRAME_HDR  r);static intmptscsih_do_cmd(MPT_SCSI_HOST  hd, INTERNAL_CMD  iocmd);static voidmptscsih_synchronize_cache(MPT_SCSI_HOST  hd, VirtDevice  vdevice);static intmptscsih_taskmgmt_reply(MPT_ADAPTER  ioc, u8 type,SCSITaskMgmtReply_t  pScsiTmReply);void mptscsih_remove(struct pci_dev  );void mptscsih_shutdown(struct pci_dev  );#ifdef CONFIG_PMint mptscsih_suspend(struct pci_dev  pdev, pm_message_t state);int mptscsih_resume(struct pci_dev  pdev);#endif =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=", "for (i = 0; i < ioc->raid_data.pIocPg3->NumPhysDisks; i++) ": "mptscsih_raid_id_to_num(MPT_ADAPTER  ioc, u8 channel, u8 id){struct inactive_raid_component_info  component_info;int i, j;RaidPhysDiskPage1_t  phys_disk;int rc = -ENXIO;int num_paths;if (!ioc->raid_data.pIocPg3)goto out;for (i = 0; i < ioc->raid_data.pIocPg3->NumPhysDisks; i++) {if ((id == ioc->raid_data.pIocPg3->PhysDisk[i].PhysDiskID) &&    (channel == ioc->raid_data.pIocPg3->PhysDisk[i].PhysDiskBus)) {rc = ioc->raid_data.pIocPg3->PhysDisk[i].PhysDiskNum;goto out;}}if (ioc->bus_type != SAS)goto out;    Check if dual path ", "#define ADD_INDEX_LOG(req_ent)do ": "mptscsih_show_info(struct seq_file  m, struct Scsi_Host  host){MPT_SCSI_HOST hd = shost_priv(host);MPT_ADAPTER ioc = hd->ioc;seq_printf(m, \"%s: %s, \", ioc->name, ioc->prod_name);seq_printf(m, \"%s%08xh, \", MPT_FW_REV_MAGIC_ID_STRING, ioc->facts.FWVersion.Word);seq_printf(m, \"Ports=%d, \", ioc->facts.NumberOfPorts);seq_printf(m, \"MaxQ=%d\\n\", ioc->req_depth);return 0;} =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=", "static voidmptscsih_info_scsiio(MPT_ADAPTER *ioc, struct scsi_cmnd *sc, SCSIIOReply_t * pScsiReply)": "mptscsih_info_scsiio - debug print info on reply frame  @ioc: Pointer to MPT_ADAPTER structure  @sc: original scsi cmnd pointer  @pScsiReply: Pointer to MPT reply frame    MPT_DEBUG_REPLY needs to be enabled to obtain this info    Refer to lsimpi.h.  ", "intmptscsih_qcmd(struct scsi_cmnd *SCpnt)": "mptscsih_qcmd - Primary Fusion MPT SCSI initiator IO start routine.  @SCpnt: Pointer to scsi_cmnd structure    (linux scsi_host_template.queuecommand routine)  This is the primary SCSI IO start routine.  Create a MPI SCSIIORequest  from a linux scsi_cmnd request and send it to the IOC.    Returns 0. (rtn value discarded by linux scsi mid-layer) ", "/* *mptscsih_change_queue_depth - This function will set a devices queue depth *@sdev: per scsi_device pointer *@qdepth: requested queue depth * *Adding support for new 'change_queue_depth' api.": "mptscsih_slave_destroy(struct scsi_device  sdev){struct Scsi_Host host = sdev->host;MPT_SCSI_HOST hd = shost_priv(host);VirtTarget vtarget;VirtDevice vdevice;struct scsi_target  starget;starget = scsi_target(sdev);vtarget = starget->hostdata;vdevice = sdev->hostdata;if (!vdevice)return;mptscsih_search_running_cmds(hd, vdevice);vtarget->num_luns--;mptscsih_synchronize_cache(hd, vdevice);kfree(vdevice);sdev->hostdata = NULL;} =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=", "/* *  Private routines... ": "mptscsih_slave_configure(struct scsi_device  sdev){struct Scsi_Host sh = sdev->host;VirtTarget vtarget;VirtDevice vdevice;struct scsi_target  starget;MPT_SCSI_HOST hd = shost_priv(sh);MPT_ADAPTER ioc = hd->ioc;starget = scsi_target(sdev);vtarget = starget->hostdata;vdevice = sdev->hostdata;dsprintk(ioc, printk(MYIOC_s_DEBUG_FMT\"device @ %p, channel=%d, id=%d, lun=%llu\\n\",ioc->name, sdev, sdev->channel, sdev->id, sdev->lun));if (ioc->bus_type == SPI)dsprintk(ioc, printk(MYIOC_s_DEBUG_FMT    \"sdtr %d wdtr %d ppr %d inq length=%d\\n\",    ioc->name, sdev->sdtr, sdev->wdtr,    sdev->ppr, sdev->inquiry_len));vdevice->configured_lun = 1;dsprintk(ioc, printk(MYIOC_s_DEBUG_FMT\"Queue depth=%d, tflags=%x\\n\",ioc->name, sdev->queue_depth, vtarget->tflags));if (ioc->bus_type == SPI)dsprintk(ioc, printk(MYIOC_s_DEBUG_FMT    \"negoFlags=%x, maxOffset=%x, SyncFactor=%x\\n\",    ioc->name, vtarget->negoFlags, vtarget->maxOffset,    vtarget->minSyncFactor));mptscsih_change_queue_depth(sdev, MPT_SCSI_CMD_PER_DEV_HIGH);dsprintk(ioc, printk(MYIOC_s_DEBUG_FMT\"tagged %d, simple %d\\n\",ioc->name,sdev->tagged_supported, sdev->simple_tags));blk_queue_dma_alignment (sdev->request_queue, 512 - 1);return 0;} =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=", "intmptscsih_abort(struct scsi_cmnd * SCpnt)": "mptscsih_abort - Abort linux scsi_cmnd routine, new_eh variant  @SCpnt: Pointer to scsi_cmnd structure, IO to be aborted    (linux scsi_host_template.eh_abort_handler routine)    Returns SUCCESS or FAILED.  ", "intmptscsih_dev_reset(struct scsi_cmnd * SCpnt)": "mptscsih_dev_reset - Perform a SCSI TARGET_RESET!  new_eh variant  @SCpnt: Pointer to scsi_cmnd structure, IO which reset is due to    (linux scsi_host_template.eh_dev_reset_handler routine)    Returns SUCCESS or FAILED.  ", "intmptscsih_bus_reset(struct scsi_cmnd * SCpnt)": "mptscsih_bus_reset - Perform a SCSI BUS_RESET!new_eh variant  @SCpnt: Pointer to scsi_cmnd structure, IO which reset is due to    (linux scsi_host_template.eh_bus_reset_handler routine)    Returns SUCCESS or FAILED.  ", "intmptscsih_host_reset(struct scsi_cmnd *SCpnt)": "mptscsih_host_reset - Perform a SCSI host adapter RESET (new_eh variant)  @SCpnt: Pointer to scsi_cmnd structure, IO which reset is due to    (linux scsi_host_template.eh_host_reset_handler routine)    Returns SUCCESS or FAILED. ", "if ((ulong)capacity >= 0x200000) ": "mptscsih_bios_param(struct scsi_device   sdev, struct block_device  bdev,sector_t capacity, int geom[]){intheads;intsectors;sector_tcylinders;ulong dummy;heads = 64;sectors = 32;dummy = heads   sectors;cylinders = capacity;sector_div(cylinders,dummy);    Handle extended translation size for logical drives   > 1Gb ", "intmptscsih_change_queue_depth(struct scsi_device *sdev, int qdepth)": "mptscsih_change_queue_depth - This function will set a devices queue depth  @sdev: per scsi_device pointer  @qdepth: requested queue depth    Adding support for new 'change_queue_depth' api.", "intmpt_raid_phys_disk_get_num_paths(MPT_ADAPTER *ioc, u8 phys_disk_num)": "mpt_raid_phys_disk_get_num_paths - returns number paths associated to this phys_num  @ioc: Pointer to a Adapter Structure  @phys_disk_num: io unit unique phys disk num generated by the ioc    Return:  returns number paths  ", "intmpt_raid_phys_disk_pg1(MPT_ADAPTER *ioc, u8 phys_disk_num,RaidPhysDiskPage1_t *phys_disk)": "mpt_raid_phys_disk_pg1 - returns phys disk page 1  @ioc: Pointer to a Adapter Structure  @phys_disk_num: io unit unique phys disk num generated by the ioc  @phys_disk: requested payload data returned    Return:  0 on success  -EFAULT if read of config page header fails or data pointer not NULL  -ENOMEM if pci_alloc failed  ", "intmpt_set_taskmgmt_in_progress_flag(MPT_ADAPTER *ioc)": "mpt_set_taskmgmt_in_progress_flag - set flags associated with task management  @ioc: Pointer to MPT_ADAPTER structure    Returns 0 for SUCCESS or -1 if FAILED.    If -1 is return, then it was not possible to set the flags  ", "voidmpt_clear_taskmgmt_in_progress_flag(MPT_ADAPTER *ioc)": "mpt_clear_taskmgmt_in_progress_flag - clear flags associated with task management  @ioc: Pointer to MPT_ADAPTER structure    ", "void __noreturnmpt_halt_firmware(MPT_ADAPTER *ioc)": "mpt_halt_firmware - Halts the firmware if it is operational and panic  the kernel  @ioc: Pointer to MPT_ADAPTER structure    ", "static voidmptbase_raid_process_event_data(MPT_ADAPTER *ioc,    MpiEventDataRaid_t * pRaidEventData)": "mpt_Soft_Hard_ResetHandler(ioc, CAN_SLEEP);mpt_free_msg_frame(ioc, mf);}goto out;}if (!(ioc->mptbase_cmds.status & MPT_MGMT_STATUS_RF_VALID)) {ret = -1;goto out;}sasIoUnitCntrReply =    (SasIoUnitControlReply_t  )ioc->mptbase_cmds.reply;if (le16_to_cpu(sasIoUnitCntrReply->IOCStatus) != MPI_IOCSTATUS_SUCCESS) {printk(KERN_DEBUG \"%s: IOCStatus=0x%X IOCLogInfo=0x%X\\n\",    __func__, sasIoUnitCntrReply->IOCStatus,    sasIoUnitCntrReply->IOCLogInfo);printk(KERN_DEBUG \"%s: failed\\n\", __func__);ret = -1;} elseprintk(KERN_DEBUG \"%s: success\\n\", __func__); out:CLEAR_MGMT_STATUS(ioc->mptbase_cmds.status)mutex_unlock(&ioc->mptbase_cmds.mutex);return ret;} =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=", "intmpt_attach(struct pci_dev *pdev, const struct pci_device_id *id)": "mpt_attach - Install a PCI intelligent MPT adapter.  @pdev: Pointer to pci_dev structure  @id: PCI device ID information    This routine performs all the steps necessary to bring the IOC of  a MPT adapter to a OPERATIONAL state.  This includes registering  memory regions, registering the interrupt, and allocating request  and reply memory pools.    This routine also pre-fetches the LAN MAC address of a Fibre Channel  MPT adapter.    Returns 0 for success, non-zero for failure.    TODO: Add support for polled controllers ", "voidmpt_detach(struct pci_dev *pdev)": "mpt_detach - Remove a PCI intelligent MPT adapter.  @pdev: Pointer to pci_dev structure ", "intmpt_resume(struct pci_dev *pdev)": "mpt_resume - Fusion MPT base driver resume routine.  @pdev: Pointer to pci_dev structure ", "intmpt_suspend(struct pci_dev *pdev, pm_message_t state)": "mpt_suspend - Fusion MPT base driver suspend routine.  @pdev: Pointer to pci_dev structure  @state: new state to enter ", "static MPT_CALLBACK MptCallbacks[MPT_MAX_PROTOCOL_DRIVERS];/* Protocol driver class lookup table ": "ioc_list);  Callback lookup table ", "u8mpt_register(MPT_CALLBACK cbfunc, MPT_DRIVER_CLASS dclass, char *func_name)": "mpt_register - Register protocol-specific main callback handler.  @cbfunc: callback function pointer  @dclass: Protocol driver's class (%MPT_DRIVER_CLASS enum value)  @func_name: call function's name    This routine is called by a protocol-specific driver (SCSI host,  LAN, SCSI target) to register its reply callback routine.  Each  protocol-specific driver must do this before it will be able to  use any IOC resources, such as obtaining request frames.    NOTES: The SCSI protocol driver currently calls this routine thrice  in order to register separate callbacks; one for \"normal\" SCSI IO;  one for MptScsiTaskMgmt requests; one for ScanDV requests.    Returns u8 valued \"handle\" in the range (and S.O.D. order)  {N,...,7,6,5,...,1} if successful.  A return value of MPT_MAX_PROTOCOL_DRIVERS (including zero!) should be  considered an error by the caller. ", "voidmpt_deregister(u8 cb_idx)": "mpt_deregister - Deregister a protocol drivers resources.  @cb_idx: previously registered callback handle    Each protocol-specific driver should call this routine when its  module is unloaded. ", "intmpt_event_register(u8 cb_idx, MPT_EVHANDLER ev_cbfunc)": "mpt_event_register - Register protocol-specific event callback handler.  @cb_idx: previously registered (via mpt_register) callback handle  @ev_cbfunc: callback function    This routine can be called by one or more protocol-specific drivers  ifwhen they choose to be notified of MPT events.    Returns 0 for success. ", "voidmpt_event_deregister(u8 cb_idx)": "mpt_event_deregister - Deregister protocol-specific event callback handler  @cb_idx: previously registered callback handle    Each protocol-specific driver should call this routine  when it does not (or can no longer) handle events,  or when its module is unloaded. ", "intmpt_reset_register(u8 cb_idx, MPT_RESETHANDLER reset_func)": "mpt_reset_register - Register protocol-specific IOC reset handler.  @cb_idx: previously registered (via mpt_register) callback handle  @reset_func: reset function    This routine can be called by one or more protocol-specific drivers  ifwhen they choose to be notified of IOC resets.    Returns 0 for success. ", "voidmpt_reset_deregister(u8 cb_idx)": "mpt_reset_deregister - Deregister protocol-specific IOC reset handler.  @cb_idx: previously registered callback handle    Each protocol-specific driver should call this routine  when it does not (or can no longer) handle IOC reset handling,  or when its module is unloaded. ", "intmpt_device_driver_register(struct mpt_pci_driver * dd_cbfunc, u8 cb_idx)": "mpt_device_driver_register - Register device driver hooks  @dd_cbfunc: driver callbacks struct  @cb_idx: MPT protocol driver index ", "voidmpt_device_driver_deregister(u8 cb_idx)": "mpt_device_driver_deregister - DeRegister device driver hooks  @cb_idx: MPT protocol driver index ", "MPT_FRAME_HDR*mpt_get_msg_frame(u8 cb_idx, MPT_ADAPTER *ioc)": "mpt_get_msg_frame - Obtain an MPT request frame from the pool  @cb_idx: Handle of registered MPT protocol driver  @ioc: Pointer to MPT adapter structure    Obtain an MPT request frame from the pool (of 1024) that are  allocated per MPT adapter.    Returns pointer to a MPT request frame or %NULL if none are available  or IOC is not active. ", "voidmpt_put_msg_frame(u8 cb_idx, MPT_ADAPTER *ioc, MPT_FRAME_HDR *mf)": "mpt_put_msg_frame - Send a protocol-specific MPT request frame to an IOC  @cb_idx: Handle of registered MPT protocol driver  @ioc: Pointer to MPT adapter structure  @mf: Pointer to MPT request frame    This routine posts an MPT request frame to the request post FIFO of a  specific MPT adapter. ", "voidmpt_put_msg_frame_hi_pri(u8 cb_idx, MPT_ADAPTER *ioc, MPT_FRAME_HDR *mf)": "mpt_put_msg_frame_hi_pri - Send a hi-pri protocol-specific MPT request frame  @cb_idx: Handle of registered MPT protocol driver  @ioc: Pointer to MPT adapter structure  @mf: Pointer to MPT request frame    Send a protocol-specific MPT request frame to an IOC using  hi-priority request queue.    This routine posts an MPT request frame to the request post FIFO of a  specific MPT adapter.  ", "if (!cb_idx || cb_idx >= MPT_MAX_PROTOCOL_DRIVERS ||MptCallbacks[cb_idx] == NULL) ": "mpt_free_msg_frame(ioc, mf);mb();return;}mr = (MPT_FRAME_HDR  ) CAST_U32_TO_PTR(pa);break;case MPI_CONTEXT_REPLY_TYPE_SCSI_TARGET:cb_idx = mpt_get_cb_idx(MPTSTM_DRIVER);mr = (MPT_FRAME_HDR  ) CAST_U32_TO_PTR(pa);break;default:cb_idx = 0;BUG();}   Check for (valid) IO callback!  ", "intmpt_send_handshake_request(u8 cb_idx, MPT_ADAPTER *ioc, int reqBytes, u32 *req, int sleepFlag)": "mpt_send_handshake_request - Send MPT request via doorbell handshake method.  @cb_idx: Handle of registered MPT protocol driver  @ioc: Pointer to MPT adapter structure  @reqBytes: Size of the request in bytes  @req: Pointer to MPT request frame  @sleepFlag: Use schedule if CAN_SLEEP else use udelay.    This routine is used exclusively to send MptScsiTaskMgmt  requests since they are required to be sent via doorbell handshake.    NOTE: It is the callers responsibility to byte-swap fields in the  request which are greater than 1 byte in size.    Returns 0 for success, non-zero for failure. ", "intmpt_verify_adapter(int iocid, MPT_ADAPTER **iocpp)": "mpt_verify_adapter - Given IOC identifier, set pointer to its adapter structure.  @iocid: IOC unique identifier (integer)  @iocpp: Pointer to pointer to IOC adapter    Given a unique IOC identifier, set pointer to the associated MPT  adapter structure.    Returns iocid and sets iocpp if iocid is found.  Returns -1 if iocid is not found. ", "hd = shost_priv(ioc->sh);ioc->schedule_dead_ioc_flush_running_cmds(hd);/*Remove the Dead Host ": "mpt_GetIocState(ioc, 0);if ((ioc_raw_state & MPI_IOC_STATE_MASK) == MPI_IOC_STATE_MASK) {printk(MYIOC_s_INFO_FMT \"%s: IOC is non-operational !!!!\\n\",    ioc->name, __func__);    Call mptscsih_flush_pending_cmds callback so that we   flush all pending commands back to OS.   This call is required to aovid deadlock at block layer.   Dead IOC will fail to do diag reset,and this call is safe   since dead ioc will never return any command back from HW. ", "sz = (ioc->req_sz * ioc->req_depth) + 128;sz = ((sz + 0x1000UL - 1UL) / 0x1000) * 0x1000;seq_printf(m, \"    ": "mpt_print_ioc_summary(MPT_ADAPTER  ioc, struct seq_file  m, int showlan);static int mpt_summary_proc_show(struct seq_file  m, void  v){MPT_ADAPTER  ioc = m->private;if (ioc) {seq_mpt_print_ioc_summary(ioc, m, 1);} else {list_for_each_entry(ioc, &ioc_list, list) {seq_mpt_print_ioc_summary(ioc, m, 1);}}return 0;}static int mpt_version_proc_show(struct seq_file  m, void  v){u8 cb_idx;int scsi, fc, sas, lan, ctl, targ;char drvname;seq_printf(m, \"%s-%s\\n\", \"mptlinux\", MPT_LINUX_VERSION_COMMON);seq_printf(m, \"  Fusion MPT base driver\\n\");scsi = fc = sas = lan = ctl = targ = 0;for (cb_idx = MPT_MAX_PROTOCOL_DRIVERS-1; cb_idx; cb_idx--) {drvname = NULL;if (MptCallbacks[cb_idx]) {switch (MptDriverClass[cb_idx]) {case MPTSPI_DRIVER:if (!scsi++) drvname = \"SPI host\";break;case MPTFC_DRIVER:if (!fc++) drvname = \"FC host\";break;case MPTSAS_DRIVER:if (!sas++) drvname = \"SAS host\";break;case MPTLAN_DRIVER:if (!lan++) drvname = \"LAN\";break;case MPTSTM_DRIVER:if (!targ++) drvname = \"SCSI target\";break;case MPTCTL_DRIVER:if (!ctl++) drvname = \"ioctl\";break;}if (drvname)seq_printf(m, \"  Fusion MPT %s driver\\n\", drvname);}}return 0;}static int mpt_iocinfo_proc_show(struct seq_file  m, void  v){MPT_ADAPTER ioc = m->private;char expVer[32];int sz;int p;mpt_get_fw_exp_ver(expVer, ioc);seq_printf(m, \"%s:\", ioc->name);if (ioc->facts.Flags & MPI_IOCFACTS_FLAGS_FW_DOWNLOAD_BOOT)seq_printf(m, \"  (fw download boot flag set)\");if (ioc->facts.IOCExceptions & MPI_IOCFACTS_EXCEPT_CONFIG_CHECKSUM_FAIL)seq_printf(m, \"  CONFIG_CHECKSUM_FAIL!\");seq_printf(m, \"\\n  ProductID = 0x%04x (%s)\\n\",ioc->facts.ProductID,ioc->prod_name);seq_printf(m, \"  FWVersion = 0x%08x%s\", ioc->facts.FWVersion.Word, expVer);if (ioc->facts.FWImageSize)seq_printf(m, \" (fw_size=%d)\", ioc->facts.FWImageSize);seq_printf(m, \"\\n  MsgVersion = 0x%04x\\n\", ioc->facts.MsgVersion);seq_printf(m, \"  FirstWhoInit = 0x%02x\\n\", ioc->FirstWhoInit);seq_printf(m, \"  EventState = 0x%02x\\n\", ioc->facts.EventState);seq_printf(m, \"  CurrentHostMfaHighAddr = 0x%08x\\n\",ioc->facts.CurrentHostMfaHighAddr);seq_printf(m, \"  CurrentSenseBufferHighAddr = 0x%08x\\n\",ioc->facts.CurrentSenseBufferHighAddr);seq_printf(m, \"  MaxChainDepth = 0x%02x frames\\n\", ioc->facts.MaxChainDepth);seq_printf(m, \"  MinBlockSize = 0x%02x bytes\\n\", 4 ioc->facts.BlockSize);seq_printf(m, \"  RequestFrames @ 0x%p (Dma @ 0x%p)\\n\",(void  )ioc->req_frames, (void  )(ulong)ioc->req_frames_dma);     Rounding UP to nearest 4-kB boundary here... ", "if (ioc->alt_ioc)ioc = ioc->alt_ioc;/* rearm the timer ": "mpt_HardResetHandler(ioc, CAN_SLEEP);printk(MYIOC_s_WARN_FMT \"%s: HardReset: %s\\n\", ioc->name,       __func__, (rc == 0) ? \"success\" : \"failed\");ioc_raw_state = mpt_GetIocState(ioc, 0);if ((ioc_raw_state & MPI_IOC_STATE_MASK) == MPI_IOC_STATE_FAULT)printk(MYIOC_s_WARN_FMT \"IOC is in FAULT state after \"    \"reset (%04xh)\\n\", ioc->name, ioc_raw_state &    MPI_DOORBELL_DATA_MASK);} else if (ioc->bus_type == SAS && ioc->sas_discovery_quiesce_io) {if ((mpt_is_discovery_complete(ioc))) {devtprintk(ioc, printk(MYIOC_s_DEBUG_FMT \"clearing \"    \"discovery_quiesce_io flag\\n\", ioc->name));ioc->sas_discovery_quiesce_io = 0;}} out:    Take turns polling alternate controller ", "static int mpt_remove_dead_ioc_func(void *arg)": "mpt_config(ioc, &cfg)))goto out;if (!hdr.ExtPageLength)goto out;buffer = dma_alloc_coherent(&ioc->pcidev->dev, hdr.ExtPageLength   4,    &dma_handle, GFP_KERNEL);if (!buffer)goto out;cfg.physAddr = dma_handle;cfg.action = MPI_CONFIG_ACTION_PAGE_READ_CURRENT;if ((mpt_config(ioc, &cfg)))goto out_free_consistent;if (!(buffer->PhyData[0].PortFlags &    MPI_SAS_IOUNIT0_PORT_FLAGS_DISCOVERY_IN_PROGRESS))rc = 1; out_free_consistent:dma_free_coherent(&ioc->pcidev->dev, hdr.ExtPageLength   4, buffer,  dma_handle); out:return rc;}      mpt_remove_dead_ioc_func - kthread context to remove dead ioc   @arg: input argument, used to derive ioc     Return 0 if controller is removed from pci subsystem.   Return -1 for other case. ", "mpt_read_ioc_pg_1(ioc);break;case FC:if ((ioc->pfacts[0].ProtocolFlags &MPI_PORTFACTS_PROTOCOL_LAN) &&    (ioc->lan_cnfg_page0.Header.PageLength == 0)) ": "mpt_findImVolumes(ioc);  Check, and possibly reset, the coalescing value ", "intmpt_alloc_fw_memory(MPT_ADAPTER *ioc, int size)": "mpt_alloc_fw_memory - allocate firmware memory  @ioc: Pointer to MPT_ADAPTER structure        @size: total FW bytes    If memory has already been allocated, the same (cached) value  is returned.    Return 0 if successful, or non-zero for failure  ", "/** *mpt_adapter_dispose - Free all resources associated with an MPT adapter *@ioc: Pointer to MPT adapter structure * *This routine unregisters h/w resources and frees all alloc'd memory *associated with a MPT adapter structure. ": "mpt_free_fw_memory(ioc);kfree(ioc->spi_data.nvram);mpt_inactive_raid_list_free(ioc);kfree(ioc->raid_data.pIocPg2);kfree(ioc->raid_data.pIocPg3);ioc->spi_data.nvram = NULL;ioc->raid_data.pIocPg3 = NULL;if (ioc->spi_data.pIocPg4 != NULL) {sz = ioc->spi_data.IocPg4Sz;dma_free_coherent(&ioc->pcidev->dev, sz,  ioc->spi_data.pIocPg4,  ioc->spi_data.IocPg4_dma);ioc->spi_data.pIocPg4 = NULL;ioc->alloc_total -= sz;}if (ioc->ReqToChain != NULL) {kfree(ioc->ReqToChain);kfree(ioc->RequestNB);ioc->ReqToChain = NULL;}kfree(ioc->ChainToChain);ioc->ChainToChain = NULL;if (ioc->HostPageBuffer != NULL) {if((ret = mpt_host_page_access_control(ioc,    MPI_DB_HPBAC_FREE_BUFFER, NO_SLEEP)) != 0) {printk(MYIOC_s_ERR_FMT   \": %s: host page buffers free failed (%d)!\\n\",    ioc->name, __func__, ret);}dexitprintk(ioc, printk(MYIOC_s_DEBUG_FMT\"HostPageBuffer free  @ %p, sz=%d bytes\\n\",ioc->name, ioc->HostPageBuffer,ioc->HostPageBuffer_sz));dma_free_coherent(&ioc->pcidev->dev, ioc->HostPageBuffer_sz,    ioc->HostPageBuffer, ioc->HostPageBuffer_dma);ioc->HostPageBuffer = NULL;ioc->HostPageBuffer_sz = 0;ioc->alloc_total -= ioc->HostPageBuffer_sz;}pci_set_drvdata(ioc->pcidev, NULL);} =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=", "static int  __init    fusion_init  (void);static void __exit    fusion_exit  (void);#define CHIPREG_READ32(addr) readl_relaxed(addr)#define CHIPREG_READ32_dmasync(addr)readl(addr)#define CHIPREG_WRITE32(addr,val) writel(val, addr)#define CHIPREG_PIO_WRITE32(addr,val)outl(val, (unsigned long)addr)#define CHIPREG_PIO_READ32(addr) inl((unsigned long)addr)static voidpci_disable_io_access(struct pci_dev *pdev)": "mptbase_sas_persist_operation(MPT_ADAPTER  ioc, u8 persist_opcode);static intmpt_GetScsiPortSettings(MPT_ADAPTER  ioc, int portnum);static intmpt_readScsiDevicePageHeaders(MPT_ADAPTER  ioc, int portnum);static void mpt_read_ioc_pg_1(MPT_ADAPTER  ioc);static void mpt_read_ioc_pg_4(MPT_ADAPTER  ioc);static voidmpt_get_manufacturing_pg_0(MPT_ADAPTER  ioc);static intSendEventNotification(MPT_ADAPTER  ioc, u8 EvSwitch,int sleepFlag);static intSendEventAck(MPT_ADAPTER  ioc, EventNotificationReply_t  evnp);static intmpt_host_page_access_control(MPT_ADAPTER  ioc, u8 access_control_value, int sleepFlag);static intmpt_host_page_alloc(MPT_ADAPTER  ioc, pIOCInit_t ioc_init);#ifdef CONFIG_PROC_FSstatic int mpt_summary_proc_show(struct seq_file  m, void  v);static int mpt_version_proc_show(struct seq_file  m, void  v);static int mpt_iocinfo_proc_show(struct seq_file  m, void  v);#endifstatic voidmpt_get_fw_exp_ver(char  buf, MPT_ADAPTER  ioc);static intProcessEventNotification(MPT_ADAPTER  ioc,EventNotificationReply_t  evReply, int  evHandlers);static voidmpt_iocstatus_info(MPT_ADAPTER  ioc, u32 ioc_status, MPT_FRAME_HDR  mf);static voidmpt_fc_log_info(MPT_ADAPTER  ioc, u32 log_info);static voidmpt_spi_log_info(MPT_ADAPTER  ioc, u32 log_info);static voidmpt_sas_log_info(MPT_ADAPTER  ioc, u32 log_info , u8 cb_idx);static intmpt_read_ioc_pg_3(MPT_ADAPTER  ioc);static voidmpt_inactive_raid_list_free(MPT_ADAPTER  ioc);  module entry point ", "intmpt_raid_phys_disk_pg0(MPT_ADAPTER *ioc, u8 phys_disk_num,RaidPhysDiskPage0_t *phys_disk)": "mpt_raid_phys_disk_pg0(ioc,    buffer->PhysDisk[i].PhysDiskNum, &phys_disk) != 0)continue;if ((component_info = kmalloc(sizeof ( component_info), GFP_KERNEL)) == NULL)continue;component_info->volumeID = id;component_info->volumeBus = channel;component_info->d.PhysDiskNum = phys_disk.PhysDiskNum;component_info->d.PhysDiskBus = phys_disk.PhysDiskBus;component_info->d.PhysDiskID = phys_disk.PhysDiskID;component_info->d.PhysDiskIOC = phys_disk.PhysDiskIOC;list_add_tail(&component_info->list,    &ioc->raid_data.inactive_list);}mutex_unlock(&ioc->raid_data.inactive_list_mutex); out:if (buffer)dma_free_coherent(&ioc->pcidev->dev, hdr.PageLength   4,  buffer, dma_handle);}    mpt_raid_phys_disk_pg0 - returns phys disk page zero  @ioc: Pointer to a Adapter Structure  @phys_disk_num: io unit unique phys disk num generated by the ioc  @phys_disk: requested payload data returned    Return:  0 on success  -EFAULT if read of config page header fails or data pointer not NULL  -ENOMEM if pci_alloc failed  ", "int superhyway_register_driver(struct superhyway_driver *drv)": "superhyway_register_driver - Register a new SuperHyway driver   @drv: SuperHyway driver to register.     This registers the passed in @drv. Any devices matching the id table will   automatically be populated and handed off to the driver's specified probe   routine. ", "int superhyway_add_device(unsigned long base, struct superhyway_device *sdev,  struct superhyway_bus *bus)": "superhyway_add_device - Add a SuperHyway module   @base: Physical address where module is mapped.   @sdev: SuperHyway device to add, or NULL to allocate a new one.   @bus: Bus where SuperHyway module resides.     This is responsible for adding a new SuperHyway module. This sets up a new   struct superhyway_device for the module being added if @sdev == NULL.     Devices are initially added in the order that they are scanned (from the   top-down of the memory map), and are assigned an ID based on the order that   they are added. Any manual addition of a module will thus get the ID after   the devices already discovered regardless of where it resides in memory.     Further work can and should be done in superhyway_scan_bus(), to be sure   that any new modules are properly discovered and subsequently registered. ", "void superhyway_unregister_driver(struct superhyway_driver *drv)": "superhyway_unregister_driver - Unregister a SuperHyway driver   @drv: SuperHyway driver to unregister.     This cleans up after superhyway_register_driver(), and should be invoked in   the exit path of any module drivers. ", "int tc_dwc_g210_config_40_bit(struct ufs_hba *hba)": "tc_dwc_g210_config_40_bit()   This function configures Local (host) Synopsys 40-bit TC specific attributes     @hba: Pointer to drivers structure     Returns 0 on success non-zero value on failure ", "int tc_dwc_g210_config_20_bit(struct ufs_hba *hba)": "tc_dwc_g210_config_20_bit()   This function configures Local (host) Synopsys 20-bit TC specific attributes     @hba: Pointer to drivers structure     Returns 0 on success non-zero value on failure ", "int ufshcd_dwc_link_startup_notify(struct ufs_hba *hba,enum ufs_notify_change_status status)": "ufshcd_dwc_link_startup_notify()   UFS Host DWC specific link startup sequence   @hba: private structure pointer   @status: Callback notify status     Returns 0 on success, non-zero value on failure ", "if (hba->ufs_version <= ufshci_version(1, 1))return UFS_UNIPRO_VER_1_41;elsereturn UFS_UNIPRO_VER_1_6;}EXPORT_SYMBOL(ufshcd_get_local_unipro_ver": "ufshcd_get_local_unipro_ver(struct ufs_hba  hba){  HCI version 1.0 and 1.1 supports UniPro 1.41 ", "int ufshcd_system_suspend(struct device *dev)": "ufshcd_system_suspend - system suspend callback   @dev: Device associated with the UFS controller.     Executed before putting the system into a sleep state in which the contents   of main memory are preserved.     Returns 0 for success and non-zero for failure ", "int ufshcd_system_resume(struct device *dev)": "ufshcd_system_resume - system resume callback   @dev: Device associated with the UFS controller.     Executed after waking the system up from a sleep state in which the contents   of main memory were preserved.     Returns 0 for success and non-zero for failure ", "int ufshcd_runtime_suspend(struct device *dev)": "ufshcd_runtime_suspend - runtime suspend callback   @dev: Device associated with the UFS controller.     Check the description of ufshcd_suspend() function for more details.     Returns 0 for success and non-zero for failure ", "int ufshcd_runtime_resume(struct device *dev)": "ufshcd_runtime_resume - runtime resume routine   @dev: Device associated with the UFS controller.     This function basically brings controller   to active state. Following operations are done in this function:     1. Turn on all the controller related clocks   2. Turn ON VCC rail ", "if (!strcmp(clki->name, \"ref_clk\"))ufshcd_parse_dev_ref_clk_freq(hba, clki->clk);if (clki->max_freq) ": "ufshcd_alloc_host(). ", "": "vesa_modes[] = {  0 640x350-85 VESA ", "list_add(&notifier->list, &notifier_list);mutex_unlock(&list_lock);return 0;err_unbind:/* * On failure, unbind all sub-devices registered through this notifier. ": "v4l2_async_nf_register(struct v4l2_async_notifier  notifier){struct v4l2_async_subdev  asd;int ret, i = 0;INIT_LIST_HEAD(&notifier->waiting);INIT_LIST_HEAD(&notifier->done);mutex_lock(&list_lock);list_for_each_entry(asd, &notifier->asd_list, asd_list) {ret = v4l2_async_nf_asd_valid(notifier, asd, i++);if (ret)goto err_unlock;list_add_tail(&asd->list, &notifier->waiting);}ret = v4l2_async_nf_try_all_subdevs(notifier);if (ret < 0)goto err_unbind;ret = v4l2_async_nf_try_complete(notifier);if (ret < 0)goto err_unbind;  Keep also completed notifiers on the list ", "if (!sd->fwnode && sd->dev)sd->fwnode = dev_fwnode(sd->dev);mutex_lock(&list_lock);INIT_LIST_HEAD(&sd->async_list);list_for_each_entry(notifier, &notifier_list, list) ": "v4l2_async_register_subdev(struct v4l2_subdev  sd){struct v4l2_async_notifier  subdev_notifier;struct v4l2_async_notifier  notifier;int ret;    No reference taken. The reference is held by the device   (struct v4l2_subdev.dev), and async sub-device does not   exist independently of the device at any point of time. ", "BUG_ON(myid != id);for (i = 0; standards[i].std; i++)if (myid == standards[i].std)break;return standards[i].descr;}EXPORT_SYMBOL(v4l2_norm_to_name": "v4l2_norm_to_name(v4l2_std_id id){u32 myid = id;int i;  HACK: ppc32 architecture doesn't have __ucmpdi2 function to handle   64 bit comparisons. So, on that architecture, with some gcc   variants, compilation fails. Currently, the max value is 30bit wide. ", "static void v4l2_m2m_schedule_next_job(struct v4l2_m2m_dev *m2m_dev,       struct v4l2_m2m_ctx *m2m_ctx)": "v4l2_m2m_buf_done_and_job_finish(). ", "/* Only valid when the video_device struct is a static. ": "video_device_release_empty(struct video_device  vdev){  Do nothing ", "vdev->minor = -1;/* the release callback MUST be present ": "__video_register_device(struct video_device  vdev,    enum vfl_devnode_type type,    int nr, int warn_if_nr_in_use,    struct module  owner){int i = 0;int ret;int minor_offset = 0;int minor_cnt = VIDEO_NUM_DEVICES;const char  name_base;  A minor value of -1 marks this video device as never   having been registered ", "void video_unregister_device(struct video_device *vdev)": "video_unregister_device - unregister a video4linux device  @vdev: the device to unregister    This unregisters the passed device. Future open calls will  be met with errors. ", "for_each_active_route(routing, route) ": "v4l2_subdev_init_finalize(struct v4l2_subdev  sd, const char  name,struct lock_class_key  key){struct v4l2_subdev_state  state;state = __v4l2_subdev_state_alloc(sd, name, key);if (IS_ERR(state))return PTR_ERR(state);sd->active_state = state;return 0;}EXPORT_SYMBOL_GPL(__v4l2_subdev_init_finalize);void v4l2_subdev_cleanup(struct v4l2_subdev  sd){__v4l2_subdev_state_free(sd->active_state);sd->active_state = NULL;}EXPORT_SYMBOL_GPL(v4l2_subdev_cleanup);#if defined(CONFIG_VIDEO_V4L2_SUBDEV_API)static intv4l2_subdev_init_stream_configs(struct v4l2_subdev_stream_configs  stream_configs,const struct v4l2_subdev_krouting  routing){struct v4l2_subdev_stream_configs new_configs = { 0 };struct v4l2_subdev_route  route;u32 idx;  Count number of formats needed ", "if (strcmp(ptr1.p_char + idx, ptr2.p_char + idx))return false;}return true;default:return !memcmp(ptr1.p_const, ptr2.p_const,       ctrl->elems * ctrl->elem_size);}}EXPORT_SYMBOL(v4l2_ctrl_type_op_equal": "v4l2_ctrl_type_op_equal(const struct v4l2_ctrl  ctrl,     union v4l2_ctrl_ptr ptr1, union v4l2_ctrl_ptr ptr2){unsigned int i;switch (ctrl->type) {case V4L2_CTRL_TYPE_BUTTON:return false;case V4L2_CTRL_TYPE_STRING:for (i = 0; i < ctrl->elems; i++) {unsigned int idx = i   ctrl->elem_size;  strings are always 0-terminated ", "list_for_each_entry_safe(ref, next_ref, &hdl->ctrl_refs, node) ": "v4l2_ctrl_handler_free(struct v4l2_ctrl_handler  hdl){struct v4l2_ctrl_ref  ref,  next_ref;struct v4l2_ctrl  ctrl,  next_ctrl;struct v4l2_subscribed_event  sev,  next_sev;if (hdl == NULL || hdl->buckets == NULL)return;v4l2_ctrl_handler_free_request(hdl);mutex_lock(hdl->lock);  Free all nodes ", "if (list_empty(&hdl->ctrl_refs) || id > node2id(hdl->ctrl_refs.prev)) ": "v4l2_ctrl_new_std(hdl, NULL, class_ctrl, 0, 0, 0, 0))return hdl->error;if (hdl->error)return hdl->error;if (allocate_req && !ctrl->is_array)size_extra_req = ctrl->elems   ctrl->elem_size;new_ref = kzalloc(sizeof( new_ref) + size_extra_req, GFP_KERNEL);if (!new_ref)return handler_set_err(hdl, -ENOMEM);new_ref->ctrl = ctrl;new_ref->from_other_dev = from_other_dev;if (size_extra_req)new_ref->p_req.p = &new_ref[1];INIT_LIST_HEAD(&new_ref->node);mutex_lock(hdl->lock);  Add immediately at the end of the list if the list is empty, or if   the last element in the list has a lower ID.   This ensures that when elements are added in ascending order the   insertion is an O(1) operation. ", "if (v4l2_ctrl_get_menu(id)) ": "v4l2_ctrl_new_std_menu_items(struct v4l2_ctrl_handler  hdl,const struct v4l2_ctrl_ops  ops, u32 id, u8 _max,u64 mask, u8 _def, const char   const  qmenu){enum v4l2_ctrl_type type;const char  name;u32 flags;u64 step;s64 min;s64 max = _max;s64 def = _def;  v4l2_ctrl_new_std_menu_items() should only be called for   standard controls without a standard menu. ", "if (!hdl || !add || hdl == add)return 0;if (hdl->error)return hdl->error;mutex_lock(add->lock);list_for_each_entry(ref, &add->ctrl_refs, node) ": "v4l2_ctrl_add_handler(struct v4l2_ctrl_handler  hdl,  struct v4l2_ctrl_handler  add,  bool ( filter)(const struct v4l2_ctrl  ctrl),  bool from_other_dev){struct v4l2_ctrl_ref  ref;int ret = 0;  Do nothing if either handler is NULL or if they are the same ", "if (WARN_ON(ncontrols == 0 || controls[0] == NULL))return;for (i = 0; i < ncontrols; i++) ": "v4l2_ctrl_cluster(unsigned ncontrols, struct v4l2_ctrl   controls){bool has_volatiles = false;int i;  The first control is the master control and it must not be NULL ", "bool inactive = !active;bool old;if (ctrl == NULL)return;if (inactive)/* set V4L2_CTRL_FLAG_INACTIVE ": "v4l2_ctrl_activate(struct v4l2_ctrl  ctrl, bool active){  invert since the actual flag is called 'inactive' ", "old = test_and_set_bit(1, &ctrl->flags);else/* clear V4L2_CTRL_FLAG_GRABBED ": "__v4l2_ctrl_grab(struct v4l2_ctrl  ctrl, bool grabbed){bool old;if (ctrl == NULL)return;lockdep_assert_held(ctrl->handler->lock);if (grabbed)  set V4L2_CTRL_FLAG_GRABBED ", "/* Skip button controls and read-only controls. ": "v4l2_ctrl_handler_setup(struct v4l2_ctrl_handler  hdl){struct v4l2_ctrl  ctrl;int ret = 0;if (hdl == NULL)return 0;lockdep_assert_held(hdl->lock);list_for_each_entry(ctrl, &hdl->ctrls, node)ctrl->done = false;list_for_each_entry(ctrl, &hdl->ctrls, node) {struct v4l2_ctrl  master = ctrl->cluster[0];int i;  Skip if this control was already handled by a cluster. ", "int v4l2_g_ext_ctrls_common(struct v4l2_ctrl_handler *hdl,    struct v4l2_ext_controls *cs,    struct video_device *vdev)": "v4l2_g_ext_ctrls_common() with 'which' set to   V4L2_CTRL_WHICH_REQUEST_VAL is only called if the request was   completed, and in that case p_req_valid is true for all controls. ", "if (WARN_ON(!ctrl->is_int))return 0;c.value = 0;get_ctrl(ctrl, &c);return c.value;}EXPORT_SYMBOL(v4l2_ctrl_g_ctrl": "v4l2_ctrl_g_ctrl(struct v4l2_ctrl  ctrl){struct v4l2_ext_control c;  It's a driver bug if this happens. ", "if (WARN_ON(ctrl->is_ptr || ctrl->type != V4L2_CTRL_TYPE_INTEGER64))return 0;c.value64 = 0;get_ctrl(ctrl, &c);return c.value64;}EXPORT_SYMBOL(v4l2_ctrl_g_ctrl_int64": "v4l2_ctrl_g_ctrl_int64(struct v4l2_ctrl  ctrl){struct v4l2_ext_control c;  It's a driver bug if this happens. ", "if (WARN_ON(!ctrl->is_int))return -EINVAL;ctrl->val = val;return set_ctrl(NULL, ctrl, 0);}EXPORT_SYMBOL(__v4l2_ctrl_s_ctrl": "__v4l2_ctrl_s_ctrl(struct v4l2_ctrl  ctrl, s32 val){lockdep_assert_held(ctrl->handler->lock);  It's a driver bug if this happens. ", "if (WARN_ON(ctrl->is_ptr || ctrl->type != V4L2_CTRL_TYPE_INTEGER64))return -EINVAL;*ctrl->p_new.p_s64 = val;return set_ctrl(NULL, ctrl, 0);}EXPORT_SYMBOL(__v4l2_ctrl_s_ctrl_int64": "__v4l2_ctrl_s_ctrl_int64(struct v4l2_ctrl  ctrl, s64 val){lockdep_assert_held(ctrl->handler->lock);  It's a driver bug if this happens. ", "if (WARN_ON(ctrl->type != V4L2_CTRL_TYPE_STRING))return -EINVAL;strscpy(ctrl->p_new.p_char, s, ctrl->maximum + 1);return set_ctrl(NULL, ctrl, 0);}EXPORT_SYMBOL(__v4l2_ctrl_s_ctrl_string": "__v4l2_ctrl_s_ctrl_string(struct v4l2_ctrl  ctrl, const char  s){lockdep_assert_held(ctrl->handler->lock);  It's a driver bug if this happens. ", "if (WARN_ON(ctrl->type != type))return -EINVAL;/* Setting dynamic arrays is not (yet?) supported. ": "__v4l2_ctrl_s_ctrl_compound(struct v4l2_ctrl  ctrl,enum v4l2_ctrl_type type, const void  p){lockdep_assert_held(ctrl->handler->lock);  It's a driver bug if this happens. ", "ref = find_ref(hdl, id);if ((qc->id & next_flags) && !list_empty(&hdl->ctrl_refs)) ": "v4l2_query_ext_ctrl(struct v4l2_ctrl_handler  hdl, struct v4l2_query_ext_ctrl  qc){const unsigned int next_flags = V4L2_CTRL_FLAG_NEXT_CTRL | V4L2_CTRL_FLAG_NEXT_COMPOUND;u32 id = qc->id & V4L2_CTRL_ID_MASK;struct v4l2_ctrl_ref  ref;struct v4l2_ctrl  ctrl;if (!hdl)return -EINVAL;mutex_lock(hdl->lock);  Try to find it ", "switch (ctrl->type) ": "v4l2_querymenu(struct v4l2_ctrl_handler  hdl, struct v4l2_querymenu  qm){struct v4l2_ctrl  ctrl;u32 i = qm->index;ctrl = v4l2_ctrl_find(hdl, qm->id);if (!ctrl)return -EINVAL;qm->reserved = 0;  Sanity checks ", "/* Keep the order of the 'case's the same as in v4l2-controls.h! ": "v4l2_ctrl_get_name(u32 id){switch (id) {  USER controls ", "*flags |= V4L2_CTRL_FLAG_READ_ONLY | V4L2_CTRL_FLAG_WRITE_ONLY;*min = *max = *step = *def = 0;break;case V4L2_CID_BG_COLOR:case V4L2_CID_COLORFX_RGB:*type = V4L2_CTRL_TYPE_INTEGER;*step = 1;*min = 0;/* Max is calculated as RGB888 that is 2^24 - 1 ": "v4l2_ctrl_fill(u32 id, const char   name, enum v4l2_ctrl_type  type,    s64  min, s64  max, u64  step, s64  def, u32  flags){ name = v4l2_ctrl_get_name(id); flags = 0;switch (id) {case V4L2_CID_AUDIO_MUTE:case V4L2_CID_AUDIO_LOUDNESS:case V4L2_CID_AUTO_WHITE_BALANCE:case V4L2_CID_AUTOGAIN:case V4L2_CID_HFLIP:case V4L2_CID_VFLIP:case V4L2_CID_HUE_AUTO:case V4L2_CID_CHROMA_AGC:case V4L2_CID_COLOR_KILLER:case V4L2_CID_AUTOBRIGHTNESS:case V4L2_CID_MPEG_AUDIO_MUTE:case V4L2_CID_MPEG_VIDEO_MUTE:case V4L2_CID_MPEG_VIDEO_GOP_CLOSURE:case V4L2_CID_MPEG_VIDEO_PULLDOWN:case V4L2_CID_EXPOSURE_AUTO_PRIORITY:case V4L2_CID_FOCUS_AUTO:case V4L2_CID_PRIVACY:case V4L2_CID_AUDIO_LIMITER_ENABLED:case V4L2_CID_AUDIO_COMPRESSION_ENABLED:case V4L2_CID_PILOT_TONE_ENABLED:case V4L2_CID_ILLUMINATORS_1:case V4L2_CID_ILLUMINATORS_2:case V4L2_CID_FLASH_STROBE_STATUS:case V4L2_CID_FLASH_CHARGE:case V4L2_CID_FLASH_READY:case V4L2_CID_MPEG_VIDEO_DECODER_MPEG4_DEBLOCK_FILTER:case V4L2_CID_MPEG_VIDEO_DECODER_SLICE_INTERFACE:case V4L2_CID_MPEG_VIDEO_DEC_DISPLAY_DELAY_ENABLE:case V4L2_CID_MPEG_VIDEO_FRAME_RC_ENABLE:case V4L2_CID_MPEG_VIDEO_MB_RC_ENABLE:case V4L2_CID_MPEG_VIDEO_H264_8X8_TRANSFORM:case V4L2_CID_MPEG_VIDEO_H264_VUI_SAR_ENABLE:case V4L2_CID_MPEG_VIDEO_MPEG4_QPEL:case V4L2_CID_MPEG_VIDEO_REPEAT_SEQ_HEADER:case V4L2_CID_MPEG_VIDEO_AU_DELIMITER:case V4L2_CID_WIDE_DYNAMIC_RANGE:case V4L2_CID_IMAGE_STABILIZATION:case V4L2_CID_RDS_RECEPTION:case V4L2_CID_RF_TUNER_LNA_GAIN_AUTO:case V4L2_CID_RF_TUNER_MIXER_GAIN_AUTO:case V4L2_CID_RF_TUNER_IF_GAIN_AUTO:case V4L2_CID_RF_TUNER_BANDWIDTH_AUTO:case V4L2_CID_RF_TUNER_PLL_LOCK:case V4L2_CID_RDS_TX_MONO_STEREO:case V4L2_CID_RDS_TX_ARTIFICIAL_HEAD:case V4L2_CID_RDS_TX_COMPRESSED:case V4L2_CID_RDS_TX_DYNAMIC_PTY:case V4L2_CID_RDS_TX_TRAFFIC_ANNOUNCEMENT:case V4L2_CID_RDS_TX_TRAFFIC_PROGRAM:case V4L2_CID_RDS_TX_MUSIC_SPEECH:case V4L2_CID_RDS_TX_ALT_FREQS_ENABLE:case V4L2_CID_RDS_RX_TRAFFIC_ANNOUNCEMENT:case V4L2_CID_RDS_RX_TRAFFIC_PROGRAM:case V4L2_CID_RDS_RX_MUSIC_SPEECH: type = V4L2_CTRL_TYPE_BOOLEAN; min = 0; max =  step = 1;break;case V4L2_CID_ROTATE: type = V4L2_CTRL_TYPE_INTEGER; flags |= V4L2_CTRL_FLAG_MODIFY_LAYOUT;break;case V4L2_CID_MPEG_VIDEO_MV_H_SEARCH_RANGE:case V4L2_CID_MPEG_VIDEO_MV_V_SEARCH_RANGE:case V4L2_CID_MPEG_VIDEO_DEC_DISPLAY_DELAY:case V4L2_CID_MPEG_VIDEO_INTRA_REFRESH_PERIOD: type = V4L2_CTRL_TYPE_INTEGER;break;case V4L2_CID_MPEG_VIDEO_LTR_COUNT: type = V4L2_CTRL_TYPE_INTEGER;break;case V4L2_CID_MPEG_VIDEO_FRAME_LTR_INDEX: type = V4L2_CTRL_TYPE_INTEGER; flags |= V4L2_CTRL_FLAG_EXECUTE_ON_WRITE;break;case V4L2_CID_MPEG_VIDEO_USE_LTR_FRAMES: type = V4L2_CTRL_TYPE_BITMASK; flags |= V4L2_CTRL_FLAG_EXECUTE_ON_WRITE;break;case V4L2_CID_MPEG_VIDEO_FORCE_KEY_FRAME:case V4L2_CID_PAN_RESET:case V4L2_CID_TILT_RESET:case V4L2_CID_FLASH_STROBE:case V4L2_CID_FLASH_STROBE_STOP:case V4L2_CID_AUTO_FOCUS_START:case V4L2_CID_AUTO_FOCUS_STOP:case V4L2_CID_DO_WHITE_BALANCE: type = V4L2_CTRL_TYPE_BUTTON; flags |= V4L2_CTRL_FLAG_WRITE_ONLY |  V4L2_CTRL_FLAG_EXECUTE_ON_WRITE; min =  max =  step =  def = 0;break;case V4L2_CID_POWER_LINE_FREQUENCY:case V4L2_CID_MPEG_AUDIO_SAMPLING_FREQ:case V4L2_CID_MPEG_AUDIO_ENCODING:case V4L2_CID_MPEG_AUDIO_L1_BITRATE:case V4L2_CID_MPEG_AUDIO_L2_BITRATE:case V4L2_CID_MPEG_AUDIO_L3_BITRATE:case V4L2_CID_MPEG_AUDIO_AC3_BITRATE:case V4L2_CID_MPEG_AUDIO_MODE:case V4L2_CID_MPEG_AUDIO_MODE_EXTENSION:case V4L2_CID_MPEG_AUDIO_EMPHASIS:case V4L2_CID_MPEG_AUDIO_CRC:case V4L2_CID_MPEG_AUDIO_DEC_PLAYBACK:case V4L2_CID_MPEG_AUDIO_DEC_MULTILINGUAL_PLAYBACK:case V4L2_CID_MPEG_VIDEO_ENCODING:case V4L2_CID_MPEG_VIDEO_ASPECT:case V4L2_CID_MPEG_VIDEO_BITRATE_MODE:case V4L2_CID_MPEG_STREAM_TYPE:case V4L2_CID_MPEG_STREAM_VBI_FMT:case V4L2_CID_EXPOSURE_AUTO:case V4L2_CID_AUTO_FOCUS_RANGE:case V4L2_CID_COLORFX:case V4L2_CID_AUTO_N_PRESET_WHITE_BALANCE:case V4L2_CID_TUNE_PREEMPHASIS:case V4L2_CID_FLASH_LED_MODE:case V4L2_CID_FLASH_STROBE_SOURCE:case V4L2_CID_MPEG_VIDEO_HEADER_MODE:case V4L2_CID_MPEG_VIDEO_FRAME_SKIP_MODE:case V4L2_CID_MPEG_VIDEO_MULTI_SLICE_MODE:case V4L2_CID_MPEG_VIDEO_H264_ENTROPY_MODE:case V4L2_CID_MPEG_VIDEO_H264_LEVEL:case V4L2_CID_MPEG_VIDEO_H264_LOOP_FILTER_MODE:case V4L2_CID_MPEG_VIDEO_H264_PROFILE:case V4L2_CID_MPEG_VIDEO_H264_VUI_SAR_IDC:case V4L2_CID_MPEG_VIDEO_H264_SEI_FP_ARRANGEMENT_TYPE:case V4L2_CID_MPEG_VIDEO_H264_FMO_MAP_TYPE:case V4L2_CID_MPEG_VIDEO_H264_HIERARCHICAL_CODING_TYPE:case V4L2_CID_MPEG_VIDEO_MPEG2_LEVEL:case V4L2_CID_MPEG_VIDEO_MPEG2_PROFILE:case V4L2_CID_MPEG_VIDEO_MPEG4_LEVEL:case V4L2_CID_MPEG_VIDEO_MPEG4_PROFILE:case V4L2_CID_JPEG_CHROMA_SUBSAMPLING:case V4L2_CID_ISO_SENSITIVITY_AUTO:case V4L2_CID_EXPOSURE_METERING:case V4L2_CID_SCENE_MODE:case V4L2_CID_DV_TX_MODE:case V4L2_CID_DV_TX_RGB_RANGE:case V4L2_CID_DV_TX_IT_CONTENT_TYPE:case V4L2_CID_DV_RX_RGB_RANGE:case V4L2_CID_DV_RX_IT_CONTENT_TYPE:case V4L2_CID_TEST_PATTERN:case V4L2_CID_DEINTERLACING_MODE:case V4L2_CID_TUNE_DEEMPHASIS:case V4L2_CID_MPEG_VIDEO_VPX_GOLDEN_FRAME_SEL:case V4L2_CID_MPEG_VIDEO_VP8_PROFILE:case V4L2_CID_MPEG_VIDEO_VP9_PROFILE:case V4L2_CID_MPEG_VIDEO_VP9_LEVEL:case V4L2_CID_DETECT_MD_MODE:case V4L2_CID_MPEG_VIDEO_HEVC_PROFILE:case V4L2_CID_MPEG_VIDEO_HEVC_LEVEL:case V4L2_CID_MPEG_VIDEO_HEVC_HIER_CODING_TYPE:case V4L2_CID_MPEG_VIDEO_HEVC_REFRESH_TYPE:case V4L2_CID_MPEG_VIDEO_HEVC_SIZE_OF_LENGTH_FIELD:case V4L2_CID_MPEG_VIDEO_HEVC_TIER:case V4L2_CID_MPEG_VIDEO_HEVC_LOOP_FILTER_MODE:case V4L2_CID_MPEG_VIDEO_AV1_PROFILE:case V4L2_CID_MPEG_VIDEO_AV1_LEVEL:case V4L2_CID_STATELESS_HEVC_DECODE_MODE:case V4L2_CID_STATELESS_HEVC_START_CODE:case V4L2_CID_STATELESS_H264_DECODE_MODE:case V4L2_CID_STATELESS_H264_START_CODE:case V4L2_CID_CAMERA_ORIENTATION:case V4L2_CID_MPEG_VIDEO_INTRA_REFRESH_PERIOD_TYPE:case V4L2_CID_HDR_SENSOR_MODE: type = V4L2_CTRL_TYPE_MENU;break;case V4L2_CID_LINK_FREQ: type = V4L2_CTRL_TYPE_INTEGER_MENU;break;case V4L2_CID_RDS_TX_PS_NAME:case V4L2_CID_RDS_TX_RADIO_TEXT:case V4L2_CID_RDS_RX_PS_NAME:case V4L2_CID_RDS_RX_RADIO_TEXT: type = V4L2_CTRL_TYPE_STRING;break;case V4L2_CID_ISO_SENSITIVITY:case V4L2_CID_AUTO_EXPOSURE_BIAS:case V4L2_CID_MPEG_VIDEO_VPX_NUM_PARTITIONS:case V4L2_CID_MPEG_VIDEO_VPX_NUM_REF_FRAMES: type = V4L2_CTRL_TYPE_INTEGER_MENU;break;case V4L2_CID_USER_CLASS:case V4L2_CID_CAMERA_CLASS:case V4L2_CID_CODEC_CLASS:case V4L2_CID_FM_TX_CLASS:case V4L2_CID_FLASH_CLASS:case V4L2_CID_JPEG_CLASS:case V4L2_CID_IMAGE_SOURCE_CLASS:case V4L2_CID_IMAGE_PROC_CLASS:case V4L2_CID_DV_CLASS:case V4L2_CID_FM_RX_CLASS:case V4L2_CID_RF_TUNER_CLASS:case V4L2_CID_DETECT_CLASS:case V4L2_CID_CODEC_STATELESS_CLASS:case V4L2_CID_COLORIMETRY_CLASS: type = V4L2_CTRL_TYPE_CTRL_CLASS;  You can neither read nor write these ", "if (!set)return ERR_PTR(-ENOMEM);new_hdl = kzalloc(sizeof(*new_hdl), GFP_KERNEL);if (!new_hdl)return ERR_PTR(-ENOMEM);obj = &new_hdl->req_obj;ret = v4l2_ctrl_handler_init(new_hdl, (hdl->nr_of_buckets - 1) * 8);if (!ret)ret = v4l2_ctrl_request_bind(req, new_hdl, hdl);if (ret) ": "v4l2_ctrl_request_complete() could not allocate a      control handler object to store the completed state in.     So return ENOMEM to indicate that there was an out-of-memory   error. ", "obj = media_request_object_find(req, &req_ops, main_hdl);if (!obj)return 0;if (obj->completed) ": "v4l2_ctrl_request_setup(struct media_request  req,    struct v4l2_ctrl_handler  main_hdl){struct media_request_object  obj;struct v4l2_ctrl_handler  hdl;struct v4l2_ctrl_ref  ref;int ret = 0;if (!req || !main_hdl)return 0;if (WARN_ON(req->state != MEDIA_REQUEST_STATE_QUEUED))return -EBUSY;    Note that it is valid if nothing was found. It means   that this request doesn't have any controls and so just   wants to leave the controls unchanged. ", "state = kzalloc(sizeof(struct s5h1409_state), GFP_KERNEL);if (state == NULL)goto error;/* setup the state ": "s5h1409_attach(const struct s5h1409_config  config,    struct i2c_adapter  i2c){struct s5h1409_state  state = NULL;u16 reg;  allocate memory for the internal state ", "ret = zl10036_read_status_reg(state);if (ret < 0) ": "zl10036_attach(struct dvb_frontend  fe,    const struct zl10036_config  config,    struct i2c_adapter  i2c){struct zl10036_state  state;int ret;if (!config) {printk(KERN_ERR \"%s: no config specified\", __func__);return NULL;}state = kzalloc(sizeof(struct zl10036_state), GFP_KERNEL);if (!state)return NULL;state->config = config;state->i2c = i2c;if (fe->ops.i2c_gate_ctrl)fe->ops.i2c_gate_ctrl(fe, 1);   open i2c_gate ", "state->config = config;state->i2c = i2c;/* check if the demod is present and has proper type ": "cx24120_attach(const struct cx24120_config  config,    struct i2c_adapter  i2c){struct cx24120_state  state;int demod_rev;info(\"Conexant cx24120cx24118 - DVBSS2 Satellite demodtuner\\n\");state = kzalloc(sizeof( state), GFP_KERNEL);if (!state) {err(\"Unable to allocate memory for cx24120_state\\n\");goto error;}  setup the state ", "struct cx24123_state *state =kzalloc(sizeof(struct cx24123_state), GFP_KERNEL);dprintk(\"\\n\");if (state == NULL) ": "cx24123_attach(const struct cx24123_config  config,    struct i2c_adapter  i2c){  allocate memory for the internal state ", "state->frontend.dtv_property_cache.atscmh_parade_id = 1;return &state->frontend;}EXPORT_SYMBOL(lg2160_attach": "lg2160_attach(const struct lg2160_config  config,   struct i2c_adapter  i2c_adap){struct lg216x_state  state = NULL;lg_dbg(\"(%d-%04x)\\n\",       i2c_adap ? i2c_adapter_id(i2c_adap) : 0,       config ? config->i2c_addr : 0);state = kzalloc(sizeof(struct lg216x_state), GFP_KERNEL);if (!state)return NULL;state->cfg = config;state->i2c_adap = i2c_adap;state->fic_ver = 0xff;state->parade_id = 0xff;switch (config->lg_chip) {default:lg_warn(\"invalid chip requested, defaulting to LG2160\");fallthrough;case LG2160:memcpy(&state->frontend.ops, &lg2160_ops,       sizeof(struct dvb_frontend_ops));break;case LG2161:memcpy(&state->frontend.ops, &lg2161_ops,       sizeof(struct dvb_frontend_ops));break;}state->frontend.demodulator_priv = state;state->current_frequency = -1;  parade 1 by default ", "state = kzalloc(sizeof(*state), GFP_KERNEL);if (!state)return NULL;/* setup the state ": "mb86a20s_attach(const struct mb86a20s_config  config,    struct i2c_adapter  i2c){struct mb86a20s_state  state;u8rev;dev_dbg(&i2c->dev, \"%s called.\\n\", __func__);  allocate memory for the internal state ", "state = kzalloc(sizeof(struct cx22702_state), GFP_KERNEL);if (state == NULL)goto error;/* setup the state ": "cx22702_attach(const struct cx22702_config  config,struct i2c_adapter  i2c){struct cx22702_state  state = NULL;  allocate memory for the internal state ", "state = kzalloc(sizeof(struct tda10023_state), GFP_KERNEL);if (state == NULL) goto error;/* setup the state ": "tda10023_attach(const struct tda10023_config  config,     struct i2c_adapter  i2c,     u8 pwm){struct tda10023_state  state = NULL;  allocate memory for the internal state ", "state = kzalloc(sizeof(struct nxt6000_state), GFP_KERNEL);if (state == NULL) goto error;/* setup the state ": "nxt6000_attach(const struct nxt6000_config  config,    struct i2c_adapter  i2c){struct nxt6000_state  state = NULL;  allocate memory for the internal state ", "state = kzalloc(sizeof(struct s5h1411_state), GFP_KERNEL);if (state == NULL)goto error;/* setup the state ": "s5h1411_attach(const struct s5h1411_config  config,    struct i2c_adapter  i2c){struct s5h1411_state  state = NULL;u16 reg;  allocate memory for the internal state ", "state = kzalloc(sizeof(struct mt312_state), GFP_KERNEL);if (state == NULL)goto error;/* setup the state ": "mt312_attach(const struct mt312_config  config,struct i2c_adapter  i2c){struct mt312_state  state = NULL;  allocate memory for the internal state ", "state = kzalloc(sizeof(struct tda1004x_state), GFP_KERNEL);if (!state) ": "tda10046_attach(const struct tda1004x_config  config,     struct i2c_adapter  i2c){struct tda1004x_state  state;int id;  allocate memory for the internal state ", "if (lgs8gxx_read_reg(priv, 0, &data) != 0) ": "lgs8gxx_attach(const struct lgs8gxx_config  config,struct i2c_adapter  i2c){struct lgs8gxx_state  priv = NULL;u8 data = 0;dprintk(\"%s()\\n\", __func__);if (config == NULL || i2c == NULL)return NULL;priv = kzalloc(sizeof(struct lgs8gxx_state), GFP_KERNEL);if (priv == NULL)goto error_out;priv->config = config;priv->i2c = i2c;  check if the demod is there ", "dprintk(\"pid filter cmd postpone\\n\");state->pid_ctrl_index++;state->pid_ctrl[state->pid_ctrl_index].cmd = DIB9000_PID_FILTER_CTRL;state->pid_ctrl[state->pid_ctrl_index].onoff = onoff;return 0;}if (mutex_lock_interruptible(&state->demod_lock) < 0) ": "dib9000_fw_pid_filter_ctrl(struct dvb_frontend  fe, u8 onoff){struct dib9000_state  state = fe->demodulator_priv;u16 val;int ret;if ((state->pid_ctrl_index != -2) && (state->pid_ctrl_index < 9)) {  postpone the pid filtering cmd ", "new_addr = first_addr + (k << 1);client.i2c_addr = default_addr;dib9000_i2c_write16(&client, 1817, 3);dib9000_i2c_write16(&client, 1796, 0);dib9000_i2c_write16(&client, 1227, 1);dib9000_i2c_write16(&client, 1227, 0);client.i2c_addr = new_addr;dib9000_i2c_write16(&client, 1817, 3);dib9000_i2c_write16(&client, 1796, 0);dib9000_i2c_write16(&client, 1227, 1);dib9000_i2c_write16(&client, 1227, 0);if (dib9000_identify(&client) == 0) ": "dib9000_i2c_enumeration(struct i2c_adapter  i2c, int no_of_demods, u8 default_addr, u8 first_addr){int k = 0, ret = 0;u8 new_addr = 0;struct i2c_device client = {.i2c_adap = i2c };client.i2c_write_buffer = kzalloc(4, GFP_KERNEL);if (!client.i2c_write_buffer) {dprintk(\"%s: not enough memory\\n\", __func__);return -ENOMEM;}client.i2c_read_buffer = kzalloc(4, GFP_KERNEL);if (!client.i2c_read_buffer) {dprintk(\"%s: not enough memory\\n\", __func__);ret = -ENOMEM;goto error_memory;}client.i2c_addr = default_addr + 16;dib9000_i2c_write16(&client, 1796, 0x0);for (k = no_of_demods - 1; k >= 0; k--) {  designated i2c address ", "if ((st->chip.d9.cfg.output_mode != OUTMODE_MPEG2_SERIAL) && (st->chip.d9.cfg.output_mode != OUTMODE_MPEG2_PAR_GATED_CLK))st->chip.d9.cfg.output_mode = OUTMODE_MPEG2_FIFO;if (dib9000_identify(&st->i2c) == 0)goto error;dibx000_init_i2c_master(&st->i2c_master, DIB7000MC, st->i2c.i2c_adap, st->i2c.i2c_addr);st->tuner_adap.dev.parent = i2c_adap->dev.parent;strscpy(st->tuner_adap.name, \"DIB9000_FW TUNER ACCESS\",sizeof(st->tuner_adap.name));st->tuner_adap.algo = &dib9000_tuner_algo;st->tuner_adap.algo_data = NULL;i2c_set_adapdata(&st->tuner_adap, st);if (i2c_add_adapter(&st->tuner_adap) < 0)goto error;st->component_bus.dev.parent = i2c_adap->dev.parent;strscpy(st->component_bus.name, \"DIB9000_FW COMPONENT BUS ACCESS\",sizeof(st->component_bus.name));st->component_bus.algo = &dib9000_component_bus_algo;st->component_bus.algo_data = NULL;st->component_bus_speed = 340;i2c_set_adapdata(&st->component_bus, st);if (i2c_add_adapter(&st->component_bus) < 0)goto component_bus_add_error;dib9000_fw_reset(fe);return fe;component_bus_add_error:i2c_del_adapter(&st->tuner_adap);error:kfree(st);return NULL;}EXPORT_SYMBOL(dib9000_attach": "dib9000_attach(struct i2c_adapter  i2c_adap, u8 i2c_addr, const struct dib9000_config  cfg){struct dvb_frontend  fe;struct dib9000_state  st;st = kzalloc(sizeof(struct dib9000_state), GFP_KERNEL);if (st == NULL)return NULL;fe = kzalloc(sizeof(struct dvb_frontend), GFP_KERNEL);if (fe == NULL) {kfree(st);return NULL;}memcpy(&st->chip.d9.cfg, cfg, sizeof(struct dib9000_config));st->i2c.i2c_adap = i2c_adap;st->i2c.i2c_addr = i2c_addr;st->i2c.i2c_write_buffer = st->i2c_write_buffer;st->i2c.i2c_read_buffer = st->i2c_read_buffer;st->gpio_dir = DIB9000_GPIO_DEFAULT_DIRECTIONS;st->gpio_val = DIB9000_GPIO_DEFAULT_VALUES;st->gpio_pwm_pos = DIB9000_GPIO_DEFAULT_PWM_POS;mutex_init(&st->platform.risc.mbx_if_lock);mutex_init(&st->platform.risc.mbx_lock);mutex_init(&st->platform.risc.mem_lock);mutex_init(&st->platform.risc.mem_mbx_lock);mutex_init(&st->demod_lock);st->get_frontend_internal = 0;st->pid_ctrl_index = -2;st->fe[0] = fe;fe->demodulator_priv = st;memcpy(&st->fe[0]->ops, &dib9000_ops, sizeof(struct dvb_frontend_ops));  Ensure the output mode remains at the previous default if it's   not specifically set by the caller. ", "u8 tx[4];struct i2c_msg m = ": "dibx000_reset_i2c_master(struct dibx000_i2c_master  mst){  initialize the i2c-master by closing the gate ", "dibx000_i2c_gate_ctrl(mst, mst->i2c_write_buffer, 0, 0);ret = (i2c_transfer(i2c_adap, mst->msg, 1) == 1);mutex_unlock(&mst->i2c_buffer_lock);return ret;}EXPORT_SYMBOL(dibx000_init_i2c_master": "dibx000_init_i2c_master(struct dibx000_i2c_master  mst, u16 device_rev,struct i2c_adapter  i2c_adap, u8 i2c_addr){int ret;mutex_init(&mst->i2c_buffer_lock);if (mutex_lock_interruptible(&mst->i2c_buffer_lock) < 0) {dprintk(\"could not acquire lock\\n\");return -EINVAL;}memset(mst->msg, 0, sizeof(struct i2c_msg));mst->msg[0].addr = i2c_addr >> 1;mst->msg[0].flags = 0;mst->msg[0].buf = mst->i2c_write_buffer;mst->msg[0].len = 4;mst->device_rev = device_rev;mst->i2c_adap = i2c_adap;mst->i2c_addr = i2c_addr >> 1;if (device_rev == DIB7000P || device_rev == DIB8000)mst->base_reg = 1024;elsemst->base_reg = 768;mst->gated_tuner_i2c_adap.dev.parent = mst->i2c_adap->dev.parent;if (i2c_adapter_init(&mst->gated_tuner_i2c_adap, &dibx000_i2c_gated_tuner_algo, \"DiBX000 tuner I2C bus\", mst) != 0)pr_err(\"could not initialize the tuner i2c_adapter\\n\");mst->master_i2c_adap_gpio12.dev.parent = mst->i2c_adap->dev.parent;if (i2c_adapter_init(&mst->master_i2c_adap_gpio12, &dibx000_i2c_master_gpio12_xfer_algo, \"DiBX000 master GPIO12 I2C bus\", mst) != 0)pr_err(\"could not initialize the master i2c_adapter\\n\");mst->master_i2c_adap_gpio34.dev.parent = mst->i2c_adap->dev.parent;if (i2c_adapter_init(&mst->master_i2c_adap_gpio34, &dibx000_i2c_master_gpio34_xfer_algo, \"DiBX000 master GPIO34 I2C bus\", mst) != 0)pr_err(\"could not initialize the master i2c_adapter\\n\");mst->master_i2c_adap_gpio67.dev.parent = mst->i2c_adap->dev.parent;if (i2c_adapter_init(&mst->master_i2c_adap_gpio67, &dibx000_i2c_gated_gpio67_algo, \"DiBX000 master GPIO67 I2C bus\", mst) != 0)pr_err(\"could not initialize the master i2c_adapter\\n\");  initialize the i2c-master by closing the gate ", "state = kzalloc(sizeof(struct l64781_state), GFP_KERNEL);if (state == NULL) goto error;/* setup the state ": "l64781_attach(const struct l64781_config  config,   struct i2c_adapter  i2c){struct l64781_state  state = NULL;int reg0x3e = -1;u8 b0 [] = { 0x1a };u8 b1 [] = { 0x00 };struct i2c_msg msg [] = { { .addr = config->demod_address, .flags = 0, .buf = b0, .len = 1 },   { .addr = config->demod_address, .flags = I2C_M_RD, .buf = b1, .len = 1 } };  allocate memory for the internal state ", "state = kzalloc(sizeof(struct stv0367_state), GFP_KERNEL);if (state == NULL)goto error;ter_state = kzalloc(sizeof(struct stv0367ter_state), GFP_KERNEL);if (ter_state == NULL)goto error;/* setup the state ": "stv0367ter_attach(const struct stv0367_config  config,   struct i2c_adapter  i2c){struct stv0367_state  state = NULL;struct stv0367ter_state  ter_state = NULL;  allocate memory for the internal state ", "state = kzalloc(sizeof(struct stv0367_state), GFP_KERNEL);if (state == NULL)goto error;cab_state = kzalloc(sizeof(struct stv0367cab_state), GFP_KERNEL);if (cab_state == NULL)goto error;/* setup the state ": "stv0367cab_attach(const struct stv0367_config  config,   struct i2c_adapter  i2c){struct stv0367_state  state = NULL;struct stv0367cab_state  cab_state = NULL;  allocate memory for the internal state ", "state = kzalloc(sizeof(struct stv0367_state), GFP_KERNEL);if (state == NULL)goto error;ter_state = kzalloc(sizeof(struct stv0367ter_state), GFP_KERNEL);if (ter_state == NULL)goto error;cab_state = kzalloc(sizeof(struct stv0367cab_state), GFP_KERNEL);if (cab_state == NULL)goto error;/* setup the state ": "stv0367ddb_attach(const struct stv0367_config  config,   struct i2c_adapter  i2c){struct stv0367_state  state = NULL;struct stv0367ter_state  ter_state = NULL;struct stv0367cab_state  cab_state = NULL;  allocate memory for the internal state ", "state = kzalloc(sizeof(struct sp887x_state), GFP_KERNEL);if (state == NULL) goto error;/* setup the state ": "sp887x_attach(const struct sp887x_config  config,   struct i2c_adapter  i2c){struct sp887x_state  state = NULL;  allocate memory for the internal state ", "state = kzalloc(sizeof(struct m88rs2000_state), GFP_KERNEL);if (state == NULL)goto error;/* setup the state ": "m88rs2000_attach(const struct m88rs2000_config  config,    struct i2c_adapter  i2c){struct m88rs2000_state  state = NULL;  allocate memory for the internal state ", "ret = i2c_transfer(i2c, msg, 2);if (fe->ops.i2c_gate_ctrl)fe->ops.i2c_gate_ctrl(fe, 0);if (ret != 2)return NULL;priv = kzalloc(sizeof(struct stb6000_priv), GFP_KERNEL);if (priv == NULL)return NULL;priv->i2c_address = addr;priv->i2c = i2c;memcpy(&fe->ops.tuner_ops, &stb6000_tuner_ops,sizeof(struct dvb_tuner_ops));fe->tuner_priv = priv;return fe;}EXPORT_SYMBOL(stb6000_attach": "stb6000_attach(struct dvb_frontend  fe, int addr,struct i2c_adapter  i2c){struct stb6000_priv  priv = NULL;u8 b0[] = { 0 };u8 b1[] = { 0, 0 };struct i2c_msg msg[2] = {{.addr = addr,.flags = 0,.buf = b0,.len = 0}, {.addr = addr,.flags = I2C_M_RD,.buf = b1,.len = 2}};int ret;dprintk(\"%s:\\n\", __func__);if (fe->ops.i2c_gate_ctrl)fe->ops.i2c_gate_ctrl(fe, 1);  is some i2c device here ? ", "state = kzalloc(sizeof(struct stv0297_state), GFP_KERNEL);if (state == NULL)goto error;/* setup the state ": "stv0297_attach(const struct stv0297_config  config,    struct i2c_adapter  i2c){struct stv0297_state  state = NULL;  allocate memory for the internal state ", "fe->ops.read_signal_strength(fe, (u16 *) &s);s >>= 8;dprintk(\"signal strength: %d\\n\", s);for (i = 0; i < sizeof(cx24113_agc_table[0]); i++)if (cx24113_agc_table[state->gain_level][i] > s)break;s = -25 - i*5;} while (cx24113_set_gain_settings(state, s));}EXPORT_SYMBOL(cx24113_agc_callback": "cx24113_agc_callback(struct dvb_frontend  fe){struct cx24113_state  state = fe->tuner_priv;s16 s, i;if (!fe->ops.read_signal_strength)return;do {  this only works with the current CX24123 implementation ", "struct cx24113_state *state = kzalloc(sizeof(*state), GFP_KERNEL);int rc;if (!state)return NULL;/* setup the state ": "cx24113_attach(struct dvb_frontend  fe,const struct cx24113_config  config, struct i2c_adapter  i2c){  allocate memory for the internal state ", "state = kzalloc(sizeof(*state), GFP_KERNEL);if (!state)return NULL;state->config = config;state->i2c = i2c;state->prevUCBS2 = 0;/* check if the demod is present ": "ds3000_attach(const struct ds3000_config  config,    struct i2c_adapter  i2c){struct ds3000_state  state;int ret;dprintk(\"%s\\n\", __func__);  allocate memory for the internal state ", "state = kzalloc(sizeof(*state), GFP_KERNEL);if (state == NULL)return NULL;state->config = config;state->i2c = i2c;/* check if the demod is present ": "cx24116_attach(const struct cx24116_config  config,struct i2c_adapter  i2c){struct cx24116_state  state;int ret;dprintk(\"%s\\n\", __func__);  allocate memory for the internal state ", "if (override_set & 0x80)isl6405->config = ISL6405_ISEL2;elseisl6405->config = ISL6405_ISEL1;isl6405->i2c = i2c;isl6405->i2c_addr = i2c_addr;fe->sec_priv = isl6405;/* bits which should be forced to '1' ": "isl6405_attach(struct dvb_frontend  fe, struct i2c_adapter  i2c,    u8 i2c_addr, u8 override_set, u8 override_clear){struct isl6405  isl6405 = kmalloc(sizeof(struct isl6405), GFP_KERNEL);if (!isl6405)return NULL;  default configuration ", "state = kzalloc(sizeof(struct tda10048_state), GFP_KERNEL);if (state == NULL)goto error;/* setup the state and clone the config ": "tda10048_attach(const struct tda10048_config  config,struct i2c_adapter  i2c){struct tda10048_state  state = NULL;dprintk(1, \"%s()\\n\", __func__);  allocate memory for the internal state ", "state = kzalloc(sizeof(struct cx24110_state), GFP_KERNEL);if (state == NULL) goto error;/* setup the state ": "cx24110_attach(const struct cx24110_config  config,    struct i2c_adapter  i2c){struct cx24110_state  state = NULL;int ret;  allocate memory for the internal state ", "reg0[2] &= ~0xc0;reg0[2] |= (config->clk_div << 6);if (fe->ops.i2c_gate_ctrl)fe->ops.i2c_gate_ctrl(fe, 1);ret = i2c_transfer(i2c, msg, 1);if (fe->ops.i2c_gate_ctrl)fe->ops.i2c_gate_ctrl(fe, 0);if (ret != 1)return NULL;priv = kzalloc(sizeof(struct stv6110_priv), GFP_KERNEL);if (priv == NULL)return NULL;priv->i2c_address = config->i2c_address;priv->i2c = i2c;priv->mclk = config->mclk;priv->clk_div = config->clk_div;priv->gain = config->gain;memcpy(&priv->regs, &reg0[1], 8);memcpy(&fe->ops.tuner_ops, &stv6110_tuner_ops,sizeof(struct dvb_tuner_ops));fe->tuner_priv = priv;printk(KERN_INFO \"STV6110 attached on addr=%x!\\n\", priv->i2c_address);return fe;}EXPORT_SYMBOL(stv6110_attach": "stv6110_attach(struct dvb_frontend  fe,const struct stv6110_config  config,struct i2c_adapter  i2c){struct stv6110_priv  priv = NULL;u8 reg0[] = { 0x00, 0x07, 0x11, 0xdc, 0x85, 0x17, 0x01, 0xe6, 0x1e };struct i2c_msg msg[] = {{.addr = config->i2c_address,.flags = 0,.buf = reg0,.len = 9}};int ret;  divisor value for the output clock ", "state = kzalloc(sizeof(struct bcm3510_state), GFP_KERNEL);if (state == NULL)goto error;/* setup the state ": "bcm3510_attach(const struct bcm3510_config  config,   struct i2c_adapter  i2c){struct bcm3510_state  state = NULL;int ret;bcm3510_register_value v;  allocate memory for the internal state ", "state = kmalloc(sizeof(struct s5h1432_state), GFP_KERNEL);if (!state)return NULL;/* setup the state ": "s5h1432_attach(const struct s5h1432_config  config,    struct i2c_adapter  i2c){struct s5h1432_state  state = NULL;printk(KERN_INFO \" Enter s5h1432_attach(). attach success!\\n\");  allocate memory for the internal state ", "new_addr          = DIB3000MC_I2C_ADDRESS[k];dmcst->i2c_addr = new_addr;if (dib3000mc_identify(dmcst) != 0) ": "dib3000mc_i2c_enumeration(struct i2c_adapter  i2c, int no_of_demods, u8 default_addr, struct dib3000mc_config cfg[]){struct dib3000mc_state  dmcst;int k;u8 new_addr;static const u8 DIB3000MC_I2C_ADDRESS[] = { 20, 22, 24, 26 };dmcst = kzalloc(sizeof(struct dib3000mc_state), GFP_KERNEL);if (dmcst == NULL)return -ENOMEM;dmcst->i2c_adap = i2c;for (k = no_of_demods-1; k >= 0; k--) {dmcst->cfg = &cfg[k];  designated i2c address ", "const u16 *rf_ramp = NULL;u8 en_pwm_rf_mux = 1;/* reset the AGC ": "dib0090_pwm_gain_reset(struct dvb_frontend  fe){struct dib0090_state  state = fe->tuner_priv;const u16  bb_ramp = bb_ramp_pwm_normal;   default baseband config ", "u8 ltg2 = (state->rf_lt_def >> 10) & 0x7;if (state->current_band == BAND_CBAND && ltg2) ": "dib0090_gain_control(struct dvb_frontend  fe){struct dib0090_state  state = fe->tuner_priv;enum frontend_tune_state  tune_state = &state->tune_state;int ret = 10;u16 wbd_val = 0;u8 apply_gain_immediatly = 1;s16 wbd_error = 0, adc_error = 0;if ( tune_state == CT_AGC_START) {state->agc_freeze = 0;dib0090_write_reg(state, 0x04, 0x0);#ifdef CONFIG_BAND_SBANDif (state->current_band == BAND_SBAND) {dib0090_set_rframp(state, rf_ramp_sband);dib0090_set_bbramp(state, bb_ramp_boost);} else#endif#ifdef CONFIG_BAND_VHFif (state->current_band == BAND_VHF && !state->identity.p1g) {dib0090_set_rframp(state, rf_ramp_pwm_vhf);dib0090_set_bbramp(state, bb_ramp_pwm_normal);} else#endif#ifdef CONFIG_BAND_CBANDif (state->current_band == BAND_CBAND && !state->identity.p1g) {dib0090_set_rframp(state, rf_ramp_pwm_cband);dib0090_set_bbramp(state, bb_ramp_pwm_normal);} else#endifif ((state->current_band == BAND_CBAND || state->current_band == BAND_VHF) && state->identity.p1g) {dib0090_set_rframp(state, rf_ramp_pwm_cband_7090p);dib0090_set_bbramp(state, bb_ramp_pwm_normal_socs);} else {dib0090_set_rframp(state, rf_ramp_pwm_uhf);dib0090_set_bbramp(state, bb_ramp_pwm_normal);}dib0090_write_reg(state, 0x32, 0);dib0090_write_reg(state, 0x39, 0);dib0090_wbd_target(state, state->current_rf);state->rf_gain_limit = state->rf_ramp[0] << WBD_ALPHA;state->current_gain = ((state->rf_ramp[0] + state->bb_ramp[0])  2) << GAIN_ALPHA; tune_state = CT_AGC_STEP_0;} else if (!state->agc_freeze) {s16 wbd = 0, i, cnt;int adc;wbd_val = dib0090_get_slow_adc_val(state);if ( tune_state == CT_AGC_STEP_0)cnt = 5;elsecnt = 1;for (i = 0; i < cnt; i++) {wbd_val = dib0090_get_slow_adc_val(state);wbd += dib0090_wbd_to_db(state, wbd_val);}wbd = cnt;wbd_error = state->wbd_target - wbd;if ( tune_state == CT_AGC_STEP_0) {if (wbd_error < 0 && state->rf_gain_limit > 0 && !state->identity.p1g) {#ifdef CONFIG_BAND_CBAND  in case of CBAND tune reduce first the lt_gain2 before adjusting the RF gain ", "state = kzalloc(sizeof(struct or51211_state), GFP_KERNEL);if (state == NULL)return NULL;/* Setup the state ": "or51211_attach(const struct or51211_config  config,    struct i2c_adapter  i2c){struct or51211_state  state = NULL;  Allocate memory for the internal state ", "if (fe->ops.i2c_gate_ctrl)fe->ops.i2c_gate_ctrl(fe, 1);/* check if this is a valid tuner ": "zl10039_attach(struct dvb_frontend  fe,u8 i2c_addr, struct i2c_adapter  i2c){struct zl10039_state  state = NULL;dprintk(\"%s\\n\", __func__);state = kmalloc(sizeof(struct zl10039_state), GFP_KERNEL);if (state == NULL)goto error;state->i2c = i2c;state->i2c_addr = i2c_addr;  Open i2c gate ", "usleep_range(4000, 6000);/* IQ Generator disable ": "horus3a_attach(struct dvb_frontend  fe,    const struct horus3a_config  config,    struct i2c_adapter  i2c){u8 buf[3], val;struct horus3a_priv  priv = NULL;priv = kzalloc(sizeof(struct horus3a_priv), GFP_KERNEL);if (priv == NULL)return NULL;priv->i2c_address = (config->i2c_address >> 1);priv->i2c = i2c;priv->set_tuner_data = config->set_tuner_priv;priv->set_tuner = config->set_tuner_callback;if (fe->ops.i2c_gate_ctrl)fe->ops.i2c_gate_ctrl(fe, 1);  wait 4ms after power on ", "state->demod_mode= config->demod_mode;state->device= config->device;/* default ": "stv090x_attach(struct stv090x_config  config,    struct i2c_adapter  i2c,    enum stv090x_demodulator demod){int ret = 0;struct stv090x_state  state = NULL;state = kzalloc(sizeof( state), GFP_KERNEL);if (!state)goto error;state->verbose= &verbose;state->config= config;state->i2c= i2c;state->frontend.ops= stv090x_ops;state->frontend.demodulator_priv= state;state->demod= demod;  Single or Dual mode ", "state = kzalloc(sizeof(struct ves1x93_state), GFP_KERNEL);if (state == NULL) goto error;/* setup the state ": "ves1x93_attach(const struct ves1x93_config  config,    struct i2c_adapter  i2c){struct ves1x93_state  state = NULL;u8 identity;  allocate memory for the internal state ", "state->internal.inversion= config->inversion;stb0899_wakeup(&state->frontend);if (stb0899_get_dev_id(state) == -ENODEV) ": "stb0899_attach(struct stb0899_config  config, struct i2c_adapter  i2c){struct stb0899_state  state = NULL;state = kzalloc(sizeof (struct stb0899_state), GFP_KERNEL);if (state == NULL)goto error;state->verbose= &verbose;state->config= config;state->i2c= i2c;state->frontend.ops= stb0899_ops;state->frontend.demodulator_priv= state;  use configured inversion as default -- we'll later autodetect inversion ", "/* FGR - NOTE - there is no obvious ChipId to check; we check * some \"known\" bits after reset, but it's still just a guess ": "lgdt3306a_attach(const struct lgdt3306a_config  config,      struct i2c_adapter  i2c_adap){struct lgdt3306a_state  state = NULL;int ret;u8 val;dbg_info(\"(%d-%04x)\\n\",       i2c_adap ? i2c_adapter_id(i2c_adap) : 0,       config ? config->i2c_addr : 0);state = kzalloc(sizeof(struct lgdt3306a_state), GFP_KERNEL);if (state == NULL)goto fail;state->cfg = config;state->i2c_adap = i2c_adap;memcpy(&state->frontend.ops, &lgdt3306a_ops,       sizeof(struct dvb_frontend_ops));state->frontend.demodulator_priv = state;  verify that we're talking to a lg3306a ", "state = kzalloc(sizeof(struct dvb_dummy_fe_state), GFP_KERNEL);if (!state)return NULL;/* create dvb_frontend ": "dvb_dummy_fe_qam_attach(void){struct dvb_dummy_fe_state  state = NULL;  allocate memory for the internal state ", "state = kzalloc(sizeof(struct ec100_state), GFP_KERNEL);if (state == NULL)goto error;/* setup the state ": "ec100_attach(const struct ec100_config  config,struct i2c_adapter  i2c){int ret;struct ec100_state  state = NULL;u8 tmp;  allocate memory for the internal state ", "instance = au8522_get_state(&state, i2c, config->demod_address);switch (instance) ": "au8522_attach(const struct au8522_config  config,   struct i2c_adapter  i2c){struct au8522_state  state = NULL;int instance;  allocate memory for the internal state ", "state = kzalloc(sizeof(struct lgs8gl5_state), GFP_KERNEL);if (state == NULL)goto error;/* Setup the state ": "lgs8gl5_attach(const struct lgs8gl5_config  config, struct i2c_adapter  i2c){struct lgs8gl5_state  state = NULL;dprintk(\"%s\\n\", __func__);  Allocate memory for the internal state ", "state = kzalloc(sizeof(struct or51132_state), GFP_KERNEL);if (state == NULL)return NULL;/* Setup the state ": "or51132_attach(const struct or51132_config  config,    struct i2c_adapter  i2c){struct or51132_state  state = NULL;  Allocate memory for the internal state ", "new_addr          = (0x40 + k) << 1;st.i2c_addr = new_addr;if (dib7000m_identify(&st) != 0) ": "dib7000m_i2c_enumeration(struct i2c_adapter  i2c, int no_of_demods,u8 default_addr, struct dib7000m_config cfg[]){struct dib7000m_state st = { .i2c_adap = i2c };int k = 0;u8 new_addr = 0;for (k = no_of_demods-1; k >= 0; k--) {st.cfg = cfg[k];  designated i2c address ", "lnbp22->config[0] = 0x00; /* ? ": "lnbp22_attach(struct dvb_frontend  fe,struct i2c_adapter  i2c){struct lnbp22  lnbp22 = kmalloc(sizeof(struct lnbp22), GFP_KERNEL);if (!lnbp22)return NULL;  default configuration ", "state = kzalloc(sizeof(struct si21xx_state), GFP_KERNEL);if (state == NULL)goto error;/* setup the state ": "si21xx_attach(const struct si21xx_config  config,struct i2c_adapter  i2c){struct si21xx_state  state = NULL;int id;dprintk(\"%s\\n\", __func__);  allocate memory for the internal state ", "state = kzalloc(sizeof(struct nxt200x_state), GFP_KERNEL);if (state == NULL)goto error;/* setup the state ": "nxt200x_attach(const struct nxt200x_config  config,   struct i2c_adapter  i2c){struct nxt200x_state  state = NULL;u8 buf [] = {0,0,0,0,0};  allocate memory for the internal state ", "data[0] = 16;/* VCO current setting ": "ascot2e_attach(struct dvb_frontend  fe,    const struct ascot2e_config  config,    struct i2c_adapter  i2c){u8 data[4];struct ascot2e_priv  priv = NULL;priv = kzalloc(sizeof(struct ascot2e_priv), GFP_KERNEL);if (priv == NULL)return NULL;priv->i2c_address = (config->i2c_address >> 1);priv->i2c = i2c;priv->set_tuner_data = config->set_tuner_priv;priv->set_tuner = config->set_tuner_callback;if (fe->ops.i2c_gate_ctrl)fe->ops.i2c_gate_ctrl(fe, 1);  16 MHz xTal frequency ", "state->uio_mask = config->antenna_gpio;/* Default gpio to DVB-C ": "drxk_attach(const struct drxk_config  config, struct i2c_adapter  i2c){struct dtv_frontend_properties  p;struct drxk_state  state = NULL;u8 adr = config->adr;int status;dprintk(1, \"\\n\");state = kzalloc(sizeof(struct drxk_state), GFP_KERNEL);if (!state)return NULL;state->i2c = i2c;state->demod_address = adr;state->single_master = config->single_master;state->microcode_name = config->microcode_name;state->qam_demod_parameter_count = config->qam_demod_parameter_count;state->no_i2c_bridge = config->no_i2c_bridge;state->antenna_gpio = config->antenna_gpio;state->antenna_dvbt = config->antenna_dvbt;state->m_chunk_size = config->chunk_size;state->enable_merr_cfg = config->enable_merr_cfg;if (config->dynamic_clk) {state->m_dvbt_static_clk = false;state->m_dvbc_static_clk = false;} else {state->m_dvbt_static_clk = true;state->m_dvbc_static_clk = true;}if (config->mpeg_out_clk_strength)state->m_ts_clockk_strength = config->mpeg_out_clk_strength & 0x07;elsestate->m_ts_clockk_strength = 0x06;if (config->parallel_ts)state->m_enable_parallel = true;elsestate->m_enable_parallel = false;  NOTE: as more UIO bits will be used, add them to the mask ", "tmp++;state->wbd_gain_current = tmp->wbd_gain_val;} elsestate->wbd_gain_current = 6;return state->wbd_offset_3_3[state->wbd_gain_current - 6];}EXPORT_SYMBOL(dib0070_wbd_offset": "dib0070_wbd_offset_calibration(struct dib0070_state  state){u8 gain;for (gain = 6; gain < 8; gain++) {state->wbd_offset_3_3[gain - 6] = ((dib0070_read_wbd_offset(state, gain)   8   18  33 + 1)  2);dprintk(\"Gain: %d, WBDOffset (3.3V) = %hd\\n\", gain, state->wbd_offset_3_3[gain-6]);}}u16 dib0070_wbd_offset(struct dvb_frontend  fe){struct dib0070_state  state = fe->tuner_priv;const struct dib0070_wbd_gain_cfg  tmp = state->cfg->wbd_gain;u32 freq = fe->dtv_property_cache.frequency1000;if (tmp != NULL) {while (freq1000 > tmp->freq)   find the right one ", "state = kzalloc(sizeof(struct tda10086_state), GFP_KERNEL);if (!state)return NULL;/* setup the state ": "tda10086_attach(const struct tda10086_config  config,     struct i2c_adapter  i2c){struct tda10086_state  state;dprintk (\"%s\\n\", __func__);  allocate memory for the internal state ", "CDRXD(state, state->config.IF ? state->config.IF : 36000000);InitHI(state);return &state->frontend;error:printk(KERN_ERR \"drxd: not found\\n\");kfree(state);return NULL;}EXPORT_SYMBOL(drxd_attach": "drxd_attach(const struct drxd_config  config, void  priv, struct i2c_adapter  i2c, struct device  dev){struct drxd_state  state = NULL;state = kzalloc(sizeof( state), GFP_KERNEL);if (!state)return NULL;state->ops = drxd_ops;state->dev = dev;state->config =  config;state->i2c = i2c;state->priv = priv;mutex_init(&state->mutex);if (Read16(state, 0, NULL, 0) < 0)goto error;state->frontend.ops = drxd_ops;state->frontend.demodulator_priv = state;ConfigureMPEGOutput(state, 0);  add few initialization to allow gate control ", "ret = lgdt3305_read_reg(state, LGDT3305_GEN_CTRL_2, &val);if ((lg_fail(ret)) | (val == 0))goto fail;ret = lgdt3305_write_reg(state, 0x0808, 0x80);if (lg_fail(ret))goto fail;ret = lgdt3305_read_reg(state, 0x0808, &val);if ((lg_fail(ret)) | (val != 0x80))goto fail;ret = lgdt3305_write_reg(state, 0x0808, 0x00);if (lg_fail(ret))goto fail;state->current_frequency = -1;state->current_modulation = -1;return &state->frontend;fail:lg_warn(\"unable to detect %s hardware\\n\",config->demod_chip ? \"LGDT3304\" : \"LGDT3305\");kfree(state);return NULL;}EXPORT_SYMBOL(lgdt3305_attach": "lgdt3305_attach(const struct lgdt3305_config  config,     struct i2c_adapter  i2c_adap){struct lgdt3305_state  state = NULL;int ret;u8 val;lg_dbg(\"(%d-%04x)\\n\",       i2c_adap ? i2c_adapter_id(i2c_adap) : 0,       config ? config->i2c_addr : 0);state = kzalloc(sizeof(struct lgdt3305_state), GFP_KERNEL);if (state == NULL)goto fail;state->cfg = config;state->i2c_adap = i2c_adap;switch (config->demod_chip) {case LGDT3304:memcpy(&state->frontend.ops, &lgdt3304_ops,       sizeof(struct dvb_frontend_ops));break;case LGDT3305:memcpy(&state->frontend.ops, &lgdt3305_ops,       sizeof(struct dvb_frontend_ops));break;default:goto fail;}state->frontend.demodulator_priv = state;  verify that we're talking to a lg dt33045 ", "if (atbm8830_read_reg(priv, REG_CHIP_ID, &data) != 0) ": "atbm8830_attach(const struct atbm8830_config  config,struct i2c_adapter  i2c){struct atbm_state  priv = NULL;u8 data = 0;dprintk(\"%s()\\n\", __func__);if (config == NULL || i2c == NULL)return NULL;priv = kzalloc(sizeof(struct atbm_state), GFP_KERNEL);if (priv == NULL)goto error_out;priv->config = config;priv->i2c = i2c;  check if the demod is there ", "state = kzalloc(sizeof(struct ves1820_state), GFP_KERNEL);if (state == NULL)goto error;/* setup the state ": "ves1820_attach(const struct ves1820_config  config,    struct i2c_adapter  i2c,    u8 pwm){struct ves1820_state  state = NULL;  allocate memory for the internal state ", "state = kzalloc(sizeof(struct tda10021_state), GFP_KERNEL);if (state == NULL) goto error;/* setup the state ": "tda10021_attach(const struct tda1002x_config  config,     struct i2c_adapter  i2c,     u8 pwm){struct tda10021_state  state = NULL;u8 id;  allocate memory for the internal state ", "isl6421->config = ISL6421_ISEL1;isl6421->i2c = i2c;isl6421->i2c_addr = i2c_addr;fe->sec_priv = isl6421;/* bits which should be forced to '1' ": "isl6421_attach(struct dvb_frontend  fe, struct i2c_adapter  i2c, u8 i2c_addr,   u8 override_set, u8 override_clear, bool override_tone){struct isl6421  isl6421 = kmalloc(sizeof(struct isl6421), GFP_KERNEL);if (!isl6421)return NULL;  default configuration ", "struct s921_state *state =kzalloc(sizeof(struct s921_state), GFP_KERNEL);dprintk(\"\\n\");if (!state) ": "s921_attach(const struct s921_config  config,    struct i2c_adapter  i2c){  allocate memory for the internal state ", "struct ts2020_config pdata;memcpy(&pdata, config, sizeof(pdata));pdata.fe = fe;pdata.attach_in_use = true;memset(&board_info, 0, sizeof(board_info));strscpy(board_info.type, \"ts2020\", I2C_NAME_SIZE);board_info.addr = config->tuner_address;board_info.platform_data = &pdata;client = i2c_new_client_device(i2c, &board_info);if (!i2c_client_has_driver(client))return NULL;return fe;}EXPORT_SYMBOL(ts2020_attach": "ts2020_attach(struct dvb_frontend  fe,const struct ts2020_config  config,struct i2c_adapter  i2c){struct i2c_client  client;struct i2c_board_info board_info;  This is only used by ts2020_probe() so can be on the stack ", "state = kzalloc(sizeof(struct dib3000_state), GFP_KERNEL);if (state == NULL)goto error;/* setup the state ": "dib3000mb_attach(const struct dib3000_config  config,      struct i2c_adapter  i2c, struct dib_fe_xfer_ops  xfer_ops){struct dib3000_state  state = NULL;  allocate memory for the internal state ", "isl6423->reg_3 = 0x02 << 5;/* SR4H = 0, SR4M = 1, SR4L = 1 ": "isl6423_attach(struct dvb_frontend  fe,    struct i2c_adapter  i2c,    const struct isl6423_config  config){struct isl6423_dev  isl6423;isl6423 = kzalloc(sizeof(struct isl6423_dev), GFP_KERNEL);if (!isl6423)return NULL;isl6423->config= config;isl6423->i2c= i2c;fe->sec_priv= isl6423;  SR3H = 0, SR3M = 1, SR3L = 0 ", "state = kzalloc(sizeof(struct cx22700_state), GFP_KERNEL);if (state == NULL) goto error;/* setup the state ": "cx22700_attach(const struct cx22700_config  config,    struct i2c_adapter  i2c){struct cx22700_state  state = NULL;  allocate memory for the internal state ", "state = kzalloc(sizeof(struct mt352_state), GFP_KERNEL);if (state == NULL) goto error;/* setup the state ": "mt352_attach(const struct mt352_config  config,  struct i2c_adapter  i2c){struct mt352_state  state = NULL;  allocate memory for the internal state ", "return 0;}if (enable)return au8522_writereg(state, 0x106, 1);elsereturn au8522_writereg(state, 0x106, 0);}EXPORT_SYMBOL(au8522_i2c_gate_ctrl": "au8522_i2c_gate_ctrl(struct dvb_frontend  fe, int enable){struct au8522_state  state = fe->demodulator_priv;dprintk(\"%s(%d)\\n\", __func__, enable);if (state->operational_mode == AU8522_ANALOG_MODE) {  We're being asked to manage the gate even though we're   not in digital mode.  This can occur if we get switched   over to analog mode before the dvb_frontend kernel thread   has completely shutdown ", "if (!led_config || !led_config->gpio_leds ||    !led_config->num_led_states || !led_config->led_states)return 0;if (led < 0) ": "au8522_led_ctrl(struct au8522_state  state, int led){struct au8522_led_config  led_config = state->config.led_cfg;int i, ret = 0;  bail out if we can't control an LED ", "state->current_frequency = 0;state->current_modulation = VSB_8;au8522_writereg(state, 0xa4, 1 << 5);au8522_i2c_gate_ctrl(fe, 1);return 0;}EXPORT_SYMBOL(au8522_init": "au8522_init(struct dvb_frontend  fe){struct au8522_state  state = fe->demodulator_priv;dprintk(\"%s()\\n\", __func__);state->operational_mode = AU8522_DIGITAL_MODE;  Clear out any state associated with the digital side of the   chip, so that when it gets powered back up it won't think   that it is already tuned ", "if (state->operational_mode == AU8522_ANALOG_MODE) ": "au8522_sleep(struct dvb_frontend  fe){struct au8522_state  state = fe->demodulator_priv;dprintk(\"%s()\\n\", __func__);  Only power down if the digital side is currently using the chip ", "state = kzalloc(sizeof(struct zl10353_state), GFP_KERNEL);if (state == NULL)goto error;/* setup the state ": "zl10353_attach(const struct zl10353_config  config,    struct i2c_adapter  i2c){struct zl10353_state  state = NULL;int id;  allocate memory for the internal state ", "state = kzalloc(sizeof(struct stv0299_state), GFP_KERNEL);if (state == NULL) goto error;/* setup the state ": "stv0299_attach(const struct stv0299_config  config,    struct i2c_adapter  i2c){struct stv0299_state  state = NULL;int id;  allocate memory for the internal state ", "state = kzalloc(sizeof(struct tda8083_state), GFP_KERNEL);if (state == NULL) goto error;/* setup the state ": "tda8083_attach(const struct tda8083_config  config,    struct i2c_adapter  i2c){struct tda8083_state  state = NULL;  allocate memory for the internal state ", "fe->tuner_priv= state;fe->ops.tuner_ops= stb6100_ops;printk(\"%s: Attaching STB6100 \\n\", __func__);return fe;}static void stb6100_release(struct dvb_frontend *fe)": "stb6100_attach(struct dvb_frontend  fe,    const struct stb6100_config  config,    struct i2c_adapter  i2c){struct stb6100_state  state = NULL;state = kzalloc(sizeof (struct stb6100_state), GFP_KERNEL);if (!state)return NULL;state->config= config;state->i2c= i2c;state->frontend= fe;state->reference= config->refclock  1000;   kHz ", "state = kzalloc(sizeof(struct stv0288_state), GFP_KERNEL);if (state == NULL)goto error;/* setup the state ": "stv0288_attach(const struct stv0288_config  config,    struct i2c_adapter  i2c){struct stv0288_state  state = NULL;int id;  allocate memory for the internal state ", "struct s5h1420_state *state = kzalloc(sizeof(struct s5h1420_state), GFP_KERNEL);u8 i;if (state == NULL)goto error;/* setup the state ": "s5h1420_attach(const struct s5h1420_config  config,    struct i2c_adapter  i2c){  allocate memory for the internal state ", "state = kzalloc(sizeof(struct drx39xxj_state), GFP_KERNEL);if (state == NULL)goto error;demod = kmemdup(&drxj_default_demod_g,sizeof(struct drx_demod_instance), GFP_KERNEL);if (demod == NULL)goto error;demod_addr = kmemdup(&drxj_default_addr_g,     sizeof(struct i2c_device_addr), GFP_KERNEL);if (demod_addr == NULL)goto error;demod_comm_attr = kmemdup(&drxj_default_comm_attr_g,  sizeof(struct drx_common_attr), GFP_KERNEL);if (demod_comm_attr == NULL)goto error;demod_ext_attr = kmemdup(&drxj_data_g, sizeof(struct drxj_data), GFP_KERNEL);if (demod_ext_attr == NULL)goto error;/* setup the state ": "drx39xxj_attach(struct i2c_adapter  i2c){struct drx39xxj_state  state = NULL;struct i2c_device_addr  demod_addr = NULL;struct drx_common_attr  demod_comm_attr = NULL;struct drxj_data  demod_ext_attr = NULL;struct drx_demod_instance  demod = NULL;struct dtv_frontend_properties  p;int result;  allocate memory for the internal state ", "pad = media_pad_remote_pad_first(pad);if (!pad || !is_media_entity_v4l2_subdev(pad->entity))break;sd = media_entity_to_v4l2_subdev(pad->entity);if (sd->grp_id == GRP_ID_FIMC_IS_SENSOR ||    sd->grp_id == GRP_ID_SENSOR)return sd;/* sink pad ": "fimc_find_remote_sensor(struct media_entity  entity){struct media_pad  pad = &entity->pads[0];struct v4l2_subdev  sd;while (pad->flags & MEDIA_PAD_FL_SINK) {  source pad ", "mtk_v4l2_err(\"[%d] cmd=%d, type=%d, dec timeout=%ums (%d %d)\",     ctx->id, command, ctx->type, timeout_ms,     ctx->int_cond[hw_id], ctx->int_type[hw_id]);} else if (-ERESTARTSYS == ret) ": "mtk_vcodec_wait_for_done_ctx(struct mtk_vcodec_ctx  ctx, int command, unsigned int timeout_ms, unsigned int hw_id){long timeout_jiff, ret;int status = 0;timeout_jiff = msecs_to_jiffies(timeout_ms);ret = wait_event_interruptible_timeout(ctx->queue[hw_id],       ctx->int_cond[hw_id],       timeout_jiff);if (!ret) {status = -1;  timeout ", "if (dst_q_data->pixelformat == V4L2_PIX_FMT_YUYV)val = VDOAC_PFS;elseval = 0;writel(val, vdoa->regs + VDOAC);writel(dst_q_data->height << 16 | dst_q_data->width,       vdoa->regs + VDOAFP);val = dst;writel(val, vdoa->regs + VDOAIEBA00);writel(src_q_data->bytesperline << 16 | dst_q_data->bytesperline,       vdoa->regs + VDOASL);if (dst_q_data->pixelformat == V4L2_PIX_FMT_NV12 ||    dst_q_data->pixelformat == V4L2_PIX_FMT_NV21)val = dst_q_data->bytesperline * dst_q_data->height;elseval = 0;writel(val, vdoa->regs + VDOAIUBO);val = src;writel(val, vdoa->regs + VDOAVEBA0);val = round_up(src_q_data->bytesperline * src_q_data->height, 4096);writel(val, vdoa->regs + VDOAVUBO);/* Enable interrupts and start transfer ": "vdoa_device_run(struct vdoa_ctx  ctx, dma_addr_t dst, dma_addr_t src){struct vdoa_q_data  src_q_data,  dst_q_data;struct vdoa_data  vdoa = ctx->vdoa;u32 val;if (vdoa->curr_ctx)vdoa_wait_for_completion(vdoa->curr_ctx);vdoa->curr_ctx = ctx;reinit_completion(&ctx->completion);ctx->submitted_job++;src_q_data = &ctx->q_data[V4L2_M2M_SRC];dst_q_data = &ctx->q_data[V4L2_M2M_DST];  Progressive, no sync, 1 frame per run ", "if (!ctx)return 0;src_q_data = &ctx->q_data[V4L2_M2M_SRC];dst_q_data = &ctx->q_data[V4L2_M2M_DST];src_q_data->width = width;src_q_data->height = height;src_q_data->bytesperline = width;src_q_data->sizeimage =round_up(src_q_data->bytesperline * height, 4096) +src_q_data->bytesperline * height / 2;dst_q_data->width = width;dst_q_data->height = height;dst_q_data->pixelformat = pixelformat;switch (pixelformat) ": "vdoa_context_configure(struct vdoa_ctx  ctx,   unsigned int width, unsigned int height,   u32 pixelformat){struct vdoa_q_data  src_q_data;struct vdoa_q_data  dst_q_data;if (width < 16 || width  > 8192 || width % 16 != 0 ||    height < 16 || height > 4096 || height % 16 != 0)return -EINVAL;if (pixelformat != V4L2_PIX_FMT_YUYV &&    pixelformat != V4L2_PIX_FMT_NV12)return -EINVAL;  If no context is passed, only check if the format is valid ", "DUMPREG(INT_CHAN_STAT(0));DUMPREG(INT_CHAN_MASK(0));DUMPREG(INT_CHAN_STAT(3));DUMPREG(INT_CHAN_MASK(3));DUMPREG(INT_CLIENT0_STAT);DUMPREG(INT_CLIENT0_MASK);DUMPREG(INT_CLIENT1_STAT);DUMPREG(INT_CLIENT1_MASK);DUMPREG(INT_LIST0_STAT);DUMPREG(INT_LIST0_MASK);/* * these are registers specific to VPE clients, we can make this * function dump client registers specific to VPE or VIP based on * who is using it ": "vpdma_dump_regs(struct vpdma_data  vpdma){struct device  dev = &vpdma->pdev->dev;#define DUMPREG(r) dev_dbg(dev, \"%-35s %08x\\n\", #r, read_reg(vpdma, VPDMA_##r))dev_dbg(dev, \"VPDMA Registers:\\n\");DUMPREG(PID);DUMPREG(LIST_ADDR);DUMPREG(LIST_ATTR);DUMPREG(LIST_STAT_SYNC);DUMPREG(BG_RGB);DUMPREG(BG_YUV);DUMPREG(SETUP);DUMPREG(MAX_SIZE1);DUMPREG(MAX_SIZE2);DUMPREG(MAX_SIZE3);    dumping registers of only group0 and group3, because VPE channels   lie within group0 and group3 registers ", "int vpdma_create_desc_list(struct vpdma_desc_list *list, size_t size, int type)": "vpdma_create_desc_list(&abort_list,size   sizeof(struct vpdma_dtd), VPDMA_LIST_TYPE_NORMAL);if (ret)return ret;for (i = 0; i < size; i++)vpdma_add_abort_channel_ctd(&abort_list, channels[i]);ret = vpdma_map_desc_buf(vpdma, &abort_list.buf);if (ret)goto free_desc;ret = vpdma_submit_descs(vpdma, &abort_list, list_num);if (ret)goto unmap_desc;while (vpdma_list_busy(vpdma, list_num) && --timeout);if (timeout == 0) {dev_err(&vpdma->pdev->dev, \"Timed out cleaning up VPDMA list\\n\");ret = -EBUSY;}unmap_desc:vpdma_unmap_desc_buf(vpdma, &abort_list.buf);free_desc:vpdma_free_desc_buf(&abort_list.buf);return ret;}EXPORT_SYMBOL(vpdma_list_cleanup);    create a descriptor list, the user of this list will append configuration,   control and data descriptors to this list, this list will be submitted to   VPDMA. VPDMA's list parser will go through each descriptor and perform the   required DMA operations ", "offset = (void *)write_dtd - list->buf.addr;write_desc_addr = list->buf.dma_addr + offset;if (drop)dtd->desc_write_addr = dtd_desc_write_addr(write_desc_addr,   1, 1, 0);elsedtd->desc_write_addr = dtd_desc_write_addr(write_desc_addr,   1, 0, 0);vpdma_map_desc_buf(vpdma, &list->buf);dump_dtd(dtd);}EXPORT_SYMBOL(vpdma_update_dma_addr": "vpdma_update_dma_addr(struct vpdma_data  vpdma,struct vpdma_desc_list  list, dma_addr_t dma_addr,void  write_dtd, int drop, int idx){struct vpdma_dtd  dtd = list->buf.addr;dma_addr_t write_desc_addr;int offset;dtd += idx;vpdma_unmap_desc_buf(vpdma, &list->buf);dtd->start_addr = dma_addr;  Calculate write address from the offset of write_dtd from start   of the list->buf ", "if ((dst_w << 1) < src_w)dst_w <<= 1;/* second level decimation ": "sc_set_hs_coeffs(struct sc_data  sc, void  addr, unsigned int src_w,unsigned int dst_w){int sixteenths;int idx;int i, j;u16  coeff_h = addr;const u16  cp;if (dst_w > src_w) {idx = HS_UP_SCALE;} else {if ((dst_w << 1) < src_w)dst_w <<= 1;  first level decimation ", "coeff_v += SC_NUM_TAPS_MEM_ALIGN - SC_V_NUM_TAPS;}sc->load_coeff_v = true;}EXPORT_SYMBOL(sc_set_vs_coeffs": "sc_set_vs_coeffs(struct sc_data  sc, void  addr, unsigned int src_h,unsigned int dst_h){int sixteenths;int idx;int i, j;u16  coeff_v = addr;const u16  cp;if (dst_h > src_h) {idx = VS_UP_SCALE;} else if (dst_h == src_h) {idx = VS_1_TO_1_SCALE;} else {sixteenths = (dst_h << 4)  src_h;if (sixteenths < 8)sixteenths = 8;idx = VS_LT_9_16_SCALE + sixteenths - 8;}cp = scaler_vs_coeffs[idx];for (i = 0; i < SC_NUM_PHASES   2; i++) {for (j = 0; j < SC_V_NUM_TAPS; j++) coeff_v++ =  cp++;    for the vertical scaler, we copy the first 5 coefficients and   skip the last 3 slots to move to the next row to hold   coefficients for the next phase ", "u32 *sc_reg9 = sc_reg8 + 1;u32 *sc_reg12 = sc_reg8 + 4;u32 *sc_reg13 = sc_reg8 + 5;u32 *sc_reg24 = sc_reg17 + 7;val = sc_reg0[0];/* clear all the features(they may get enabled elsewhere later) ": "sc_config_scaler(struct sc_data  sc, u32  sc_reg0, u32  sc_reg8,u32  sc_reg17, unsigned int src_w, unsigned int src_h,unsigned int dst_w, unsigned int dst_h){struct device  dev = &sc->pdev->dev;u32 val;int dcm_x, dcm_shift;bool use_rav;unsigned long lltmp;u32 lin_acc_inc, lin_acc_inc_u;u32 col_acc_offset;u16 factor = 0;int row_acc_init_rav = 0, row_acc_init_rav_b = 0;u32 row_acc_inc = 0, row_acc_offset = 0, row_acc_offset_b = 0;    location of SC register in payload memory with respect to the first   register in the mmr address data block ", "int vpif_set_video_params(struct vpif_params *vpifparams, u8 channel_id)": "vpif_set_video_params   This function is used to set video parameters in VPIF register ", "if (type == CX18_ENC_STREAM_TYPE_IDX) ": "cx18_claim_stream(struct cx18_open_id  id, int type){struct cx18  cx = id->cx;struct cx18_stream  s = &cx->streams[type];struct cx18_stream  s_assoc;  Nothing should ever try to directly claim the IDX stream ", "return;}if (s->type == CX18_ENC_STREAM_TYPE_VBI &&test_bit(CX18_F_S_INTERNAL_USE, &s->s_flags)) ": "cx18_release_stream(struct cx18_stream  s){struct cx18  cx = s->cx;struct cx18_stream  s_assoc;s->id = -1;if (s->type == CX18_ENC_STREAM_TYPE_IDX) {    The IDX stream is only used internally, and can   only be indirectly unclaimed by unclaiming the MPG stream. ", "captype = CAPTURE_CHANNEL_TYPE_VBI;#endifcx->vbi.frame = 0;cx->vbi.inserted_frame = 0;memset(cx->vbi.sliced_mpeg_size,0, sizeof(cx->vbi.sliced_mpeg_size));break;default:return -EINVAL;}/* Clear Streamoff flags in case left from last capture ": "cx18_start_v4l2_encode_stream(struct cx18_stream  s){u32 data[MAX_MB_ARGUMENTS];struct cx18  cx = s->cx;int captype = 0;struct cx18_stream  s_idx;if (!cx18_stream_enabled(s))return -EINVAL;CX18_DEBUG_INFO(\"Start encoder stream %s\\n\", s->name);switch (s->type) {case CX18_ENC_STREAM_TYPE_MPG:captype = CAPTURE_CHANNEL_TYPE_MPEG;cx->mpg_data_received = cx->vbi_data_inserted = 0;cx->dualwatch_jiffies = jiffies;cx->dualwatch_stereo_mode = v4l2_ctrl_g_ctrl(cx->cxhdl.audio_mode);cx->search_pack_header = 0;break;case CX18_ENC_STREAM_TYPE_IDX:captype = CAPTURE_CHANNEL_TYPE_INDEX;break;case CX18_ENC_STREAM_TYPE_TS:captype = CAPTURE_CHANNEL_TYPE_TS;break;case CX18_ENC_STREAM_TYPE_YUV:captype = CAPTURE_CHANNEL_TYPE_YUV;break;case CX18_ENC_STREAM_TYPE_PCM:captype = CAPTURE_CHANNEL_TYPE_PCM;break;case CX18_ENC_STREAM_TYPE_VBI:#ifdef CX18_ENCODER_PARSES_SLICEDcaptype = cx18_raw_vbi(cx) ?     CAPTURE_CHANNEL_TYPE_VBI : CAPTURE_CHANNEL_TYPE_SLICED_VBI;#else    Currently we set things up so that Sliced VBI from the   digitizer is handled as Raw VBI by the encoder ", "CX18_DEBUG_INFO(\"Stop Capture\\n\");if (atomic_read(&cx->tot_capturing) == 0)return 0;set_bit(CX18_F_S_STOPPING, &s->s_flags);if (s->type == CX18_ENC_STREAM_TYPE_MPG)cx18_vapi(cx, CX18_CPU_CAPTURE_STOP, 2, s->handle, !gop_end);elsecx18_vapi(cx, CX18_CPU_CAPTURE_STOP, 1, s->handle);if (s->type == CX18_ENC_STREAM_TYPE_MPG && gop_end) ": "cx18_stop_v4l2_encode_stream(s, 0);}}int cx18_stop_v4l2_encode_stream(struct cx18_stream  s, int gop_end){struct cx18  cx = s->cx;if (!cx18_stream_enabled(s))return -EINVAL;  This function assumes that you are allowed to stop the capture   and that we are actually capturing ", "instructions  = fields * (1 + ((bpl + padding) * lines) /  PAGE_SIZE + lines);instructions += 4;risc->size = instructions * 8;risc->dma = 0;risc->cpu = dma_alloc_coherent(&pci->dev, risc->size, &risc->dma,       GFP_KERNEL);if (!risc->cpu)return -ENOMEM;/* write risc instructions ": "cx88_risc_buffer(struct pci_dev  pci, struct cx88_riscmem  risc,     struct scatterlist  sglist,     unsigned int top_offset, unsigned int bottom_offset,     unsigned int bpl, unsigned int padding, unsigned int lines){u32 instructions, fields;__le32  rp;fields = 0;if (top_offset != UNSET)fields++;if (bottom_offset != UNSET)fields++;    estimate risc mem: worst case is one write per page border +   one write per scan line + syncs + jump (all 2 dwords).  Padding   can cause next bpl to start close to a page border.  First DMA   region may be smaller than PAGE_SIZE ", "instructions  = 1 + (bpl * lines) / PAGE_SIZE + lines;instructions += 3;risc->size = instructions * 8;risc->dma = 0;risc->cpu = dma_alloc_coherent(&pci->dev, risc->size, &risc->dma,       GFP_KERNEL);if (!risc->cpu)return -ENOMEM;/* write risc instructions ": "cx88_risc_databuffer(struct pci_dev  pci, struct cx88_riscmem  risc, struct scatterlist  sglist, unsigned int bpl, unsigned int lines, unsigned int lpi){u32 instructions;__le32  rp;    estimate risc mem: worst case is one write per page border +   one write per scan line + syncs + jump (all 2 dwords).  Here   there is no padding and no sync.  First DMA region may be smaller   than PAGE_SIZE ", ".fifo_start = 0x185400,       /* same as audio IN ": "cx88_sram_channels[] = {[SRAM_CH21] = {.name       = \"video y  packed\",.cmds_start = 0x180040,.ctrl_start = 0x180400,.cdt        = 0x180400 + 64,.fifo_start = 0x180c00,.fifo_size  = 0x002800,.ptr1_reg   = MO_DMA21_PTR1,.ptr2_reg   = MO_DMA21_PTR2,.cnt1_reg   = MO_DMA21_CNT1,.cnt2_reg   = MO_DMA21_CNT2,},[SRAM_CH22] = {.name       = \"video u\",.cmds_start = 0x180080,.ctrl_start = 0x1804a0,.cdt        = 0x1804a0 + 64,.fifo_start = 0x183400,.fifo_size  = 0x000800,.ptr1_reg   = MO_DMA22_PTR1,.ptr2_reg   = MO_DMA22_PTR2,.cnt1_reg   = MO_DMA22_CNT1,.cnt2_reg   = MO_DMA22_CNT2,},[SRAM_CH23] = {.name       = \"video v\",.cmds_start = 0x1800c0,.ctrl_start = 0x180540,.cdt        = 0x180540 + 64,.fifo_start = 0x183c00,.fifo_size  = 0x000800,.ptr1_reg   = MO_DMA23_PTR1,.ptr2_reg   = MO_DMA23_PTR2,.cnt1_reg   = MO_DMA23_CNT1,.cnt2_reg   = MO_DMA23_CNT2,},[SRAM_CH24] = {.name       = \"vbi\",.cmds_start = 0x180100,.ctrl_start = 0x1805e0,.cdt        = 0x1805e0 + 64,.fifo_start = 0x184400,.fifo_size  = 0x001000,.ptr1_reg   = MO_DMA24_PTR1,.ptr2_reg   = MO_DMA24_PTR2,.cnt1_reg   = MO_DMA24_CNT1,.cnt2_reg   = MO_DMA24_CNT2,},[SRAM_CH25] = {.name       = \"audio from\",.cmds_start = 0x180140,.ctrl_start = 0x180680,.cdt        = 0x180680 + 64,.fifo_start = 0x185400,.fifo_size  = 0x001000,.ptr1_reg   = MO_DMA25_PTR1,.ptr2_reg   = MO_DMA25_PTR2,.cnt1_reg   = MO_DMA25_CNT1,.cnt2_reg   = MO_DMA25_CNT2,},[SRAM_CH26] = {.name       = \"audio to\",.cmds_start = 0x180180,.ctrl_start = 0x180720,.cdt        = 0x180680 + 64,    same as audio IN ", "cdt   = ch->cdt;lines = ch->fifo_size / bpl;if (lines > 6)lines = 6;WARN_ON(lines < 2);/* write CDT ": "cx88_sram_channel_setup(struct cx88_core  core,    const struct sram_channel  ch,    unsigned int bpl, u32 risc){unsigned int i, lines;u32 cdt;bpl   = (bpl + 7) & ~7;   alignment ", "cx_write(MO_DEV_CNTRL2, 0);/* stop dma transfers ": "cx88_shutdown(struct cx88_core  core){  disable RISC controller + IRQs ", "cx_write(MO_VID_INTSTAT, 0xFFFFFFFF); // Clear PIV intcx_write(MO_PCI_INTSTAT, 0xFFFFFFFF); // Clear PCI intcx_write(MO_INT1_STAT,   0xFFFFFFFF); // Clear RISC int/* wait a bit ": "cx88_reset(struct cx88_core  core){dprintk(1, \"\");cx88_shutdown(core);  clear irq status ", "cx_andor(MO_FILTER_ODD,   0x7ffc7f, value);dprintk(1, \"set_scale: filter  0x%04x\\n\", value);return 0;}EXPORT_SYMBOL(cx88_set_scale": "cx88_set_scale(struct cx88_core  core, unsigned int width,   unsigned int height, enum v4l2_field field){unsigned int swidth  = norm_swidth(core->tvnorm);unsigned int sheight = norm_maxh(core->tvnorm);u32 value;dprintk(1, \"set_scale: %dx%d [%s%s,%s]\\n\", width, height,V4L2_FIELD_HAS_TOP(field)    ? \"T\" : \"\",V4L2_FIELD_HAS_BOTTOM(field) ? \"B\" : \"\",v4l2_norm_to_name(core->tvnorm));if (!V4L2_FIELD_HAS_BOTH(field))height  = 2; recalc H delay and scale registersvalue = (width   norm_hdelay(core->tvnorm))  swidth;value &= 0x3fe;cx_write(MO_HDELAY_EVEN,  value);cx_write(MO_HDELAY_ODD,   value);dprintk(1, \"set_scale: hdelay  0x%04x (width %d)\\n\", value, swidth);value = (swidth   4096  width) - 4096;cx_write(MO_HSCALE_EVEN,  value);cx_write(MO_HSCALE_ODD,   value);dprintk(1, \"set_scale: hscale  0x%04x\\n\", value);cx_write(MO_HACTIVE_EVEN, width);cx_write(MO_HACTIVE_ODD,  width);dprintk(1, \"set_scale: hactive 0x%04x\\n\", width); recalc V scale Register (delay is constant)cx_write(MO_VDELAY_EVEN, norm_vdelay(core->tvnorm));cx_write(MO_VDELAY_ODD,  norm_vdelay(core->tvnorm));dprintk(1, \"set_scale: vdelay  0x%04x\\n\", norm_vdelay(core->tvnorm));value = (0x10000 - (sheight   512  height - 512)) & 0x1fff;cx_write(MO_VSCALE_EVEN,  value);cx_write(MO_VSCALE_ODD,   value);dprintk(1, \"set_scale: vscale  0x%04x\\n\", value);cx_write(MO_VACTIVE_EVEN, sheight);cx_write(MO_VACTIVE_ODD,  sheight);dprintk(1, \"set_scale: vactive 0x%04x\\n\", sheight); setup filtersvalue = 0;value |= (1 << 19);         CFILT (default)if (core->tvnorm & V4L2_STD_SECAM) {value |= (1 << 15);value |= (1 << 16);}if (INPUT(core->input).type == CX88_VMUX_SVIDEO)value |= (1 << 13) | (1 << 5);if (field == V4L2_FIELD_INTERLACED)value |= (1 << 3);  VINT (interlaced vertical scaling)if (width < 385)value |= (1 << 0);  3-tap interpolationif (width < 193)value |= (1 << 1);  5-tap interpolationif (nocomb)value |= (3 << 5);  disable comb filtercx_andor(MO_FILTER_EVEN,  0x7ffc7f, value);   preserve PEAKEN, PSEL ", "return 35468950;      // 4.43361875 MHz +/- 5 Hz}static inline unsigned int norm_htotal(v4l2_std_id norm)": "cx88_set_tvnorm already handles that.     The same FSC applies to PALBGDKIH, PAL60, NTSC4.43 and PALN ", "vfd->v4l2_dev = &core->v4l2_dev;vfd->dev_parent = &pci->dev;vfd->release = video_device_release_empty;vfd->lock = &core->lock;snprintf(vfd->name, sizeof(vfd->name), \"%s %s (%s)\", core->name, type, core->board.name);}EXPORT_SYMBOL(cx88_vdev_init": "cx88_vdev_init(struct cx88_core  core,    struct pci_dev  pci,    struct video_device  vfd,    const struct video_device  template_,    const char  type){ vfd =  template_;    The dev pointer of v4l2_device is NULL, instead we set the   video_device dev_parent pointer to the correct PCI bus device.   This driver is a rare example where there is one v4l2_device,   but the video nodes have different parent (PCI) devices. ", "cx88_sram_channel_setup(core, &cx88_sram_channels[SRAM_CH28],dev->ts_packet_size, buf->risc.dma);/* write TS length to chip ": "cx8802_start_dma(struct cx8802_dev     dev,     struct cx88_dmaqueue  q,     struct cx88_buffer    buf){struct cx88_core  core = dev->core;dprintk(1, \"w: %d, h: %d, f: %d\\n\",core->width, core->height, core->field);  setup fifo + format ", "buf->risc.cpu[1] = cpu_to_le32(buf->risc.dma + 8);buf->risc.jmp[0] = cpu_to_le32(RISC_JUMP | RISC_CNT_INC);buf->risc.jmp[1] = cpu_to_le32(buf->risc.dma + 8);if (list_empty(&cx88q->active)) ": "cx8802_buf_queue(struct cx8802_dev  dev, struct cx88_buffer  buf){struct cx88_buffer     prev;struct cx88_dmaqueue   cx88q = &dev->mpegq;dprintk(1, \"\\n\");  add jump to start ", "driver = kzalloc(sizeof(*drv), GFP_KERNEL);if (!driver) ": "cx8802_register_driver(struct cx8802_driver  drv){struct cx8802_dev  dev;struct cx8802_driver  driver;int err, i = 0;pr_info(\"registering cx8802 driver, type: %s access: %s\\n\",drv->type_id == CX88_MPEG_DVB ? \"dvb\" : \"blackbird\",drv->hw_access == CX8802_DRVCTL_SHARED ?  \"shared\" : \"exclusive\");err = cx8802_check_driver(drv);if (err) {pr_err(\"cx8802_driver is invalid\\n\");return err;}mutex_lock(&cx8802_mutex);list_for_each_entry(dev, &cx8802_devlist, devlist) {pr_info(\"subsystem: %04x:%04x, board: %s [card=%d]\\n\",dev->pci->subsystem_vendor,dev->pci->subsystem_device, dev->core->board.name,dev->core->boardnr);  Bring up a new struct for each driver instance ", "if (d->type_id != drv->type_id)continue;err = d->remove(d);if (err == 0) ": "cx8802_unregister_driver(struct cx8802_driver  drv){struct cx8802_dev  dev;struct cx8802_driver  d,  dtmp;int err = 0;pr_info(\"unregistering cx8802 driver, type: %s access: %s\\n\",drv->type_id == CX88_MPEG_DVB ? \"dvb\" : \"blackbird\",drv->hw_access == CX8802_DRVCTL_SHARED ?  \"shared\" : \"exclusive\");mutex_lock(&cx8802_mutex);list_for_each_entry(dev, &cx8802_devlist, devlist) {pr_info(\"subsystem: %04x:%04x, board: %s [card=%d]\\n\",dev->pci->subsystem_vendor,dev->pci->subsystem_device, dev->core->board.name,dev->core->boardnr);mutex_lock(&dev->core->lock);list_for_each_entry_safe(d, dtmp, &dev->drvlist, drvlist) {  only unregister the correct driver type ", "set_audio_standard_A2(core, EN_A2_FORCE_MONO1);/* * set nicam mode - otherwise * AUD_NICAM_STATUS2 contains wrong values ": "cx88_set_tvaudio(struct cx88_core  core){switch (core->tvaudio) {case WW_BTSC:set_audio_standard_BTSC(core, 0, EN_BTSC_AUTO_STEREO);break;case WW_BG:case WW_DK:case WW_M:case WW_I:case WW_L:  prepare all dsp registers ", "break;}/* If software stereo detection is not supported... ": "cx88_get_stereo(struct cx88_core  core, struct v4l2_tuner  t){static const char   const m[] = { \"stereo\", \"dual mono\",  \"mono\",   \"sap\" };static const char   const p[] = { \"no pilot\", \"pilot c1\",  \"pilot c2\", \"?\" };u32 reg, mode, pilot;reg = cx_read(AUD_STATUS);mode = reg & 0x03;pilot = (reg >> 2) & 0x03;if (core->astat != reg)dprintk(\"AUD_STATUS: 0x%x [%s%s] ctl=%s\\n\",reg, m[mode], p[pilot],aud_ctl_names[cx_read(AUD_CTL) & 63]);core->astat = reg;t->capability = V4L2_TUNER_CAP_STEREO | V4L2_TUNER_CAP_SAP |    V4L2_TUNER_CAP_LANG1 | V4L2_TUNER_CAP_LANG2;t->rxsubchans = UNSET;t->audmode = V4L2_TUNER_MODE_MONO;switch (mode) {case 0:t->audmode = V4L2_TUNER_MODE_STEREO;break;case 1:t->audmode = V4L2_TUNER_MODE_LANG2;break;case 2:t->audmode = V4L2_TUNER_MODE_MONO;break;case 3:t->audmode = V4L2_TUNER_MODE_SAP;break;}switch (core->tvaudio) {case WW_BTSC:case WW_BG:case WW_DK:case WW_M:case WW_EIAJ:if (!core->use_nicam) {t->rxsubchans = cx88_dsp_detect_stereo_sap(core);break;}break;case WW_NONE:case WW_I:case WW_L:case WW_I2SPT:case WW_FM:case WW_I2SADC:  nothing ", "set_audio_standard_A2(core, EN_A2_FORCE_MONO1);} else ": "cx88_set_stereo(struct cx88_core  core, u32 mode, int manual){u32 ctl = UNSET;u32 mask = UNSET;if (manual) {core->audiomode_manual = mode;} else {if (core->audiomode_manual != UNSET)return;}core->audiomode_current = mode;switch (core->tvaudio) {case WW_BTSC:switch (mode) {case V4L2_TUNER_MODE_MONO:set_audio_standard_BTSC(core, 0, EN_BTSC_FORCE_MONO);break;case V4L2_TUNER_MODE_LANG1:set_audio_standard_BTSC(core, 0, EN_BTSC_AUTO_STEREO);break;case V4L2_TUNER_MODE_LANG2:set_audio_standard_BTSC(core, 1, EN_BTSC_FORCE_SAP);break;case V4L2_TUNER_MODE_STEREO:case V4L2_TUNER_MODE_LANG1_LANG2:set_audio_standard_BTSC(core, 0, EN_BTSC_FORCE_STEREO);break;}break;case WW_BG:case WW_DK:case WW_M:case WW_I:case WW_L:if (core->use_nicam == 1) {switch (mode) {case V4L2_TUNER_MODE_MONO:case V4L2_TUNER_MODE_LANG1:set_audio_standard_NICAM(core, EN_NICAM_FORCE_MONO1);break;case V4L2_TUNER_MODE_LANG2:set_audio_standard_NICAM(core, EN_NICAM_FORCE_MONO2);break;case V4L2_TUNER_MODE_STEREO:case V4L2_TUNER_MODE_LANG1_LANG2:set_audio_standard_NICAM(core, EN_NICAM_FORCE_STEREO);break;}} else {if ((core->tvaudio == WW_I) ||    (core->tvaudio == WW_L)) {  fall back to fm  am mono ", "memset(&t, 0, sizeof(t));cx88_get_stereo(core, &t);if (core->audiomode_manual != UNSET)/* manually set, don't do anything. ": "cx88_audio_thread(void  data){struct cx88_core  core = data;struct v4l2_tuner t;u32 mode = 0;dprintk(\"cx88: tvaudio thread started\\n\");set_freezable();for (;;) {msleep_interruptible(1000);if (kthread_should_stop())break;try_to_freeze();switch (core->tvaudio) {case WW_BG:case WW_DK:case WW_M:case WW_I:case WW_L:if (core->use_nicam)goto hw_autodetect;  just monitor the audio status for now ... ", "cx_write(MO_DDSCFG_IO, 0x5); /* enable ": "cx88_ir_start(void  priv){struct cx88_core  core = priv;struct cx88_IR  ir;if (!core || !core->ir)return -EINVAL;ir = core->ir;if (ir->polling) {hrtimer_init(&ir->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);ir->timer.function = cx88_ir_work;hrtimer_start(&ir->timer,      ktime_set(0, ir->polling   1000000),      HRTIMER_MODE_REL);}if (ir->sampling) {core->pci_irqmask |= PCI_INT_IR_SMPINT;cx_write(MO_DDS_IO, 0x33F286   ir_samplerate);   samplerate ", "dprintk(1, \"video_mux: %d [vmux=%d,gpio=0x%x,0x%x,0x%x,0x%x]\\n\",input, INPUT(input).vmux,INPUT(input).gpio0, INPUT(input).gpio1,INPUT(input).gpio2, INPUT(input).gpio3);core->input = input;cx_andor(MO_INPUT_FORMAT, 0x03 << 14, INPUT(input).vmux << 14);cx_write(MO_GP3_IO, INPUT(input).gpio3);cx_write(MO_GP0_IO, INPUT(input).gpio0);cx_write(MO_GP1_IO, INPUT(input).gpio1);cx_write(MO_GP2_IO, INPUT(input).gpio2);switch (INPUT(input).type) ": "cx88_video_mux(struct cx88_core  core, unsigned int input){  struct cx88_core  core = dev->core; ", "usleep_range(10000, 20000);cx88_set_tvaudio(core);return 0;}EXPORT_SYMBOL(cx88_set_freq": "cx88_set_freq(struct cx88_core   core,  const struct v4l2_frequency  f){struct v4l2_frequency new_freq =  f;if (unlikely(core->board.tuner_type == UNSET))return -EINVAL;if (unlikely(f->tuner != 0))return -EINVAL;cx88_newstation(core);call_all(core, tuner, s_frequency, f);call_all(core, tuner, g_frequency, &new_freq);core->freq = new_freq.frequency;  When changing channels it is required to reset TVAUDIO ", "if (!(cx_read(MO_AUD_DMACNTRL) & 0x04))return ret;if (!(cx_read(AUD_CTL) & EN_FMRADIO_EN_RDS))return ret;/* Wait at least 500 ms after an audio standard change ": "cx88_dsp_detect_stereo_sap(struct cx88_core  core){s16  samples;u32 N = 0;s32 ret = UNSET;  If audio RDS fifo is disabled, we can't read the samples ", "if (itv->output_mode == OUT_PASSTHROUGH) ": "ivtv_start_v4l2_encode_stream(struct ivtv_stream  s){u32 data[CX2341X_MBOX_MAX_DATA];struct ivtv  itv = s->itv;int captype = 0, subtype = 0;int enable_passthrough = 0;if (s->vdev.v4l2_dev == NULL)return -EINVAL;IVTV_DEBUG_INFO(\"Start encoder stream %s\\n\", s->name);switch (s->type) {case IVTV_ENC_STREAM_TYPE_MPG:captype = 0;subtype = 3;  Stop Passthrough ", "IVTV_DEBUG_INFO(\"Stop Capture\\n\");if (s->type == IVTV_DEC_STREAM_TYPE_VOUT)return 0;if (atomic_read(&itv->capturing) == 0)return 0;switch (s->type) ": "ivtv_stop_v4l2_encode_stream(s, 0);}}}int ivtv_stop_v4l2_encode_stream(struct ivtv_stream  s, int gop_end){struct ivtv  itv = s->itv;DECLARE_WAITQUEUE(wait, current);int cap_type;int stopmode;if (s->vdev.v4l2_dev == NULL)return -EINVAL;  This function assumes that you are allowed to stop the capture   and that we are actually capturing ", "if (s->fh == &id->fh) ": "ivtv_claim_stream(struct ivtv_open_id  id, int type){struct ivtv  itv = id->itv;struct ivtv_stream  s = &itv->streams[type];struct ivtv_stream  s_vbi;int vbi_type;if (test_and_set_bit(IVTV_F_S_CLAIMED, &s->s_flags)) {  someone already claimed this stream ", "return;}if (!test_and_clear_bit(IVTV_F_S_CLAIMED, &s->s_flags)) ": "ivtv_release_stream(struct ivtv_stream  s){struct ivtv  itv = s->itv;struct ivtv_stream  s_vbi;s->fh = NULL;if ((s->type == IVTV_DEC_STREAM_TYPE_VBI || s->type == IVTV_ENC_STREAM_TYPE_VBI) &&test_bit(IVTV_F_S_INTERNAL_USE, &s->s_flags)) {  this stream is still in use internally ", "int ivtv_msleep_timeout(unsigned int msecs, int intr)": "ivtv_clear_irq_mask(struct ivtv  itv, u32 mask){itv->irqmask &= ~mask;write_reg_sync(itv->irqmask, IVTV_REG_IRQMASK);}void ivtv_set_irq_mask(struct ivtv  itv, u32 mask){itv->irqmask |= mask;write_reg_sync(itv->irqmask, IVTV_REG_IRQMASK);}int ivtv_set_output_mode(struct ivtv  itv, int mode){    int old_mode;    spin_lock(&itv->lock);    old_mode = itv->output_mode;    if (old_mode == 0)itv->output_mode = old_mode = mode;    spin_unlock(&itv->lock);    return old_mode;}struct ivtv_stream  ivtv_get_output_stream(struct ivtv  itv){switch (itv->output_mode) {case OUT_MPG:return &itv->streams[IVTV_DEC_STREAM_TYPE_MPG];case OUT_YUV:return &itv->streams[IVTV_DEC_STREAM_TYPE_YUV];default:return NULL;}}int ivtv_waitq(wait_queue_head_t  waitq){DEFINE_WAIT(wait);prepare_to_wait(waitq, &wait, TASK_INTERRUPTIBLE);schedule();finish_wait(waitq, &wait);return signal_pending(current) ? -EINTR : 0;}  Generic utility functions ", "retval = ivtv_setup_pci(itv, pdev, pci_id);if (retval == -EIO)goto free_worker;if (retval == -ENXIO)goto free_mem;/* map io memory ": "ivtv_api_func;IVTV_DEBUG_INFO(\"base addr: 0x%llx\\n\", (u64)itv->base_addr);  PCI Device Setup ", "request_module(\"ivtv-alsa\");/* Initialize ivtv-alsa for this instance of the cx18 device ": "ivtv_fw_debug;#endifstatic int tunertype = -1;static int newi2c = -1;module_param_array(tuner, int, &tuner_c, 0644);module_param_array(radio, int, &radio_c, 0644);module_param_array(cardtype, int, &cardtype_c, 0644);module_param_string(pal, pal, sizeof(pal), 0644);module_param_string(secam, secam, sizeof(secam), 0644);module_param_string(ntsc, ntsc, sizeof(ntsc), 0644);module_param_named(debug,ivtv_debug, int, 0644);#ifdef CONFIG_VIDEO_ADV_DEBUGmodule_param_named(fw_debug, ivtv_fw_debug, int, 0644);#endifmodule_param(ivtv_pci_latency, int, 0644);module_param(ivtv_yuv_mode, int, 0644);module_param(ivtv_yuv_threshold, int, 0644);module_param(ivtv_first_minor, int, 0644);module_param(enc_mpg_buffers, int, 0644);module_param(enc_yuv_buffers, int, 0644);module_param(enc_vbi_buffers, int, 0644);module_param(enc_pcm_buffers, int, 0644);module_param(dec_mpg_buffers, int, 0644);module_param(dec_yuv_buffers, int, 0644);module_param(dec_vbi_buffers, int, 0644);module_param(tunertype, int, 0644);module_param(newi2c, int, 0644);module_param_array(i2c_clock_period, int, &i2c_clock_period_c, 0644);MODULE_PARM_DESC(tuner, \"Tuner type selection,\\n\"\"\\t\\t\\tsee tuner.h for values\");MODULE_PARM_DESC(radio, \"Enable or disable the radio. Use only if autodetection\\n\" \"\\t\\t\\tfails. 0 = disable, 1 = enable\");MODULE_PARM_DESC(cardtype, \"Only use this option if your card is not detected properly.\\n\" \"\\t\\tSpecify card type:\\n\" \"\\t\\t\\t 1 = WinTV PVR 250\\n\" \"\\t\\t\\t 2 = WinTV PVR 350\\n\" \"\\t\\t\\t 3 = WinTV PVR-150 or PVR-500\\n\" \"\\t\\t\\t 4 = AVerMedia M179\\n\" \"\\t\\t\\t 5 = YUAN MPG600Kuroutoshikou iTVC16-STVLP\\n\" \"\\t\\t\\t 6 = YUAN MPG160Kuroutoshikou iTVC15-STVLP\\n\" \"\\t\\t\\t 7 = YUAN PG600DIAMONDMM PVR-550 (CX Falcon 2)\\n\" \"\\t\\t\\t 8 = Adaptec AVC-2410\\n\" \"\\t\\t\\t 9 = Adaptec AVC-2010\\n\" \"\\t\\t\\t10 = NAGASE TRANSGEAR 5000TV\\n\" \"\\t\\t\\t11 = AOpen VA2000MAX-STN6\\n\" \"\\t\\t\\t12 = YUAN MPG600GRKuroutoshikou CX23416GYC-STVLP\\n\" \"\\t\\t\\t13 = IO Data GV-MVPRX\\n\" \"\\t\\t\\t14 = IO Data GV-MVPRX2E\\n\" \"\\t\\t\\t15 = GOTVIEW PCI DVD\\n\" \"\\t\\t\\t16 = GOTVIEW PCI DVD2 Deluxe\\n\" \"\\t\\t\\t17 = Yuan MPC622\\n\" \"\\t\\t\\t18 = Digital Cowboy DCT-MTVP1\\n\" \"\\t\\t\\t19 = Yuan PG600V2GotView PCI DVD Lite\\n\" \"\\t\\t\\t20 = Club3D ZAP-TV1x01\\n\" \"\\t\\t\\t21 = AverTV MCE 116 Plus\\n\" \"\\t\\t\\t22 = ASUS Falcon2\\n\" \"\\t\\t\\t23 = AverMedia PVR-150 Plus\\n\" \"\\t\\t\\t24 = AverMedia EZMaker PCI Deluxe\\n\" \"\\t\\t\\t25 = AverMedia M104 (not yet working)\\n\" \"\\t\\t\\t26 = Buffalo PC-MV5LPCI\\n\" \"\\t\\t\\t27 = AVerMedia UltraTV 1500 MCE\\n\" \"\\t\\t\\t28 = Sony VAIO Giga Pocket (ENX Kikyou)\\n\" \"\\t\\t\\t 0 = Autodetect (default)\\n\" \"\\t\\t\\t-1 = Ignore this card\\n\\t\\t\");MODULE_PARM_DESC(pal, \"Set PAL standard: BGH, DK, I, M, N, Nc, 60\");MODULE_PARM_DESC(secam, \"Set SECAM standard: BGH, DK, L, LC\");MODULE_PARM_DESC(ntsc, \"Set NTSC standard: M, J (Japan), K (South Korea)\");MODULE_PARM_DESC(tunertype,\"Specify tuner type:\\n\"\"\\t\\t\\t 0 = tuner for PAL-BGHDKI, SECAM-BGHDKLLc\\n\"\"\\t\\t\\t 1 = tuner for NTSC-MJK, PAL-MNNc\\n\"\"\\t\\t\\t-1 = Autodetect (default)\\n\");MODULE_PARM_DESC(debug, \"Debug level (bitmask). Default: 0\\n\" \"\\t\\t\\t   10x0001: warning\\n\" \"\\t\\t\\t   20x0002: info\\n\" \"\\t\\t\\t   40x0004: mailbox\\n\" \"\\t\\t\\t   80x0008: ioctl\\n\" \"\\t\\t\\t  160x0010: file\\n\" \"\\t\\t\\t  320x0020: dma\\n\" \"\\t\\t\\t  640x0040: irq\\n\" \"\\t\\t\\t 1280x0080: decoder\\n\" \"\\t\\t\\t 2560x0100: yuv\\n\" \"\\t\\t\\t 5120x0200: i2c\\n\" \"\\t\\t\\t10240x0400: high volume\\n\");#ifdef CONFIG_VIDEO_ADV_DEBUGMODULE_PARM_DESC(fw_debug, \"Enable code for debugging firmware problems.  Default: 0\\n\");#endifMODULE_PARM_DESC(ivtv_pci_latency, \"Change the PCI latency to 64 if lower: 0 = No, 1 = Yes,\\n\" \"\\t\\t\\tDefault: Yes\");MODULE_PARM_DESC(ivtv_yuv_mode, \"Specify the yuv playback mode:\\n\" \"\\t\\t\\t0 = interlaced\\n\\t\\t\\t1 = progressive\\n\\t\\t\\t2 = auto\\n\" \"\\t\\t\\tDefault: 0 (interlaced)\");MODULE_PARM_DESC(ivtv_yuv_threshold, \"If ivtv_yuv_mode is 2 (auto) then playback content as\\n\\t\\tprogressive if src height <= ivtv_yuvthreshold\\n\" \"\\t\\t\\tDefault: 480\");MODULE_PARM_DESC(enc_mpg_buffers, \"Encoder MPG Buffers (in MB)\\n\" \"\\t\\t\\tDefault: \" __stringify(IVTV_DEFAULT_ENC_MPG_BUFFERS));MODULE_PARM_DESC(enc_yuv_buffers, \"Encoder YUV Buffers (in MB)\\n\" \"\\t\\t\\tDefault: \" __stringify(IVTV_DEFAULT_ENC_YUV_BUFFERS));MODULE_PARM_DESC(enc_vbi_buffers, \"Encoder VBI Buffers (in MB)\\n\" \"\\t\\t\\tDefault: \" __stringify(IVTV_DEFAULT_ENC_VBI_BUFFERS));MODULE_PARM_DESC(enc_pcm_buffers, \"Encoder PCM buffers (in kB)\\n\" \"\\t\\t\\tDefault: \" __stringify(IVTV_DEFAULT_ENC_PCM_BUFFERS));MODULE_PARM_DESC(dec_mpg_buffers, \"Decoder MPG buffers (in MB)\\n\" \"\\t\\t\\tDefault: \" __stringify(IVTV_DEFAULT_DEC_MPG_BUFFERS));MODULE_PARM_DESC(dec_yuv_buffers, \"Decoder YUV buffers (in MB)\\n\" \"\\t\\t\\tDefault: \" __stringify(IVTV_DEFAULT_DEC_YUV_BUFFERS));MODULE_PARM_DESC(dec_vbi_buffers, \"Decoder VBI buffers (in kB)\\n\" \"\\t\\t\\tDefault: \" __stringify(IVTV_DEFAULT_DEC_VBI_BUFFERS));MODULE_PARM_DESC(newi2c, \"Use new I2C implementation\\n\" \"\\t\\t\\t-1 is autodetect, 0 is off, 1 is on\\n\" \"\\t\\t\\tDefault is autodetect\");MODULE_PARM_DESC(i2c_clock_period, \"Period of SCL for the I2C bus controlled by the CX234156\\n\" \"\\t\\t\\tMin: 10 usec (100 kHz), Max: 4500 usec (222 Hz)\\n\" \"\\t\\t\\tDefault: \" __stringify(IVTV_DEFAULT_I2C_CLOCK_PERIOD));MODULE_PARM_DESC(ivtv_first_minor, \"Set device node number assigned to first card\");MODULE_AUTHOR(\"Kevin Thayer, Chris Kennedy, Hans Verkuil\");MODULE_DESCRIPTION(\"CX23415CX23416 driver\");MODULE_LICENSE(\"GPL\");MODULE_VERSION(IVTV_VERSION);#if defined(CONFIG_MODULES) && defined(MODULE)static void request_module_async(struct work_struct  work){struct ivtv  dev = container_of(work, struct ivtv, request_module_wk);  Make sure ivtv-alsa module is loaded ", "struct ivtv_open_id fh;int fw_retry_count = 3;int video_input;fh.itv = itv;fh.type = IVTV_ENC_STREAM_TYPE_MPG;if (test_bit(IVTV_F_I_FAILED, &itv->i_flags))return -ENXIO;if (test_and_set_bit(IVTV_F_I_INITED, &itv->i_flags))return 0;while (--fw_retry_count > 0) ": "ivtv_init_on_first_open(struct ivtv  itv){struct v4l2_frequency vf;  Needed to call ioctls later ", "if (pin_number >= 47)return;if (pin_number > 31) ": "cx25821_set_gpiopin_direction(struct cx25821_dev  dev,   int pin_number, int pin_logic_value){int bit = pin_number;u32 gpio_oe_reg = GPIO_LO_OE;u32 gpio_register = 0;u32 value = 0;  Check for valid pinNumber ", "cdt = ch->cdt;lines = ch->fifo_size / bpl;if (lines > 3)lines = 3;/* for AUDIO ": "cx25821_sram_channel_setup_audio(struct cx25821_dev  dev,     const struct sram_channel  ch,     unsigned int bpl, u32 risc){unsigned int i, lines;u32 cdt;if (ch->cmds_start == 0) {cx_write(ch->ptr1_reg, 0);cx_write(ch->ptr2_reg, 0);cx_write(ch->cnt2_reg, 0);cx_write(ch->cnt1_reg, 0);return 0;}bpl = (bpl + 7) & ~7;  alignment ", "pr_warn(\"ctrl + 0x%2x (0x%08x): iq %x: \",i * 4, ch->ctrl_start + 4 * i, i);n = cx25821_risc_decode(risc);for (j = 1; j < n; j++) ": "cx25821_sram_channel_dump_audio(struct cx25821_dev  dev,     const struct sram_channel  ch){static const char   const name[] = {\"init risc lo\",\"init risc hi\",\"cdt base\",\"cdt size\",\"iq base\",\"iq size\",\"risc pc lo\",\"risc pc hi\",\"iq wr ptr\",\"iq rd ptr\",\"cdt current\",\"pci target lo\",\"pci target hi\",\"line  byte\",};u32 risc, value, tmp;unsigned int i, j, n;pr_info(\"\\n%s: %s - dma Audio channel status dump\\n\",dev->name, ch->name);for (i = 0; i < ARRAY_SIZE(name); i++)pr_info(\"%s: cmds + 0x%2x:   %-15s: 0x%08x\\n\",dev->name, i   4, name[i],cx_read(ch->cmds_start + 4   i));j = i   4;for (i = 0; i < 4;) {risc = cx_read(ch->cmds_start + 4   (i + 14));pr_warn(\"cmds + 0x%2x:   risc%d: \", j + i   4, i);i += cx25821_risc_decode(risc);}for (i = 0; i < (64 >> 2); i += n) {risc = cx_read(ch->ctrl_start + 4   i);  No consideration for bits 63-32 ", "continue;/* * TODO: enable when video output is properly * supported.if (i == SRAM_CH09 || i == SRAM_CH10)cx25821_free_mem_upstream(&dev->channels[i]); ": "cx25821_dev_unregister(struct cx25821_dev  dev){int i;if (!dev->base_io_addr)return;release_mem_region(dev->base_io_addr, pci_resource_len(dev->pci, 0));for (i = 0; i < MAX_VID_CAP_CHANNEL_NUM - 1; i++) {if (i == SRAM_CH08)   audio channel ", "/* Jump and write need an extra dword ": "cx25821_risc_databuffer_audio(struct pci_dev  pci,  struct cx25821_riscmem  risc,  struct scatterlist  sglist,  unsigned int bpl,  unsigned int lines, unsigned int lpi){u32 instructions;__le32  rp;int rc;  estimate risc mem: worst case is one write per page border +   one write per scan line + syncs + jump (all 2 dwords).  Here   there is no padding and no sync.  First DMA region may be smaller   than PAGE_SIZE ", " 0,   /* n_slots ": "altera_ci_init(struct altera_ci_config  config, int ci_nr){struct altera_ci_state  state;struct fpga_inode  temp_int = find_inode(config->dev);struct fpga_internal  inter = NULL;int ret = 0;u8 store = 0;state = kzalloc(sizeof(struct altera_ci_state), GFP_KERNEL);ci_dbg_print(\"%s\\n\", __func__);if (!state) {ret = -ENOMEM;goto err;}if (temp_int != NULL) {inter = temp_int->internal;(inter->cis_used)++;inter->fpga_rw = config->fpga_rw;ci_dbg_print(\"%s: Find Internal Structure!\\n\", __func__);} else {inter = kzalloc(sizeof(struct fpga_internal), GFP_KERNEL);if (!inter) {ret = -ENOMEM;goto err;}temp_int = append_internal(inter);if (!temp_int) {ret = -ENOMEM;goto err;}inter->cis_used = 1;inter->dev = config->dev;inter->fpga_rw = config->fpga_rw;mutex_init(&inter->fpga_mutex);inter->strt_wrk = 1;ci_dbg_print(\"%s: Create New Internal Structure!\\n\", __func__);}ci_dbg_print(\"%s: setting state = %p for ci = %d\\n\", __func__,state, ci_nr - 1);state->internal = inter;state->nr = ci_nr - 1;state->ca.owner = THIS_MODULE;state->ca.read_attribute_mem = altera_ci_read_attribute_mem;state->ca.write_attribute_mem = altera_ci_write_attribute_mem;state->ca.read_cam_control = altera_ci_read_cam_ctl;state->ca.write_cam_control = altera_ci_write_cam_ctl;state->ca.slot_reset = altera_ci_slot_reset;state->ca.slot_shutdown = altera_ci_slot_shutdown;state->ca.slot_ts_enable = altera_ci_slot_ts_ctl;state->ca.poll_slot_status = altera_poll_ci_slot_status;state->ca.data = state;ret = dvb_ca_en50221_init(config->adapter,   &state->ca,     flags ", "if (dst_probe(state) < 0) ": "dst_attach(struct dst_state  state, struct dvb_adapter  dvb_adapter){  check if the ASIC is there ", "#include <linux/module.h>#include <linux/kernel.h>#include <linux/pci.h>#include <linux/pgtable.h>#include <asm/io.h>#include <linux/ioport.h>#include <asm/page.h>#include <linux/types.h>#include <linux/interrupt.h>#include <linux/kmod.h>#include <linux/vmalloc.h>#include <linux/init.h>#include <media/dmxdev.h>#include <media/dvbdev.h>#include \"bt878.h\"#include \"dst_priv.h\"/*************************************": "bt878.c: part of the driver for the Pinnacle PCTV Sat DVB PCI card     Copyright (C) 2002 Peter Hettkamp <peter.hettkamp@htp-tel.de>     large parts based on the bttv driver   Copyright (C) 1996,97,98 Ralph  Metzler (rjkm@metzlerbros.de)                          & Marcus Metzler (mocm@metzlerbros.de)   (c) 1999,2000 Gerd Knorr <kraxel@goldbach.in-berlin.de> ", "bt878_risc_program(bt, op_sync_orin);controlreg &= ~0x1f;controlreg |= 0x1b;btwrite(bt->risc_dma, BT878_ARISC_START);/* original int mask had : *    6    2    8    4    0 * 1111 1111 1000 0000 0000 * SCERR|OCERR|PABORT|RIPERR|FDSR|FTRGT|FBUS|RISCI * Hacked for DST to: * SCERR | OCERR | FDSR | FTRGT | FBUS | RISCI ": "bt878_start(struct bt878  bt, u32 controlreg, u32 op_sync_orin,u32 irq_err_ignore){u32 int_mask;dprintk(\"bt878 debug: bt878_start (ctl=%8.8x)\\n\", controlreg);  complete the writing of the risc dma program now we have   the card specifics ", "switch (cmd) ": "sof_ipc3_do_rx_work(struct snd_sof_dev  sdev, struct sof_ipc_cmd_hdr  hdr, void  msg_buf){ipc3_rx_callback rx_callback = NULL;u32 cmd;int err;ipc3_log_header(sdev->dev, \"ipc rx\", hdr->cmd);if (hdr->size < sizeof(hdr) || hdr->size > SOF_IPC_MSG_MAX_SIZE) {dev_err(sdev->dev, \"The received message size is invalid: %u\\n\",hdr->size);return;}cmd = hdr->cmd & SOF_GLB_TYPE_MASK;  check message type ", "state = kzalloc(sizeof(struct ddbridge_dummy_fe_state), GFP_KERNEL);if (!state)return NULL;/* create dvb_frontend ": "ddbridge_dummy_fe_qam_attach(void){struct ddbridge_dummy_fe_state  state = NULL;  allocate memory for the internal state ", "case 1:core_dbg(\"setting GPIO%d to static %d\\n\", bit_no, value);/* turn sync mode off if necessary ": "saa7134_set_gpio(struct saa7134_dev  dev, int bit_no, int value){u32 index, bitval;index = 1 << bit_no;switch (value) {case 0:   static value ", "media_device_for_each_entity(entity, dev->media_dev) ": "saa7134_boards[i].name);for (p = 0; saa7134_pci_tbl[p].driver_data; p++) {if (saa7134_pci_tbl[p].driver_data != i)continue;pr_cont(\" %04x:%04x\",       saa7134_pci_tbl[p].subvendor,       saa7134_pci_tbl[p].subdevice);}pr_cont(\"\\n\");}}static void saa7134_unregister_media_device(struct saa7134_dev  dev){#ifdef CONFIG_MEDIA_CONTROLLERif (!dev->media_dev)return;media_device_unregister(dev->media_dev);media_device_cleanup(dev->media_dev);kfree(dev->media_dev);dev->media_dev = NULL;#endif}static void saa7134_media_release(struct saa7134_dev  dev){#ifdef CONFIG_MEDIA_CONTROLLERint i;for (i = 0; i < SAA7134_INPUT_MAX + 1; i++)media_device_unregister_entity(&dev->input_ent[i]);#endif}#if defined(CONFIG_MEDIA_CONTROLLER)static void saa7134_create_entities(struct saa7134_dev  dev){int ret, i;struct media_entity  entity;struct media_entity  decoder = NULL;  Check if it is using an external analog TV demod ", "saa_andorb(SAA7134_GPIO_GPMODE3,SAA7134_GPIO_GPRESCAN,0);saa_andorb(SAA7134_GPIO_GPMODE3,SAA7134_GPIO_GPRESCAN,SAA7134_GPIO_GPRESCAN);mode   = saa_readl(SAA7134_GPIO_GPMODE0   >> 2) & 0xfffffff;status = saa_readl(SAA7134_GPIO_GPSTATUS0 >> 2) & 0xfffffff;core_dbg(\"%s: gpio: mode=0x%07lx in=0x%07lx out=0x%07lx [%s]\\n\",       dev->name, mode, (~mode) & status, mode & status, msg);}void saa7134_set_gpio(struct saa7134_dev *dev, int bit_no, int value)": "saa7134_dmasound_exit)(struct saa7134_dev  dev);#define core_dbg(fmt, arg...) do { \\if (core_debug) \\printk(KERN_DEBUG pr_fmt(\"core: \" fmt), ## arg); \\} while (0)#define irq_dbg(level, fmt, arg...)  do {\\if (irq_debug > level) \\printk(KERN_DEBUG pr_fmt(\"irq: \" fmt), ## arg); \\} while (0)void saa7134_track_gpio(struct saa7134_dev  dev, const char  msg){unsigned long mode,status;if (!gpio_tracking)return;  rising SAA7134_GPIO_GPRESCAN reads the status ", "int saa7134_buffer_queue(struct saa7134_dev *dev, struct saa7134_dmaqueue *q, struct saa7134_buf *buf)": "saa7134_pgtable_alloc(struct pci_dev  pci, struct saa7134_pgtable  pt){__le32        cpu;dma_addr_t   dma_addr = 0;cpu = dma_alloc_coherent(&pci->dev, SAA7134_PGTABLE_SIZE, &dma_addr, GFP_KERNEL);if (NULL == cpu)return -ENOMEM;pt->size = SAA7134_PGTABLE_SIZE;pt->cpu  = cpu;pt->dma  = dma_addr;return 0;}int saa7134_pgtable_build(struct pci_dev  pci, struct saa7134_pgtable  pt,  struct scatterlist  list, unsigned int length,  unsigned int startpage){__le32         ptr;unsigned int  i, p;BUG_ON(NULL == pt || NULL == pt->cpu);ptr = pt->cpu + startpage;for (i = 0; i < length; i++, list = sg_next(list)) {for (p = 0; p   4096 < sg_dma_len(list); p++, ptr++) ptr = cpu_to_le32(sg_dma_address(list) +list->offset + p   4096);}return 0;}void saa7134_pgtable_free(struct pci_dev  pci, struct saa7134_pgtable  pt){if (NULL == pt->cpu)return;dma_free_coherent(&pci->dev, pt->size, pt->cpu, pt->dma);pt->cpu = NULL;}  ------------------------------------------------------------------ ", "saa_writeb(SAA7134_REGION_ENABLE, 0x00);saa_writeb(SAA7134_REGION_ENABLE, 0x80);saa_writeb(SAA7134_REGION_ENABLE, 0x00);/* flag current buffer as failed,   try to start over with the next one. ": "saa7134_set_dmabits(dev);del_timer(&q->timeout);}}void saa7134_buffer_timeout(struct timer_list  t){struct saa7134_dmaqueue  q = from_timer(q, t, timeout);struct saa7134_dev  dev = q->dev;unsigned long flags;spin_lock_irqsave(&dev->slock, flags);  try to reset the hardware (SWRST) ", "if (0 != card(dev).gpiomask) ": "saa_dsp_writel(struct saa7134_dev  dev, int reg, u32 value){int err;audio_dbg(2, \"dsp write reg 0x%x = 0x%06x\\n\",  (reg << 2) & 0xffffffff, value);err = saa_dsp_wait_bit(dev,SAA7135_DSP_RWSTATE_WRR);if (err < 0)return err;saa_writel(reg,value);err = saa_dsp_wait_bit(dev,SAA7135_DSP_RWSTATE_WRR);if (err < 0)return err;return 0;}static int getstereo_7133(struct saa7134_dev  dev){int retval = V4L2_TUNER_SUB_MONO;u32 value;value = saa_readl(0x528 >> 2);if (value & 0x20)retval = V4L2_TUNER_SUB_MONO | V4L2_TUNER_SUB_STEREO;if (value & 0x40)retval = V4L2_TUNER_SUB_LANG1 | V4L2_TUNER_SUB_LANG2;return retval;}static int mute_input_7133(struct saa7134_dev  dev){u32 reg = 0;u32 xbarin, xbarout;int mask;struct saa7134_input  in;xbarin = 0x03;switch (dev->input->amux) {case TV:reg = 0x02;xbarin = 0;break;case LINE1:reg = 0x00;break;case LINE2:case LINE2_LEFT:reg = 0x09;break;}saa_dsp_writel(dev, 0x464 >> 2, xbarin);if (dev->ctl_mute) {reg = 0x07;xbarout = 0xbbbbbb;} elsexbarout = 0xbbbb10;saa_dsp_writel(dev, 0x46c >> 2, xbarout);saa_writel(0x594 >> 2, reg);  switch gpio-connected external audio mux ", "for (audio = UNSET, i = 0; i < TVAUDIO; i++) ": "saa7134_tvaudio_setmute(dev);  find the exact tv audio norm ", "u32 band = 0;switch (tea->band) ": "snd_tea575x_set_freq(struct snd_tea575x  tea){u32 freq = tea->freq  16;  to kHz ", "if (i != tea->band) ": "snd_tea575x_s_hw_freq_seek(struct file  file, struct snd_tea575x  tea,const struct v4l2_hw_freq_seek  a){unsigned long timeout;int i, spacing;if (tea->cannot_read_data)return -ENOTTY;if (a->tuner || a->wrap_around)return -EINVAL;if (file->f_flags & O_NONBLOCK)return -EWOULDBLOCK;if (a->rangelow || a->rangehigh) {for (i = 0; i < ARRAY_SIZE(bands); i++) {if ((i == BAND_FM && tea->tea5759) ||    (i == BAND_FM_JAPAN && !tea->tea5759) ||    (i == BAND_AM && !tea->has_am))continue;if (bands[i].rangelow  == a->rangelow &&    bands[i].rangehigh == a->rangehigh)break;}if (i == ARRAY_SIZE(bands))return -EINVAL;   No matching band found ", "if (!tea->cannot_read_data) ": "snd_tea575x_hw_init(struct snd_tea575x  tea){tea->mute = true;  Not all devices can or know how to read the data back.   Such devices can set cannot_read_data to true. ", "if (tea->cannot_read_data)v4l2_disable_ioctl(&tea->vd, VIDIOC_S_HW_FREQ_SEEK);if (!tea->cannot_mute) ": "snd_tea575x_init(struct snd_tea575x  tea, struct module  owner){int retval = snd_tea575x_hw_init(tea);if (retval)return retval;tea->vd = tea575x_radio;video_set_drvdata(&tea->vd, tea);mutex_init(&tea->mutex);strscpy(tea->vd.name, tea->v4l2_dev->name, sizeof(tea->vd.name));tea->vd.lock = &tea->mutex;tea->vd.v4l2_dev = tea->v4l2_dev;tea->vd.device_caps = V4L2_CAP_TUNER | V4L2_CAP_RADIO;if (!tea->cannot_read_data)tea->vd.device_caps |= V4L2_CAP_HW_FREQ_SEEK;tea->fops = tea575x_fops;tea->fops.owner = owner;tea->vd.fops = &tea->fops;  disable hw_freq_seek if we can't use it ", "int ir_raw_gen_manchester(struct ir_raw_event **ev, unsigned int max,  const struct ir_raw_timings_manchester *timings,  unsigned int n, u64 data)": "ir_raw_gen_manchester() - Encode data with Manchester (bi-phase) modulation.   @ev:Pointer to pointer to next free event.  @ev is incremented for  each raw event filled.   @max:Maximum number of raw events to fill.   @timings:Manchester modulation timings.   @n:Number of bits of data.   @data:Data bits to encode.     Encodes the @n least significant bits of @data using Manchester (bi-phase)   modulation with the timing characteristics described by @timings, writing up   to @max raw IR events using the  @ev pointer.     Returns:0 on success.  -ENOBUFS if there isn't enough space in the array to fit the  full encoded data. In this case all @max events will have been  written. ", "int ir_raw_gen_pd(struct ir_raw_event **ev, unsigned int max,  const struct ir_raw_timings_pd *timings,  unsigned int n, u64 data)": "ir_raw_gen_pd() - Encode data to raw events with pulse-distance modulation.   @ev:Pointer to pointer to next free event.  @ev is incremented for  each raw event filled.   @max:Maximum number of raw events to fill.   @timings:Pulse distance modulation timings.   @n:Number of bits of data.   @data:Data bits to encode.     Encodes the @n least significant bits of @data using pulse-distance   modulation with the timing characteristics described by @timings, writing up   to @max raw IR events using the  @ev pointer.     Returns:0 on success.  -ENOBUFS if there isn't enough space in the array to fit the  full encoded data. In this case all @max events will have been  written. ", "int ir_raw_gen_pl(struct ir_raw_event **ev, unsigned int max,  const struct ir_raw_timings_pl *timings,  unsigned int n, u64 data)": "ir_raw_gen_pl() - Encode data to raw events with pulse-length modulation.   @ev:Pointer to pointer to next free event.  @ev is incremented for  each raw event filled.   @max:Maximum number of raw events to fill.   @timings:Pulse distance modulation timings.   @n:Number of bits of data.   @data:Data bits to encode.     Encodes the @n least significant bits of @data using space-distance   modulation with the timing characteristics described by @timings, writing up   to @max raw IR events using the  @ev pointer.     Returns:0 on success.  -ENOBUFS if there isn't enough space in the array to fit the  full encoded data. In this case all @max events will have been  written. ", "int ir_raw_encode_scancode(enum rc_proto protocol, u32 scancode,   struct ir_raw_event *events, unsigned int max)": "ir_raw_encode_scancode() - Encode a scancode as raw events     @protocol:protocol   @scancode:scancode filter describing a single scancode   @events:array of raw events to write into   @max:max number of raw events     Attempts to encode the scancode as raw events.     Returns:The number of events written.  -ENOBUFS if there isn't enough space in the array to fit the  encoding. In this case all @max events will have been written.  -EINVAL if the scancode is ambiguous or invalid, or if no  compatible encoder was found. ", "int ir_raw_encode_carrier(enum rc_proto protocol)": "ir_raw_encode_carrier() - Get carrier used for protocol     @protocol:protocol     Attempts to find the carrier for the specified protocol     Returns:The carrier in Hz  -EINVAL if the protocol is invalid, or if no  compatible encoder was found. ", "hx->data[0] = 1;ret = usb_cypress_writemem(udev, cypress[type].cs_reg, hx->data, 1);if (ret != 1) ": "cypress_load_firmware(struct usb_device  udev,const struct firmware  fw, int type){struct hexline  hx;int ret, pos = 0;hx = kmalloc(sizeof( hx), GFP_KERNEL);if (!hx)return -ENOMEM;  stop the CPU ", "int i, j, len, done, beenhere, tag, start;int tuner1 = 0, t_format1 = 0, audioic = -1;const char *t_name1 = NULL;const char *t_fmt_name1[8] = ": "tveeprom_hauppauge_analog(struct tveeprom  tvee,       unsigned char  eeprom_data){  ----------------------------------------------   The hauppauge eeprom format is tagged     if packet[0] == 0x84, then packet[0..1] == length   else length = packet[0] & 3f;   if packet[0] & f8 == f8, then EOD and packet[1] == checksum     In our (ivtv) case we're interested in the following:   tuner type:   tag [00].05 or [0a].01 (index into hauppauge_tuner)   tuner fmts:   tag [00].04 or [0a].00 (bitmask index into   hauppauge_tuner_fmt)   radio:        tag [00].{last} or [0e].00  (bitmask.  bit2=FM)   audio proc:   tag [02].01 or [05].00 (mask with 0x7f)   decoder proc: tag [09].01)   Fun info:   model:      tag [00].07-08 or [06].00-01   revision:   tag [00].09-0b or [06].04-06   serial#:    tag [01].05-07 or [04].04-06   # of inputsoutputs ???", "case V4L2_CID_MPEG_CX2341X_VIDEO_SPATIAL_FILTER_MODE:case V4L2_CID_MPEG_CX2341X_VIDEO_SPATIAL_FILTER:case V4L2_CID_MPEG_CX2341X_VIDEO_LUMA_SPATIAL_FILTER_TYPE:case V4L2_CID_MPEG_CX2341X_VIDEO_CHROMA_SPATIAL_FILTER_TYPE:case V4L2_CID_MPEG_CX2341X_VIDEO_TEMPORAL_FILTER_MODE:case V4L2_CID_MPEG_CX2341X_VIDEO_TEMPORAL_FILTER:case V4L2_CID_MPEG_CX2341X_VIDEO_MEDIAN_FILTER_TYPE:case V4L2_CID_MPEG_CX2341X_VIDEO_LUMA_MEDIAN_FILTER_TOP:case V4L2_CID_MPEG_CX2341X_VIDEO_LUMA_MEDIAN_FILTER_BOTTOM:case V4L2_CID_MPEG_CX2341X_VIDEO_CHROMA_MEDIAN_FILTER_TOP:case V4L2_CID_MPEG_CX2341X_VIDEO_CHROMA_MEDIAN_FILTER_BOTTOM:case V4L2_CID_MPEG_CX2341X_STREAM_INSERT_NAV_PACKETS:cx2341x_ctrl_fill(qctrl->id, &name, &qctrl->type,&min, &max, &step, &def, &qctrl->flags);qctrl->minimum = min;qctrl->maximum = max;qctrl->step = step;qctrl->default_value = def;qctrl->reserved[0] = qctrl->reserved[1] = 0;strscpy(qctrl->name, name, sizeof(qctrl->name));return 0;default:return v4l2_ctrl_query_fill(qctrl, min, max, step, def);}}int cx2341x_ctrl_query(const struct cx2341x_mpeg_params *params,       struct v4l2_queryctrl *qctrl)": "cx2341x_ctrl_query_fill(struct v4l2_queryctrl  qctrl,   s32 min, s32 max, s32 step, s32 def){const char  name;switch (qctrl->id) {  MPEG controls ", "1,/* MPEG-2 TS ": "cx2341x_update(void  priv, cx2341x_mbox_func func,   const struct cx2341x_mpeg_params  old,   const struct cx2341x_mpeg_params  new){static int mpeg_stream_type[] = {0,  MPEG-2 PS ", "printk(KERN_INFO \"%s: Stream: %s\",prefix,cx2341x_menu_item(p, V4L2_CID_MPEG_STREAM_TYPE));if (p->stream_insert_nav_packets)printk(KERN_CONT \" (with navigation packets)\");printk(KERN_CONT \"\\n\");printk(KERN_INFO \"%s: VBI Format: %s\\n\",prefix,cx2341x_menu_item(p, V4L2_CID_MPEG_STREAM_VBI_FMT));/* Video ": "cx2341x_log_status(const struct cx2341x_mpeg_params  p, const char  prefix){int is_mpeg1 = p->video_encoding == V4L2_MPEG_VIDEO_ENCODING_MPEG_1;  Stream ", "cxhdl->stream_type = cx2341x_ctrl_new_menu(hdl,V4L2_CID_MPEG_STREAM_TYPE,V4L2_MPEG_STREAM_TYPE_MPEG2_SVCD, has_ts ? 0 : 2,V4L2_MPEG_STREAM_TYPE_MPEG2_PS);cxhdl->stream_vbi_fmt = cx2341x_ctrl_new_menu(hdl,V4L2_CID_MPEG_STREAM_VBI_FMT,V4L2_MPEG_STREAM_VBI_FMT_IVTV, has_sliced_vbi ? 0 : 2,V4L2_MPEG_STREAM_VBI_FMT_NONE);cxhdl->audio_sampling_freq = cx2341x_ctrl_new_menu(hdl,V4L2_CID_MPEG_AUDIO_SAMPLING_FREQ,V4L2_MPEG_AUDIO_SAMPLING_FREQ_32000, 0,V4L2_MPEG_AUDIO_SAMPLING_FREQ_48000);cxhdl->audio_encoding = cx2341x_ctrl_new_menu(hdl,V4L2_CID_MPEG_AUDIO_ENCODING,V4L2_MPEG_AUDIO_ENCODING_AC3, has_ac3 ? ~0x12 : ~0x2,V4L2_MPEG_AUDIO_ENCODING_LAYER_2);cxhdl->audio_l2_bitrate = cx2341x_ctrl_new_menu(hdl,V4L2_CID_MPEG_AUDIO_L2_BITRATE,V4L2_MPEG_AUDIO_L2_BITRATE_384K, 0x1ff,V4L2_MPEG_AUDIO_L2_BITRATE_224K);cxhdl->audio_mode = cx2341x_ctrl_new_menu(hdl,V4L2_CID_MPEG_AUDIO_MODE,V4L2_MPEG_AUDIO_MODE_MONO, 0,V4L2_MPEG_AUDIO_MODE_STEREO);cxhdl->audio_mode_extension = cx2341x_ctrl_new_menu(hdl,V4L2_CID_MPEG_AUDIO_MODE_EXTENSION,V4L2_MPEG_AUDIO_MODE_EXTENSION_BOUND_16, 0,V4L2_MPEG_AUDIO_MODE_EXTENSION_BOUND_4);cxhdl->audio_emphasis = cx2341x_ctrl_new_menu(hdl,V4L2_CID_MPEG_AUDIO_EMPHASIS,V4L2_MPEG_AUDIO_EMPHASIS_CCITT_J17, 0,V4L2_MPEG_AUDIO_EMPHASIS_NONE);cxhdl->audio_crc = cx2341x_ctrl_new_menu(hdl,V4L2_CID_MPEG_AUDIO_CRC,V4L2_MPEG_AUDIO_CRC_CRC16, 0,V4L2_MPEG_AUDIO_CRC_NONE);cx2341x_ctrl_new_std(hdl, V4L2_CID_MPEG_AUDIO_MUTE, 0, 1, 1, 0);if (has_ac3)cxhdl->audio_ac3_bitrate = cx2341x_ctrl_new_menu(hdl,V4L2_CID_MPEG_AUDIO_AC3_BITRATE,V4L2_MPEG_AUDIO_AC3_BITRATE_448K, 0x03,V4L2_MPEG_AUDIO_AC3_BITRATE_224K);cxhdl->video_encoding = cx2341x_ctrl_new_menu(hdl,V4L2_CID_MPEG_VIDEO_ENCODING,V4L2_MPEG_VIDEO_ENCODING_MPEG_2, 0,V4L2_MPEG_VIDEO_ENCODING_MPEG_2);cx2341x_ctrl_new_menu(hdl,V4L2_CID_MPEG_VIDEO_ASPECT,V4L2_MPEG_VIDEO_ASPECT_221x100, 0,V4L2_MPEG_VIDEO_ASPECT_4x3);cxhdl->video_b_frames = cx2341x_ctrl_new_std(hdl,V4L2_CID_MPEG_VIDEO_B_FRAMES, 0, 33, 1, 2);cxhdl->video_gop_size = cx2341x_ctrl_new_std(hdl,V4L2_CID_MPEG_VIDEO_GOP_SIZE,1, 34, 1, cxhdl->is_50hz ? 12 : 15);cx2341x_ctrl_new_std(hdl, V4L2_CID_MPEG_VIDEO_GOP_CLOSURE, 0, 1, 1, 1);cxhdl->video_bitrate_mode = cx2341x_ctrl_new_menu(hdl,V4L2_CID_MPEG_VIDEO_BITRATE_MODE,V4L2_MPEG_VIDEO_BITRATE_MODE_CBR, 0,V4L2_MPEG_VIDEO_BITRATE_MODE_VBR);cxhdl->video_bitrate = cx2341x_ctrl_new_std(hdl,V4L2_CID_MPEG_VIDEO_BITRATE,0, 27000000, 1, 6000000);cxhdl->video_bitrate_peak = cx2341x_ctrl_new_std(hdl,V4L2_CID_MPEG_VIDEO_BITRATE_PEAK,0, 27000000, 1, 8000000);cx2341x_ctrl_new_std(hdl,V4L2_CID_MPEG_VIDEO_TEMPORAL_DECIMATION, 0, 255, 1, 0);cxhdl->video_mute = cx2341x_ctrl_new_std(hdl,V4L2_CID_MPEG_VIDEO_MUTE, 0, 1, 1, 0);cxhdl->video_mute_yuv = cx2341x_ctrl_new_std(hdl,V4L2_CID_MPEG_VIDEO_MUTE_YUV, 0, 0xffffff, 1, 0x008080);/* CX23415/6 specific ": "cx2341x_handler_init(struct cx2341x_handler  cxhdl, unsigned nr_of_controls_hint){struct v4l2_ctrl_handler  hdl = &cxhdl->hdl;u32 caps = cxhdl->capabilities;int has_sliced_vbi = caps & CX2341X_CAP_HAS_SLICED_VBI;int has_ac3 = caps & CX2341X_CAP_HAS_AC3;int has_ts = caps & CX2341X_CAP_HAS_TS;cxhdl->width = 720;cxhdl->height = 480;v4l2_ctrl_handler_init(hdl, nr_of_controls_hint);  Add controls in ascending control ID order for fastest   insertion time. ", "dprintk(\"Couldn't read from EEPROM: not there?\\n\");eth_zero_addr(proposed_mac);return ret;}ret = getmac_tt(decodedMAC, encodedMAC);if( ret != 0 ) ": "ttpci_eeprom_parse_mac(struct i2c_adapter  adapter, u8  proposed_mac){int ret;u8 encodedMAC[20];u8 decodedMAC[6];ret = ttpci_eeprom_read_encodedMAC(adapter, encodedMAC);if (ret != 0) {  Will only be -ENODEV ", "if (dvbdmxfeed->index >= max_pid_filter)fc->extra_feedcount += onoff ? 1 : -1;/* toggle complete-TS-streaming when: * - pid_filtering is not enabled and it is the first or last feed requested * - pid_filtering is enabled, *   - but the number of requested feeds is exceeded *   - or the requested pid is 0x2000 ": "flexcop_pid_feed_control(struct flexcop_device  fc,struct dvb_demux_feed  dvbdmxfeed, int onoff){int max_pid_filter = 6;max_pid_filter -= 6   fc->skip_6_hw_pid_filter;max_pid_filter += 32   fc->has_32_hw_pid_filter;fc->feedcount += onoff ? 1 : -1;   the number of PIDsFeed currently requested ", "return 0;}EXPORT_SYMBOL(flexcop_sram_set_dest": "flexcop_sram_set_dest(struct flexcop_device  fc, flexcop_sram_dest_t dest, flexcop_sram_dest_target_t target){flexcop_ibi_value v;v = fc->read_ibi_reg(fc, sram_dest_reg_714);if (fc->rev != FLEXCOP_III && target == FC_SRAM_DEST_TARGET_FC3_CA) {err(\"SRAM destination target to available on FlexCopII(b)\\n\");return -EINVAL;}deb_sram(\"sram dest: %x target: %x\\n\", dest, target);if (dest & FC_SRAM_DEST_NET)v.sram_dest_reg_714.NET_Dest = target;if (dest & FC_SRAM_DEST_CAI)v.sram_dest_reg_714.CAI_Dest = target;if (dest & FC_SRAM_DEST_CAO)v.sram_dest_reg_714.CAO_Dest = target;if (dest & FC_SRAM_DEST_MEDIA)v.sram_dest_reg_714.MEDIA_Dest = target;fc->write_ibi_reg(fc,sram_dest_reg_714,v);udelay(1000);   TODO delay really necessary ", "if (i2c->no_base_addr && len == 0 && op == FC_WRITE) ": "flexcop_i2c_request(struct flexcop_i2c_adapter  i2c,flexcop_access_op_t op, u8 chipaddr,u8 start_addr, u8  buf, u16 size){int ret;int len = size;u8  p;u8 addr = start_addr;u16 bytes_to_transfer;flexcop_ibi_value r100;deb_i2c(\"port %d %s(%02x): register %02x, size: %d\\n\",i2c->port,op == FC_READ ? \"rd\" : \"wr\",chipaddr, start_addr, size);r100.raw = 0;r100.tw_sm_c_100.chipaddr = chipaddr;r100.tw_sm_c_100.twoWS_rw = op;r100.tw_sm_c_100.twoWS_port_reg = i2c->port;  in that case addr is the only value ->   we write it twice as baseaddr and val0   BBTI is doing it like that for ISL6421 at least ", "ret = flexcop_i2c_init(fc);if (ret)goto error;/* do the MAC address reading after initializing the dvb_adapter ": "flexcop_device_initialize(struct flexcop_device  fc){int ret;ibi_zero.raw = 0;flexcop_reset(fc);flexcop_determine_revision(fc);flexcop_sram_init(fc);flexcop_hw_filter_init(fc);flexcop_smc_ctrl(fc, 0);ret = flexcop_dvb_init(fc);if (ret)goto error;  i2c has to be done before doing EEProm stuff -   because the EEProm is accessed via i2c ", "if (mem_priv && call_memop(vb, num_users, mem_priv) > 1)return true;}return false;}EXPORT_SYMBOL(vb2_buffer_in_use": "vb2_buffer_in_use(struct vb2_queue  q, struct vb2_buffer  vb){unsigned int plane;for (plane = 0; plane < vb->num_planes; ++plane) {void  mem_priv = vb->planes[plane].mem_priv;    If num_users() has not been provided, call_memop   will return 0, apparently nobody cares about this   case anyway. If num_users() returns more than 1,   we are not the only user of the plane's memory. ", "if (memory == VB2_MEMORY_MMAP && __verify_mmap_ops(q)) ": "vb2_verify_memory_type(struct vb2_queue  q,enum vb2_memory memory, unsigned int type){if (memory != VB2_MEMORY_MMAP && memory != VB2_MEMORY_USERPTR &&    memory != VB2_MEMORY_DMABUF) {dprintk(q, 1, \"unsupported memory type\\n\");return -EINVAL;}if (type != q->type) {dprintk(q, 1, \"requested type is incorrect\\n\");return -EINVAL;}    Make sure all the required memory ops for given memory type   are available. ", "res = vb2_dvb_register_adapter(f, module, adapter_priv, device, mdev,fe->dvb.name, adapter_nr, mfe_shared);if (res < 0) ": "vb2_dvb_get_frontend(f, 1);if (!fe) {pr_warn(\"Unable to register the adapter which has no frontends\\n\");return -EINVAL;}  Bring up the adapter ", "dvb_frontend_detach(fe->dvb.frontend);list_del(list); /* remove list entry ": "vb2_dvb_dealloc_frontends(f);dvb_unregister_adapter(&f->adapter);}EXPORT_SYMBOL(vb2_dvb_unregister_bus);struct vb2_dvb_frontend  vb2_dvb_get_frontend(struct vb2_dvb_frontends  f, int id){struct list_head  list,  q;struct vb2_dvb_frontend  fe,  ret = NULL;mutex_lock(&f->lock);list_for_each_safe(list, q, &f->felist) {fe = list_entry(list, struct vb2_dvb_frontend, felist);if (fe->id == id) {ret = fe;break;}}mutex_unlock(&f->lock);return ret;}EXPORT_SYMBOL(vb2_dvb_get_frontend);int vb2_dvb_find_frontend(struct vb2_dvb_frontends  f,struct dvb_frontend  p){struct list_head  list,  q;struct vb2_dvb_frontend  fe = NULL;int ret = 0;mutex_lock(&f->lock);list_for_each_safe(list, q, &f->felist) {fe = list_entry(list, struct vb2_dvb_frontend, felist);if (fe->dvb.frontend == p) {ret = fe->id;break;}}mutex_unlock(&f->lock);return ret;}EXPORT_SYMBOL(vb2_dvb_find_frontend);struct vb2_dvb_frontend  vb2_dvb_alloc_frontend(struct vb2_dvb_frontends  f, int id){struct vb2_dvb_frontend  fe;fe = kzalloc(sizeof(struct vb2_dvb_frontend), GFP_KERNEL);if (fe == NULL)return NULL;fe->id = id;mutex_init(&fe->dvb.lock);mutex_lock(&f->lock);list_add_tail(&fe->felist, &f->felist);mutex_unlock(&f->lock);return fe;}EXPORT_SYMBOL(vb2_dvb_alloc_frontend);void vb2_dvb_dealloc_frontends(struct vb2_dvb_frontends  f){struct list_head  list,  q;struct vb2_dvb_frontend  fe;mutex_lock(&f->lock);list_for_each_safe(list, q, &f->felist) {fe = list_entry(list, struct vb2_dvb_frontend, felist);if (fe->dvb.net.dvbdev) {dvb_net_release(&fe->dvb.net);fe->dvb.demux.dmx.remove_frontend(&fe->dvb.demux.dmx,&fe->dvb.fe_mem);fe->dvb.demux.dmx.remove_frontend(&fe->dvb.demux.dmx,&fe->dvb.fe_hw);dvb_dmxdev_release(&fe->dvb.dmxdev);dvb_dmx_release(&fe->dvb.demux);dvb_unregister_frontend(fe->dvb.frontend);}if (fe->dvb.frontend)  always allocated, may have been reset ", "int vb2_querybuf(struct vb2_queue *q, struct v4l2_buffer *b)": "vb2_querybuf() - query video buffer information   @q:vb2 queue   @b:buffer struct passed from userspace to vidioc_querybuf handler  in driver     Should be called from vidioc_querybuf ioctl handler in driver.   This function will verify the passed v4l2_buffer structure and fill the   relevant information for the userspace.     The return values from this function are intended to be directly returned   from vidioc_querybuf handler in driver. ", "struct frame_vector *vb2_create_framevec(unsigned long start, unsigned long length, bool write)": "vb2_create_framevec() - map virtual addresses to pfns   @start:Virtual user address where we start mapping   @length:Length of a range to map   @write:Should we map for writing into the area     This function allocates and fills in a vector with pfns corresponding to   virtual address range passed in arguments. If pfns have corresponding pages,   page references are also grabbed to pin pages in memory. The function   returns pointer to the vector on success and error pointer in case of   failure. Returned vector needs to be freed via vb2_destroy_pfnvec(). ", "void vb2_destroy_framevec(struct frame_vector *vec)": "vb2_destroy_framevec() - release vector of mapped pfns   @vec:vector of pfns  pages to release     This releases references to all pages in the vector @vec (if corresponding   pfns are backed by pages) and frees the passed vector. ", "int get_vaddr_frames(unsigned long start, unsigned int nr_frames, bool write,     struct frame_vector *vec)": "get_vaddr_frames() - map virtual addresses to pfns   @start:starting user address   @nr_frames:number of pages  pfns from start to map   @write:the mapped address has write permission   @vec:structure which receives pages  pfns of the addresses mapped.  It should have space for at least nr_frames entries.     This function maps virtual addresses from @start and fills @vec structure   with page frame numbers or page pointers to corresponding pages (choice   depends on the type of the vma underlying the virtual address). If @start   belongs to a normal vma, the function grabs reference to each of the pages   to pin them in memory. If @start belongs to VM_IO | VM_PFNMAP vma, we don't   touch page structures and the caller must make sure pfns aren't reused for   anything else while he is using them.     The function returns number of pages mapped which may be less than   @nr_frames. In particular we stop mapping if there are more vmas of   different type underlying the specified range of virtual addresses.   When the function isn't able to map a single page, it returns error.     This function takes care of grabbing mmap_lock as necessary. ", "void put_vaddr_frames(struct frame_vector *vec)": "put_vaddr_frames() - drop references to pages if get_vaddr_frames() acquired  them   @vec:frame vector to put     Drop references to pages if get_vaddr_frames() acquired them. We also   invalidate the frame vector so that it is prepared for the next call into   get_vaddr_frames(). ", "int frame_vector_to_pages(struct frame_vector *vec)": "frame_vector_to_pages - convert frame vector to contain page pointers   @vec:frame vector to convert     Convert @vec to contain array of page pointers.  If the conversion is   successful, return 0. Otherwise return an error. Note that we do not grab   page references for the page structures. ", "void frame_vector_to_pfns(struct frame_vector *vec)": "frame_vector_to_pfns - convert frame vector to contain pfns   @vec:frame vector to convert     Convert @vec to contain array of pfns. ", "struct frame_vector *frame_vector_create(unsigned int nr_frames)": "frame_vector_create() - allocate & initialize structure for pinned pfns   @nr_frames:number of pfns slots we should reserve     Allocate and initialize struct pinned_pfns to be able to hold @nr_pfns   pfns. ", "void frame_vector_destroy(struct frame_vector *vec)": "frame_vector_destroy() - free memory allocated to carry frame vector   @vec:Frame vector to free     Free structure allocated by frame_vector_create() to carry frames. ", "result = 0;for (i = 4; i < 4 + 32 * 4; i += 4) ": "af9005_rc_decode(struct dvb_usb_device  d, u8   data, int len, u32   event,     int  state){u16 mark, space;u32 result;u8 cust, dat, invdat;int i;if (len >= 6) {mark = (u16) (data[0] << 8) + data[1];space = (u16) (data[2] << 8) + data[3];if (space   3 < mark) {for (i = 0; i < ARRAY_SIZE(repeatable_keys); i++) {if (d->last_event == repeatable_keys[i]) { state = REMOTE_KEY_REPEAT; event = d->last_event;deb_decode(\"repeat key, event %x\\n\",    event);return 0;}}deb_decode(\"repeated key ignored (non repeatable)\\n\");return 0;} else if (len >= 33   4) { 32 bits + start code ", "if (dvb_attach(dvb_pll_attach, adap->fe_adap[0].fe, 0x60, tun_i2c, DVB_PLL_ENV57H1XD5) == NULL)return -ENOMEM;} else ": "dibusb_dib3000mc_tuner_attach(struct dvb_usb_adapter  adap){struct dibusb_state  st = adap->priv;u8 a,b;u16 if1 = 1220;struct i2c_adapter  tun_i2c; First IF calibration for Liteon Sticksif (le16_to_cpu(adap->dev->udev->descriptor.idVendor) == USB_VID_LITEON &&    le16_to_cpu(adap->dev->udev->descriptor.idProduct) == USB_PID_LITEON_DVB_T_WARM) {dibusb_read_eeprom_byte(adap->dev,0x7E,&a);dibusb_read_eeprom_byte(adap->dev,0x7F,&b);if (a == 0x00)if1 += b;else if (a == 0x80)if1 -= b;elsewarn(\"LITE-ON DVB-T: Strange IF1 calibration :%2X %2X\\n\", a, b);} else if (le16_to_cpu(adap->dev->udev->descriptor.idVendor) == USB_VID_DIBCOM &&   le16_to_cpu(adap->dev->udev->descriptor.idProduct) == USB_PID_DIBCOM_MOD3001_WARM) {u8 desc;dibusb_read_eeprom_byte(adap->dev, 7, &desc);if (desc == 2) {a = 127;do {dibusb_read_eeprom_byte(adap->dev, a, &desc);a--;} while (a > 7 && (desc == 0xff || desc == 0x00));if (desc & 0x80)if1 -= (0xff - desc);elseif1 += desc;}}tun_i2c = dib3000mc_get_tuner_i2c_master(adap->fe_adap[0].fe, 1);if (dvb_attach(mt2060_attach, adap->fe_adap[0].fe, tun_i2c, &stk3000p_mt2060_config, if1) == NULL) {  not found - use panasonic pll parameters ", "buf[0] = 1;if (usb_cypress_writemem(udev, cpu_cs_register, buf, 1) != 1)err(\"could not stop the USB controller CPU.\");while ((ret = dvb_usb_get_hexline(fw, hx, &pos)) > 0) ": "usb_cypress_load_firmware(struct usb_device  udev, const struct firmware  fw, int type){struct hexline  hx;u8  buf;int ret, pos = 0;u16 cpu_cs_register = cypress[type].cpu_cs_register;buf = kmalloc(sizeof( hx), GFP_KERNEL);if (!buf)return -ENOMEM;hx = (struct hexline  )buf;  stop the CPU ", "buf[0] = 0;if (usb_cypress_writemem(udev, cpu_cs_register, buf, 1) != 1) ": "dvb_usb_get_hexline(fw, hx, &pos)) > 0) {deb_fw(\"writing to address 0x%04x (buffer: 0x%02x %02x)\\n\", hx->addr, hx->len, hx->chk);ret = usb_cypress_writemem(udev, hx->addr, hx->data, hx->len);if (ret != hx->len) {err(\"error while transferring firmware (transferred size: %d, block size: %d)\",ret, hx->len);ret = -EINVAL;break;}}if (ret < 0) {err(\"firmware download failed at %d with %d\",pos,ret);kfree(buf);return ret;}if (ret == 0) {  restart the CPU ", "for (i = 0; i < d->props.rc.legacy.rc_map_size; i++)if (rc5_custom(&keymap[i]) == keybuf[1] &&rc5_data(&keymap[i]) == keybuf[3]) ": "dvb_usb_nec_rc_key_to_event(struct dvb_usb_device  d,u8 keybuf[5], u32  event, int  state){int i;struct rc_map_table  keymap = d->props.rc.legacy.rc_map_table; event = 0; state = REMOTE_NO_KEY_PRESSED;switch (keybuf[0]) {case DVB_USB_RC_NEC_EMPTY:break;case DVB_USB_RC_NEC_KEY_PRESSED:if ((u8) ~keybuf[1] != keybuf[2] ||(u8) ~keybuf[3] != keybuf[4]) {deb_err(\"remote control checksum failed.\\n\");break;}  See if we can match the raw key code. ", "if (!ret && rbuf && rlen) ": "dvb_usb_generic_rw(struct dvb_usb_device  d, u8  wbuf, u16 wlen, u8  rbuf,u16 rlen, int delay_ms){int actlen = 0, ret = -ENOMEM;if (!d || wbuf == NULL || wlen == 0)return -EINVAL;if (d->props.generic_bulk_ctrl_endpoint == 0) {err(\"endpoint for generic control not specified.\");return -EINVAL;}if ((ret = mutex_lock_interruptible(&d->usb_mutex)))return ret;deb_xfer(\">>> \");debug_dump(wbuf,wlen,deb_xfer);ret = usb_bulk_msg(d->udev,usb_sndbulkpipe(d->udev,d->props.generic_bulk_ctrl_endpoint), wbuf,wlen,&actlen,2000);if (ret)err(\"bulk message failed: %d (%d%d)\",ret,wlen,actlen);elseret = actlen != wlen ? -1 : 0;  an answer is expected, and no error before ", "go7007_write_addr(go, 0x3c82, 0x0009);msleep(50);go7007_write_addr(go, 0x3c82, 0x000d);}for (i = 0; i < num_i2c_devs; ++i)init_i2c_module(&go->i2c_adapter, &go->board_info->i2c_devs[i]);if (go->tuner_type >= 0) ": "go7007_register_encoder(struct go7007  go, unsigned num_i2c_devs){int i, ret;dev_info(go->dev, \"go7007: registering new %s\\n\", go->name);go->v4l2_dev.release = go7007_remove;ret = v4l2_device_register(go->dev, &go->v4l2_dev);if (ret < 0)return ret;mutex_lock(&go->hw_lock);ret = go7007_init_encoder(go);mutex_unlock(&go->hw_lock);if (ret < 0)return ret;ret = go7007_v4l2_ctrl_init(go);if (ret < 0)return ret;if (!go->i2c_adapter_online &&go->board_info->flags & GO7007_BOARD_USE_ONBOARD_I2C) {ret = go7007_i2c_init(go);if (ret < 0)return ret;go->i2c_adapter_online = 1;}if (go->i2c_adapter_online) {if (go->board_id == GO7007_BOARDID_ADS_USBAV_709) {  Reset the TW9906 ", "break;case 0x01:go->state = STATE_00_00_01;break;case 0xFF:store_byte(vb, 0x00);store_byte(vb, 0x00);go->state = STATE_FF;break;default:store_byte(vb, 0x00);store_byte(vb, 0x00);store_byte(vb, buf[i]);go->state = STATE_DATA;break;}break;case STATE_00_00_01:if (buf[i] == 0xF8 && go->modet_enable == 0) ": "go7007_parse_video_stream(struct go7007  go, u8  buf, int length){struct go7007_buffer  vb = go->active_buf;int i, seq_start_code = -1, gop_start_code = -1, frame_start_code = -1;switch (go->format) {case V4L2_PIX_FMT_MPEG4:seq_start_code = 0xB0;gop_start_code = 0xB3;frame_start_code = 0xB6;break;case V4L2_PIX_FMT_MPEG1:case V4L2_PIX_FMT_MPEG2:seq_start_code = 0xB3;gop_start_code = 0xB8;frame_start_code = 0x00;break;}for (i = 0; i < length; ++i) {if (vb && vb->vb.vb2_buf.planes[0].bytesused >=GO7007_BUF_SIZE - 3) {v4l2_info(&go->v4l2_dev, \"dropping oversized frame\\n\");vb2_set_plane_payload(&vb->vb.vb2_buf, 0, 0);vb->frame_offset = 0;vb->modet_active = 0;vb = go->active_buf = NULL;}switch (go->state) {case STATE_DATA:switch (buf[i]) {case 0x00:go->state = STATE_00;break;case 0xFF:go->state = STATE_FF;break;default:store_byte(vb, buf[i]);break;}break;case STATE_00:switch (buf[i]) {case 0x00:go->state = STATE_00_00;break;case 0xFF:store_byte(vb, 0x00);go->state = STATE_FF;break;default:store_byte(vb, 0x00);store_byte(vb, buf[i]);go->state = STATE_DATA;break;}break;case STATE_00_00:switch (buf[i]) {case 0x00:store_byte(vb, 0x00);  go->state remains STATE_00_00 ", ";} else if (ret == RECONNECTS_USB) ": "dvb_usbv2_probe(struct usb_interface  intf,const struct usb_device_id  id){int ret;struct dvb_usb_device  d;struct usb_device  udev = interface_to_usbdev(intf);struct dvb_usb_driver_info  driver_info =(struct dvb_usb_driver_info  ) id->driver_info;dev_dbg(&udev->dev, \"%s: bInterfaceNumber=%d\\n\", __func__,intf->cur_altsetting->desc.bInterfaceNumber);if (!id->driver_info) {dev_err(&udev->dev, \"%s: driver_info failed\\n\", KBUILD_MODNAME);ret = -ENODEV;goto err;}d = kzalloc(sizeof(struct dvb_usb_device), GFP_KERNEL);if (!d) {dev_err(&udev->dev, \"%s: kzalloc() failed\\n\", KBUILD_MODNAME);ret = -ENOMEM;goto err;}d->intf = intf;d->name = driver_info->name;d->rc_map = driver_info->rc_map;d->udev = udev;d->props = driver_info->props;if (intf->cur_altsetting->desc.bInterfaceNumber !=d->props->bInterfaceNumber) {ret = -ENODEV;goto err_kfree_d;}mutex_init(&d->usb_mutex);mutex_init(&d->i2c_mutex);if (d->props->size_of_priv) {d->priv = kzalloc(d->props->size_of_priv, GFP_KERNEL);if (!d->priv) {dev_err(&d->udev->dev, \"%s: kzalloc() failed\\n\",KBUILD_MODNAME);ret = -ENOMEM;goto err_kfree_d;}}if (d->props->probe) {ret = d->props->probe(d);if (ret)goto err_kfree_priv;}if (d->props->identify_state) {const char  name = NULL;ret = d->props->identify_state(d, &name);if (ret == COLD) {dev_info(&d->udev->dev,\"%s: found a '%s' in cold state\\n\",KBUILD_MODNAME, d->name);if (!name)name = d->props->firmware;ret = dvb_usbv2_download_firmware(d, name);if (ret == 0) {  device is warm, continue initialization ", "if (d->rc_polling_active)cancel_delayed_work_sync(&d->rc_query_work);for (i = MAX_NO_OF_ADAPTER_PER_DEVICE - 1; i >= 0; i--) ": "dvb_usbv2_suspend(struct usb_interface  intf, pm_message_t msg){struct dvb_usb_device  d = usb_get_intfdata(intf);int ret = 0, i, active_fe;struct dvb_frontend  fe;dev_dbg(&d->udev->dev, \"%s:\\n\", __func__);  stop remote controller poll ", "usb_urb_submitv2(&d->adapter[i].stream, NULL);if (d->props->streaming_ctrl)d->props->streaming_ctrl(fe, 1);d->adapter[i].suspend_resume_active = false;}}/* start remote controller poll ": "dvb_usbv2_resume_common(struct dvb_usb_device  d){int ret = 0, i, active_fe;struct dvb_frontend  fe;dev_dbg(&d->udev->dev, \"%s:\\n\", __func__);for (i = 0; i < MAX_NO_OF_ADAPTER_PER_DEVICE; i++) {active_fe = d->adapter[i].active_fe;if (d->adapter[i].dvb_adap.priv && active_fe != -1) {fe = d->adapter[i].fe[active_fe];ret = dvb_frontend_resume(fe);  resume usb streaming ", "steps = abs(desired_avg_lum - avg_lum) / deadzone;gspca_dbg(gspca_dev, D_FRAM, \"autogain: lum: %d, desired: %d, steps: %d\\n\",  avg_lum, desired_avg_lum, steps);for (i = 0; i < steps; i++) ": "gspca_expo_autogain(struct gspca_dev  gspca_dev,int avg_lum,int desired_avg_lum,int deadzone,int gain_knee,int exposure_knee){s32 gain, orig_gain, exposure, orig_exposure;int i, steps, retval = 0;if (v4l2_ctrl_g_ctrl(gspca_dev->autogain) == 0)return 0;orig_gain = gain = v4l2_ctrl_g_ctrl(gspca_dev->gain);orig_exposure = exposure = v4l2_ctrl_g_ctrl(gspca_dev->exposure);  If we are of a multiple of deadzone, do multiple steps to reach the   desired lumination fast (with the risc of a slight overshoot) ", "steps = (desired_avg_lum - avg_lum) / deadzone;gspca_dbg(gspca_dev, D_FRAM, \"autogain: lum: %d, desired: %d, steps: %d\\n\",  avg_lum, desired_avg_lum, steps);if ((gain + steps) > gain_high &&    exposure < gspca_dev->exposure->maximum) ": "gspca_coarse_grained_expo_autogain(struct gspca_dev  gspca_dev,int avg_lum,int desired_avg_lum,int deadzone){s32 gain_low, gain_high, gain, orig_gain, exposure, orig_exposure;int steps, retval = 0;if (v4l2_ctrl_g_ctrl(gspca_dev->autogain) == 0)return 0;orig_gain = gain = v4l2_ctrl_g_ctrl(gspca_dev->gain);orig_exposure = exposure = v4l2_ctrl_g_ctrl(gspca_dev->exposure);gain_low  = (s32)(gspca_dev->gain->maximum - gspca_dev->gain->minimum)     5   2 + gspca_dev->gain->minimum;gain_high = (s32)(gspca_dev->gain->maximum - gspca_dev->gain->minimum)     5   4 + gspca_dev->gain->minimum;  If we are of a multiple of deadzone, do multiple steps to reach the   desired lumination fast (with the risc of a slight overshoot) ", "if (!buf) ": "gspca_frame_add(struct gspca_dev  gspca_dev,enum gspca_packet_type packet_type,const u8  data,int len){struct gspca_buffer  buf;unsigned long flags;gspca_dbg(gspca_dev, D_PACK, \"add t:%d l:%d\\n\",packet_type, len);spin_lock_irqsave(&gspca_dev->qlock, flags);buf = list_first_entry_or_null(&gspca_dev->buf_list,       typeof( buf), list);spin_unlock_irqrestore(&gspca_dev->qlock, flags);if (packet_type == FIRST_PACKET) {  if there is no queued buffer, discard the whole frame ", "if (dev_size < sizeof *gspca_dev)dev_size = sizeof *gspca_dev;gspca_dev = kzalloc(dev_size, GFP_KERNEL);if (!gspca_dev) ": "gspca_dev_probe2(struct usb_interface  intf,const struct usb_device_id  id,const struct sd_desc  sd_desc,int dev_size,struct module  module){struct gspca_dev  gspca_dev;struct usb_device  dev = interface_to_usbdev(intf);struct vb2_queue  q;int ret;pr_info(\"%s-\" GSPCA_VERSION \" probing %04x:%04x\\n\",sd_desc->name, id->idVendor, id->idProduct);  create the device ", "v4l2_device_put(&gspca_dev->v4l2_dev);}EXPORT_SYMBOL(gspca_disconnect": "gspca_disconnect(struct usb_interface  intf){struct gspca_dev  gspca_dev = usb_get_intfdata(intf);#if IS_ENABLED(CONFIG_INPUT)struct input_dev  input_dev;#endifgspca_dbg(gspca_dev, D_PROBE, \"%s disconnect\\n\",  video_device_node_name(&gspca_dev->vdev));mutex_lock(&gspca_dev->usb_lock);gspca_dev->present = false;destroy_urbs(gspca_dev);gspca_input_destroy_urb(gspca_dev);vb2_queue_error(&gspca_dev->queue);#if IS_ENABLED(CONFIG_INPUT)input_dev = gspca_dev->input_dev;if (input_dev) {gspca_dev->input_dev = NULL;input_unregister_device(input_dev);}#endifv4l2_device_disconnect(&gspca_dev->v4l2_dev);video_unregister_device(&gspca_dev->vdev);mutex_unlock(&gspca_dev->usb_lock);  (this will call gspca_release() immediately or on last close) ", "gspca_dev->usb_err = 0;if (gspca_dev->sd_desc->stopN)gspca_dev->sd_desc->stopN(gspca_dev);destroy_urbs(gspca_dev);gspca_set_alt0(gspca_dev);if (gspca_dev->sd_desc->stop0)gspca_dev->sd_desc->stop0(gspca_dev);mutex_unlock(&gspca_dev->usb_lock);return 0;}EXPORT_SYMBOL(gspca_suspend": "gspca_suspend(struct usb_interface  intf, pm_message_t message){struct gspca_dev  gspca_dev = usb_get_intfdata(intf);gspca_input_destroy_urb(gspca_dev);if (!vb2_start_streaming_called(&gspca_dev->queue))return 0;mutex_lock(&gspca_dev->usb_lock);gspca_dev->frozen = 1;  avoid urb error messages ", "streaming = vb2_start_streaming_called(&gspca_dev->queue);if (streaming)ret = gspca_init_transfer(gspca_dev);elsegspca_input_create_urb(gspca_dev);mutex_unlock(&gspca_dev->usb_lock);return ret;}EXPORT_SYMBOL(gspca_resume": "gspca_resume(struct usb_interface  intf){struct gspca_dev  gspca_dev = usb_get_intfdata(intf);int streaming, ret = 0;mutex_lock(&gspca_dev->usb_lock);gspca_dev->frozen = 0;gspca_dev->usb_err = 0;gspca_dev->sd_desc->init(gspca_dev);    Most subdrivers send all ctrl values on sd_start and thus   only write to the device registers on s_ctrl when streaming ->   Clear streaming to avoid setting all ctrls twice. ", "state = kmalloc(sizeof(struct ttusbdecfe_state), GFP_KERNEL);if (state == NULL)return NULL;/* setup the state ": "ttusbdecfe_dvbs_attach(const struct ttusbdecfe_config  config){struct ttusbdecfe_state  state = NULL;  allocate memory for the internal state ", "if (mt2060_readreg(priv,REG_PART_REV,&id) != 0) ": "mt2060_attach(struct dvb_frontend  fe, struct i2c_adapter  i2c, struct mt2060_config  cfg, u16 if1){struct mt2060_priv  priv = NULL;u8 id = 0;priv = kzalloc(sizeof(struct mt2060_priv), GFP_KERNEL);if (priv == NULL)return NULL;priv->cfg      = cfg;priv->i2c      = i2c;priv->if1_freq = if1;priv->i2c_max_regs = ~0;if (fe->ops.i2c_gate_ctrl)fe->ops.i2c_gate_ctrl(fe, 1);   open i2c_gate ", "ret = fc0012_readreg(priv, 0x00, &chip_id);if (ret < 0)goto err;dev_dbg(&i2c->dev, \"%s: chip_id=%02x\\n\", __func__, chip_id);switch (chip_id) ": "fc0012_attach(struct dvb_frontend  fe,struct i2c_adapter  i2c, const struct fc0012_config  cfg){struct fc0012_priv  priv;int ret;u8 chip_id;if (fe->ops.i2c_gate_ctrl)fe->ops.i2c_gate_ctrl(fe, 1);priv = kzalloc(sizeof(struct fc0012_priv), GFP_KERNEL);if (!priv) {ret = -ENOMEM;dev_err(&i2c->dev, \"%s: kzalloc() failed\\n\", KBUILD_MODNAME);goto err;}priv->cfg = cfg;priv->i2c = i2c;  check if the tuner is there ", "ret = mc44s803_readreg(priv, MC44S803_REG_ID, &reg);if (ret)goto error;id = MC44S803_REG_MS(reg, MC44S803_ID);if (id != 0x14) ": "mc44s803_attach(struct dvb_frontend  fe, struct i2c_adapter  i2c, struct mc44s803_config  cfg){struct mc44s803_priv  priv;u32 reg;u8 id;int ret;reg = 0;priv = kzalloc(sizeof(struct mc44s803_priv), GFP_KERNEL);if (priv == NULL)return NULL;priv->cfg = cfg;priv->i2c = i2c;priv->fe  = fe;if (fe->ops.i2c_gate_ctrl)fe->ops.i2c_gate_ctrl(fe, 1);   open i2c_gate ", "priv->bandwidth = 6000000;/* set default configuration ": "xc4000_attach(struct dvb_frontend  fe,   struct i2c_adapter  i2c,   struct xc4000_config  cfg){struct xc4000_priv  priv = NULL;intinstance;u16id = 0;dprintk(1, \"%s(%d-%04x)\\n\", __func__,i2c ? i2c_adapter_id(i2c) : -1,cfg ? cfg->i2c_address : -1);mutex_lock(&xc4000_list_mutex);instance = hybrid_tuner_request_state(struct xc4000_priv, priv,      hybrid_tuner_instance_list,      i2c, cfg->i2c_address, \"xc4000\");switch (instance) {case 0:goto fail;case 1:  new tuner instance ", "/*    History of this driver (Steven Toth):      I was given a public release of a linux driver that included      support for the MaxLinear MXL5005S silicon tuner. Analysis of      the tuner driver showed clearly three things.      1. The tuner driver didn't support the LinuxTV tuner API so the code Realtek added had to be removed.      2. A significant amount of the driver is reference driver code from MaxLinear, I felt it was important to identify and preserve this.      3. New code has to be added to interface correctly with the LinuxTV API, as a regular kernel module.      Other than the reference driver enum's, I've clearly marked      sections of the code and retained the copyright of the      respective owners.": "mxl5005s_attach()    Copyright (C) 2008 Realtek    Copyright (C) 2008 Jan Hoogenraad      Functions:mxl5005s_SetRfFreqHz()    This program is free software; you can redistribute it andor modify    it under the terms of the GNU General Public License as published by    the Free Software Foundation; either version 2 of the License, or    (at your option) any later version.    This program is distributed in the hope that it will be useful,    but WITHOUT ANY WARRANTY; without even the implied warranty of    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the    GNU General Public License for more details.    You should have received a copy of the GNU General Public License    along with this program; if not, write to the Free Software    Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.", "/* Try to detect tuner chip. Probably this is not correct register. ": "qt1010_attach(struct dvb_frontend  fe,    struct i2c_adapter  i2c,    struct qt1010_config  cfg){struct qt1010_priv  priv = NULL;u8 id;priv = kzalloc(sizeof(struct qt1010_priv), GFP_KERNEL);if (priv == NULL)return NULL;priv->cfg = cfg;priv->i2c = i2c;if (fe->ops.i2c_gate_ctrl)fe->ops.i2c_gate_ctrl(fe, 1);   open i2c_gate ", "priv->bandwidth = 6000000;fe->tuner_priv = priv;priv->fe = fe;INIT_DELAYED_WORK(&priv->timer_sleep, xc5000_do_timer_sleep);break;default:/* existing tuner instance ": "xc5000_attach(struct dvb_frontend  fe,   struct i2c_adapter  i2c,   const struct xc5000_config  cfg){struct xc5000_priv  priv = NULL;int instance;u16 id = 0;dprintk(1, \"%s(%d-%04x)\\n\", __func__,i2c ? i2c_adapter_id(i2c) : -1,cfg ? cfg->i2c_address : -1);mutex_lock(&xc5000_list_mutex);instance = hybrid_tuner_request_state(struct xc5000_priv, priv,      hybrid_tuner_instance_list,      i2c, cfg->i2c_address, \"xc5000\");switch (instance) {case 0:goto fail;case 1:  new tuner instance ", "static u8 def_regs[] = ": "tda18218_attach(struct dvb_frontend  fe,struct i2c_adapter  i2c, struct tda18218_config  cfg){struct tda18218_priv  priv = NULL;u8 val;int ret;  chip default registers values ", "/* The following was taken from dvb-pll.c: ": "tuners that contain a tda988x chip, and then we  can remove this setting from the various card structs.    FIXME: Right now, all tuners are using the first tuner_params[]  array element for analog mode. In the future, we will be merging  similar tuner definitions together, such that each tuner definition  will have a tuner_params struct for each available video standard.  At that point, the tuner_params[] array element will be chosen  based on the video standard in use. ", "/* push rc_cal value, get rc_cal value ": "fc0013_rc_cal_add(struct dvb_frontend  fe, int rc_val){struct fc0013_priv  priv = fe->tuner_priv;int ret;u8 rc_cal;int val;if (fe->ops.i2c_gate_ctrl)fe->ops.i2c_gate_ctrl(fe, 1);   open I2C-gate ", "ret = fc0013_writereg(priv, 0x0d, 0x01);if (!ret)ret = fc0013_writereg(priv, 0x10, 0x00);if (fe->ops.i2c_gate_ctrl)fe->ops.i2c_gate_ctrl(fe, 0); /* close I2C-gate ": "fc0013_rc_cal_reset(struct dvb_frontend  fe){struct fc0013_priv  priv = fe->tuner_priv;int ret;if (fe->ops.i2c_gate_ctrl)fe->ops.i2c_gate_ctrl(fe, 1);   open I2C-gate ", "goto fail;case 1:/* new tuner instance ": "xc2028_attach(struct dvb_frontend  fe,   struct xc2028_config  cfg){struct xc2028_data  priv;int instance;if (debug)printk(KERN_DEBUG \"xc2028: Xcv20283028 init called!\\n\");if (NULL == cfg)return NULL;if (!fe) {printk(KERN_ERR \"xc2028: No frontend!\\n\");return NULL;}mutex_lock(&xc2028_list_mutex);instance = hybrid_tuner_request_state(struct xc2028_data, priv,      hybrid_tuner_instance_list,      cfg->i2c_adap, cfg->i2c_addr,      \"xc2028\");switch (instance) {case 0:  memory allocation failure ", "dvb_frontend_get(fe);sema_init(&fepriv->sem, 1);init_waitqueue_head(&fepriv->wait_queue);init_waitqueue_head(&fepriv->events.wait_queue);mutex_init(&fepriv->events.mtx);fe->dvb = dvb;fepriv->inversion = INVERSION_OFF;dev_info(fe->dvb->device, \"DVB: registering adapter %i frontend %i (%s)...\\n\", fe->dvb->num, fe->id, fe->ops.info.name);ret = dvb_register_device(fe->dvb, &fepriv->dvbdev, &dvbdev_template,    fe, DVB_DEVICE_FRONTEND, 0);if (ret) ": "dvb_frontend_detach(). ", "unsigned int msb;unsigned int logentry;unsigned int significand;unsigned int interpolation;if (unlikely(value == 0)) ": "intlog2(u32 value){    returns: log2(value)   2^24  wrong result if value = 0 (log2(0) is undefined) ", "u64 log;if (unlikely(value == 0)) ": "intlog10(u32 value){    returns: log10(value)   2^24  wrong result if value = 0 (log10(0) is undefined) ", "list_for_each_entry(node, &dvbdevfops_list, list_head) ": "dvb_register_device(struct dvb_adapter  adap, struct dvb_device   pdvbdev,const struct dvb_device  template, void  priv,enum dvb_device_type type, int demux_sink_pads){struct dvb_device  dvbdev;struct file_operations  dvbdevfops = NULL;struct dvbdevfops_node  node = NULL,  new_node = NULL;struct device  clsdev;int minor;int id, ret;mutex_lock(&dvbdev_register_lock);id = dvbdev_get_free_id(adap, type);if (id < 0) {mutex_unlock(&dvbdev_register_lock); pdvbdev = NULL;pr_err(\"%s: couldn't find free device id\\n\", __func__);return -ENFILE;} pdvbdev = dvbdev = kzalloc(sizeof( dvbdev), GFP_KERNEL);if (!dvbdev) {mutex_unlock(&dvbdev_register_lock);return -ENOMEM;}    When a device of the same type is probe()d more than once,   the first allocated fops are used. This prevents memory leaks   that can occur when the same device is probe()d repeatedly. ", "return 0;default:return 0;}dvbdev->entity = kzalloc(sizeof(*dvbdev->entity), GFP_KERNEL);if (!dvbdev->entity)return -ENOMEM;dvbdev->entity->name = dvbdev->name;if (npads) ": "dvb_device_get(dvbdev);replace_fops(file, new_fops);if (file->f_op->open)err = file->f_op->open(inode, file);up_read(&minor_rwsem);mutex_unlock(&dvbdev_mutex);return err;}fail:up_read(&minor_rwsem);mutex_unlock(&dvbdev_mutex);return -ENODEV;}static const struct file_operations dvb_device_fops = {.owner =THIS_MODULE,.open =dvb_device_open,.llseek =noop_llseek,};static struct cdev dvb_device_cdev;int dvb_generic_open(struct inode  inode, struct file  file){struct dvb_device  dvbdev = file->private_data;if (!dvbdev)return -ENODEV;if (!dvbdev->users)return -EBUSY;if ((file->f_flags & O_ACCMODE) == O_RDONLY) {if (!dvbdev->readers)return -EBUSY;dvbdev->readers--;} else {if (!dvbdev->writers)return -EBUSY;dvbdev->writers--;}dvbdev->users--;return 0;}EXPORT_SYMBOL(dvb_generic_open);int dvb_generic_release(struct inode  inode, struct file  file){struct dvb_device  dvbdev = file->private_data;if (!dvbdev)return -ENODEV;if ((file->f_flags & O_ACCMODE) == O_RDONLY)dvbdev->readers++;elsedvbdev->writers++;dvbdev->users++;dvb_device_put(dvbdev);return 0;}EXPORT_SYMBOL(dvb_generic_release);long dvb_generic_ioctl(struct file  file,       unsigned int cmd, unsigned long arg){struct dvb_device  dvbdev = file->private_data;if (!dvbdev)return -ENODEV;if (!dvbdev->kernel_ioctl)return -EINVAL;return dvb_usercopy(file, cmd, arg, dvbdev->kernel_ioctl);}EXPORT_SYMBOL(dvb_generic_ioctl);static int dvbdev_get_free_id(struct dvb_adapter  adap, int type){u32 id = 0;while (id < DVB_MAX_IDS) {struct dvb_device  dev;list_for_each_entry(dev, &adap->device_list, list_head)if (dev->type == type && dev->id == id)goto skip;return id;skip:id++;}return -ENFILE;}static void dvb_media_device_free(struct dvb_device  dvbdev){#if defined(CONFIG_MEDIA_CONTROLLER_DVB)if (dvbdev->entity) {media_device_unregister_entity(dvbdev->entity);kfree(dvbdev->entity);kfree(dvbdev->pads);dvbdev->entity = NULL;dvbdev->pads = NULL;}if (dvbdev->tsout_entity) {int i;for (i = 0; i < dvbdev->tsout_num_entities; i++) {media_device_unregister_entity(&dvbdev->tsout_entity[i]);kfree(dvbdev->tsout_entity[i].name);}kfree(dvbdev->tsout_entity);kfree(dvbdev->tsout_pads);dvbdev->tsout_entity = NULL;dvbdev->tsout_pads = NULL;dvbdev->tsout_num_entities = 0;}if (dvbdev->intf_devnode) {media_devnode_remove(dvbdev->intf_devnode);dvbdev->intf_devnode = NULL;}if (dvbdev->adapter->conn) {media_device_unregister_entity(dvbdev->adapter->conn);kfree(dvbdev->adapter->conn);dvbdev->adapter->conn = NULL;kfree(dvbdev->adapter->conn_pads);dvbdev->adapter->conn_pads = NULL;}#endif}#if defined(CONFIG_MEDIA_CONTROLLER_DVB)static int dvb_create_tsout_entity(struct dvb_device  dvbdev,   const char  name, int npads){int i;dvbdev->tsout_pads = kcalloc(npads, sizeof( dvbdev->tsout_pads),     GFP_KERNEL);if (!dvbdev->tsout_pads)return -ENOMEM;dvbdev->tsout_entity = kcalloc(npads, sizeof( dvbdev->tsout_entity),       GFP_KERNEL);if (!dvbdev->tsout_entity)return -ENOMEM;dvbdev->tsout_num_entities = npads;for (i = 0; i < npads; i++) {struct media_pad  pads = &dvbdev->tsout_pads[i];struct media_entity  entity = &dvbdev->tsout_entity[i];int ret;entity->name = kasprintf(GFP_KERNEL, \"%s #%d\", name, i);if (!entity->name)return -ENOMEM;entity->function = MEDIA_ENT_F_IO_DTV;pads->flags = MEDIA_PAD_FL_SINK;ret = media_entity_pads_init(entity, 1, pads);if (ret < 0)return ret;ret = media_device_register_entity(dvbdev->adapter->mdev,   entity);if (ret < 0)return ret;}return 0;}#define DEMUX_TSOUT\"demux-tsout\"#define DVR_TSOUT\"dvr-tsout\"static int dvb_create_media_entity(struct dvb_device  dvbdev,   int type, int demux_sink_pads){int i, ret, npads;switch (type) {case DVB_DEVICE_FRONTEND:npads = 2;break;case DVB_DEVICE_DVR:ret = dvb_create_tsout_entity(dvbdev, DVR_TSOUT,      demux_sink_pads);return ret;case DVB_DEVICE_DEMUX:npads = 1 + demux_sink_pads;ret = dvb_create_tsout_entity(dvbdev, DEMUX_TSOUT,      demux_sink_pads);if (ret < 0)return ret;break;case DVB_DEVICE_CA:npads = 2;break;case DVB_DEVICE_NET:    We should be creating entities for the MPEULE   decapsulation hardware (or software implementation).     However, the number of for the MPEULE decaps may not be   fixed. As we don't have yet dynamic support for PADs at   the Media Controller, let's not create the decap   entities yet. ", "if (dvbdev_check_free_adapter_num(num))break;} else ": "dvb_register_adapter(struct dvb_adapter  adap, const char  name, struct module  module, struct device  device, short  adapter_nums){int i, num;mutex_lock(&dvbdev_register_lock);for (i = 0; i < DVB_MAX_ADAPTERS; ++i) {num = adapter_nums[i];if (num >= 0  &&  num < DVB_MAX_ADAPTERS) {  use the one the driver asked for ", "void dvb_ca_en50221_camchange_irq(struct dvb_ca_en50221 *pubca, int slot,  int change_type)": "dvb_ca_en50221_camchange_irq - A CAMCHANGE IRQ has occurred.     @pubca: CA instance.   @slot: Slot concerned.   @change_type: One of the DVB_CA_CAMCHANGE_  values. ", "void dvb_ca_en50221_camready_irq(struct dvb_ca_en50221 *pubca, int slot)": "dvb_ca_en50221_camready_irq - A CAMREADY IRQ has occurred.     @pubca: CA instance.   @slot: Slot concerned. ", "void dvb_ca_en50221_frda_irq(struct dvb_ca_en50221 *pubca, int slot)": "dvb_ca_en50221_frda_irq - An FR or DA IRQ has occurred.     @pubca: CA instance.   @slot: Slot concerned. ", "int dvb_ca_en50221_init(struct dvb_adapter *dvb_adapter,struct dvb_ca_en50221 *pubca, int flags, int slot_count)": "dvb_ca_en50221_init - Initialise a new DVB CA EN50221 interface device.     @dvb_adapter: DVB adapter to attach the new CA device to.   @pubca: The dvb_ca instance.   @flags: Flags describing the CA device (DVB_CA_FLAG_ ).   @slot_count: Number of slots supported.     return: 0 on success, nonzero on failure ", "void dvb_ca_en50221_release(struct dvb_ca_en50221 *pubca)": "dvb_ca_en50221_release - Release a DVB CA EN50221 interface device.     @pubca: The associated dvb_ca instance. ", "free = READ_ONCE(rbuf->pread) - rbuf->pwrite;if (free <= 0)free += rbuf->size;return free-1;}ssize_t dvb_ringbuffer_avail(struct dvb_ringbuffer *rbuf)": "dvb_ringbuffer_read(),   dvb_ringbuffer_read_user(), dvb_ringbuffer_flush(),   or dvb_ringbuffer_reset() ", "return (rbuf->pread == smp_load_acquire(&rbuf->pwrite));}ssize_t dvb_ringbuffer_free(struct dvb_ringbuffer *rbuf)": "dvb_ringbuffer_write_user(), or dvb_ringbuffer_reset()     for memory barriers also see Documentationcore-apicircular-buffers.rst ", "avail = smp_load_acquire(&rbuf->pwrite) - rbuf->pread;if (avail < 0)avail += rbuf->size;return avail;}void dvb_ringbuffer_flush(struct dvb_ringbuffer *rbuf)": "dvb_ringbuffer_avail(struct dvb_ringbuffer  rbuf){ssize_t avail;  smp_load_acquire() to load write pointer on reader side   this pairs with smp_store_release() in dvb_ringbuffer_write(),   dvb_ringbuffer_write_user(), or dvb_ringbuffer_reset() ", "smp_store_release(&rbuf->pread, 0);}if (copy_to_user(buf, rbuf->data+rbuf->pread, todo))return -EFAULT;/* smp_store_release() to update read pointer, see above ": "dvb_ringbuffer_flush_spinlock_wakeup(struct dvb_ringbuffer  rbuf){unsigned long flags;spin_lock_irqsave(&rbuf->lock, flags);dvb_ringbuffer_flush(rbuf);spin_unlock_irqrestore(&rbuf->lock, flags);wake_up(&rbuf->queue);}ssize_t dvb_ringbuffer_read_user(struct dvb_ringbuffer  rbuf, u8 __user  buf, size_t len){size_t todo = len;size_t split;split = (rbuf->pread + len > rbuf->size) ? rbuf->size - rbuf->pread : 0;if (split > 0) {if (copy_to_user(buf, rbuf->data+rbuf->pread, split))return -EFAULT;buf += split;todo -= split;  smp_store_release() for read pointer update to ensure   that buf is not overwritten until read is complete,   this pairs with READ_ONCE() in dvb_ringbuffer_free() ", "if (sec->secbuf[0] != 0xff || sec->secbuf[n - 1] != 0xff) ": "dvb_dmx_swfilter_payload(struct dvb_demux_feed  feed,   const u8  buf){int count = payload(buf);int p;int ccok;u8 cc;if (count == 0)return -1;p = 188 - count;cc = buf[3] & 0x0f;ccok = ((feed->cc + 1) & 0x0f) == cc;if (!ccok) {set_buf_flags(feed, DMX_BUFFER_FLAG_DISCONTINUITY_DETECTED);dprintk_sect_loss(\"missed packet: %d instead of %d!\\n\",  cc, (feed->cc + 1) & 0x0f);}feed->cc = cc;if (buf[1] & 0x40) PUSI ?feed->peslen = 0xfffa;feed->peslen += count;return feed->cb.ts(&buf[p], count, NULL, 0, &feed->feed.ts,   &feed->buffer_flags);}static int dvb_dmx_swfilter_sectionfilter(struct dvb_demux_feed  feed,  struct dvb_demux_filter  f){u8 neq = 0;int i;for (i = 0; i < DVB_DEMUX_MASK_MAX; i++) {u8 xor = f->filter.filter_value[i] ^ feed->feed.sec.secbuf[i];if (f->maskandmode[i] & xor)return 0;neq |= f->maskandnotmode[i] & xor;}if (f->doneq && !neq)return 0;return feed->cb.sec(feed->feed.sec.secbuf, feed->feed.sec.seclen,    NULL, 0, &f->filter, &feed->buffer_flags);}static inline int dvb_dmx_swfilter_section_feed(struct dvb_demux_feed  feed){struct dvb_demux  demux = feed->demux;struct dvb_demux_filter  f = feed->filter;struct dmx_section_feed  sec = &feed->feed.sec;int section_syntax_indicator;if (!sec->is_filtering)return 0;if (!f)return 0;if (sec->check_crc) {section_syntax_indicator = ((sec->secbuf[1] & 0x80) != 0);if (section_syntax_indicator &&    demux->check_crc32(feed, sec->secbuf, sec->seclen)) {set_buf_flags(feed, DMX_BUFFER_FLAG_HAD_CRC32_DISCARD);return -1;}}do {if (dvb_dmx_swfilter_sectionfilter(feed, f) < 0)return -1;} while ((f = f->next) && sec->is_filtering);sec->seclen = 0;return 0;}static void dvb_dmx_swfilter_section_new(struct dvb_demux_feed  feed){struct dmx_section_feed  sec = &feed->feed.sec;if (sec->secbufp < sec->tsfeedp) {int n = sec->tsfeedp - sec->secbufp;    Section padding is done with 0xff bytes entirely.   Due to speed reasons, we won't check all of them   but just first and last. ", "isac_rme_irq(isac);if (val & 0x40)/* RPF ": "mISDNisac_irq(struct isac_hw  isac, u8 val){if (unlikely(!val))return IRQ_NONE;pr_debug(\"%s: ISAC interrupt %02x\\n\", isac->name, val);if (isac->type & IPAC_TYPE_ISACX) {if (val & ISACX__CIC)isacsx_cic_irq(isac);if (val & ISACX__ICD) {val = ReadISAC(isac, ISACX_ISTAD);pr_debug(\"%s: ISTAD %02x\\n\", isac->name, val);if (val & ISACX_D_XDU) {pr_debug(\"%s: ISAC XDU\\n\", isac->name);#ifdef ERROR_STATISTICisac->dch.err_tx++;#endifisac_retransmit(isac);}if (val & ISACX_D_XMR) {pr_debug(\"%s: ISAC XMR\\n\", isac->name);#ifdef ERROR_STATISTICisac->dch.err_tx++;#endifisac_retransmit(isac);}if (val & ISACX_D_XPR)isac_xpr_irq(isac);if (val & ISACX_D_RFO) {pr_debug(\"%s: ISAC RFO\\n\", isac->name);WriteISAC(isac, ISACX_CMDRD, ISACX_CMDRD_RMC);}if (val & ISACX_D_RME)isacsx_rme_irq(isac);if (val & ISACX_D_RPF)isac_empty_fifo(isac, 0x20);}} else {if (val & 0x80)  RME ", "mISDNisac_irq(isac, istad);}if (ista & (IPAC__ICA | IPAC__EXA))ipac_irq(&ipac->hscx[0], ista);if (ista & (IPAC__ICB | IPAC__EXB))ipac_irq(&ipac->hscx[1], ista);ista = ReadIPAC(ipac, IPAC_ISTA);}} else if (ipac->type & IPAC_TYPE_HSCX) ": "mISDNipac_irq(struct ipac_hw  ipac, int maxloop){int cnt = maxloop + 1;u8 ista, istad;struct isac_hw   isac = &ipac->isac;if (ipac->type & IPAC_TYPE_IPACX) {ista = ReadIPAC(ipac, ISACX_ISTA);while (ista && --cnt) {pr_debug(\"%s: ISTA %02x\\n\", ipac->name, ista);if (ista & IPACX__ICA)ipac_irq(&ipac->hscx[0], ista);if (ista & IPACX__ICB)ipac_irq(&ipac->hscx[1], ista);if (ista & (ISACX__ICD | ISACX__CIC))mISDNisac_irq(&ipac->isac, ista);ista = ReadIPAC(ipac, ISACX_ISTA);}} else if (ipac->type & IPAC_TYPE_IPAC) {ista = ReadIPAC(ipac, IPAC_ISTA);while (ista && --cnt) {pr_debug(\"%s: ISTA %02x\\n\", ipac->name, ista);if (ista & (IPAC__ICD | IPAC__EXD)) {istad = ReadISAC(isac, ISAC_ISTA);pr_debug(\"%s: ISTAD %02x\\n\", ipac->name, istad);if (istad & IPAC_D_TIN2)pr_debug(\"%s TIN2 irq\\n\", ipac->name);if (ista & IPAC__EXD)istad |= 1;   ISAC EXI ", "ipac->hscx[i].slot = (i == 0) ? 0x2f : 0x03;}ipac->init = ipac_init;ipac->release = free_ipac;ret =(1 << (ISDN_P_B_RAW & ISDN_P_B_MASK)) |(1 << (ISDN_P_B_HDLC & ISDN_P_B_MASK));return ret;}EXPORT_SYMBOL(mISDNipac_init": "mISDNipac_init(struct ipac_hw  ipac, void  hw){u32 ret;u8 i;ipac->hw = hw;if (ipac->isac.dch.debug & DEBUG_HW)pr_notice(\"%s: ipac type %x\\n\", ipac->name, ipac->type);if (ipac->type & IPAC_TYPE_HSCX) {ipac->isac.type = IPAC_TYPE_ISAC;ipac->hscx[0].off = 0;ipac->hscx[1].off = 0x40;ipac->hscx[0].fifo_size = 32;ipac->hscx[1].fifo_size = 32;} else if (ipac->type & IPAC_TYPE_IPAC) {ipac->isac.type = IPAC_TYPE_IPAC | IPAC_TYPE_ISAC;ipac->hscx[0].off = 0;ipac->hscx[1].off = 0x40;ipac->hscx[0].fifo_size = 64;ipac->hscx[1].fifo_size = 64;} else if (ipac->type & IPAC_TYPE_IPACX) {ipac->isac.type = IPAC_TYPE_IPACX | IPAC_TYPE_ISACX;ipac->hscx[0].off = IPACX_OFF_ICA;ipac->hscx[1].off = IPACX_OFF_ICB;ipac->hscx[0].fifo_size = 64;ipac->hscx[1].fifo_size = 64;} elsereturn 0;mISDNisac_init(&ipac->isac, hw);ipac->isac.dch.dev.D.ctrl = ipac_dctrl;for (i = 0; i < 2; i++) {ipac->hscx[i].bch.nr = i + 1;set_channelmap(i + 1, ipac->isac.dch.dev.channelmap);list_add(&ipac->hscx[i].bch.ch.list, &ipac->isac.dch.dev.bchannels);mISDN_initbchannel(&ipac->hscx[i].bch, MAX_DATA_MEM,   ipac->hscx[i].fifo_size);ipac->hscx[i].bch.ch.nr = i + 1;ipac->hscx[i].bch.ch.send = &hscx_l2l1;ipac->hscx[i].bch.ch.ctrl = hscx_bctrl;ipac->hscx[i].bch.hw = hw;ipac->hscx[i].ip = ipac;  default values for IOM time slots   can be overwritten by card ", "int isdnhdlc_decode(struct isdnhdlc_vars *hdlc, const u8 *src, int slen,    int *count, u8 *dst, int dsize)": "isdnhdlc_decode - decodes HDLC frames from a transparent bit stream.  The source buffer is scanned for valid HDLC frames looking for  flags (01111110) to indicate the start of a frame. If the start of  the frame is found, the bit stuffing is removed (0 after 5 1's).  When a new flag is found, the complete frame has been received  and the CRC is checked.  If a valid frame is found, the function returns the frame length  excluding the CRC with the bit HDLC_END_OF_FRAME set.  If the beginning of a valid frame is found, the function returns  the length.  If a framing error is found (too many 1s and not a flag) the function  returns the length with the bit HDLC_FRAMING_ERROR set.  If a CRC error is found the function returns the length with the  bit HDLC_CRC_ERROR set.  If the frame length exceeds the destination buffer size, the function  returns the length with the bit HDLC_LENGTH_ERROR set.  src - source buffer  slen - source buffer length  count - number of bytes removed (decoded) from the source buffer  dst _ destination buffer  dsize - destination buffer size  returns - number of decoded bytes in the destination buffer and status  flag.", "int isdnhdlc_encode(struct isdnhdlc_vars *hdlc, const u8 *src, u16 slen,    int *count, u8 *dst, int dsize)": "isdnhdlc_encode - encodes HDLC frames to a transparent bit stream.  The bit stream starts with a beginning flag (01111110). After  that each byte is added to the bit stream with bit stuffing added  (0 after 5 1's).  When the last byte has been removed from the source buffer, the  CRC (2 bytes is added) and the frame terminates with the ending flag.  For the dchannel, the idle character (all 1's) is also added at the end.  If this function is called with empty source buffer (slen=0), flags or  idle character will be generated.  src - source buffer  slen - source buffer length  count - number of bytes removed (encoded) from source buffer  dst _ destination buffer  dsize - destination buffer size  returns - number of encoded bytes in the destination buffer", "void capi_ctr_handle_message(struct capi_ctr *ctr, u16 appl,     struct sk_buff *skb)": "capi_ctr_handle_message() - handle incoming CAPI message   @ctr:controller descriptor structure.   @appl:application ID.   @skb:message.     Called by hardware driver to pass a CAPI message to the application. ", "void capi_ctr_ready(struct capi_ctr *ctr)": "capi_ctr_ready() - signal CAPI controller ready   @ctr:controller descriptor structure.     Called by hardware driver to signal that the controller is up and running. ", "void capi_ctr_down(struct capi_ctr *ctr)": "capi_ctr_down() - signal CAPI controller not ready   @ctr:controller descriptor structure.     Called by hardware driver to signal that the controller is down and   unavailable for use. ", "int attach_capi_ctr(struct capi_ctr *ctr)": "attach_capi_ctr() - register CAPI controller   @ctr:controller descriptor structure.     Called by hardware driver to register a controller with the CAPI subsystem.   Return value: 0 on success, error code < 0 on error ", "int detach_capi_ctr(struct capi_ctr *ctr)": "detach_capi_ctr() - unregister CAPI controller   @ctr:controller descriptor structure.     Called by hardware driver to remove the registration of a controller   with the CAPI subsystem.   Return value: 0 on success, error code < 0 on error ", "cq->p2 = bch->dropcnt;if (cq->p1)test_and_set_bit(FLG_RX_OFF, &bch->Flags);elsetest_and_clear_bit(FLG_RX_OFF, &bch->Flags);bch->dropcnt = 0;break;case MISDN_CTRL_RX_BUFFER:if (cq->p2 > MISDN_CTRL_RX_SIZE_IGNORE)bch->next_maxlen = cq->p2;if (cq->p1 > MISDN_CTRL_RX_SIZE_IGNORE)bch->next_minlen = cq->p1;/* we return the old values ": "mISDN_ctrl_bchannel(struct bchannel  bch, struct mISDN_ctrl_req  cq){int ret = 0;switch (cq->op) {case MISDN_CTRL_GETOP:cq->op = MISDN_CTRL_RX_BUFFER | MISDN_CTRL_FILL_EMPTY | MISDN_CTRL_RX_OFF;break;case MISDN_CTRL_FILL_EMPTY:if (cq->p1) {memset(bch->fill, cq->p2 & 0xff, MISDN_BCH_FILL_SIZE);test_and_set_bit(FLG_FILLEMPTY, &bch->Flags);} else {test_and_clear_bit(FLG_FILLEMPTY, &bch->Flags);}break;case MISDN_CTRL_RX_OFF:  read back dropped byte count ", "dev_kfree_skb(dch->rx_skb);dch->rx_skb = NULL;return;}hh = mISDN_HEAD_P(dch->rx_skb);hh->prim = PH_DATA_IND;hh->id = get_sapi_tei(dch->rx_skb->data);skb_queue_tail(&dch->rqueue, dch->rx_skb);dch->rx_skb = NULL;schedule_event(dch, FLG_RECVQUEUE);}EXPORT_SYMBOL(recv_Dchannel": "recv_Dchannel(struct dchannel  dch){struct mISDNhead  hh;if (dch->rx_skb->len < 2) {   at least 2 for sapi  tei ", "dev_kfree_skb(ech->rx_skb);ech->rx_skb = NULL;return;}hh = mISDN_HEAD_P(ech->rx_skb);hh->prim = PH_DATA_E_IND;hh->id = get_sapi_tei(ech->rx_skb->data);skb_queue_tail(&dch->rqueue, ech->rx_skb);ech->rx_skb = NULL;schedule_event(dch, FLG_RECVQUEUE);}EXPORT_SYMBOL(recv_Echannel": "recv_Echannel(struct dchannel  ech, struct dchannel  dch){struct mISDNhead  hh;if (ech->rx_skb->len < 2) {   at least 2 for sapi  tei ", "if (unlikely(!bch->rx_skb))return;if (unlikely(!bch->rx_skb->len)) ": "recv_Bchannel(struct bchannel  bch, unsigned int id, bool force){struct mISDNhead  hh;  if allocation did fail upper functions still may call us ", "confirm_Bsend(bch);return 1;} else ": "get_next_bframe(struct bchannel  bch){bch->tx_idx = 0;if (test_bit(FLG_TX_NEXT, &bch->Flags)) {bch->tx_skb = bch->next_skb;if (bch->tx_skb) {bch->next_skb = NULL;test_and_clear_bit(FLG_TX_NEXT, &bch->Flags);  confirm imediately to allow next data ", "if (skb->len <= 0) ": "bchannel_senddata(struct bchannel  ch, struct sk_buff  skb){  check oversize ", "recv_Bchannel(bch, 0, true);} else ": "bchannel_get_rxbuf(struct bchannel  bch, int reqlen){int len;if (bch->rx_skb) {len = skb_tailroom(bch->rx_skb);if (len < reqlen) {pr_warn(\"B%d no space for %d (only %d) bytes\\n\",bch->nr, reqlen, len);if (test_bit(FLG_TRANSPARENT, &bch->Flags)) {  send what we have now and try a new buffer ", "#include <linux/slab.h>#include <linux/types.h>#include <linux/stddef.h>#include <linux/spinlock.h>#include <linux/ktime.h>#include <linux/mISDNif.h>#include <linux/export.h>#include \"core.h\"static u_int *debug;static LIST_HEAD(iclock_list);static DEFINE_RWLOCK(iclock_lock);static u16 iclock_count;/* counter of last clock ": "mISDN_clock_get. The signed short value   counts the number of samples since. Time since last clock event is added. ", "device_del(&dev->dev);dev_set_drvdata(&dev->dev, NULL);test_and_clear_bit(dev->id, (u_long *)&device_ids);delete_stack(dev);put_device(&dev->dev);}EXPORT_SYMBOL(mISDN_unregister_device": "mISDN_unregister_device(struct mISDNdevice  dev) {if (debug & DEBUG_CORE)printk(KERN_DEBUG \"mISDN_unregister %s %d\\n\",       dev_name(&dev->dev), dev->id);  sysfs_remove_link(&dev->dev.kobj, \"device\"); ", "void (*apm_get_power_status)(struct apm_power_info *) = __apm_get_power_status;EXPORT_SYMBOL(apm_get_power_status": "apm_get_power_status(struct apm_power_info  info){}    This allows machines to provide their own \"apm get power status\" function. ", "void apm_queue_event(apm_event_t event)": "apm_queue_event - queue an APM event for kapmd   @event: APM event     Queue an APM event for kapmd to process and ultimately take the   appropriate action.  Only a subset of events are handled:     %APM_LOW_BATTERY     %APM_POWER_STATUS_CHANGE     %APM_USER_SUSPEND     %APM_SYS_SUSPEND     %APM_CRITICAL_SUSPEND ", "int misc_register(struct miscdevice *misc)": "misc_register-register a miscellaneous device  @misc: device structure    Register a miscellaneous device with the kernel. If the minor  number is set to %MISC_DYNAMIC_MINOR a minor number is assigned  and placed in the minor field of the structure. For other cases  the minor number requested is used.    The structure passed is linked into the kernel and may not be  destroyed until it has been unregistered. By default, an open()  syscall to the device sets file->private_data to point to the  structure. Drivers don't need open in fops for this.    A zero is returned on success and a negative errno code for  failure. ", "void misc_deregister(struct miscdevice *misc)": "misc_deregister - unregister a miscellaneous device  @misc: device to unregister    Unregister a miscellaneous device that was previously  successfully registered with misc_register(). ", "amp->gpio_dump(amp, m);break;case '\\n':/* end of settings string, do nothing ": "nsc_gpio_write(struct file  file, const char __user  data,       size_t len, loff_t  ppos){unsigned m = iminor(file_inode(file));struct nsc_gpio_ops  amp = file->private_data;struct device  dev = amp->dev;size_t i;int err = 0;for (i = 0; i < len; ++i) {char c;if (get_user(c, data + i))return -EFAULT;switch (c) {case '0':amp->gpio_set(m, 0);break;case '1':amp->gpio_set(m, 1);break;case 'O':dev_dbg(dev, \"GPIO%d output enabled\\n\", m);amp->gpio_config(m, ~1, 1);break;case 'o':dev_dbg(dev, \"GPIO%d output disabled\\n\", m);amp->gpio_config(m, ~1, 0);break;case 'T':dev_dbg(dev, \"GPIO%d output is push pull\\n\", m);amp->gpio_config(m, ~2, 2);break;case 't':dev_dbg(dev, \"GPIO%d output is open drain\\n\", m);amp->gpio_config(m, ~2, 0);break;case 'P':dev_dbg(dev, \"GPIO%d pull up enabled\\n\", m);amp->gpio_config(m, ~4, 4);break;case 'p':dev_dbg(dev, \"GPIO%d pull up disabled\\n\", m);amp->gpio_config(m, ~4, 0);break;case 'v':  View Current pin settings ", "EXPORT_SYMBOL(nsc_gpio_write);EXPORT_SYMBOL(nsc_gpio_read": "nsc_gpio_read(struct file  file, char __user   buf,      size_t len, loff_t   ppos){unsigned m = iminor(file_inode(file));int value;struct nsc_gpio_ops  amp = file->private_data;value = amp->gpio_get(m);if (put_user(value ? '1' : '0', buf))return -EFAULT;return 1;}  common file-ops routines for both scx200_gpio and pc87360_gpio ", "u32 config = amp->gpio_config(index, ~0, 0);/* user requested via 'v' command, so its INFO ": "nsc_gpio_dump(struct nsc_gpio_ops  amp, unsigned index){  retrieve current config wo changing it ", "static void try_to_generate_entropy(void);/* * Wait for the input pool to be seeded and thus guaranteed to supply * cryptographically secure random numbers. This applies to: the /dev/urandom * device, the get_random_bytes function, and the get_random_": "wait_for_random_bytes(), and considered an entropy collector, below. ", "bool rng_is_initialized(void)": "get_random_bytes function, and the get_random_{u8,   u16,u32,u64,long} family of functions.     Returns: true if the input pool has been seeded.            false if the input pool has not been seeded. ", "u32 rand = get_random_u32();u64 mult;/* * This function is technically undefined for ceil == 0, and in fact * for the non-underscored constant version in the header, we build bug * on that. But for the non-constant case, it's convenient to have that * evaluate to being a straight call to get_random_u32(), so that * get_random_u32_inclusive() can work over its whole range without * undefined behavior. ": "__get_random_u32_below(u32 ceil){    This is the slow path for variable ceil. It is still fast, most of   the time, by doing traditional reciprocal multiplication and   opportunistically comparing the lower half to ceil itself, before   falling back to computing a larger bound, and then rejecting samples   whose lower half would indicate a range indivisible by ceil. The use   of `-ceil % ceil` is analogous to `2^32 % ceil`, but is computable   in 32-bits. ", "static bool trust_cpu __initdata = true;static bool trust_bootloader __initdata = true;static int __init parse_trust_cpu(char *arg)": "add_device_randomness(const void  buf, size_t len);  void add_hwgenerator_randomness(const void  buf, size_t len, size_t entropy, bool sleep_after);  void add_bootloader_randomness(const void  buf, size_t len);  void add_vmfork_randomness(const void  unique_vm_id, size_t len);  void add_interrupt_randomness(int irq);  void add_input_randomness(unsigned int type, unsigned int code, unsigned int value);  void add_disk_randomness(struct gendisk  disk);     add_device_randomness() adds data to the input pool that   is likely to differ between two devices (or possibly even per boot).   This would be things like MAC addresses or serial numbers, or the   read-out of the RTC. This does  not  credit any actual entropy to   the pool, but it initializes the pool to different values for devices   that might otherwise be identical and have very little entropy   available to them (particularly common in the embedded world).     add_hwgenerator_randomness() is for true hardware RNGs, and will credit   entropy as specified by the caller. If the entropy pool is full it will   block until more entropy is needed.     add_bootloader_randomness() is called by bootloader drivers, such as EFI   and device tree, and credits its input depending on whether or not the   command line option 'random.trust_bootloader'.     add_vmfork_randomness() adds a unique (but not necessarily secret) ID   representing the current instance of a VM to the pool, without crediting,   and then force-reseeds the crng so that it takes effect immediately.     add_interrupt_randomness() uses the interrupt timing as random   inputs to the entropy pool. Using the cycle counters and the irq source   as inputs, it feeds the input pool roughly once a second or after 64   interrupts, crediting 1 bit of entropy for whichever comes first.     add_input_randomness() uses the input layer interrupt timing, as well   as the event type information from the hardware.     add_disk_randomness() uses what amounts to the seek time of block   layer request events, on a per-disk_devt basis, as input to the   entropy pool. Note that high-speed solid state drives with very low   seek times do not make for good sources of entropy, as their seek   times are usually fairly consistent.     The last two routines try to estimate how many bits of entropy   to credit. They do this by keeping track of the first and second   order deltas of the event timings.                                                                        ", "void agp_free_memory(struct agp_memory *curr)": "agp_free_memory - free memory associated with an agp_memory pointer.    @curr:agp_memory pointer to be freed.    It is the only function that can be called when the backend is not owned  by the caller.  (So it can free memory on client death.) ", "struct agp_memory *agp_allocate_memory(struct agp_bridge_data *bridge,size_t page_count, u32 type)": "agp_generic_free_by_type(curr);return;}if (curr->type != 0) {curr->bridge->driver->free_by_type(curr);return;}if (curr->page_count != 0) {if (curr->bridge->driver->agp_destroy_pages) {curr->bridge->driver->agp_destroy_pages(curr);} else {for (i = 0; i < curr->page_count; i++) {curr->bridge->driver->agp_destroy_page(curr->pages[i],AGP_PAGE_DESTROY_UNMAP);}for (i = 0; i < curr->page_count; i++) {curr->bridge->driver->agp_destroy_page(curr->pages[i],AGP_PAGE_DESTROY_FREE);}}}agp_free_key(curr->key);agp_free_page_array(curr);kfree(curr);}EXPORT_SYMBOL(agp_free_memory);#define ENTRIES_PER_PAGE(PAGE_SIZE  sizeof(unsigned long))    agp_allocate_memory  -  allocate a group of pages of a certain type.    @bridge: an agp_bridge_data struct allocated for the AGP host bridge.  @page_count:size_t argument of the number of pages  @type:u32 argument of the type of memory to be allocated.    Every agp bridge device will allow you to allocate AGP_NORMAL_MEMORY which  maps to physical ram.  Any other type is device dependent.    It returns NULL whenever memory is unavailable. ", "int agp_copy_info(struct agp_bridge_data *bridge, struct agp_kern_info *info)": "agp_copy_info  -  copy bridge state information    @bridge: an agp_bridge_data struct allocated for the AGP host bridge.  @info:agp_kern_info pointer.  The caller should insure that this pointer is valid.    This function copies information about the agp bridge device and the state of  the agp backend into an agp_kern_info pointer. ", "int agp_bind_memory(struct agp_memory *curr, off_t pg_start)": "agp_bind_memory  -  Bind an agp_memory structure into the GATT.    @curr:agp_memory pointer  @pg_start:an offset into the graphics aperture translation table    It returns -EINVAL if the pointer == NULL.  It returns -EBUSY if the area of the table requested is already in use. ", "u32 agp_collect_device_status(struct agp_bridge_data *bridge, u32 requested_mode, u32 bridge_agpstat)": "agp_collect_device_status - determine correct agp_cmd from various agp_stat's   @bridge: an agp_bridge_data struct allocated for the AGP host bridge.   @requested_mode: requested agp_stat from userspace (Typically from X)   @bridge_agpstat: current agp_stat from AGP bridge.     This function will hunt for an AGP graphics card, and try to match   the requested mode to the capabilities of both the bridge and the card. ", "if (bridge->major_version != 0)return;pci_read_config_dword(bridge->dev, bridge->capndx, &ncapid);bridge->major_version = (ncapid >> AGP_MAJOR_VERSION_SHIFT) & 0xf;bridge->minor_version = (ncapid >> AGP_MINOR_VERSION_SHIFT) & 0xf;}EXPORT_SYMBOL(get_agp_version": "get_agp_version(struct agp_bridge_data  bridge){u32 ncapid;  Exit early if already set by errata workarounds. ", "return;bridge_agpstat |= AGPSTAT_AGP_ENABLE;/* Do AGP version specific frobbing. ": "agp_generic_enable(struct agp_bridge_data  bridge, u32 requested_mode){u32 bridge_agpstat, temp;get_agp_version(agp_bridge);dev_info(&agp_bridge->dev->dev, \"AGP %d.%d bridge\\n\", agp_bridge->major_version, agp_bridge->minor_version);pci_read_config_dword(agp_bridge->dev,      agp_bridge->capndx + PCI_AGP_STATUS, &bridge_agpstat);bridge_agpstat = agp_collect_device_status(agp_bridge, requested_mode, bridge_agpstat);if (bridge_agpstat == 0)  Something bad happened. FIXME: Return error code? ", "if (bridge->driver->size_type == LVL2_APER_SIZE)return -EINVAL;table = NULL;i = bridge->aperture_size_idx;temp = bridge->current_size;page_order = num_entries = 0;if (bridge->driver->size_type != FIXED_APER_SIZE) ": "agp_generic_create_gatt_table(struct agp_bridge_data  bridge){char  table;char  table_end;int page_order;int num_entries;int i;void  temp;struct page  page;  The generic routines can't handle 2 level gatt's ", "return -EINVAL;default:page_order = 0;break;}/* Do not worry about freeing memory, because if this is * called, then all agp memory is deallocated and removed * from the table. ": "agp_generic_free_gatt_table(struct agp_bridge_data  bridge){int page_order;char  table,  table_end;void  temp;struct page  page;temp = bridge->current_size;switch (bridge->driver->size_type) {case U8_APER_SIZE:page_order = A_SIZE_8(temp)->page_order;break;case U16_APER_SIZE:page_order = A_SIZE_16(temp)->page_order;break;case U32_APER_SIZE:page_order = A_SIZE_32(temp)->page_order;break;case FIXED_APER_SIZE:page_order = A_SIZE_FIX(temp)->page_order;break;case LVL2_APER_SIZE:  The generic routines can't deal with 2 level gatt's ", "return -EINVAL;default:num_entries = 0;break;}num_entries -= agp_memory_reserved/PAGE_SIZE;if (num_entries < 0) num_entries = 0;if (type != mem->type)return -EINVAL;mask_type = bridge->driver->agp_type_to_mask_type(bridge, type);if (mask_type != 0) ": "agp_generic_insert_memory(struct agp_memory   mem, off_t pg_start, int type){int num_entries;size_t i;off_t j;void  temp;struct agp_bridge_data  bridge;int mask_type;bridge = mem->bridge;if (!bridge)return -EINVAL;if (mem->page_count == 0)return 0;temp = bridge->current_size;switch (bridge->driver->size_type) {case U8_APER_SIZE:num_entries = A_SIZE_8(temp)->num_entries;break;case U16_APER_SIZE:num_entries = A_SIZE_16(temp)->num_entries;break;case U32_APER_SIZE:num_entries = A_SIZE_32(temp)->num_entries;break;case FIXED_APER_SIZE:num_entries = A_SIZE_FIX(temp)->num_entries;break;case LVL2_APER_SIZE:  The generic routines can't deal with 2 level gatt's ", "return -EINVAL;}/* AK: bogus, should encode addresses > 4GB ": "agp_generic_remove_memory(struct agp_memory  mem, off_t pg_start, int type){size_t i;struct agp_bridge_data  bridge;int mask_type, num_entries;bridge = mem->bridge;if (!bridge)return -EINVAL;if (mem->page_count == 0)return 0;if (type != mem->type)return -EINVAL;num_entries = agp_num_entries();if (((pg_start + mem->page_count) > num_entries) ||    ((pg_start + mem->page_count) < pg_start))return -EINVAL;mask_type = bridge->driver->agp_type_to_mask_type(bridge, type);if (mask_type != 0) {  The generic routines know nothing of memory types ", "static int agp_return_size(void)": "agp_generic_alloc_user(page_count, type);if (new)new->bridge = bridge;return new;}if (type != 0) {new = bridge->driver->alloc_by_type(page_count, type);if (new)new->bridge = bridge;return new;}scratch_pages = (page_count + ENTRIES_PER_PAGE - 1)  ENTRIES_PER_PAGE;new = agp_create_memory(scratch_pages);if (new == NULL)return NULL;if (bridge->driver->agp_alloc_pages) {if (bridge->driver->agp_alloc_pages(bridge, new, page_count)) {agp_free_memory(new);return NULL;}new->bridge = bridge;return new;}for (i = 0; i < page_count; i++) {struct page  page = bridge->driver->agp_alloc_page(bridge);if (page == NULL) {agp_free_memory(new);return NULL;}new->pages[i] = page;new->page_count++;}new->bridge = bridge;return new;}EXPORT_SYMBOL(agp_allocate_memory);  End - Generic routines for handling agp_memory structures ", "if (page == NULL)goto out;#ifndef CONFIG_X86map_page_into_agp(page);#endifget_page(page);atomic_inc(&agp_bridge->current_memory_agp);mem->pages[i] = page;mem->page_count++;}#ifdef CONFIG_X86set_pages_array_uc(mem->pages, num_pages);#endifret = 0;out:return ret;}EXPORT_SYMBOL(agp_generic_alloc_pages": "agp_generic_alloc_pages(struct agp_bridge_data  bridge, struct agp_memory  mem, size_t num_pages){struct page   page;int i, ret = -ENOMEM;for (i = 0; i < num_pages; i++) {page = alloc_page(GFP_KERNEL | GFP_DMA32 | __GFP_ZERO);  agp_free_memory() needs gart address ", "if (page == NULL)goto out;#ifndef CONFIG_X86map_page_into_agp(page);#endifget_page(page);atomic_inc(&agp_bridge->current_memory_agp);mem->pages[i] = page;mem->page_count++;}#ifdef CONFIG_X86set_pages_array_uc(mem->pages, num_pages);#endifret = 0;out:return ret;}EXPORT_SYMBOL(agp_generic_alloc_page": "agp_generic_alloc_pages(struct agp_bridge_data  bridge, struct agp_memory  mem, size_t num_pages){struct page   page;int i, ret = -ENOMEM;for (i = 0; i < num_pages; i++) {page = alloc_page(GFP_KERNEL | GFP_DMA32 | __GFP_ZERO);  agp_free_memory() needs gart address ", "void agp_enable(struct agp_bridge_data *bridge, u32 mode)": "agp_enable  -  initialise the agp point-to-point connection.     @bridge: an agp_bridge_data struct allocated for the AGP host bridge.   @mode:agp mode register value to configure with. ", "if (bridge->driver->masks)return addr | bridge->driver->masks[0].mask;elsereturn addr;}EXPORT_SYMBOL(agp_generic_mask_memory": "agp_generic_mask_memory(struct agp_bridge_data  bridge,      dma_addr_t addr, int type){  memory type is ignored in the generic routine ", "pci_write_config_word(agp_bridge->dev, agp_bridge->capndx+AGPAPSIZE, current_size->size_value);/* set gart pointer ": "agp3_generic_configure(void){u32 temp;struct aper_size_info_16  current_size;current_size = A_SIZE_16(agp_bridge->current_size);agp_bridge->gart_bus_addr = pci_bus_address(agp_bridge->dev,    AGP_APERTURE_BAR);  set aperture size ", "struct agp_bridge_data *agp_backend_acquire(struct pci_dev *pdev)": "agp_backend_acquire  -  attempt to acquire an agp backend.  @pdev: the PCI device   ", "void agp_backend_release(struct agp_bridge_data *bridge)": "agp_backend_release  -  release the lock on the agp backend.  @bridge: the AGP backend to release    The caller must insure that the graphics aperture translation table  is read for use by another entity.    (Ensure that all memory it bound is unbound.) ", "struct agp_bridge_data *agp_alloc_bridge(void)": "agp_alloc_bridge and agp_generic_find_bridge need to be updated ", "if (!try_module_get(bridge->driver->owner)) ": "agp_off) {error = -ENODEV;goto err_put_bridge;}if (!bridge->dev) {printk (KERN_DEBUG PFX \"Erk, registering with no pci_dev!\\n\");error = -EINVAL;goto err_put_bridge;}  Grab reference on the chipset driver. ", "if (INTEL_GTT_GEN >= 3)writel(0, intel_private.registers+GFX_FLSH_CNTL);reg = intel_private.registers+I810_PGETBL_CTL;writel(intel_private.PGETBL_save, reg);if (HAS_PGTBL_EN && (readl(reg) & I810_PGETBL_ENABLED) == 0) ": "intel_gmch_enable_gtt(void){u8 __iomem  reg;if (INTEL_GTT_GEN == 2) {u16 gmch_ctrl;pci_read_config_word(intel_private.bridge_dev,     I830_GMCH_CTRL, &gmch_ctrl);gmch_ctrl |= I830_GMCH_ENABLED;pci_write_config_word(intel_private.bridge_dev,      I830_GMCH_CTRL, gmch_ctrl);pci_read_config_word(intel_private.bridge_dev,     I830_GMCH_CTRL, &gmch_ctrl);if ((gmch_ctrl & I830_GMCH_ENABLED) == 0) {dev_err(&intel_private.pcidev->dev,\"failed to enable the GTT: GMCH_CTRL=%x\\n\",gmch_ctrl);return false;}}  On the resume path we may be adjusting the PGTBL value, so   be paranoid and flush all chipset write buffers... ", "for_each_sg(st->sgl, sg, st->nents, i) ": "intel_gmch_gtt_insert_sg_entries(struct sg_table  st,      unsigned int pg_start,      unsigned int flags){struct scatterlist  sg;unsigned int len, m;int i, j;j = pg_start;  sg may merge pages, but we have to separate   per-page addr for GTT ", "if (intel_private.refcount++)return 1;intel_private.bridge_dev = pci_dev_get(bridge_pdev);dev_info(&bridge_pdev->dev, \"Intel %s Chipset\\n\", intel_gtt_chipsets[i].name);if (bridge) ": "intel_gmch_probe(struct pci_dev  bridge_pdev, struct pci_dev  gpu_pdev,     struct agp_bridge_data  bridge){int i, mask;for (i = 0; intel_gtt_chipsets[i].name != NULL; i++) {if (gpu_pdev) {if (gpu_pdev->device ==    intel_gtt_chipsets[i].gmch_chip_id) {intel_private.pcidev = pci_dev_get(gpu_pdev);intel_private.driver =intel_gtt_chipsets[i].gtt_driver;break;}} else if (find_gmch(intel_gtt_chipsets[i].gmch_chip_id)) {intel_private.driver =intel_gtt_chipsets[i].gtt_driver;break;}}if (!intel_private.driver)return 0;#if IS_ENABLED(CONFIG_AGP_INTEL)if (bridge) {if (INTEL_GTT_GEN > 1)return 0;bridge->driver = &intel_fake_agp_driver;bridge->dev_private_data = &intel_private;bridge->dev = bridge_pdev;}#endif    Can be called from the fake agp driver but also directly from   drmi915.ko. Hence we need to check whether everything is set up   already. ", "kobject_put(&unit->cdev->kobj);goto unregister_chrdev;}for (i = 0; i < num_nodes; i++) ": "xillybus_init_chrdev(struct device  dev, const struct file_operations  fops, struct module  owner, void  private_data, unsigned char  idt, unsigned int len, int num_nodes, const char  prefix, bool enumerate){int rc;dev_t mdev;int i;char devname[48];struct device  device;size_t namelen;struct xilly_unit  unit,  u;unit = kzalloc(sizeof( unit), GFP_KERNEL);if (!unit)return -ENOMEM;mutex_lock(&unit_mutex);if (!enumerate)snprintf(unit->name, UNITNAMELEN, \"%s\", prefix);for (i = 0; enumerate; i++) {snprintf(unit->name, UNITNAMELEN, \"%s_%02d\", prefix, i);enumerate = false;list_for_each_entry(u, &unit_list, list_entry)if (!strcmp(unit->name, u->name)) {enumerate = true;break;}}rc = alloc_chrdev_region(&mdev, 0, num_nodes, unit->name);if (rc) {dev_warn(dev, \"Failed to obtain majorminors\");goto fail_obtain;}unit->major = MAJOR(mdev);unit->lowest_minor = MINOR(mdev);unit->num_nodes = num_nodes;unit->private_data = private_data;unit->cdev = cdev_alloc();if (!unit->cdev) {rc = -ENOMEM;goto unregister_chrdev;}unit->cdev->ops = fops;unit->cdev->owner = owner;rc = cdev_add(unit->cdev, MKDEV(unit->major, unit->lowest_minor),      unit->num_nodes);if (rc) {dev_err(dev, \"Failed to add cdev.\\n\");  kobject_put() is normally done by cdev_del() ", "irqreturn_t xillybus_isr(int irq, void *data)": "xillybus_isr assumes the interrupt is allocated exclusively to it,   which is the natural case MSI and several other hardware-oriented   interrupts. Sharing is not allowed. ", "unsigned char bogus_idt[8] = ": "xillybus_endpoint_discovery(struct xilly_endpoint  endpoint){int rc;long t;void  bootstrap_resources;int idtbuffersize = (1 << PAGE_SHIFT);struct device  dev = endpoint->dev;    The bogus IDT is used during bootstrap for allocating the initial   message buffer, and then the message buffer and space for the IDT   itself. The initial message buffer is of a single page's size, but   it's soon replaced with a more modest one (and memory is freed). ", "flush_workqueue(xillybus_wq);}EXPORT_SYMBOL(xillybus_endpoint_remove": "xillybus_endpoint_remove(struct xilly_endpoint  endpoint){xillybus_cleanup_chrdev(endpoint, endpoint->dev);xilly_quiesce(endpoint);    Flushing is done upon endpoint release to prevent access to memory   just about to be released. This makes the quiesce complete. ", "return addr_src_to_str[src];}EXPORT_SYMBOL(ipmi_addr_src_to_str": "ipmi_addr_src_to_str(enum ipmi_addr_src src){if (src >= SI_LAST)src = 0;   Invalid ", "rv = ipmi_init_msghandler();if (rv)return rv;mutex_lock(&smi_watchers_mutex);list_add(&watcher->link, &smi_watchers);index = srcu_read_lock(&ipmi_interfaces_srcu);list_for_each_entry_rcu(intf, &ipmi_interfaces, link,lockdep_is_held(&smi_watchers_mutex)) ": "ipmi_smi_watcher_register(struct ipmi_smi_watcher  watcher){struct ipmi_smi  intf;int index, rv;    Make sure the driver is actually initialized, this handles   problems with initialization order. ", "if (handler == NULL)return -EINVAL;/* * Make sure the driver is actually initialized, this handles * problems with initialization order. ": "ipmi_create_user(unsigned int          if_num,     const struct ipmi_user_hndl  handler,     void                   handler_data,     struct ipmi_user        user){unsigned long flags;struct ipmi_user  new_user;int           rv, index;struct ipmi_smi  intf;    There is no module usecount here, because it's not   required.  Since this can only be used by and called from   other modules, they will implicitly use this module, and   thus this can't be removed unless the other modules are   removed. ", "return -EINVAL;found:if (!intf->handlers->get_smi_info)rv = -ENOTTY;elserv = intf->handlers->get_smi_info(intf->send_info, data);srcu_read_unlock(&ipmi_interfaces_srcu, index);return rv;}EXPORT_SYMBOL(ipmi_get_smi_info": "ipmi_get_smi_info(int if_num, struct ipmi_smi_info  data){int rv, index;struct ipmi_smi  intf;index = srcu_read_lock(&ipmi_interfaces_srcu);list_for_each_entry_rcu(intf, &ipmi_interfaces, link) {if (intf->intf_num == if_num)goto found;}srcu_read_unlock(&ipmi_interfaces_srcu, index);  Not found, return an error ", "synchronize_srcu(&user->release_barrier);return;}rcu_assign_pointer(user->self, NULL);release_ipmi_user(user, i);synchronize_srcu(&user->release_barrier);if (user->handler->shutdown)user->handler->shutdown(user->handler_data);if (user->handler->ipmi_watchdog_pretimeout)smi_remove_watch(intf, IPMI_WATCH_MASK_CHECK_WATCHDOG);if (user->gets_events)atomic_dec(&intf->event_waiters);/* Remove the user from the interface's sequence table. ": "ipmi_destroy_user(struct ipmi_user  user){struct ipmi_smi   intf = user->intf;int              i;unsigned long    flags;struct cmd_rcvr   rcvr;struct cmd_rcvr   rcvrs = NULL;struct module     owner;if (!acquire_ipmi_user(user, &i)) {    The user has already been cleaned up, just make sure   nothing is using it and return. ", "goto out;/* Deliver any queued events. ": "ipmi_set_gets_events(struct ipmi_user  user, bool val){unsigned long        flags;struct ipmi_smi       intf = user->intf;struct ipmi_recv_msg  msg,  msg2;struct list_head     msgs;int index;user = acquire_ipmi_user(user, &index);if (!user)return -ENODEV;INIT_LIST_HEAD(&msgs);spin_lock_irqsave(&intf->events_lock, flags);if (user->gets_events == val)goto out;user->gets_events = val;if (val) {if (atomic_inc_return(&intf->event_waiters) == 1)need_waiter(intf);} else {atomic_dec(&intf->event_waiters);}if (intf->delivering_events)    Another thread is delivering events for this, so   let it handle any new events. ", "if (!is_cmd_rcvr_exclusive(intf, netfn, cmd, chans)) ": "ipmi_register_for_cmd(struct ipmi_user  user,  unsigned char netfn,  unsigned char cmd,  unsigned int  chans){struct ipmi_smi  intf = user->intf;struct cmd_rcvr  rcvr;int rv = 0, index;user = acquire_ipmi_user(user, &index);if (!user)return -ENODEV;rcvr = kmalloc(sizeof( rcvr), GFP_KERNEL);if (!rcvr) {rv = -ENOMEM;goto out_release;}rcvr->cmd = cmd;rcvr->netfn = netfn;rcvr->chans = chans;rcvr->user = user;mutex_lock(&intf->cmd_rcvrs_mutex);  Make sure the commandnetfn is not already registered. ", "static int __bmc_get_device_id(struct ipmi_smi *intf, struct bmc_device *bmc,       struct ipmi_device_id *id,       bool *guid_set, guid_t *guid, int intf_num)": "ipmi_add_smi()),   this will always return good data; ", "device_remove_file(intf->si_dev, &intf->nr_msgs_devattr);device_remove_file(intf->si_dev, &intf->nr_users_devattr);/* * Call all the watcher interfaces to tell them that * an interface is going away. ": "ipmi_unregister_smi(struct ipmi_smi  intf){struct ipmi_smi_watcher  w;int intf_num, index;if (!intf)return;intf_num = intf->intf_num;mutex_lock(&ipmi_interfaces_mutex);intf->intf_num = -1;intf->in_shutdown = true;list_del_rcu(&intf->link);mutex_unlock(&ipmi_interfaces_mutex);synchronize_srcu(&ipmi_interfaces_srcu);  At this point no users can be added to the interface. ", "int run_to_completion = intf->run_to_completion;/* * To preserve message order, we keep a queue and deliver from * a tasklet. ": "ipmi_smi_msg_received(struct ipmi_smi  intf,   struct ipmi_smi_msg  msg){unsigned long flags = 0;   keep us warning-free. ", "kref_get(&user->refcount);recv_msg->msgid = msgid;/* * Store the message to send in the receive message so timeout * responses can get the proper response data. ": "ipmi_alloc_smi_msg();if (smi_msg == NULL) {if (!supplied_recv)ipmi_free_recv_msg(recv_msg);rv = -ENOMEM;goto out;}}rcu_read_lock();if (intf->in_shutdown) {rv = -ENODEV;goto out_err;}recv_msg->user = user;if (user)  The put happens when the message is freed. ", "mutex_lock(&intf->cmd_rcvrs_mutex);INIT_LIST_HEAD(&list);list_splice_init_rcu(&intf->cmd_rcvrs, &list, synchronize_rcu);mutex_unlock(&intf->cmd_rcvrs_mutex);list_for_each_entry_safe(rcvr, rcvr2, &list, link)kfree(rcvr);for (i = 0; i < IPMI_IPMB_NUM_SEQ; i++) ": "ipmi_free_recv_msg(msg);}}static void free_smi_msg_list(struct list_head  q){struct ipmi_smi_msg  msg,  msg2;list_for_each_entry_safe(msg, msg2, q, link) {list_del(&msg->link);ipmi_free_smi_msg(msg);}}static void clean_up_interface_data(struct ipmi_smi  intf){int              i;struct cmd_rcvr   rcvr,  rcvr2;struct list_head list;tasklet_kill(&intf->recv_tasklet);free_smi_msg_list(&intf->waiting_rcv_msgs);free_recv_msg_list(&intf->waiting_events);    Wholesale remove all the entries from the list in the   interface and wait for RCU to know that none are in use. ", "pdev = platform_device_alloc(name, inst);if (!pdev) ": "ipmi_platform_add(const char  name, unsigned int inst,  struct ipmi_plat_data  p){struct platform_device  pdev;unsigned int num_r = 1, size = 0, pidx = 0;struct resource r[4];struct property_entry pr[6];u32 flags;int rv;memset(pr, 0, sizeof(pr));memset(r, 0, sizeof(r));if (p->iftype == IPMI_PLAT_IF_SI) {if (p->type == SI_BT)size = 3;else if (p->type != SI_TYPE_INVALID)size = 2;if (p->regsize == 0)p->regsize = DEFAULT_REGSIZE;if (p->regspacing == 0)p->regspacing = p->regsize;pr[pidx++] = PROPERTY_ENTRY_U8(\"ipmi-type\", p->type);} else if (p->iftype == IPMI_PLAT_IF_SSIF) {pr[pidx++] = PROPERTY_ENTRY_U16(\"i2c-addr\", p->addr);}if (p->slave_addr)pr[pidx++] = PROPERTY_ENTRY_U8(\"slave-addr\", p->slave_addr);pr[pidx++] = PROPERTY_ENTRY_U8(\"addr-source\", p->addr_source);if (p->regshift)pr[pidx++] = PROPERTY_ENTRY_U8(\"reg-shift\", p->regshift);pr[pidx++] = PROPERTY_ENTRY_U8(\"reg-size\", p->regsize);  Last entry must be left NULL to terminate it. ", "tpm_dev->io_lpcpd = devm_gpiod_get_optional(dev, \"lpcpd\",    GPIOD_OUT_HIGH);ret = PTR_ERR_OR_ZERO(tpm_dev->io_lpcpd);if (ret) ": "st33zp24_probe(void  phy_id, const struct st33zp24_phy_ops  ops,   struct device  dev, int irq){int ret;u8 intmask = 0;struct tpm_chip  chip;struct st33zp24_dev  tpm_dev;chip = tpmm_chip_alloc(dev, &st33zp24_tpm);if (IS_ERR(chip))return PTR_ERR(chip);tpm_dev = devm_kzalloc(dev, sizeof(struct st33zp24_dev),       GFP_KERNEL);if (!tpm_dev)return -ENOMEM;tpm_dev->phy_id = phy_id;tpm_dev->ops = ops;dev_set_drvdata(&chip->dev, tpm_dev);chip->timeout_a = msecs_to_jiffies(TIS_SHORT_TIMEOUT);chip->timeout_b = msecs_to_jiffies(TIS_LONG_TIMEOUT);chip->timeout_c = msecs_to_jiffies(TIS_SHORT_TIMEOUT);chip->timeout_d = msecs_to_jiffies(TIS_SHORT_TIMEOUT);tpm_dev->locality = LOCALITY0;if (ACPI_COMPANION(dev)) {ret = devm_acpi_dev_add_driver_gpios(dev, acpi_st33zp24_gpios);if (ret)return ret;}    Get LPCPD GPIO. If lpcpd pin is not specified. This is not an   issue as power management can be also managed by TPM specific   commands. ", "writeb(address, bp->i2c_control_regs + 0x1);writeb(I2C_PCF_START, bp->i2c_control_regs + 0x0);if (wait_for_pin(bp, &status))goto out;/* Set PIN back to one so the device sends the first * byte. ": "bbc_i2c_readb(struct bbc_i2c_client  client, unsigned char  byte, int off){struct bbc_i2c_bus  bp = client->bp;unsigned char address = client->address, status;int ret = -1;if (bp->i2c_bussel_reg != NULL)writeb(client->bus, bp->i2c_bussel_reg);writeb(address, bp->i2c_control_regs + 0x1);writeb(I2C_PCF_START, bp->i2c_control_regs + 0x0);if (wait_for_pin(bp, &status))goto out;writeb(off, bp->i2c_control_regs + 0x1);if (wait_for_pin(bp, &status) ||    (status & I2C_PCF_LRB) != 0)goto out;writeb(I2C_PCF_STOP, bp->i2c_control_regs + 0x0);address |= 0x1;   READ ", "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt#include <linux/platform_device.h>#include <linux/dma-mapping.h>#include <linux/init.h>#include <linux/module.h>#include <linux/mm.h>#include <linux/device.h>#include <linux/dmaengine.h>#include <linux/hardirq.h>#include <linux/spinlock.h>#include <linux/percpu.h>#include <linux/rcupdate.h>#include <linux/mutex.h>#include <linux/jiffies.h>#include <linux/rculist.h>#include <linux/idr.h>#include <linux/slab.h>#include <linux/acpi.h>#include <linux/acpi_dma.h>#include <linux/of_dma.h>#include <linux/mempool.h>#include <linux/numa.h>#include \"dmaengine.h\"static DEFINE_MUTEX(dma_list_mutex);static DEFINE_IDA(dma_ida);static LIST_HEAD(dma_device_list);static long dmaengine_ref_count;/* --- debugfs implementation --- ": "dmaengine_get() followed   by dma_find_channel(), or if it has need for an exclusive channel it can call   dma_request_channel().  Once a channel is allocated a reference is taken   against its corresponding driver to disable removal.     Each device has a channels list, which runs unlocked but is never modified   once the device is registered, it's just setup by the driver.     See Documentationdriver-apidmaengine for more details ", "void dma_issue_pending_all(void)": "dma_issue_pending_all - flush all pending operations across all channels ", "void dmaengine_put(void)": "dmaengine_put - let DMA drivers be removed when ref_count == 0 ", "int dma_async_device_register(struct dma_device *device)": "dma_async_device_unregister() is called and no further references are taken. ", "int dmaenginem_async_device_register(struct dma_device *device)": "dmaenginem_async_device_register - registers DMA devices found   @device:pointer to &struct dma_device     The operation is managed and will be undone on driver detach. ", "iowrite32(EDMA_CR_ERGA | EDMA_CR_ERCA, regs->cr);return 0;}static int mcf_edma_remove(struct platform_device *pdev)": "mcf_edma_filter_fn;mcf_edma->dma_dev.filter.map = pdata->slave_map;mcf_edma->dma_dev.filter.mapcnt = pdata->slavecnt;platform_set_drvdata(pdev, mcf_edma);ret = dma_async_device_register(&mcf_edma->dma_dev);if (ret) {dev_err(&pdev->dev,\"Can't register Freescale eDMA engine. (%d)\\n\", ret);return ret;}  Enable round robin arbitration ", "void xdma_disable_user_irq(struct platform_device *pdev, u32 irq_num)": "xdma_disable_user_irq - Disable user interrupt   @pdev: Pointer to the platform_device structure   @irq_num: System IRQ number ", "int xdma_enable_user_irq(struct platform_device *pdev, u32 irq_num)": "xdma_enable_user_irq - Enable user logic interrupt   @pdev: Pointer to the platform_device structure   @irq_num: System IRQ number ", "int xdma_get_user_irq(struct platform_device *pdev, u32 user_irq_index)": "xdma_get_user_irq - Get system IRQ number   @pdev: Pointer to the platform_device structure   @user_irq_index: User logic IRQ wire index     Return: The system IRQ number allocated for the given wire index. ", "int xilinx_vdma_channel_set_config(struct dma_chan *dchan,struct xilinx_vdma_config *cfg)": "xilinx_vdma_channel_set_config - Configure VDMA channel   Run-time configuration for Axi VDMA, supports:   . halt the channel   . configure interrupt coalescing and inter-packet delay threshold   . startstop parking   . enable genlock     @dchan: DMA channel   @cfg: VDMA device configuration pointer     Return: '0' on success and failure value on error ", "if (chan->device->device_alloc_chan_resources !=    shdma_alloc_chan_resources)return false;schan = to_shdma_chan(chan);sdev = to_shdma_dev(chan->device);/* * For DT, the schan->slave_id field is generated by the * set_slave function from the slave ID that is passed in * from xlate. For the non-DT case, the slave ID is * directly passed into the filter function by the driver ": "shdma_chan_filter(struct dma_chan  chan, void  arg){struct shdma_chan  schan;struct shdma_dev  sdev;int slave_id = (long)arg;int ret;  Only support channels handled by this driver. ", "shdma_for_each_chan(schan, sdev, i) ": "shdma_reset(struct shdma_dev  sdev){const struct shdma_ops  ops = sdev->ops;struct shdma_chan  schan;unsigned int handled = 0;int i;  Reset all channels ", "schan->dma_chan.device = &sdev->dma_dev;dma_cookie_init(&schan->dma_chan);schan->dev = sdev->dma_dev.dev;schan->id = id;if (!schan->max_xfer_len)schan->max_xfer_len = PAGE_SIZE;spin_lock_init(&schan->chan_lock);/* Init descripter manage list ": "shdma_chan_probe(struct shdma_dev  sdev,   struct shdma_chan  schan, int id){schan->pm_state = SHDMA_PM_ESTABLISHED;  reference struct dma_device ", "if (!sdev->ops ||    !sdev->desc_size ||    !sdev->ops->embedded_desc ||    !sdev->ops->start_xfer ||    !sdev->ops->setup_xfer ||    !sdev->ops->set_slave ||    !sdev->ops->desc_setup ||    !sdev->ops->slave_addr ||    !sdev->ops->channel_busy ||    !sdev->ops->halt_channel ||    !sdev->ops->desc_completed)return -EINVAL;sdev->schan = kcalloc(chan_num, sizeof(*sdev->schan), GFP_KERNEL);if (!sdev->schan)return -ENOMEM;INIT_LIST_HEAD(&dma_dev->channels);/* Common and MEMCPY operations ": "shdma_init(struct device  dev, struct shdma_dev  sdev,    int chan_num){struct dma_device  dma_dev = &sdev->dma_dev;    Require all call-backs for now, they can trivially be made optional   later as required ", "const struct pci_device_id *pci_match_id(const struct pci_device_id *ids, struct pci_dev *dev)": "pci_match_id - See if a PCI device matches a given pci_id table   @ids: array of PCI device ID structures to search in   @dev: the PCI device structure to match against.     Used by a driver to check whether a PCI device is in its list of   supported devices.  Returns the matching pci_device_id structure or   %NULL if there is no match.     Deprecated; don't use this as it will not catch any dynamic IDs   that a driver might want to check for. ", "int __pci_register_driver(struct pci_driver *drv, struct module *owner,  const char *mod_name)": "__pci_register_driver - register a new pci driver   @drv: the driver structure to register   @owner: owner module of drv   @mod_name: module name string     Adds the driver structure to the list of registered drivers.   Returns a negative value on error, otherwise 0.   If no error occurred, the driver remains registered even if   no device was claimed during registration. ", "void pci_unregister_driver(struct pci_driver *drv)": "pci_unregister_driver - unregister a pci driver   @drv: the driver structure to unregister     Deletes the driver structure from the list of registered PCI drivers,   gives it a chance to clean up by calling its remove() function for   each device it was responsible for, and marks those devices as   driverless. ", "struct pci_driver *pci_dev_driver(const struct pci_dev *dev)": "pci_dev_driver - get the pci_driver of a device   @dev: the device to query     Returns the appropriate pci_driver structure or %NULL if there is no   registered driver for the device. ", "pm_runtime_put_sync(dev);/* * If the device is still on, set the power state as \"unknown\", * since it might change by the next time we load the driver. ": "pci_dev_put(pci_dev);}return error;}static void pci_device_remove(struct device  dev){struct pci_dev  pci_dev = to_pci_dev(dev);struct pci_driver  drv = pci_dev->driver;if (drv->remove) {pm_runtime_get_sync(dev);drv->remove(pci_dev);pm_runtime_put_noidle(dev);}pcibios_free_irq(pci_dev);pci_dev->driver = NULL;pci_iov_remove(pci_dev);  Undo the runtime PM settings in local_pci_probe() ", "return driver_register(&drv->driver);}EXPORT_SYMBOL(__pci_register_driver);/** * pci_unregister_driver - unregister a pci driver * @drv: the driver structure to unregister * * Deletes the driver structure from the list of registered PCI drivers, * gives it a chance to clean up by calling its remove() function for * each device it was responsible for, and marks those devices as * driverless. ": "pci_bus_type;drv->driver.owner = owner;drv->driver.mod_name = mod_name;drv->driver.groups = drv->groups;drv->driver.dev_groups = drv->dev_groups;spin_lock_init(&drv->dynids.lock);INIT_LIST_HEAD(&drv->dynids.list);  register with core ", "return;}pci_do_fixups(dev, start, end);}EXPORT_SYMBOL(pci_fixup_device": "pci_fixup_device(enum pci_fixup_pass pass, struct pci_dev  dev){struct pci_fixup  start,  end;switch (pass) {case pci_fixup_early:start = __start_pci_fixups_early;end = __end_pci_fixups_early;break;case pci_fixup_header:start = __start_pci_fixups_header;end = __end_pci_fixups_header;break;case pci_fixup_final:if (!pci_apply_fixup_final_quirks)return;start = __start_pci_fixups_final;end = __end_pci_fixups_final;break;case pci_fixup_enable:start = __start_pci_fixups_enable;end = __end_pci_fixups_enable;break;case pci_fixup_resume:start = __start_pci_fixups_resume;end = __end_pci_fixups_resume;break;case pci_fixup_resume_early:start = __start_pci_fixups_resume_early;end = __end_pci_fixups_resume_early;break;case pci_fixup_suspend:start = __start_pci_fixups_suspend;end = __end_pci_fixups_suspend;break;case pci_fixup_suspend_late:start = __start_pci_fixups_suspend_late;end = __end_pci_fixups_suspend_late;break;default:  stupid compiler warning, you would think with an enum... ", "if (res->flags & IORESOURCE_ROM_SHADOW)return 0;root = pci_find_parent_resource(dev, res);if (!root) ": "pci_claim_resource(struct pci_dev  dev, int resource){struct resource  res = &dev->resource[resource];struct resource  root,  conflict;if (res->flags & IORESOURCE_UNSET) {pci_info(dev, \"can't claim BAR %d %pR: no address assigned\\n\", resource, res);return -EINVAL;}    If we have a shadow copy in RAM, the PCI device doesn't respond   to the shadow range, so we don't need to claim it, and upstream   bridges don't need to route the range to the device. ", "ret = pci_bus_alloc_resource(bus, res, size, align, min,     IORESOURCE_PREFETCH | IORESOURCE_MEM_64,     pcibios_align_resource, dev);if (ret == 0)return 0;/* * If the prefetchable window is only 32 bits wide, we can put * 64-bit prefetchable resources in it. ": "pci_assign_resource(struct pci_bus  bus, struct pci_dev  dev,int resno, resource_size_t size, resource_size_t align){struct resource  res = dev->resource + resno;resource_size_t min;int ret;min = (res->flags & IORESOURCE_IO) ? PCIBIOS_MIN_IO : PCIBIOS_MIN_MEM;    First, try exact prefetching match.  Even if a 64-bit   prefetchable bridge window is below 4GB, we can't put a 32-bit   prefetchable resource in it because pbus_size_mem() assumes a   64-bit window will contain no 32-bit resources.  If we assign   things differently than they were sized, not everything will fit. ", "host = pci_find_host_bridge(dev->bus);if (host->preserve_config)return -ENOTSUPP;/* Make sure the resource isn't assigned before resizing it. ": "pci_resize_resource(struct pci_dev  dev, int resno, int size){struct resource  res = dev->resource + resno;struct pci_host_bridge  host;int old, ret;u32 sizes;u16 cmd;  Check if we must preserve the firmware's resource assignment ", "static int pci_scan_bridge_extend(struct pci_bus *bus, struct pci_dev *dev,  int max, unsigned int available_buses,  int pass)": "pci_scan_bridge_extend() - Scan buses behind a bridge   @bus: Parent bus the bridge is on   @dev: Bridge itself   @max: Starting subordinate number of buses behind this bridge   @available_buses: Total number of buses available for this bridge and       the devices below. After the minimal bus space has       been allocated the remaining buses will be       distributed equally between hotplug-capable bridges.   @pass: Either %0 (scan already configured bridges) or %1 (scan bridges          that need to be reconfigured.     If it's a bridge, configure it and scan the bus behind it.   For CardBus bridges, we don't scan behind as the devices will   be handled by the bridge driver itself.     We need to process bridges in two passes -- first we scan those   already configured by the BIOS and after we are done with all of   them, we proceed to assigning numbers to the remaining buses in   order to avoid overlaps between old and new bus numbers.     Return: New subordinate number covering all buses behind this bridge. ", "bool pcie_relaxed_ordering_enabled(struct pci_dev *dev)": "pcie_relaxed_ordering_enabled - Probe for PCIe relaxed ordering enable   @dev: PCI device to query     Returns true if the device has enabled relaxed ordering attribute. ", "if (bridge && bridge->vendor == PCI_VENDOR_ID_IDT &&    bridge->device == 0x80b5)return pci_idt_bus_quirk(bus, devfn, l, timeout);#endifreturn pci_bus_generic_read_dev_vendor_id(bus, devfn, l, timeout);}EXPORT_SYMBOL(pci_bus_read_dev_vendor_id": "pci_bus_read_dev_vendor_id(struct pci_bus  bus, int devfn, u32  l,int timeout){#ifdef CONFIG_PCI_QUIRKSstruct pci_dev  bridge = bus->self;    Certain IDT switches have an issue where they improperly trigger   ACS Source Validation errors on completions for config reads. ", "int pci_scan_slot(struct pci_bus *bus, int devfn)": "pci_scan_slot - Scan a PCI slot on a bus for devices   @bus: PCI bus to scan   @devfn: slot number to scan (must have zero function)     Scan a PCI slot on the specified PCI bus for devices, adding   discovered devices to the @bus->devices list.  New devices   will not have is_added set.     Returns the number of new devices found. ", "if (pci_has_flag(PCI_PROBE_ONLY)) ": "pci_scan_root_bus_bridge(bridge);if (ret < 0) {dev_err(bridge->dev.parent, \"Scanning root bridge failed\");return ret;}bus = bridge->bus;    We insert PCI resources into the iomem_resource and   ioport_resource trees in either pci_bus_claim_resources()   or pci_bus_assign_resources(). ", "if (pci_read_vpd_any(dev, off + 1, 2, &header[1]) != 2) ": "pci_read_vpd_any(dev, off, 1, header) == 1) {size = 0;if (off == 0 && (header[0] == 0x00 || header[0] == 0xff))goto error;if (header[0] & PCI_VPD_LRDT) {  Large Resource Data Type Tag ", "while (i + PCI_VPD_LRDT_TAG_SIZE <= len && buf[i] & PCI_VPD_LRDT) ": "pci_write_vpd(dev, off, count, buf);}static BIN_ATTR(vpd, 0600, vpd_read, vpd_write, 0);static struct bin_attribute  vpd_attrs[] = {&bin_attr_vpd,NULL,};static umode_t vpd_attr_is_visible(struct kobject  kobj,   struct bin_attribute  a, int n){struct pci_dev  pdev = to_pci_dev(kobj_to_dev(kobj));if (!pdev->vpd.cap)return 0;return a->attr.mode;}const struct attribute_group pci_dev_vpd_attr_group = {.bin_attrs = vpd_attrs,.is_bin_visible = vpd_attr_is_visible,};void  pci_vpd_alloc(struct pci_dev  dev, unsigned int  size){unsigned int len;void  buf;int cnt;if (!pci_vpd_available(dev, true))return ERR_PTR(-ENODEV);len = dev->vpd.len;buf = kmalloc(len, GFP_KERNEL);if (!buf)return ERR_PTR(-ENOMEM);cnt = pci_read_vpd(dev, 0, len, buf);if (cnt != len) {kfree(buf);return ERR_PTR(-EIO);}if (size) size = len;return buf;}EXPORT_SYMBOL_GPL(pci_vpd_alloc);static int pci_vpd_find_tag(const u8  buf, unsigned int len, u8 rdt, unsigned int  size){int i = 0;  look for LRDT tags only, end tag is the only SRDT tag ", "#if !defined(CONFIG_VGA_CONSOLE)int vga_remove_vgacon(struct pci_dev *pdev)": "vga_remove_vgacon - deactivete vga console     Unbind and unregister vgacon in case pdev is the default vga   device.  Can be called by gpu drivers on initialization to make   sure vga register access done by vgacon will not disturb the   device.     @pdev: pci device. ", "struct pci_dev *vga_default_device(void)": "vga_get()... ", "if ((rsrc & VGA_RSRC_NORMAL_IO) && vgadev->io_norm_cnt > 0) ": "vga_put(struct vga_device  vgadev, unsigned int rsrc){struct device  dev = &vgadev->pdev->dev;unsigned int old_locks = vgadev->locks;vgaarb_dbg(dev, \"%s\\n\", __func__);  Update our counters, and account for equivalent legacy resources   if we decode them ", "if (userspace && vgadev->set_decode)goto bail;/* update the device decodes + counter ": "vga_set_legacy_decoding(struct pci_dev  pdev,      unsigned int decodes,      bool userspace){struct vga_device  vgadev;unsigned long flags;decodes &= VGA_RSRC_LEGACY_MASK;spin_lock_irqsave(&vga_lock, flags);vgadev = vgadev_find(pdev);if (vgadev == NULL)goto bail;  don't let userspace futz with kernel driver decodes ", "int vga_client_register(struct pci_dev *pdev,unsigned int (*set_decode)(struct pci_dev *pdev, bool decode))": "vga_client_register - register or unregister a VGA arbitration client   @pdev: pci device of the VGA client   @set_decode: vga decode change callback     Clients have two callback mechanisms they can use.     @set_decode callback: If a client can disable its GPU VGA resource, it   will get a callback from this to set the encodedecode state.     Rationale: we cannot disable VGA decode resources unconditionally some single   GPU laptops seem to require ACPI or BIOS access to the VGA registers to   control things like backlights etc.  Hopefully newer multi-GPU laptops do   something saner, and desktops won't have any special ACPI for this. The   driver will get a callback when VGA arbitration is first used by userspace   since some older X servers have issues.     This function does not check whether a client for @pdev has been registered   already.     To unregister just call vga_client_unregister().     Returns: 0 on success, -1 on failure ", "int pci_bus_alloc_resource(struct pci_bus *bus, struct resource *res,resource_size_t size, resource_size_t align,resource_size_t min, unsigned long type_mask,resource_size_t (*alignf)(void *,  const struct resource *,  resource_size_t,  resource_size_t),void *alignf_data)": "pci_bus_alloc_resource - allocate a resource from a parent bus   @bus: PCI bus   @res: resource to allocate   @size: size of resource to allocate   @align: alignment of resource to allocate   @min: minimum prociomem address to allocate   @type_mask: IORESOURCE_  type flags   @alignf: resource alignment function   @alignf_data: data argument for resource alignment function     Given the PCI bus a device resides on, the size, minimum address,   alignment and type, try to find an acceptable resource allocation   for a specific device resource. ", "void pci_bus_add_devices(const struct pci_bus *bus)": "pci_bus_add_devices - start driver for PCI devices   @bus: bus to check for new devices     Start driver for PCI devices and add some sysfs entries. ", "void __iomem *pci_map_rom(struct pci_dev *pdev, size_t *size)": "pci_map_rom - map a PCI ROM to kernel space   @pdev: pointer to pci device struct   @size: pointer to receive size of pci window over ROM     Return: kernel virtual pointer to image of ROM     Map a PCI ROM into kernel space. If ROM is boot video ROM,   the shadow BIOS copy will be returned instead of the   actual ROM. ", "void pci_unmap_rom(struct pci_dev *pdev, void __iomem *rom)": "pci_unmap_rom - unmap the ROM from kernel space   @pdev: pointer to pci device struct   @rom: virtual address of the previous mapping     Remove a mapping of a previously mapped ROM ", "struct pci_bus *pci_find_bus(int domain, int busnr)": "pci_find_bus - locate PCI bus from a given domain and bus number   @domain: number of PCI domain to search   @busnr: number of desired PCI bus     Given a PCI bus number and domain number, the desired PCI bus is located   in the global list of PCI buses.  If the bus is found, a pointer to its   data structure is returned.  If no bus is found, %NULL is returned. ", "struct pci_bus *pci_find_next_bus(const struct pci_bus *from)": "pci_find_next_bus(bus)) != NULL)  {if (pci_domain_nr(bus) != domain)continue;tmp_bus = pci_do_find_bus(bus, busnr);if (tmp_bus)return tmp_bus;}return NULL;}EXPORT_SYMBOL(pci_find_bus);     pci_find_next_bus - begin or continue searching for a PCI bus   @from: Previous PCI bus found, or %NULL for new search.     Iterates through the list of known PCI buses.  A new search is   initiated by passing %NULL as the @from argument.  Otherwise if   @from is not %NULL, searches continue from next device on the   global list. ", "struct pci_dev *pci_get_slot(struct pci_bus *bus, unsigned int devfn)": "pci_get_slot - locate PCI device for a given PCI slot   @bus: PCI bus on which desired PCI device resides   @devfn: encodes number of PCI slot in which the desired PCI   device resides and the logical device number within that slot   in case of multi-function devices.     Given a PCI bus and slotfunction number, the desired PCI device   is located in the list of PCI devices.   If the device is found, its reference count is increased and this   function returns a pointer to its data structure.  The caller must   decrement the reference count by calling pci_dev_put().   If no device is found, %NULL is returned. ", "struct pci_dev *pci_get_domain_bus_and_slot(int domain, unsigned int bus,    unsigned int devfn)": "pci_get_domain_bus_and_slot - locate PCI device for a given PCI domain (segment), bus, and slot   @domain: PCI domainsegment on which the PCI device resides.   @bus: PCI bus on which desired PCI device resides   @devfn: encodes number of PCI slot in which the desired PCI device   resides and the logical device number within that slot in case of   multi-function devices.     Given a PCI domain, bus, and slotfunction number, the desired PCI   device is located in the list of PCI devices. If the device is   found, its reference count is increased and this function returns a   pointer to its data structure.  The caller must decrement the   reference count by calling pci_dev_put().  If no device is found,   %NULL is returned. ", "struct pci_dev *pci_get_subsys(unsigned int vendor, unsigned int device,       unsigned int ss_vendor, unsigned int ss_device,       struct pci_dev *from)": "pci_get_subsys - begin or continue searching for a PCI device by vendorsubvendordevicesubdevice id   @vendor: PCI vendor id to match, or %PCI_ANY_ID to match all vendor ids   @device: PCI device id to match, or %PCI_ANY_ID to match all device ids   @ss_vendor: PCI subsystem vendor id to match, or %PCI_ANY_ID to match all vendor ids   @ss_device: PCI subsystem device id to match, or %PCI_ANY_ID to match all device ids   @from: Previous PCI device found in search, or %NULL for new search.     Iterates through the list of known PCI devices.  If a PCI device is found   with a matching @vendor, @device, @ss_vendor and @ss_device, a pointer to its   device structure is returned, and the reference count to the device is   incremented.  Otherwise, %NULL is returned.  A new search is initiated by   passing %NULL as the @from argument.  Otherwise if @from is not %NULL,   searches continue from next device on the global list.   The reference count for @from is always decremented if it is not %NULL. ", "struct pci_dev *pci_get_device(unsigned int vendor, unsigned int device,       struct pci_dev *from)": "pci_get_device - begin or continue searching for a PCI device by vendordevice id   @vendor: PCI vendor id to match, or %PCI_ANY_ID to match all vendor ids   @device: PCI device id to match, or %PCI_ANY_ID to match all device ids   @from: Previous PCI device found in search, or %NULL for new search.     Iterates through the list of known PCI devices.  If a PCI device is   found with a matching @vendor and @device, the reference count to the   device is incremented and a pointer to its device structure is returned.   Otherwise, %NULL is returned.  A new search is initiated by passing %NULL   as the @from argument.  Otherwise if @from is not %NULL, searches continue   from next device on the global list.  The reference count for @from is   always decremented if it is not %NULL. ", "struct pci_dev *pci_get_class(unsigned int class, struct pci_dev *from)": "pci_get_class - begin or continue searching for a PCI device by class   @class: search for a PCI device with this class designation   @from: Previous PCI device found in search, or %NULL for new search.     Iterates through the list of known PCI devices.  If a PCI device is   found with a matching @class, the reference count to the device is   incremented and a pointer to its device structure is returned.   Otherwise, %NULL is returned.   A new search is initiated by passing %NULL as the @from argument.   Otherwise if @from is not %NULL, searches continue from next device   on the global list.  The reference count for @from is always decremented   if it is not %NULL. ", "int pci_dev_present(const struct pci_device_id *ids)": "pci_dev_present - Returns 1 if device matching the device list is present, 0 if not.   @ids: A pointer to a null terminated list of struct pci_device_id structures   that describe the type of PCI device the caller is trying to find.     Obvious fact: You do not have a reference to any device that might be found   by this function, so if that device is removed from the system right after   this function is finished, the value will be stale.  Use this function to   find devices that are usually built into a system, or for a general hint as   to if another device happens to be present at this specific moment in time. ", "int pci_request_irq(struct pci_dev *dev, unsigned int nr, irq_handler_t handler,irq_handler_t thread_fn, void *dev_id, const char *fmt, ...)": "pci_request_irq - allocate an interrupt line for a PCI device   @dev:PCI device to operate on   @nr:device-relative interrupt vector index (0-based).   @handler:Function to be called when the IRQ occurs.  Primary handler for threaded interrupts.  If NULL and thread_fn != NULL the default primary handler is  installed.   @thread_fn:Function called from the IRQ handler thread  If NULL, no IRQ thread is created   @dev_id:Cookie passed back to the handler function   @fmt:Printf-like format string naming the handler     This call allocates interrupt resources and enables the interrupt line and   IRQ handling. From the point this call is made @handler and @thread_fn may   be invoked.  All interrupts requested using this function might be shared.     @dev_id must not be NULL and must be globally unique. ", "void pci_free_irq(struct pci_dev *dev, unsigned int nr, void *dev_id)": "pci_free_irq - free an interrupt allocated with pci_request_irq   @dev:PCI device to operate on   @nr:device-relative interrupt vector index (0-based).   @dev_id:Device identity to free     Remove an interrupt handler. The handler is removed and if the interrupt   line is no longer in use by any driver it is disabled.  The caller must   ensure the interrupt is disabled on the device before calling this function.   The function does not return until any executing interrupts for this IRQ   have completed.     This function must not be called from interrupt context. ", "pci_info(bridge, \"  bridge window %pR\\n\", res);pci_write_config_dword(bridge, PCI_CB_IO_BASE_0,region.start);pci_write_config_dword(bridge, PCI_CB_IO_LIMIT_0,region.end);}res = bus->resource[1];pcibios_resource_to_bus(bridge->bus, &region, res);if (res->flags & IORESOURCE_IO) ": "pci_setup_cardbus(struct pci_bus  bus){struct pci_dev  bridge = bus->self;struct resource  res;struct pci_bus_region region;pci_info(bridge, \"CardBus bridge to %pR\\n\", &bus->busn_res);res = bus->resource[0];pcibios_resource_to_bus(bridge->bus, &region, res);if (res->flags & IORESOURCE_IO) {    The IO resource is allocated a range twice as large as it   would normally need.  This allows us to set both IO regs. ", "if (pci_is_root_bus(bus)) ": "pci_bus_size_bridges(struct pci_bus  bus, struct list_head  realloc_head){struct pci_dev  dev;unsigned long mask, prefmask, type2 = 0, type3 = 0;resource_size_t additional_io_size = 0, additional_mmio_size = 0,additional_mmio_pref_size = 0;struct resource  pref;struct pci_host_bridge  host;int hdr_type, ret;list_for_each_entry(dev, &bus->devices, bus_list) {struct pci_bus  b = dev->subordinate;if (!b)continue;switch (dev->hdr_type) {case PCI_HEADER_TYPE_CARDBUS:pci_bus_size_cardbus(b, realloc_head);break;case PCI_HEADER_TYPE_BRIDGE:default:__pci_bus_size_bridges(b, realloc_head);break;}}  The root bus? ", "struct pci_ops *pci_bus_set_ops(struct pci_bus *bus, struct pci_ops *ops)": "pci_bus_set_ops - Set raw operations of pci bus   @bus:pci bus struct   @ops:new raw operations     Return previous raw operations ", "if (ret)*val = 0;return ret;}/* * For Functions that do not implement the Slot Capabilities, * Slot Status, and Slot Control registers, these spaces must * be hardwired to 0b, with the exception of the Presence Detect * State bit in the Slot Status register of Downstream Ports, * which must be hardwired to 1b.  (PCIe Base Spec 3.0, sec 7.8) ": "pci_read_config_word(dev, pci_pcie_cap(dev) + pos, val);    Reset  val to 0 if pci_read_config_word() fails; it may   have been written as 0xFFFF (PCI_ERROR_RESPONSE) if the   config read failed on PCI. ", "if (ret)*val = 0;return ret;}if (pci_is_pcie(dev) && pcie_downstream_port(dev) &&    pos == PCI_EXP_SLTSTA)*val = PCI_EXP_SLTSTA_PDS;return 0;}EXPORT_SYMBOL(pcie_capability_read_dword": "pcie_capability_read_dword(struct pci_dev  dev, int pos, u32  val){int ret; val = 0;if (pos & 3)return PCIBIOS_BAD_REGISTER_NUMBER;if (pcie_capability_reg_implemented(dev, pos)) {ret = pci_read_config_dword(dev, pci_pcie_cap(dev) + pos, val);    Reset  val to 0 if pci_read_config_dword() fails; it may   have been written as 0xFFFFFFFF (PCI_ERROR_RESPONSE) if   the config read failed on PCI. ", "if (ret)*val = 0;return ret;}if (pci_is_pcie(dev) && pcie_downstream_port(dev) &&    pos == PCI_EXP_SLTSTA)*val = PCI_EXP_SLTSTA_PDS;return 0;}EXPORT_SYMBOL(pcie_capability_read_dword);int pcie_capability_write_word(struct pci_dev *dev, int pos, u16 val)": "pci_read_config_dword(dev, pci_pcie_cap(dev) + pos, val);    Reset  val to 0 if pci_read_config_dword() fails; it may   have been written as 0xFFFFFFFF (PCI_ERROR_RESPONSE) if   the config read failed on PCI. ", "u8 pci_find_capability(struct pci_dev *dev, int cap)": "pci_find_capability - query for devices' capabilities   @dev: PCI device to query   @cap: capability code     Tell if a device supports a given PCI capability.   Returns the address of the requested capability structure within the   device's PCI configuration space or 0 in case the device does not   support it.  Possible values for @cap include:      %PCI_CAP_ID_PM           Power Management    %PCI_CAP_ID_AGP          Accelerated Graphics Port    %PCI_CAP_ID_VPD          Vital Product Data    %PCI_CAP_ID_SLOTID       Slot Identification    %PCI_CAP_ID_MSI          Message Signalled Interrupts    %PCI_CAP_ID_CHSWP        CompactPCI HotSwap    %PCI_CAP_ID_PCIX         PCI-X    %PCI_CAP_ID_EXP          PCI Express ", "u8 pci_bus_find_capability(struct pci_bus *bus, unsigned int devfn, int cap)": "pci_bus_find_capability - query for devices' capabilities   @bus: the PCI bus to query   @devfn: PCI device to query   @cap: capability code     Like pci_find_capability() but works for PCI devices that do not have a   pci_dev structure set up yet.     Returns the address of the requested capability structure within the   device's PCI configuration space or 0 in case the device does not   support it. ", "struct resource *pci_find_parent_resource(const struct pci_dev *dev,  struct resource *res)": "pci_find_parent_resource - return resource region of parent bus of given        region   @dev: PCI device structure contains resources to be searched   @res: child resource record for which parent is sought     For given resource region of given device, return the resource region of   parent bus the given region is contained in. ", "struct resource *pci_find_resource(struct pci_dev *dev, struct resource *res)": "pci_find_resource - Return matching PCI device resource   @dev: PCI device to query   @res: Resource to look for     Goes over standard PCI resources (BARs) and checks if the given resource   is partially or fully contained in any of them. In that case the   matching resource is returned, %NULL otherwise. ", "void pci_update_current_state(struct pci_dev *dev, pci_power_t state)": "pci_choose_state(struct pci_dev  dev){if (pci_use_mid_pm())return PCI_POWER_ERROR;return acpi_pci_choose_state(dev);}static inline int platform_pci_set_wakeup(struct pci_dev  dev, bool enable){if (pci_use_mid_pm())return PCI_POWER_ERROR;return acpi_pci_wakeup(dev, enable);}static inline bool platform_pci_need_resume(struct pci_dev  dev){if (pci_use_mid_pm())return false;return acpi_pci_need_resume(dev);}static inline bool platform_pci_bridge_d3(struct pci_dev  dev){if (pci_use_mid_pm())return false;return acpi_pci_bridge_d3(dev);}     pci_update_current_state - Read power state of given device and cache it   @dev: PCI device to handle.   @state: State to cache in case the device doesn't have the PM capability     The power state is read from the PMCSR register, which however is   inaccessible in D3cold.  The platform firmware is therefore queried first   to detect accessibility of the register.  In case the platform firmware   reports an incorrect state or the device isn't power manageable by the   platform at all, we try to detect D3cold by testing accessibility of the   vendor ID in config space. ", "int pci_save_state(struct pci_dev *dev)": "pci_save_state - save the PCI configuration space of a device before      suspending   @dev: PCI device that we're dealing with ", "static int pci_set_full_power_state(struct pci_dev *dev)": "pci_restore_state() is going to be called right after a power state change   to D0, it is more efficient to use pci_power_up() directly instead of this   function. ", "int pci_reenable_device(struct pci_dev *dev)": "pci_enable_device(struct pci_dev  dev, int bars){int err;struct pci_dev  bridge;u16 cmd;u8 pin;err = pci_set_power_state(dev, PCI_D0);if (err < 0 && err != -EIO)return err;bridge = pci_upstream_bridge(dev);if (bridge)pcie_aspm_powersave_config_link(bridge);err = pcibios_enable_device(dev, bars);if (err < 0)return err;pci_fixup_device(pci_fixup_enable, dev);if (dev->msi_enabled || dev->msix_enabled)return 0;pci_read_config_byte(dev, PCI_INTERRUPT_PIN, &pin);if (pin) {pci_read_config_word(dev, PCI_COMMAND, &cmd);if (cmd & PCI_COMMAND_INTX_DISABLE)pci_write_config_word(dev, PCI_COMMAND,      cmd & ~PCI_COMMAND_INTX_DISABLE);}return 0;}     pci_reenable_device - Resume abandoned device   @dev: PCI device to be resumed     NOTE: This function is a backend of pci_default_resume() and is not supposed   to be called by normal code, write proper resume handler and use it instead. ", "int pci_enable_device_io(struct pci_dev *dev)": "pci_enable_device_io - Initialize a device for use with IO space   @dev: PCI device to be initialized     Initialize device before it's used by a driver. Ask low-level code   to enable IO resources. Wake up the device if it was suspended.   Beware, this function can fail. ", "int pci_enable_device_mem(struct pci_dev *dev)": "pci_enable_device_mem - Initialize a device for use with Memory space   @dev: PCI device to be initialized     Initialize device before it's used by a driver. Ask low-level code   to enable Memory resources. Wake up the device if it was suspended.   Beware, this function can fail. ", "int pcim_enable_device(struct pci_dev *pdev)": "pci_clear_mwi(dev);if (this->restore_intx)pci_intx(dev, this->orig_intx);if (this->enabled && !this->pinned)pci_disable_device(dev);}static struct pci_devres  get_pci_dr(struct pci_dev  pdev){struct pci_devres  dr,  new_dr;dr = devres_find(&pdev->dev, pcim_release, NULL, NULL);if (dr)return dr;new_dr = devres_alloc(pcim_release, sizeof( new_dr), GFP_KERNEL);if (!new_dr)return NULL;return devres_get(&pdev->dev, new_dr, NULL, NULL);}static struct pci_devres  find_pci_dr(struct pci_dev  pdev){if (pci_is_managed(pdev))return devres_find(&pdev->dev, pcim_release, NULL, NULL);return NULL;}     pcim_enable_device - Managed pci_enable_device()   @pdev: PCI device to be initialized     Managed pci_enable_device(). ", "void pcim_pin_device(struct pci_dev *pdev)": "pcim_pin_device - Pin managed PCI device   @pdev: PCI device to pin     Pin managed PCI device @pdev.  Pinned device won't be disabled on   driver detach.  @pdev must have been enabled with   pcim_enable_device(). ", "bool pci_pme_capable(struct pci_dev *dev, pci_power_t state)": "pci_pme_capable - check the capability of PCI device to generate PME#   @dev: PCI device to handle.   @state: PCI state from which device will issue PME#. ", "pmcsr |= PCI_PM_CTRL_PME_STATUS | PCI_PM_CTRL_PME_ENABLE;if (!enable)pmcsr &= ~PCI_PM_CTRL_PME_ENABLE;pci_write_config_word(dev, dev->pm_cap + PCI_PM_CTRL, pmcsr);}/** * pci_pme_restore - Restore PME configuration after config space restore. * @dev: PCI device to update. ": "pci_pme_active(struct pci_dev  dev, bool enable){u16 pmcsr;if (!dev->pme_support)return;pci_read_config_word(dev, dev->pm_cap + PCI_PM_CTRL, &pmcsr);  Clear PME_Status by writing 1 to it and enable PME# ", "static int __pci_enable_wake(struct pci_dev *dev, pci_power_t state, bool enable)": "pci_enable_wake - enable PCI device as wakeup event source   @dev: PCI device affected   @state: PCI state from which device will issue wakeup events   @enable: True to enable event generation; false to disable     This enables the device as a wakeup event source, or disables it.   When such events involves platform-specific hooks, those hooks are   called automatically by this routine.     Devices with legacy power management (no standard PCI PM capabilities)   always require such platform hooks.     RETURN VALUE:   0 is returned on success   -EINVAL is returned if device is not supposed to wake up the system   Error code depending on the platform is returned if both the platform and   the native mechanism fail to enable the generation of wake-up events ", "int pci_wake_from_d3(struct pci_dev *dev, bool enable)": "pci_wake_from_d3 - enabledisable device to wake up from D3_hot or D3_cold   @dev: PCI device to prepare   @enable: True to enable wake-up event generation; false to disable     Many drivers want the device to wake up the system from D3_hot or D3_cold   and this function allows them to set that up cleanly - pci_enable_wake()   should not be called twice in a row to enable wake-up due to PCI PM vs ACPI   ordering constraints.     This function only returns error code if the device is not allowed to wake   up the system from sleep or it is not capable of generating PME# from both   D3_hot and D3_cold and the platform is unable to enable wake-up power for it. ", "int pci_prepare_to_sleep(struct pci_dev *dev)": "pci_prepare_to_sleep - prepare PCI device for system-wide transition    into a sleep state   @dev: Device to handle.     Choose the power state appropriate for the device depending on whether   it can wake up the system andor is power manageable by the platform   (PCI_D3hot is the default) and put the device into that state. ", "int pci_back_from_sleep(struct pci_dev *dev)": "pci_back_from_sleep - turn PCI device on during system-wide transition   into working state   @dev: Device to handle.     Disable device's system wake-up capability and put it into D0. ", "u32 pci_rebar_get_possible_sizes(struct pci_dev *pdev, int bar)": "pci_rebar_get_possible_sizes - get possible sizes for BAR   @pdev: PCI device   @bar: BAR to query     Get the possible sizes of a resizable BAR as bitmask defined in the spec   (bit 0=1MB, bit 19=512GB). Returns 0 if BAR isn't resizable. ", "int pci_enable_atomic_ops_to_root(struct pci_dev *dev, u32 cap_mask)": "pci_enable_atomic_ops_to_root - enable AtomicOp requests to root port   @dev: the PCI device   @cap_mask: mask of desired AtomicOp sizes, including one or more of:  PCI_EXP_DEVCAP2_ATOMIC_COMP32  PCI_EXP_DEVCAP2_ATOMIC_COMP64  PCI_EXP_DEVCAP2_ATOMIC_COMP128     Return 0 if all upstream bridges support AtomicOp routing, egress   blocking is disabled on all upstream ports, and the root port supports   the requested completion capabilities (32-bit, 64-bit andor 128-bit   AtomicOp completion), or negative otherwise. ", "void pci_release_region(struct pci_dev *pdev, int bar)": "pci_request_region()   @bar: BAR to release     Releases the PCI IO and memory resources previously reserved by a   successful call to pci_request_region().  Call this function only   after all use of the PCI regions has ceased. ", "void pci_release_selected_regions(struct pci_dev *pdev, int bars)": "pci_release_selected_regions - Release selected PCI IO and memory resources   @pdev: PCI device whose resources were previously reserved   @bars: Bitmask of BARs to be released     Release selected PCI IO and memory resources previously reserved.   Call this function only after all use of the PCI regions has ceased. ", "int pci_request_selected_regions(struct pci_dev *pdev, int bars, const char *res_name)": "pci_request_selected_regions(struct pci_dev  pdev, int bars,  const char  res_name, int excl){int i;for (i = 0; i < PCI_STD_NUM_BARS; i++)if (bars & (1 << i))if (__pci_request_region(pdev, i, res_name, excl))goto err_out;return 0;err_out:while (--i >= 0)if (bars & (1 << i))pci_release_region(pdev, i);return -EBUSY;}     pci_request_selected_regions - Reserve selected PCI IO and memory resources   @pdev: PCI device whose resources are to be reserved   @bars: Bitmask of BARs to be requested   @res_name: Name to be associated with resource ", "void pci_release_regions(struct pci_dev *pdev)": "pci_request_regions()     Releases all PCI IO and memory resources previously reserved by a   successful call to pci_request_regions().  Call this function only   after all use of the PCI regions has ceased. ", "int pci_request_regions_exclusive(struct pci_dev *pdev, const char *res_name)": "pci_request_regions_exclusive - Reserve PCI IO and memory resources   @pdev: PCI device whose resources are to be reserved   @res_name: Name to be associated with resource.     Mark all PCI regions associated with PCI device @pdev as being reserved   by owner @res_name.  Do not access any address inside the PCI regions   unless this call returns successfully.     pci_request_regions_exclusive() will mark the region so that devmem   and the sysfs MMIO access will not be allowed.     Returns 0 on success, or %EBUSY on error.  A warning message is also   printed on failure. ", "#ifndef pci_remap_iospaceint pci_remap_iospace(const struct resource *res, phys_addr_t phys_addr)": "pci_remap_iospace - Remap the memory mapped IO space   @res: Resource describing the IO space   @phys_addr: physical address of range to be mapped     Remap the memory mapped IO space described by the @res and the CPU   physical address @phys_addr into virtual address space.  Only   architectures that have memory mapped IO functions defined (and the   PCI_IOBASE value defined) should call this function. ", "void pci_unmap_iospace(struct resource *res)": "pci_unmap_iospace - Unmap the memory mapped IO space   @res: resource to be unmapped     Unmap the CPU virtual address @res from virtual address space.  Only   architectures that have memory mapped IO functions defined (and the   PCI_IOBASE value defined) should call this function. ", "int devm_pci_remap_iospace(struct device *dev, const struct resource *res,   phys_addr_t phys_addr)": "devm_pci_remap_iospace - Managed pci_remap_iospace()   @dev: Generic device to remap IO address for   @res: Resource describing the IO space   @phys_addr: physical address of range to be mapped     Managed pci_remap_iospace().  Map is automatically unmapped on driver   detach. ", "void __iomem *devm_pci_remap_cfgspace(struct device *dev,      resource_size_t offset,      resource_size_t size)": "devm_pci_remap_cfgspace - Managed pci_remap_cfgspace()   @dev: Generic device to remap IO address for   @offset: Resource address to map   @size: Size of map     Managed pci_remap_cfgspace().  Map is automatically unmapped on driver   detach. ", "void __iomem *devm_pci_remap_cfg_resource(struct device *dev,  struct resource *res)": "devm_pci_remap_cfg_resource - check, request region and ioremap cfg resource   @dev: generic device to handle the resource for   @res: configuration space resource to be handled     Checks that a resource is a valid memory region, requests the memory   region and ioremaps with pci_remap_cfgspace() API that ensures the   proper PCI configuration space memory attributes are guaranteed.     All operations are managed and will be undone on driver detach.     Returns a pointer to the remapped memory or an ERR_PTR() encoded error code   on failure. Usage example::    res = platform_get_resource(pdev, IORESOURCE_MEM, 0);  base = devm_pci_remap_cfg_resource(&pdev->dev, res);  if (IS_ERR(base))  return PTR_ERR(base); ", "pci_update_current_state(dev, dev->current_state);if (atomic_inc_return(&dev->enable_cnt) > 1)return 0;/* already enabled ": "pci_set_master(dev);return;}retval = pci_enable_device(dev);if (retval)pci_err(dev, \"Error enabling bridge (%d), continuing\\n\",retval);pci_set_master(dev);}static int pci_enable_device_flags(struct pci_dev  dev, unsigned long flags){struct pci_dev  bridge;int err;int i, bars = 0;    Power state could be unknown at this point, either due to a fresh   boot or a device removal call.  So get the current power state   so that things like MSI message writing will behave as expected   (e.g. if the device really is in D0 at enable time). ", "void pci_clear_master(struct pci_dev *dev)": "pci_clear_master - disables bus-mastering for device dev   @dev: the PCI device to disable ", "int pci_set_cacheline_size(struct pci_dev *dev)": "pci_set_mwi.   Originally copied from driversnetacenic.c.   Copyright 1998-2001 by Jes Sorensen, <jes@trained-monkey.org>.     RETURNS: An appropriate -ERRNO error value on error, or zero for success. ", "int pcim_set_mwi(struct pci_dev *dev)": "pcim_set_mwi - a device-managed pci_set_mwi()   @dev: the PCI device for which MWI is enabled     Managed pci_set_mwi().     RETURNS: An appropriate -ERRNO error value on error, or zero for success. ", "int pci_try_set_mwi(struct pci_dev *dev)": "pci_try_set_mwi - enables memory-write-invalidate PCI transaction   @dev: the PCI device for which MWI is enabled     Enables the Memory-Write-Invalidate transaction in %PCI_COMMAND.   Callers are not required to check the return value.     RETURNS: An appropriate -ERRNO error value on error, or zero for success. ", "int pci_wait_for_pending_transaction(struct pci_dev *dev)": "pci_wait_for_pending_transaction - wait for pending transaction   @dev: the PCI device to operate on     Return 0 if transaction is pending 1 otherwise. ", "int pcix_get_max_mmrbc(struct pci_dev *dev)": "pcix_get_max_mmrbc - get PCI-X maximum designed memory read byte count   @dev: PCI device to query     Returns mmrbc: maximum designed memory read count in bytes or   appropriate error value. ", "int pcix_get_mmrbc(struct pci_dev *dev)": "pcix_get_mmrbc - get PCI-X maximum memory read byte count   @dev: PCI device to query     Returns mmrbc: maximum memory read count in bytes or appropriate error   value. ", "int pcix_set_mmrbc(struct pci_dev *dev, int mmrbc)": "pcix_set_mmrbc - set PCI-X maximum memory read byte count   @dev: PCI device to query   @mmrbc: maximum memory read count in bytes      valid values are 512, 1024, 2048, 4096     If possible sets maximum memory read byte count, some bridges have errata   that prevent this. ", "int pcie_get_readrq(struct pci_dev *dev)": "pcie_get_readrq - get PCI Express read request size   @dev: PCI device to query     Returns maximum memory read request in bytes or appropriate error value. ", "int pcie_set_readrq(struct pci_dev *dev, int rq)": "pcie_set_readrq - set PCI Express maximum memory read request   @dev: PCI device to query   @rq: maximum memory read count in bytes      valid values are 128, 256, 512, 1024, 2048, 4096     If possible sets maximum memory read request in bytes ", "int pcie_get_mps(struct pci_dev *dev)": "pcie_get_mps(dev);if (mps < rq)rq = mps;}v = (ffs(rq) - 8) << 12;if (bridge->no_inc_mrrs) {int max_mrrs = pcie_get_readrq(dev);if (rq > max_mrrs) {pci_info(dev, \"can't set Max_Read_Request_Size to %d; max is %d\\n\", rq, max_mrrs);return -EINVAL;}}ret = pcie_capability_clear_and_set_word(dev, PCI_EXP_DEVCTL,  PCI_EXP_DEVCTL_READRQ, v);return pcibios_err_to_errno(ret);}EXPORT_SYMBOL(pcie_set_readrq);     pcie_get_mps - get PCI Express maximum payload size   @dev: PCI device to query     Returns maximum payload size in bytes ", "int pcie_set_mps(struct pci_dev *dev, int mps)": "pcie_set_mps - set PCI Express maximum payload size   @dev: PCI device to query   @mps: maximum payload size in bytes      valid values are 128, 256, 512, 1024, 2048, 4096     If possible sets maximum payload size ", "u32 pcie_bandwidth_available(struct pci_dev *dev, struct pci_dev **limiting_dev,     enum pci_bus_speed *speed,     enum pcie_link_width *width)": "pcie_bandwidth_available - determine minimum link settings of a PCIe        device and its bandwidth limitation   @dev: PCI device to query   @limiting_dev: storage for device causing the bandwidth limitation   @speed: storage for speed of limiting device   @width: storage for width of limiting device     Walk up the PCI device chain and find the point where the minimum   bandwidth is available.  Return the bandwidth available there and (if   limiting_dev, speed, and width pointers are supplied) information about   that point.  The bandwidth returned is in Mbs, i.e., megabitssecond of   raw bandwidth. ", "if (!dev->link_active_reporting)return -ENOTTY;pcie_capability_read_word(dev, PCI_EXP_LNKSTA, &status);if (!(status & PCI_EXP_LNKSTA_DLLLA))return -ENOTTY;return pci_dev_wait(child, reset_type,    PCIE_RESET_READY_POLL_MS - PCI_RESET_WAIT);}pci_dbg(dev, \"waiting %d ms for downstream link, after activation\\n\",delay);if (!pcie_wait_for_link_delay(dev, true, delay)) ": "pcie_get_speed_cap(dev) <= PCIE_SPEED_5_0GT) {u16 status;pci_dbg(dev, \"waiting %d ms for downstream link\\n\", delay);msleep(delay);if (!pci_dev_wait(child, reset_type, PCI_RESET_WAIT - delay))return 0;    If the port supports active link reporting we now check   whether the link is active and if not bail out early with   the assumption that the device is not present anymore. ", "enum pcie_link_width pcie_get_width_cap(struct pci_dev *dev)": "pcie_get_width_cap - query for the PCI device's link width capability   @dev: PCI device to query     Query the PCI device width capability.  Return the maximum link width   supported by the device. ", "void __pcie_print_link_status(struct pci_dev *dev, bool verbose)": "pcie_print_link_status - Report the PCI device's link speed and width   @dev: PCI device to query   @verbose: Print info even when enough bandwidth is available     If the available bandwidth at the device is less than the device is   capable of, report the device's maximum possible bandwidth and the   upstream link that limits its performance.  If @verbose, always print   the available bandwidth, even if the device isn't constrained. ", "int pci_select_bars(struct pci_dev *dev, unsigned long flags)": "pci_select_bars - Make BAR mask from the type of resource   @dev: the PCI device for which BAR mask is made   @flags: resource type mask to be selected     This helper routine makes bar mask from the type of resource. ", "void pci_stop_and_remove_bus_device(struct pci_dev *dev)": "pci_stop_and_remove_bus_device - remove a PCI device and any children   @dev: the device to remove     Remove a PCI device from the device lists, informing the drivers   that the device has been removed.  We also remove any subordinate   buses and children in a depth-first manner.     For each device we remove, delete the device structure from the   device lists, remove the proc entry, and notify userspace   (sbinhotplug). ", "msi->eq_cpu = dma_alloc_coherent(pcie->dev, msi->nr_eq_region * EQ_MEM_REGION_SIZE, &msi->eq_dma, GFP_KERNEL);if (!msi->eq_cpu) ": "iproc_msi_init(struct iproc_pcie  pcie, struct device_node  node){struct iproc_msi  msi;int i, ret;unsigned int cpu;if (!of_device_is_compatible(node, \"brcm,iproc-msi\"))return -ENODEV;if (!of_find_property(node, \"msi-controller\", NULL))return -ENODEV;if (pcie->msi)return -EBUSY;msi = devm_kzalloc(pcie->dev, sizeof( msi), GFP_KERNEL);if (!msi)return -ENOMEM;msi->pcie = pcie;pcie->msi = msi;msi->msi_addr = pcie->base_addr;mutex_init(&msi->bitmap_lock);msi->nr_cpus = num_possible_cpus();if (msi->nr_cpus == 1)iproc_msi_domain_info.flags |=  MSI_FLAG_MULTI_PCI_MSI;msi->nr_irqs = of_irq_count(node);if (!msi->nr_irqs) {dev_err(pcie->dev, \"found no MSI GIC interrupt\\n\");return -ENODEV;}if (msi->nr_irqs > NR_HW_IRQS) {dev_warn(pcie->dev, \"too many MSI GIC interrupts defined %d\\n\", msi->nr_irqs);msi->nr_irqs = NR_HW_IRQS;}if (msi->nr_irqs < msi->nr_cpus) {dev_err(pcie->dev,\"not enough GIC interrupts for MSI affinity\\n\");return -EINVAL;}if (msi->nr_irqs % msi->nr_cpus != 0) {msi->nr_irqs -= msi->nr_irqs % msi->nr_cpus;dev_warn(pcie->dev, \"Reducing number of interrupts to %d\\n\", msi->nr_irqs);}switch (pcie->type) {case IPROC_PCIE_PAXB_BCMA:case IPROC_PCIE_PAXB:msi->reg_offsets = iproc_msi_reg_paxb;msi->nr_eq_region = 1;msi->nr_msi_region = 1;break;case IPROC_PCIE_PAXC:msi->reg_offsets = iproc_msi_reg_paxc;msi->nr_eq_region = msi->nr_irqs;msi->nr_msi_region = msi->nr_irqs;break;default:dev_err(pcie->dev, \"incompatible iProc PCIe interface\\n\");return -EINVAL;}if (of_find_property(node, \"brcm,pcie-msi-inten\", NULL))msi->has_inten_reg = true;msi->nr_msi_vecs = msi->nr_irqs   EQ_LEN;msi->bitmap = devm_bitmap_zalloc(pcie->dev, msi->nr_msi_vecs, GFP_KERNEL);if (!msi->bitmap)return -ENOMEM;msi->grps = devm_kcalloc(pcie->dev, msi->nr_irqs, sizeof( msi->grps), GFP_KERNEL);if (!msi->grps)return -ENOMEM;for (i = 0; i < msi->nr_irqs; i++) {unsigned int irq = irq_of_parse_and_map(node, i);if (!irq) {dev_err(pcie->dev, \"unable to parsemap interrupt\\n\");ret = -ENODEV;goto free_irqs;}msi->grps[i].gic_irq = irq;msi->grps[i].msi = msi;msi->grps[i].eq = i;}  Reserve memory for event queue and make sure memories are zeroed ", "axi_addr -= ob->axi_offset;/* iterate through all OARR/OMAP mapping windows ": "iproc_pcie_setup_ob(struct iproc_pcie  pcie, u64 axi_addr,       u64 pci_addr, resource_size_t size){struct iproc_pcie_ob  ob = &pcie->ob;struct device  dev = pcie->dev;int ret = -EINVAL, window_idx, size_idx;if (axi_addr < ob->axi_offset) {dev_err(dev, \"axi address %pap less than offset %pap\\n\",&axi_addr, &ob->axi_offset);return -EINVAL;}    Translate the AXI address to the internal address used by the iProc   PCIe core before programming the OARR ", "if (aspm_disabled) ": "pci_disable_link_state(struct pci_dev  pdev, int state, bool sem){struct pcie_link_state  link = pcie_aspm_get_link(pdev);if (!link)return -EINVAL;    A driver requested that ASPM be disabled on this device, but   if we don't have permission to manage ASPM (e.g., on ACPI   systems we have to observe the FADT ACPI_FADT_NO_ASPM bit and   the _OSC method), we can't honor that request.  Windows has   a similar mechanism using \"PciASPMOptOut\", which is also   ignored in this situation. ", "int pci_enable_link_state(struct pci_dev *pdev, int state)": "pci_enable_link_state - Clear and set the default device link state so that   the link may be allowed to enter the specified states. Note that if the   BIOS didn't grant ASPM control to the OS, this does nothing because we can't   touch the LNKCTL register. Also note that this does not enable states   disabled by pci_disable_link_state(). Return 0 or a negative errno.     @pdev: PCI device   @state: Mask of ASPM link states to enable ", "static int __pci_enable_ptm(struct pci_dev *dev)": "pci_enable_ptm(dev, NULL);}void pci_save_ptm_state(struct pci_dev  dev){u16 ptm = dev->ptm_cap;struct pci_cap_saved_state  save_state;u32  cap;if (!ptm)return;save_state = pci_find_saved_ext_cap(dev, PCI_EXT_CAP_ID_PTM);if (!save_state)return;cap = (u32  )&save_state->cap.data[0];pci_read_config_dword(dev, ptm + PCI_PTM_CTRL, cap);}void pci_restore_ptm_state(struct pci_dev  dev){u16 ptm = dev->ptm_cap;struct pci_cap_saved_state  save_state;u32  cap;if (!ptm)return;save_state = pci_find_saved_ext_cap(dev, PCI_EXT_CAP_ID_PTM);if (!save_state)return;cap = (u32  )&save_state->cap.data[0];pci_write_config_dword(dev, ptm + PCI_PTM_CTRL,  cap);}  Enable PTM in the Control register if possible ", "void pci_disable_ptm(struct pci_dev *dev)": "pci_disable_ptm(struct pci_dev  dev){u16 ptm = dev->ptm_cap;u32 ctrl;if (!ptm)return;pci_read_config_dword(dev, ptm + PCI_PTM_CTRL, &ctrl);ctrl &= ~(PCI_PTM_CTRL_ENABLE | PCI_PTM_CTRL_ROOT);pci_write_config_dword(dev, ptm + PCI_PTM_CTRL, ctrl);}     pci_disable_ptm() - Disable Precision Time Measurement   @dev: PCI device     Disable Precision Time Measurement for @dev. ", "int acpi_get_hp_hw_control_from_firmware(struct pci_dev *pdev)": "acpi_get_hp_hw_control_from_firmware   @pdev: the pci_dev of the bridge that has a hotplug controller     Attempt to take hotplug control from firmware. ", "int pci_enable_msi(struct pci_dev *dev)": "pci_free_irq_vectors() API   pair should, in general, be used instead.     Return: 0 on success, errno otherwise ", "int pci_msix_vec_count(struct pci_dev *dev)": "pci_msi_enabled() || !dev || !dev->msi_enabled)return;msi_lock_descs(&dev->dev);pci_msi_shutdown(dev);pci_free_msi_irqs(dev);msi_unlock_descs(&dev->dev);}EXPORT_SYMBOL(pci_disable_msi);     pci_msix_vec_count() - Get number of MSI-X interrupt vectors on device   @dev: the PCI device to operate on     Return: number of MSI-X interrupt vectors available on this device   (i.e., the device's MSI-X capability structure \"table size\"), -EINVAL   if the device is not MSI-X capable, other errnos otherwise. ", "int pci_enable_msix_range(struct pci_dev *dev, struct msix_entry *entries,  int minvec, int maxvec)": "pci_disable_msix() on cleanup.     NOTE: The newer pci_alloc_irq_vectors()  pci_free_irq_vectors() API   pair should, in general, be used instead.     Return: number of MSI-X vectors allocated (which might be smaller   than @maxvecs), where Linux IRQ numbers for such allocated vectors   are saved back in the @entries array elements' \"vector\" field. Return   -ENOSPC if less than @minvecs interrupt vectors are available.   Return -EINVAL if one of the passed @entries members \"entry\" field   was invalid or a duplicate, or if plain MSI interrupts mode was   earlier enabled on device. Return other errnos otherwise. ", "int pci_alloc_irq_vectors_affinity(struct pci_dev *dev, unsigned int min_vecs,   unsigned int max_vecs, unsigned int flags,   struct irq_affinity *affd)": "pci_alloc_irq_vectors_affinity(dev, min_vecs, max_vecs,      flags, NULL);}EXPORT_SYMBOL(pci_alloc_irq_vectors);     pci_alloc_irq_vectors_affinity() - Allocate multiple device interrupt                                      vectors with affinity requirements   @dev:      the PCI device to operate on   @min_vecs: minimum required number of vectors (must be >= 1)   @max_vecs: maximum desired number of vectors   @flags:    allocation flags, as in pci_alloc_irq_vectors()   @affd:     affinity requirements (can be %NULL).     Same as pci_alloc_irq_vectors(), but with the extra @affd parameter.   Check that function docs, and &struct irq_affinity, for more details. ", "int pci_alloc_irq_vectors(struct pci_dev *dev, unsigned int min_vecs,  unsigned int max_vecs, unsigned int flags)": "pci_irq_vector()   to get the Linux IRQ number to be passed to request_threaded_irq().   The driver must call pci_free_irq_vectors() on cleanup.     Return: number of allocated vectors (which might be smaller than   @max_vecs), -ENOSPC if less than @min_vecs interrupt vectors are   available, other errnos otherwise. ", "const struct cpumask *pci_irq_get_affinity(struct pci_dev *dev, int nr)": "pci_irq_get_affinity() - Get a device interrupt vector affinity   @dev: the PCI device to operate on   @nr:  device-relative interrupt vector index (0-based); has different         meanings, depending on interrupt mode:               MSI-X     the index in the MSI-X vector table             MSI       the index of the enabled MSI vectors             INTx      must be 0     Return: MSIMSI-X vector affinity, NULL if @nr is out of range or if   the MSI(-X) vector was allocated without explicit affinity   requirements (e.g., by pci_enable_msi(), pci_enable_msix_range(), or   pci_alloc_irq_vectors() without the %PCI_IRQ_AFFINITY flag). Return a   generic set of CPU IDs representing all possible CPUs available   during system boot if the device is in legacy INTx mode. ", "int pci_msi_vec_count(struct pci_dev *dev)": "pci_msi_vec_count(dev);if (nvec < 0)return nvec;if (nvec < minvec)return -ENOSPC;if (nvec > maxvec)nvec = maxvec;rc = pci_setup_msi_context(dev);if (rc)return rc;if (!pci_setup_msi_device_domain(dev))return -ENODEV;for (;;) {if (affd) {nvec = irq_calc_affinity_vectors(minvec, nvec, affd);if (nvec < minvec)return -ENOSPC;}rc = msi_capability_init(dev, nvec, affd);if (rc == 0)return nvec;if (rc < 0)return rc;if (rc < minvec)return -ENOSPC;nvec = rc;}}     pci_msi_vec_count - Return the number of MSI vectors a device can send   @dev: device to report about     This function returns the number of MSI vectors a device requested via   Multiple Message Capable register. It returns a negative errno if the   device is not capable sending MSI interrupts. Otherwise, the call succeeds   and returns a power of two, up to a maximum of 2^5 (32), according to the   MSI specification.  ", "void pci_msi_mask_irq(struct irq_data *data)": "msi_desc_to_pci_dev(desc), desc->pci.mask_pos,       desc->pci.msi_mask);raw_spin_unlock_irqrestore(lock, flags);}     pci_msi_mask_irq - Generic IRQ chip callback to mask PCIMSI interrupts   @data:pointer to irqdata associated to that interrupt ", "return -ENODEV;}pc_host = container_of(dev->bus->ops, struct bcma_drv_pci_host,       pci_ops);pr_info(\"PCI: Fixing up device %s\\n\", pci_name(dev));/* Fix up interrupt lines ": "bcma_core_pci_plat_dev_init(struct pci_dev  dev){struct bcma_drv_pci_host  pc_host;int readrq;if (dev->bus->ops->read != bcma_core_pci_hostmode_read_config) {  This is not a device on the PCI-core bridge. ", "return -ENODEV;}pc_host = container_of(dev->bus->ops, struct bcma_drv_pci_host,       pci_ops);return bcma_core_irq(pc_host->pdev->core, 0);}EXPORT_SYMBOL(bcma_core_pci_pcibios_map_irq": "bcma_core_pci_pcibios_map_irq(const struct pci_dev  dev){struct bcma_drv_pci_host  pc_host;if (dev->bus->ops->read != bcma_core_pci_hostmode_read_config) {  This is not a device on the PCI-core bridge. ", "int dio_register_driver(struct dio_driver *drv)": "dio_register_driver - register a new DIO driver    @drv: the driver structure to register      Adds the driver structure to the list of registered drivers    Returns zero or a negative error value. ", "void dio_unregister_driver(struct dio_driver *drv)": "dio_unregister_driver - unregister a DIO driver    @drv: the driver structure to unregister      Deletes the driver structure from the list of registered DIO drivers,    gives it a chance to clean up by calling its remove() function for    each device it was responsible for, and marks those devices as    driverless. ", "return driver_register(&drv->driver);}/** *  dio_unregister_driver - unregister a DIO driver *  @drv: the driver structure to unregister * *  Deletes the driver structure from the list of registered DIO drivers, *  gives it a chance to clean up by calling its remove() function for *  each device it was responsible for, and marks those devices as *  driverless. ": "dio_bus_type;  register with core ", "int devm_extcon_register_notifier(struct device *dev, struct extcon_dev *edev,unsigned int id, struct notifier_block *nb)": "devm_extcon_register_notifier() - Resource-managed extcon_register_notifier()   @dev:the device owning the extcon device being created   @edev:the extcon device   @id:the unique id among the extcon enumeration   @nb:a notifier block to be registered     This function manages automatically the notifier of extcon device using   device resource management and simplify the control of unregistering   the notifier of extcon device.     Note that the second parameter given to the callback of nb (val) is   \"old_state\", not the current state. The current state can be retrieved   by looking at the third pameter (edev pointer)'s state value.     Returns 0 if success or negaive error number if failure. ", "void devm_extcon_unregister_notifier(struct device *dev,struct extcon_dev *edev, unsigned int id,struct notifier_block *nb)": "devm_extcon_unregister_notifier()  - Resource-managed extcon_unregister_notifier()   @dev:the device owning the extcon device being created   @edev:the extcon device   @id:the unique id among the extcon enumeration   @nb:a notifier block to be registered ", "int devm_extcon_register_notifier_all(struct device *dev, struct extcon_dev *edev,struct notifier_block *nb)": "devm_extcon_register_notifier_all()  - Resource-managed extcon_register_notifier_all()   @dev:the device owning the extcon device being created   @edev:the extcon device   @nb:a notifier block to be registered     This function manages automatically the notifier of extcon device using   device resource management and simplify the control of unregistering   the notifier of extcon device. To get more information, refer that function.     Returns 0 if success or negaive error number if failure. ", "void devm_extcon_unregister_notifier_all(struct device *dev,struct extcon_dev *edev,struct notifier_block *nb)": "devm_extcon_unregister_notifier_all()  - Resource-managed extcon_unregister_notifier_all()   @dev:the device owning the extcon device being created   @edev:the extcon device   @nb:a notifier block to be registered ", "int dmi_check_system(const struct dmi_system_id *list)": "dmi_check_system - check system DMI data  @list: array of dmi_system_id structures to match against  All non-null elements of the list must match  their slot's (field index's) data (i.e., each  list string must be a substring of the specified  DMI slot's string data) to be considered a  successful match.    Walk the blacklist table running matching functions until someone  returns non zero or we hit the end. Callback function is called for  each successful match. Returns the number of matches.    dmi_setup must be called before this function is called. ", "const struct dmi_system_id *dmi_first_match(const struct dmi_system_id *list)": "dmi_first_match - find dmi_system_id structure matching system DMI data  @list: array of dmi_system_id structures to match against  All non-null elements of the list must match  their slot's (field index's) data (i.e., each  list string must be a substring of the specified  DMI slot's string data) to be considered a  successful match.    Walk the blacklist table until the first match is found.  Return the  pointer to the matching entry or NULL if there's no match.    dmi_setup must be called before this function is called. ", "static int __init dmi_present(const u8 *buf)": "dmi_get_system_info(DMI_SYS_VENDOR));c += scnprintf(buf + c, len - c, \" \");c += print_filtered(buf + c, len - c,    dmi_get_system_info(DMI_PRODUCT_NAME));board = dmi_get_system_info(DMI_BOARD_NAME);if (board) {c += scnprintf(buf + c, len - c, \"\");c += print_filtered(buf + c, len - c, board);}c += scnprintf(buf + c, len - c, \", BIOS \");c += print_filtered(buf + c, len - c,    dmi_get_system_info(DMI_BIOS_VERSION));c += scnprintf(buf + c, len - c, \" \");c += print_filtered(buf + c, len - c,    dmi_get_system_info(DMI_BIOS_DATE));}    Check for DMISMBIOS headers in the system firmware image.  Any   SMBIOS header must start 16 bytes before the DMI header, so take a   32 byte buffer and check for DMI at offset 16 and SMBIOS at offset   0.  If the DMI header is present, set dmi_ver accordingly (SMBIOS   takes precedence) and return 0.  Otherwise return 1. ", "int dmi_name_in_vendors(const char *str)": "dmi_name_in_vendors - Check if string is in the DMI system or board vendor name  @str: Case sensitive Name ", "if ((*d & 0x80) == 0)continue;dmi_save_one_device(*d & 0x7f, dmi_string_nosave(dm, *(d + 1)));}}static void __init dmi_save_oem_strings_devices(const struct dmi_header *dm)": "dmi_find_device(type, name, NULL))return;dev = dmi_alloc(sizeof( dev) + strlen(name) + 1);if (!dev)return;dev->type = type;strcpy((char  )(dev + 1), name);dev->name = (char  )(dev + 1);dev->device_data = NULL;list_add(&dev->list, &dmi_devices);}static void __init dmi_save_devices(const struct dmi_header  dm){int i, count = (dm->length - sizeof(struct dmi_header))  2;for (i = 0; i < count; i++) {const char  d = (char  )(dm + 1) + (i   2);  Skip disabled device ", "bool dmi_get_date(int field, int *yearp, int *monthp, int *dayp)": "dmi_get_date - parse a DMI date  @field:data index (see enum dmi_field)  @yearp: optional out parameter for the year  @monthp: optional out parameter for the month  @dayp: optional out parameter for the day    The date field is assumed to be in the form resembling  [mm[dd]]yy[yy] and the result is stored in the out  parameters any or all of which can be omitted.    If the field doesn't exist, all out parameters are set to zero  and false is returned.  Otherwise, true is returned with any  invalid part of date set to zero.    On return, year, month and day are guaranteed to be in the  range of [0,9999], [0,12] and [0,31] respectively. ", "int dmi_get_bios_year(void)": "dmi_get_bios_year - get a year out of DMI_BIOS_DATE field    Returns year on success, -ENXIO if DMI is not selected,  or a different negative error code if DMI field is not present  or not parseable. ", "int qcom_scm_set_warm_boot_addr(void *entry)": "qcom_scm_set_warm_boot_addr() - Set the warm boot address for all cpus   @entry: Entry point function for the cpus     Set the Linux entry point for the SCM to transfer control to when coming   out of a power down. CPU power down may be executed on cpuidle or hotplug. ", "int qcom_scm_set_cold_boot_addr(void *entry)": "qcom_scm_set_cold_boot_addr() - Set the cold boot address for all cpus   @entry: Entry point function for the cpus ", "void qcom_scm_cpu_power_down(u32 flags)": "qcom_scm_cpu_power_down() - Power down the cpu   @flags:Flags to flush cache     This is an end point to power down cpu. If there was a pending interrupt,   the control would return from this function, otherwise, the cpu jumps to the   warm boot entry point set for this cpu upon reset. ", "int qcom_scm_pas_init_image(u32 peripheral, const void *metadata, size_t size,    struct qcom_scm_pas_metadata *ctx)": "qcom_scm_io_writel(__scm->dload_mode_addr,enable ? QCOM_SCM_BOOT_SET_DLOAD_MODE : 0);} else {dev_err(__scm->dev,\"No available mechanism for setting download mode\\n\");}if (ret)dev_err(__scm->dev, \"failed to set download mode: %d\\n\", ret);}     qcom_scm_pas_init_image() - Initialize peripheral authentication service         state machine for a given peripheral, using the         metadata   @peripheral: peripheral id   @metadata:pointer to memory containing ELF header, program header table  and optional blob of data used for authenticating the metadata  and the rest of the firmware   @size:size of the metadata   @ctx:optional metadata context     Return: 0 on success.     Upon successful return, the PAS metadata context (@ctx) will be used to   track the metadata allocation, this needs to be released by invoking   qcom_scm_pas_metadata_release() by the caller. ", "int qcom_scm_pas_mem_setup(u32 peripheral, phys_addr_t addr, phys_addr_t size)": "qcom_scm_pas_mem_setup() - Prepare the memory related to a given peripheral        for firmware loading   @peripheral:peripheral id   @addr:start address of memory area to prepare   @size:size of the memory area to prepare     Returns 0 on success. ", "int qcom_scm_pas_auth_and_reset(u32 peripheral)": "qcom_scm_pas_auth_and_reset() - Authenticate the given peripheral firmware     and reset the remote processor   @peripheral:peripheral id     Return 0 on success. ", "int qcom_scm_pas_shutdown(u32 peripheral)": "qcom_scm_pas_shutdown() - Shut down the remote processor   @peripheral: peripheral id     Returns 0 on success. ", "bool qcom_scm_pas_supported(u32 peripheral)": "qcom_scm_pas_supported() - Check if the peripheral authentication service is        available for the given peripherial   @peripheral:peripheral id     Returns true if PAS is supported for this peripheral, otherwise false. ", "bool qcom_scm_restore_sec_cfg_available(void)": "qcom_scm_restore_sec_cfg_available() - Check if secure environment   supports restore security config interface.     Return true if restore-cfg interface is supported, false if not. ", "if (ret == -EPERM)ret = 0;return ret;}EXPORT_SYMBOL(qcom_scm_iommu_secure_ptbl_init": "qcom_scm_iommu_secure_ptbl_init(u64 addr, u32 size, u32 spare){struct qcom_scm_desc desc = {.svc = QCOM_SCM_SVC_MP,.cmd = QCOM_SCM_MP_IOMMU_SECURE_PTBL_INIT,.arginfo = QCOM_SCM_ARGS(3, QCOM_SCM_RW, QCOM_SCM_VAL, QCOM_SCM_VAL),.args[0] = addr,.args[1] = size,.args[2] = spare,.owner = ARM_SMCCC_OWNER_SIP,};int ret;ret = qcom_scm_call(__scm->dev, &desc, NULL);  the pg table has been initialized already, ignore the error ", "int qcom_scm_assign_mem(phys_addr_t mem_addr, size_t mem_sz,u64 *srcvm,const struct qcom_scm_vmperm *newvm,unsigned int dest_cnt)": "qcom_scm_assign_mem(struct device  dev, phys_addr_t mem_region, size_t mem_sz, phys_addr_t src, size_t src_sz, phys_addr_t dest, size_t dest_sz){int ret;struct qcom_scm_desc desc = {.svc = QCOM_SCM_SVC_MP,.cmd = QCOM_SCM_MP_ASSIGN,.arginfo = QCOM_SCM_ARGS(7, QCOM_SCM_RO, QCOM_SCM_VAL, QCOM_SCM_RO, QCOM_SCM_VAL, QCOM_SCM_RO, QCOM_SCM_VAL, QCOM_SCM_VAL),.args[0] = mem_region,.args[1] = mem_sz,.args[2] = src,.args[3] = src_sz,.args[4] = dest,.args[5] = dest_sz,.args[6] = 0,.owner = ARM_SMCCC_OWNER_SIP,};struct qcom_scm_res res;ret = qcom_scm_call(dev, &desc, &res);return ret ? : res.result[0];}     qcom_scm_assign_mem() - Make a secure call to reassign memory ownership   @mem_addr: mem region whose ownership need to be reassigned   @mem_sz:   size of the region.   @srcvm:    vmid for current set of owners, each set bit in              flag indicate a unique owner   @newvm:    array having new owners and corresponding permission              flags   @dest_cnt: number of owners in next set.     Return negative errno on failure or 0 on success with @srcvm updated. ", "bool qcom_scm_ocmem_lock_available(void)": "qcom_scm_ocmem_lock_available() - is OCMEM lockunlock interface available ", "int qcom_scm_ocmem_unlock(enum qcom_scm_ocmem_client id, u32 offset, u32 size)": "qcom_scm_ocmem_unlock() - call OCMEM unlock interface to release an OCMEM   region from the specified initiator     @id:     tz initiator id   @offset: OCMEM offset   @size:   OCMEM size ", "bool qcom_scm_ice_available(void)": "qcom_scm_ice_set_key() are available. ", "bool qcom_scm_hdcp_available(void)": "qcom_scm_hdcp_available() - Check if secure environment supports HDCP.     Return true if HDCP is supported, false if not. ", "int qcom_scm_hdcp_req(struct qcom_scm_hdcp_req *req, u32 req_cnt, u32 *resp)": "qcom_scm_hdcp_req() - Send HDCP request.   @req: HDCP request array   @req_cnt: HDCP request array count   @resp: response buffer passed to SCM     Write HDCP register(s) through SCM. ", ".owner = ARM_SMCCC_OWNER_SIP,};return qcom_scm_call(__scm->dev, &desc, NULL);}EXPORT_SYMBOL(qcom_scm_iommu_set_pt_format": "qcom_scm_iommu_set_pt_format(u32 sec_id, u32 ctx_num, u32 pt_fmt){struct qcom_scm_desc desc = {.svc = QCOM_SCM_SVC_SMMU_PROGRAM,.cmd = QCOM_SCM_SMMU_PT_FORMAT,.arginfo = QCOM_SCM_ARGS(3),.args[0] = sec_id,.args[1] = ctx_num,.args[2] = pt_fmt,   0: LPAE AArch32 - 1: AArch64 ", "bool qcom_scm_is_available(void)": "qcom_scm_is_available() - Checks if SCM is available ", "int tee_bnxt_fw_load(void)": "tee_bnxt_fw_load() - Load the bnxt firmware      Uses an OP-TEE call to start a secure      boot process.   Returns 0 on success, negative errno otherwise. ", "int tee_bnxt_copy_coredump(void *buf, u32 offset, u32 size)": "tee_bnxt_copy_coredump() - Copy coredump from the allocated memory      Uses an OP-TEE call to copy coredump   @buf:destination buffer where core dump is copied into   @offset:offset from the base address of core dump area   @size:size of the dump     Returns 0 on success, negative errno otherwise. ", "var = &nvram_buf[sizeof(struct nvram_header)];end = nvram_buf + sizeof(nvram_buf);while (var < end && *var) ": "bcm47xx_nvram_getenv(const char  name, char  val, size_t val_len){char  var,  value,  end,  eq;int err;if (!name)return -EINVAL;if (!nvram_len) {err = nvram_init();if (err)return err;}  Look for name=value and return value ", "for (i = 0; i < NVRAM_MAX_GPIO_ENTRIES; i++) ": "bcm47xx_nvram_gpio_pin(const char  name){int i, err;char nvram_var[] = \"gpioXX\";char buf[NVRAM_MAX_GPIO_VALUE_LEN];  TODO: Optimize it to don't call getenv so many times ", "smp_rmb();tegra_ivc_invalidate_frame(ivc, ivc->rx.phys, ivc->rx.position, 0,   ivc->frame_size);return tegra_ivc_frame_virt(ivc, &ivc->rx.map, ivc->rx.position, map);}EXPORT_SYMBOL(tegra_ivc_read_get_next_frame": "tegra_ivc_read_get_next_frame(struct tegra_ivc  ivc, struct iosys_map  map){int err;if (WARN_ON(ivc == NULL))return -EINVAL;err = tegra_ivc_check_read(ivc);if (err < 0)return err;    Order observation of ivc->rx.position potentially indicating new   data before data read. ", "err = tegra_ivc_check_read(ivc);if (err < 0)return err;tegra_ivc_advance_rx(ivc);tegra_ivc_flush(ivc, ivc->rx.phys + rx);/* * Ensure our write to ivc->rx.position occurs before our read from * ivc->tx.position. ": "tegra_ivc_read_advance(struct tegra_ivc  ivc){unsigned int rx = offsetof(struct tegra_ivc_header, rx.count);unsigned int tx = offsetof(struct tegra_ivc_header, tx.count);int err;    No read barriers or synchronization here: the caller is expected to   have already observed the channel non-empty. This check is just to   catch programming errors. ", "smp_wmb();tegra_ivc_advance_tx(ivc);tegra_ivc_flush(ivc, ivc->tx.phys + tx);/* * Ensure our write to ivc->tx.position occurs before our read from * ivc->rx.position. ": "tegra_ivc_write_advance(struct tegra_ivc  ivc){unsigned int tx = offsetof(struct tegra_ivc_header, tx.count);unsigned int rx = offsetof(struct tegra_ivc_header, rx.count);int err;err = tegra_ivc_check_write(ivc);if (err < 0)return err;tegra_ivc_flush_frame(ivc, ivc->tx.phys, ivc->tx.position, 0,      ivc->frame_size);    Order any possible stores to the frame before update of   ivc->tx.position. ", "int tegra_ivc_notified(struct tegra_ivc *ivc)": "tegra_ivc_notified()   =======================================================    localremoteaction  ----------------------------------------------  SYNCEST<none>  SYNCACKreset counters; move to EST; notify  SYNCSYNCreset counters; move to ACK; notify  ACKESTmove to EST; notify  ACKACKmove to EST; notify  ACKSYNCreset counters; move to ACK; notify  ESTEST<none>  ESTACK<none>  ESTSYNCreset counters; move to ACK; notify     =============================================================== ", "if (frame_size > INT_MAX)return -E2BIG;err = tegra_ivc_check_params(iosys_map_get_address(rx), iosys_map_get_address(tx),     num_frames, frame_size);if (err < 0)return err;queue_size = tegra_ivc_total_queue_size(num_frames * frame_size);if (peer) ": "tegra_ivc_init(struct tegra_ivc  ivc, struct device  peer, const struct iosys_map  rx,   dma_addr_t rx_phys, const struct iosys_map  tx, dma_addr_t tx_phys,   unsigned int num_frames, size_t frame_size,   void ( notify)(struct tegra_ivc  ivc, void  data),   void  data){size_t queue_size;int err;if (WARN_ON(!ivc || !notify))return -EINVAL;    All sizes that can be returned by communication functions should   fit in an int. ", "int meson_sm_call(struct meson_sm_firmware *fw, unsigned int cmd_index,  u32 *ret, u32 arg0, u32 arg1, u32 arg2, u32 arg3, u32 arg4)": "meson_sm_call(u32 cmd, u32 arg0, u32 arg1, u32 arg2,   u32 arg3, u32 arg4){struct arm_smccc_res res;arm_smccc_smc(cmd, arg0, arg1, arg2, arg3, arg4, 0, 0, &res);return res.a0;}static void __iomem  meson_sm_map_shmem(u32 cmd_shmem, unsigned int size){u32 sm_phy_base;sm_phy_base = __meson_sm_call(cmd_shmem, 0, 0, 0, 0, 0);if (!sm_phy_base)return NULL;return ioremap_cache(sm_phy_base, size);}     meson_sm_call - generic SMC32 call to the secure-monitor     @fw:Pointer to secure-monitor firmware   @cmd_index:Index of the SMC32 function ID   @ret:Returned value   @arg0:SMC32 Argument 0   @arg1:SMC32 Argument 1   @arg2:SMC32 Argument 2   @arg3:SMC32 Argument 3   @arg4:SMC32 Argument 4     Return:0 on success, a negative value on error ", "int meson_sm_call_read(struct meson_sm_firmware *fw, void *buffer,       unsigned int bsize, unsigned int cmd_index, u32 arg0,       u32 arg1, u32 arg2, u32 arg3, u32 arg4)": "meson_sm_call_read - retrieve data from secure-monitor     @fw:Pointer to secure-monitor firmware   @buffer:Buffer to store the retrieved data   @bsize:Size of the buffer   @cmd_index:Index of the SMC32 function ID   @arg0:SMC32 Argument 0   @arg1:SMC32 Argument 1   @arg2:SMC32 Argument 2   @arg3:SMC32 Argument 3   @arg4:SMC32 Argument 4     Return:size of read data on success, a negative value on error  When 0 is returned there is no guarantee about the amount of  data read and bsize bytes are copied in buffer. ", "int meson_sm_call_write(struct meson_sm_firmware *fw, void *buffer,unsigned int size, unsigned int cmd_index, u32 arg0,u32 arg1, u32 arg2, u32 arg3, u32 arg4)": "meson_sm_call_write - send data to secure-monitor     @fw:Pointer to secure-monitor firmware   @buffer:Buffer containing data to send   @size:Size of the data to send   @cmd_index:Index of the SMC32 function ID   @arg0:SMC32 Argument 0   @arg1:SMC32 Argument 1   @arg2:SMC32 Argument 2   @arg3:SMC32 Argument 3   @arg4:SMC32 Argument 4     Return:size of sent data on success, a negative value on error ", "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt#include <linux/kobject.h>#include <linux/module.h>#include <linux/init.h>#include <linux/debugfs.h>#include <linux/device.h>#include <linux/efi.h>#include <linux/of.h>#include <linux/initrd.h>#include <linux/io.h>#include <linux/kexec.h>#include <linux/platform_device.h>#include <linux/random.h>#include <linux/reboot.h>#include <linux/slab.h>#include <linux/acpi.h>#include <linux/ucs2_string.h>#include <linux/memblock.h>#include <linux/security.h>#include <asm/early_ioremap.h>struct efi __read_mostly efi = ": "efi.c - EFI subsystem     Copyright (C) 2001,2003,2004 Dell <Matt_Domsch@dell.com>   Copyright (C) 2004 Intel Corporation <matthew.e.tolentino@intel.com>   Copyright (C) 2013 Tom Gundersen <teg@jklm.no>     This code registers sysfirmwareefi{,efivars} when EFI is supported,   allowing the efivarfs to be mounted or the efivars module to be loaded.   The existance of sysfirmwareefi may also be used by userspace to   determine that the system supports EFI. ", "hdr = msg;ret = hdr->func;/* * Some special SCU firmware APIs do NOT have return value * in hdr->func, but they do have response data, those special * APIs are defined as void function in SCU firmware, so they * should be treated as return success always. ": "imx_scu_call_rpc(struct imx_sc_ipc  sc_ipc, void  msg, bool have_resp){uint8_t saved_svc, saved_func;struct imx_sc_rpc_msg  hdr;int ret;if (WARN_ON(!sc_ipc || !msg))return -EINVAL;mutex_lock(&sc_ipc->lock);reinit_completion(&sc_ipc->done);if (have_resp) {sc_ipc->msg = msg;saved_svc = ((struct imx_sc_rpc_msg  )msg)->svc;saved_func = ((struct imx_sc_rpc_msg  )msg)->func;}sc_ipc->count = 0;ret = imx_scu_ipc_write(sc_ipc, msg);if (ret < 0) {dev_err(sc_ipc->dev, \"RPC send msg failed: %d\\n\", ret);goto out;}if (have_resp) {if (!wait_for_completion_timeout(&sc_ipc->done, MAX_RX_TIMEOUT)) {dev_err(sc_ipc->dev, \"RPC send msg timeout\\n\");mutex_unlock(&sc_ipc->lock);return -ETIMEDOUT;}  response status is stored in hdr->func field ", "imx_scu_call_rpc(ipc, &msg, true);return hdr->func;}EXPORT_SYMBOL(imx_sc_rm_is_resource_owned": "imx_sc_rm_is_resource_owned(struct imx_sc_ipc  ipc, u16 resource){struct imx_sc_msg_rm_rsrc_owned msg;struct imx_sc_rpc_msg  hdr = &msg.hdr;hdr->ver = IMX_SC_RPC_VERSION;hdr->svc = IMX_SC_RPC_SVC_RM;hdr->func = IMX_SC_RM_FUNC_IS_RESOURCE_OWNED;hdr->size = 2;msg.resource = resource;    SCU firmware only returns value 0 or 1   for resource owned check which means not owned or owned.   So it is always successful. ", "int imx_dsp_ring_doorbell(struct imx_dsp_ipc *ipc, unsigned int idx)": "imx_dsp_ring_doorbell - triggers an interrupt on the other side (DSP)     @dsp: DSP IPC handle   @chan_idx: index of the channel where to trigger the interrupt     Returns non-negative value for success, negative value for error ", "ch = mbox_request_channel_byname(cl, \"gip3\");if (IS_ERR(ch)) ": "imx_scu_enable_general_irq_channel(struct device  dev){struct of_phandle_args spec;struct mbox_client  cl;struct mbox_chan  ch;int ret = 0, i = 0;ret = imx_scu_get_handle(&imx_sc_irq_ipc_handle);if (ret)return ret;cl = devm_kzalloc(dev, sizeof( cl), GFP_KERNEL);if (!cl)return -ENOMEM;cl->dev = dev;cl->rx_callback = imx_scu_irq_callback;  SCU general IRQ uses general interrupt channel 3 ", "int w1_register_family(struct w1_family *newf)": "w1_register_family() - register a device family driver   @newf:family to register ", "void w1_unregister_family(struct w1_family *fent)": "w1_unregister_family() - unregister a device family driver   @fent:family to unregister ", "int w1_add_master_device(struct w1_bus_master *master)": "w1_add_master_device() - registers a new master device   @master:master bus device to register ", "atomic_set(&dev->refcnt, 1);INIT_LIST_HEAD(&dev->slist);INIT_LIST_HEAD(&dev->async_list);mutex_init(&dev->mutex);mutex_init(&dev->bus_mutex);mutex_init(&dev->list_mutex);memcpy(&dev->dev, device, sizeof(struct device));dev_set_name(&dev->dev, \"w1_bus_master%u\", dev->id);snprintf(dev->name, sizeof(dev->name), \"w1_bus_master%u\", dev->id);dev->dev.init_name = dev->name;dev->driver = driver;dev->seq = 1;err = device_register(&dev->dev);if (err) ": "w1_remove_master_device to decrement ", "struct usb_driver *usb_cdc_wdm_register(struct usb_interface *intf,struct usb_endpoint_descriptor *ep,int bufsize, enum wwan_port_type type,int (*manage_power)(struct usb_interface *, int))": "usb_cdc_wdm_register - register a WDM subdriver   @intf: usb interface the subdriver will associate with   @ep: interrupt endpoint to monitor for notifications   @bufsize: maximum message size to support for readwrite   @type: Typeprotocol of the transported data (MBIM, QMI...)   @manage_power: call-back invoked during open and release to                  manage the device's power   Create WDM usb class character device and associate it with intf   without binding, allowing another driver to manage the interface.     The subdriver will manage the given interrupt endpoint exclusively   and will issue control requests referring to the given intf. It   will otherwise avoid interferring, and in particular not do   usb_set_intfdatausb_get_intfdata on intf.     The return value is a pointer to the subdriver's struct usb_driver.   The registering driver is responsible for calling this subdriver's   disconnect, suspend, resume, pre_reset and post_reset methods from   its own. ", "int cdc_parse_cdc_header(struct usb_cdc_parsed_header *hdr,struct usb_interface *intf,u8 *buffer,int buflen)": "cdc_parse_cdc_header - parse the extra headers present in CDC devices   @hdr: the place to put the results of the parsing   @intf: the interface for which parsing is requested   @buffer: pointer to the extra headers to be parsed   @buflen: length of the extra headers     This evaluates the extra headers present in CDC devices which   bind the interfaces for data and control and provide details   about the capabilities of the device.     Return: number of descriptors parsed or -EINVAL   if the header is contradictory beyond salvage ", "if (serial->suspend_count++)return 0;/* * serial->type->suspend() MUST return 0 in system sleep context, * otherwise, the resume callback has to recover device from * previous suspend failure. ": "usb_serial_suspend(struct usb_interface  intf, pm_message_t message){struct usb_serial  serial = usb_get_intfdata(intf);int i, r;  suspend when called for first sibling interface ", "if (--serial->suspend_count)return 0;usb_serial_unpoison_port_urbs(serial);if (serial->type->resume)rv = serial->type->resume(serial);elserv = usb_serial_generic_resume(serial);return rv;}EXPORT_SYMBOL(usb_serial_resume": "usb_serial_resume(struct usb_interface  intf){struct usb_serial  serial = usb_get_intfdata(intf);int rv;  resume when called for last sibling interface ", "portdata->rts_state = on;portdata->dtr_state = on;usb_wwan_send_setup(port);}EXPORT_SYMBOL(usb_wwan_dtr_rts": "usb_wwan_dtr_rts(struct usb_serial_port  port, int on){struct usb_wwan_port_private  portdata;struct usb_wwan_intf_private  intfdata;intfdata = usb_get_serial_data(port->serial);if (!intfdata->use_send_setup)return;portdata = usb_get_serial_port_data(port);  FIXME: locking ", "if (set & TIOCM_RTS)portdata->rts_state = 1;if (set & TIOCM_DTR)portdata->dtr_state = 1;if (clear & TIOCM_RTS)portdata->rts_state = 0;if (clear & TIOCM_DTR)portdata->dtr_state = 0;return usb_wwan_send_setup(port);}EXPORT_SYMBOL(usb_wwan_tiocmset": "usb_wwan_tiocmset(struct tty_struct  tty,      unsigned int set, unsigned int clear){struct usb_serial_port  port = tty->driver_data;struct usb_wwan_port_private  portdata;struct usb_wwan_intf_private  intfdata;portdata = usb_get_serial_port_data(port);intfdata = usb_get_serial_data(port->serial);if (!intfdata->use_send_setup)return -EINVAL;  FIXME: what locks portdata fields ? ", "int err;unsigned long flags;portdata = usb_get_serial_port_data(port);intfdata = usb_get_serial_data(port->serial);dev_dbg(&port->dev, \"%s: write (%d chars)\\n\", __func__, count);left = count;for (i = 0; left > 0 && i < N_OUT_URB; i++) ": "usb_wwan_write(struct tty_struct  tty, struct usb_serial_port  port,   const unsigned char  buf, int count){struct usb_wwan_port_private  portdata;struct usb_wwan_intf_private  intfdata;int i;int left, todo;struct urb  this_urb = NULL;  spurious ", "if (this_urb && test_bit(i, &portdata->out_busy))data_len += this_urb->transfer_buffer_length;}dev_dbg(&port->dev, \"%s: %u\\n\", __func__, data_len);return data_len;}EXPORT_SYMBOL(usb_wwan_chars_in_buffer": "usb_wwan_chars_in_buffer(struct tty_struct  tty){struct usb_serial_port  port = tty->driver_data;struct usb_wwan_port_private  portdata;int i;unsigned int data_len = 0;struct urb  this_urb;portdata = usb_get_serial_port_data(port);for (i = 0; i < N_OUT_URB; i++) {this_urb = portdata->out_urbs[i];  FIXME: This locking is insufficient as this_urb may   go unused during the test ", "for (i = 0; i < N_IN_URB; i++) ": "usb_wwan_open(struct tty_struct  tty, struct usb_serial_port  port){struct usb_wwan_port_private  portdata;struct usb_wwan_intf_private  intfdata;struct usb_serial  serial = port->serial;int i, err;struct urb  urb;portdata = usb_get_serial_port_data(port);intfdata = usb_get_serial_data(serial);if (port->interrupt_in_urb) {err = usb_submit_urb(port->interrupt_in_urb, GFP_KERNEL);if (err) {dev_err(&port->dev, \"%s: submit int urb failed: %d\\n\",__func__, err);}}  Start reading from the IN endpoint ", "spin_lock_irq(&intfdata->susp_lock);if (--intfdata->open_ports == 0)serial->interface->needs_remote_wakeup = 0;spin_unlock_irq(&intfdata->susp_lock);for (;;) ": "usb_wwan_close(struct usb_serial_port  port){int i;struct usb_serial  serial = port->serial;struct usb_wwan_port_private  portdata;struct usb_wwan_intf_private  intfdata = usb_get_serial_data(serial);struct urb  urb;portdata = usb_get_serial_port_data(port);    Need to take susp_lock to make sure port is not already being   resumed, but no need to hold it due to the tty-port initialized   flag. ", "if ((sg->muxmode == GEMINI_MUXMODE_2) &&    !is_ata1)return false;if ((sg->muxmode == GEMINI_MUXMODE_3) &&    is_ata1)return false;return true;}EXPORT_SYMBOL(gemini_sata_bridge_enabled": "gemini_sata_bridge_enabled(struct sata_gemini  sg, bool is_ata1){if (!sg->sata_bridge)return false;    In muxmode 2 and 3 one of the ATA controllers is   actually not connected to any SATA bridge. ", "ret = gemini_sata_setup_bridge(sg, bridge);if (ret)clk_disable(pclk);return ret;}EXPORT_SYMBOL(gemini_sata_start_bridge": "gemini_sata_start_bridge(struct sata_gemini  sg, unsigned int bridge){struct clk  pclk;int ret;if (bridge == 0)pclk = sg->sata0_pclk;elsepclk = sg->sata1_pclk;clk_enable(pclk);msleep(10);  Do not keep clocking a bridge that is not online ", "ata_scsi_port_error_handler(host, ap);/* finish or retry handled scmd's and clean up ": "ata_scsi_cmd_error_handler(host, ap, &eh_work_q);  If we timed raced normal completion and there is nothing to   recover nr_timedout == 0 why exactly are we doing error recovery ? ", "void ata_std_end_eh(struct ata_port *ap)": "ata_std_end_eh - non-libsas ata_ports complete eh with this common routine   @ap: ATA port to end EH for     In the libata object model there is a 1:1 mapping of ata_port to   shost, so host fields can be directly manipulated under ap->lock, in   the libsas case we need to hold a lock at the ha->level to coordinate   these events.    LOCKING:  spin_lock_irqsave(host lock) ", "int iio_push_event(struct iio_dev *indio_dev, u64 ev_code, s64 timestamp)": "iio_push_event() - try to add event to the list for userspace reading   @indio_dev:IIO device structure   @ev_code:What event   @timestamp:When the event occurred     Note: The caller must make sure that this function is not running   concurrently for the same indio_dev more than once.     This function may be safely used as soon as a valid reference to iio_dev has   been obtained via iio_device_alloc(), but any events that are submitted   before iio_device_register() has successfully completed will be silently   discarded.  ", "int iio_device_set_clock(struct iio_dev *indio_dev, clockid_t clock_id)": "iio_device_set_clock() - Set current timestamping clock for the device   @indio_dev: IIO device structure containing the device   @clock_id: timestamping clock posix identifier to set. ", "clockid_t iio_device_get_clock(const struct iio_dev *indio_dev)": "iio_device_get_clock() - Retrieve current timestamping clock for the device   @indio_dev: IIO device structure containing the device ", "s64 iio_get_time_ns(const struct iio_dev *indio_dev)": "iio_get_time_ns() - utility function to get a time stamp for events etc   @indio_dev: device ", "int iio_read_mount_matrix(struct device *dev, struct iio_mount_matrix *matrix)": "iio_read_mount_matrix() - retrieve iio device mounting matrix from                             device \"mount-matrix\" property   @dev:device the mounting matrix property is assigned to   @matrix:where to store retrieved matrix     If device is assigned no mounting matrix property, a default 3x3 identity   matrix will be filled in.     Return: 0 if success, or a negative error code on failure. ", "struct iio_dev *iio_device_alloc(struct device *parent, int sizeof_priv)": "iio_device_alloc() - allocate an iio_dev from a driver   @parent:Parent device.   @sizeof_priv:Space to allocate for private structure.  ", "void iio_device_free(struct iio_dev *dev)": "iio_device_free() - free an iio_dev from a driver   @dev:the iio_dev associated with the device  ", "if (dev_fwnode(&indio_dev->dev))fwnode = dev_fwnode(&indio_dev->dev);elsefwnode = dev_fwnode(indio_dev->dev.parent);device_set_node(&indio_dev->dev, fwnode);fwnode_property_read_string(fwnode, \"label\", &indio_dev->label);ret = iio_check_unique_scan_index(indio_dev);if (ret < 0)return ret;ret = iio_check_extended_name(indio_dev);if (ret < 0)return ret;iio_device_register_debugfs(indio_dev);ret = iio_buffers_alloc_sysfs_and_mask(indio_dev);if (ret) ": "__iio_device_register(struct iio_dev  indio_dev, struct module  this_mod){struct iio_dev_opaque  iio_dev_opaque = to_iio_dev_opaque(indio_dev);struct fwnode_handle  fwnode;int ret;if (!indio_dev->info)return -EINVAL;iio_dev_opaque->driver_module = this_mod;  If the calling driver did not initialize firmware node, do it here ", "static ssize_t iio_read_channel_ext_info(struct device *dev,     struct device_attribute *attr,     char *buf)": "iio_device_unregister_debugfs(struct iio_dev  indio_dev){struct iio_dev_opaque  iio_dev_opaque = to_iio_dev_opaque(indio_dev);debugfs_remove_recursive(iio_dev_opaque->debugfs_dentry);}static void iio_device_register_debugfs(struct iio_dev  indio_dev){struct iio_dev_opaque  iio_dev_opaque;if (indio_dev->info->debugfs_reg_access == NULL)return;if (!iio_debugfs_dentry)return;iio_dev_opaque = to_iio_dev_opaque(indio_dev);iio_dev_opaque->debugfs_dentry =debugfs_create_dir(dev_name(&indio_dev->dev),   iio_debugfs_dentry);debugfs_create_file(\"direct_reg_access\", 0644,    iio_dev_opaque->debugfs_dentry, indio_dev,    &iio_debugfs_reg_fops);}#elsestatic void iio_device_register_debugfs(struct iio_dev  indio_dev){}static void iio_device_unregister_debugfs(struct iio_dev  indio_dev){}#endif   CONFIG_DEBUG_FS ", "dev_set_name(&trig_info->dev, \"trigger%d\", trig_info->id);ret = device_add(&trig_info->dev);if (ret)goto error_unregister_id;/* Add to list of available triggers held by the IIO core ": "iio_trigger_register(struct iio_trigger  trig_info){int ret;trig_info->id = ida_alloc(&iio_trigger_ida, GFP_KERNEL);if (trig_info->id < 0)return trig_info->id;  Set the name used for the sysfs directory etc ", "device_del(&trig_info->dev);}EXPORT_SYMBOL(iio_trigger_unregister": "iio_trigger_unregister(struct iio_trigger  trig_info){mutex_lock(&iio_trigger_list_lock);list_del(&trig_info->list);mutex_unlock(&iio_trigger_list_lock);ida_free(&iio_trigger_ida, trig_info->id);  Possible issue in here ", "void iio_trigger_poll(struct iio_trigger *trig)": "iio_trigger_notify_done_atomic(struct iio_trigger  trig){if (atomic_dec_and_test(&trig->use_count) && trig->ops &&    trig->ops->reenable)schedule_work(&trig->reenable_work);}     iio_trigger_poll() - Call the IRQ trigger handler of the consumers   @trig: trigger which occurred     This function should only be called from a hard IRQ context. ", "void iio_trigger_poll_nested(struct iio_trigger *trig)": "iio_trigger_poll_nested() - Call the threaded trigger handler of the   consumers   @trig: trigger which occurred     This function should only be called from a kernel thread context. ", "struct iio_trigger *__iio_trigger_alloc(struct device *parent,struct module *this_mod,const char *fmt, ...)": "__iio_trigger_alloc - Allocate a trigger   @parent:Device to allocate iio_trigger for   @this_mod:module allocating the trigger   @fmt:trigger name format. If it includes format  specifiers, the additional arguments following  format are formatted and inserted in the resulting  string replacing their respective specifiers.   RETURNS:   Pointer to allocated iio_trigger on success, NULL on failure. ", "int iio_trigger_validate_own_device(struct iio_trigger *trig,    struct iio_dev *indio_dev)": "iio_trigger_validate_own_device - Check if a trigger and IIO device belong to    the same device   @trig: The IIO trigger to check   @indio_dev: the IIO device to check     This function can be used as the validate_device callback for triggers that   can only be attached to their own device.     Return: 0 if both the trigger and the IIO device belong to the same   device, -EINVAL otherwise. ", "int iio_triggered_event_setup(struct iio_dev *indio_dev,      irqreturn_t (*h)(int irq, void *p),      irqreturn_t (*thread)(int irq, void *p))": "iio_triggered_event_cleanup(). ", "int iio_triggered_buffer_setup_ext(struct iio_dev *indio_dev,irqreturn_t (*h)(int irq, void *p),irqreturn_t (*thread)(int irq, void *p),enum iio_buffer_direction direction,const struct iio_buffer_setup_ops *setup_ops,const struct iio_dev_attr **buffer_attrs)": "iio_triggered_buffer_cleanup(). ", "*output = fixp_linear_interpolate(pts[i - 1].x, pts[i - 1].y,  pts[i].x, pts[i].y,  input);}return 0;}static s32 qcom_vadc_map_temp_voltage(const struct vadc_map_pt *pts,      u32 tablesize, int input)": "qcom_vadc_scale_hw_calib_volt(const struct u32_fract  prescale,const struct adc5_data  data,u16 adc_code, int  result_uv);static int qcom_vadc_scale_hw_calib_therm(const struct u32_fract  prescale,const struct adc5_data  data,u16 adc_code, int  result_mdec);static int qcom_vadc7_scale_hw_calib_therm(const struct u32_fract  prescale,const struct adc5_data  data,u16 adc_code, int  result_mdec);static int qcom_vadc_scale_hw_smb_temp(const struct u32_fract  prescale,const struct adc5_data  data,u16 adc_code, int  result_mdec);static int qcom_vadc_scale_hw_chg5_temp(const struct u32_fract  prescale,const struct adc5_data  data,u16 adc_code, int  result_mdec);static int qcom_vadc_scale_hw_calib_die_temp(const struct u32_fract  prescale,const struct adc5_data  data,u16 adc_code, int  result_mdec);static int qcom_vadc7_scale_hw_calib_die_temp(const struct u32_fract  prescale,const struct adc5_data  data,u16 adc_code, int  result_mdec);static struct qcom_adc5_scale_type scale_adc5_fn[] = {[SCALE_HW_CALIB_DEFAULT] = {qcom_vadc_scale_hw_calib_volt},[SCALE_HW_CALIB_THERM_100K_PULLUP] = {qcom_vadc_scale_hw_calib_therm},[SCALE_HW_CALIB_XOTHERM] = {qcom_vadc_scale_hw_calib_therm},[SCALE_HW_CALIB_THERM_100K_PU_PM7] = {qcom_vadc7_scale_hw_calib_therm},[SCALE_HW_CALIB_PMIC_THERM] = {qcom_vadc_scale_hw_calib_die_temp},[SCALE_HW_CALIB_PMIC_THERM_PM7] = {qcom_vadc7_scale_hw_calib_die_temp},[SCALE_HW_CALIB_PM5_CHG_TEMP] = {qcom_vadc_scale_hw_chg5_temp},[SCALE_HW_CALIB_PM5_SMB_TEMP] = {qcom_vadc_scale_hw_smb_temp},};static int qcom_vadc_map_voltage_temp(const struct vadc_map_pt  pts,      u32 tablesize, s32 input, int  output){u32 i = 0;if (!pts)return -EINVAL;while (i < tablesize && pts[i].x > input)i++;if (i == 0) { output = pts[0].y;} else if (i == tablesize) { output = pts[tablesize - 1].y;} else {  interpolate linearly ", "bool is_stm32_lptim_trigger(struct iio_trigger *trig)": "is_stm32_lptim_trigger   @trig: trigger to be checked     return true if the trigger is a valid STM32 IIO Low-Power Timer Trigger   either return false ", "mutex_lock(&priv->lock);if (sms == 6 && !priv->enabled) ": "is_stm32_timer_trigger(trig))return -EINVAL;while (cur &&  cur) {if (!strncmp(trig->name,  cur, strlen(trig->name))) {regmap_update_bits(priv->regmap,   TIM_SMCR, TIM_SMCR_TS,   i << TIM_SMCR_TS_SHIFT);return 0;}cur++;i++;}return -EINVAL;}static const struct iio_info stm32_trigger_info = {.validate_trigger = stm32_counter_validate_trigger,.read_raw = stm32_counter_read_raw,.write_raw = stm32_counter_write_raw};static const char  const stm32_trigger_modes[] = {\"trigger\",};static int stm32_set_trigger_mode(struct iio_dev  indio_dev,  const struct iio_chan_spec  chan,  unsigned int mode){struct stm32_timer_trigger  priv = iio_priv(indio_dev);regmap_update_bits(priv->regmap, TIM_SMCR, TIM_SMCR_SMS, TIM_SMCR_SMS);return 0;}static int stm32_get_trigger_mode(struct iio_dev  indio_dev,  const struct iio_chan_spec  chan){struct stm32_timer_trigger  priv = iio_priv(indio_dev);u32 smcr;regmap_read(priv->regmap, TIM_SMCR, &smcr);return (smcr & TIM_SMCR_SMS) == TIM_SMCR_SMS ? 0 : -EINVAL;}static const struct iio_enum stm32_trigger_mode_enum = {.items = stm32_trigger_modes,.num_items = ARRAY_SIZE(stm32_trigger_modes),.set = stm32_set_trigger_mode,.get = stm32_get_trigger_mode};static const char  const stm32_enable_modes[] = {\"always\",\"gated\",\"triggered\",};static int stm32_enable_mode2sms(int mode){switch (mode) {case 0:return 0;case 1:return 5;case 2:return 6;}return -EINVAL;}static int stm32_set_enable_mode(struct iio_dev  indio_dev, const struct iio_chan_spec  chan, unsigned int mode){struct stm32_timer_trigger  priv = iio_priv(indio_dev);int sms = stm32_enable_mode2sms(mode);if (sms < 0)return sms;    Triggered mode sets CEN bit automatically by hardware. So, first   enable counter clock, so it can use it. Keeps it in sync with CEN. ", "int amba_driver_register(struct amba_driver *drv)": "amba_driver_register(&amba_proxy_drv);}late_initcall_sync(amba_stub_drv_init);    amba_driver_register - register an AMBA device driver  @drv: amba device driver structure    Register an AMBA device driver with the Linux device model  core.  If devices pre-exist, the drivers probe function will  be called. ", "void amba_driver_unregister(struct amba_driver *drv)": "amba_driver_unregister - remove an AMBA device driver  @drv: AMBA device driver structure to remove    Unregister an AMBA device driver from the Linux device  model.  The device model will call the drivers remove function  for each device the device driver is currently handling. ", "int amba_device_register(struct amba_device *dev, struct resource *parent)": "amba_device_register - register an AMBA device  @dev: AMBA device to register  @parent: parent memory resource    Setup the AMBA device, reading the cell ID if present.  Claim the resource, and register the AMBA device with  the Linux device manager. ", "void amba_device_unregister(struct amba_device *dev)": "amba_device_unregister - unregister an AMBA device  @dev: AMBA device to remove    Remove the specified AMBA device from the Linux device  manager.  All files associated with this object will be  destroyed, and device drivers notified that the device has  been removed.  The AMBA device's resources including  the amba_device structure will be freed once all  references to it have been dropped. ", "int amba_request_regions(struct amba_device *dev, const char *name)": "amba_request_regions - request all mem regions associated with device  @dev: amba_device structure for device  @name: name, or NULL to use driver name ", "void amba_release_regions(struct amba_device *dev)": "amba_release_regions - release mem regions associated with device  @dev: amba_device structure for device    Release regions claimed by a successful call to amba_request_regions. ", "int ntb_msi_init(struct ntb_dev *ntb, void (*desc_changed)(void *ctx))": "ntb_msi_init() - Initialize the MSI context   @ntb:NTB device context     This function must be called before any other ntb_msi function.   It initializes the context for MSI operations and maps   the peer memory windows.     This function reserves the last N outbound memory windows (where N   is the number of peers).     Return: Zero on success, otherwise a negative error number. ", "int ntb_msi_setup_mws(struct ntb_dev *ntb)": "ntb_msi_setup_mws() - Initialize the MSI inbound memory windows   @ntb:NTB device context     This function sets up the required inbound memory windows. It should be   called from a work function after a link up event.     Over the entire network, this function will reserves the last N   inbound memory windows for each peer (where N is the number of peers).     ntb_msi_init() must be called before this function.     Return: Zero on success, otherwise a negative error number. ", "void ntb_msi_clear_mws(struct ntb_dev *ntb)": "ntb_msi_clear_mws() - Clear all inbound memory windows   @ntb:NTB device context     This function tears down the resources used by ntb_msi_setup_mws(). ", "int ntbm_msi_request_threaded_irq(struct ntb_dev *ntb, irq_handler_t handler,  irq_handler_t thread_fn,  const char *name, void *dev_id,  struct ntb_msi_desc *msi_desc)": "ntbm_msi_request_threaded_irq() - allocate an MSI interrupt   @ntb:NTB device context   @handler:Function to be called when the IRQ occurs   @thread_fn:  Function to be called in a threaded interrupt context. NULL                for clients which handle everything in @handler   @name:    An ascii name for the claiming device, dev_name(dev) if NULL   @dev_id:     A cookie passed back to the handler function   @msi_desc:MSI descriptor data which triggers the interrupt     This function assigns an interrupt handler to an unused   MSI interrupt and returns the descriptor used to trigger   it. The descriptor can then be sent to a peer to trigger   the interrupt.     The interrupt resource is managed with devres so it will   be automatically freed when the NTB device is torn down.     If an IRQ allocated with this function needs to be freed   separately, ntbm_free_irq() must be used.     Return: IRQ number assigned on success, otherwise a negative error number. ", "void ntbm_msi_free_irq(struct ntb_dev *ntb, unsigned int irq, void *dev_id)": "ntbm_msi_free_irq() - free an interrupt   @ntb:NTB device context   @irq:Interrupt line to free   @dev_id:Device identity to free     This function should be used to manually free IRQs allocated with   ntbm_request_[threaded_]irq(). ", "int ntb_msi_peer_trigger(struct ntb_dev *ntb, int peer, struct ntb_msi_desc *desc)": "ntb_msi_peer_trigger() - Trigger an interrupt handler on a peer   @ntb:NTB device context   @peer:Peer index   @desc:MSI descriptor data which triggers the interrupt     This function triggers an interrupt on a peer. It requires   the descriptor structure to have been passed from that peer   by some other means.     Return: Zero on success, otherwise a negative error number. ", "int ntb_msi_peer_addr(struct ntb_dev *ntb, int peer,      struct ntb_msi_desc *desc,      phys_addr_t *msi_addr)": "ntb_msi_peer_addr() - Get the DMA address to trigger a peer's MSI interrupt   @ntb:NTB device context   @peer:Peer index   @desc:MSI descriptor data which triggers the interrupt   @msi_addr:   Physical address to trigger the interrupt     This function allows using DMA engines to trigger an interrupt   (for example, trigger an interrupt to process the data after   sending it). To trigger the interrupt, write @desc.data to the address   returned in @msi_addr     Return: Zero on success, otherwise a negative error number. ", "int tty_register_ldisc(struct tty_ldisc_ops *new_ldisc)": "tty_register_ldisc-install a line discipline   @new_ldisc: pointer to the ldisc object     Installs a new line discipline into the kernel. The discipline is set up as   unreferenced and then made available to the kernel from this point onwards.     Locking: takes %tty_ldiscs_lock to guard against ldisc races ", "void tty_unregister_ldisc(struct tty_ldisc_ops *ldisc)": "tty_unregister_ldisc-unload a line discipline   @ldisc: ldisc number     Remove a line discipline from the kernel providing it is not currently in   use.     Locking: takes %tty_ldiscs_lock to guard against ldisc races ", "void tty_port_init(struct tty_port *port)": "tty_port_tty_get(port);if (tty) {tty_wakeup(tty);tty_kref_put(tty);}}const struct tty_port_client_operations tty_port_default_client_ops = {.receive_buf = tty_port_default_receive_buf,.lookahead_buf = tty_port_default_lookahead_buf,.write_wakeup = tty_port_default_wakeup,};EXPORT_SYMBOL_GPL(tty_port_default_client_ops);     tty_port_init -- initialize tty_port   @port: tty_port to initialize     Initializes the state of struct tty_port. When a port was initialized using   this function, one has to destroy the port by tty_port_destroy(). Either   indirectly by using &tty_port refcounting (tty_port_put()) or directly if   refcounting is not used. ", "mutex_lock(&port->buf_mutex);if (port->xmit_buf == NULL) ": "tty_port_alloc_xmit_buf(struct tty_port  port){  We may sleep in get_zeroed_page() ", "void tty_port_tty_set(struct tty_port *port, struct tty_struct *tty)": "tty_port_tty_set-set the tty of a port   @port: tty port   @tty: the tty     Associate the port and tty pair. Manages any internal refcounts. Pass %NULL   to deassociate a port. ", "static void tty_port_shutdown(struct tty_port *port, struct tty_struct *tty)": "tty_port_close(). Its task is to   shutdown the device if it was initialized (note consoles remain   functioning). It lowers DTRRTS (if @tty has HUPCL set) and invokes   @port->ops->shutdown(). ", "bool tty_port_carrier_raised(struct tty_port *port)": "tty_port_carrier_raised-carrier raised check   @port: tty port     Wrapper for the carrier detect logic. For the moment this is used   to hide some internal details. This will eventually become entirely   internal to the tty port. ", "void tty_port_raise_dtr_rts(struct tty_port *port)": "tty_port_raise_dtr_rts-Raise DTRRTS   @port: tty port     Wrapper for the DTRRTS raise logic. For the moment this is used to hide   some internal details. This will eventually become entirely internal to the   tty port. ", "void tty_port_hangup(struct tty_port *port)": "tty_port_lower_dtr_rts(port);if (port->ops->shutdown)port->ops->shutdown(port);}out:mutex_unlock(&port->mutex);}     tty_port_hangup-hangup helper   @port: tty port     Perform port level tty hangup flag and count changes. Drop the tty   reference.     Caller holds tty lock. ", "int tty_port_block_til_ready(struct tty_port *port,struct tty_struct *tty, struct file *filp)": "tty_port_block_til_ready-Waiting logic for tty open   @port: the tty port being opened   @tty: the tty device being bound   @filp: the file pointer of the opener or %NULL     Implement the core POSIXSuS tty behaviour when opening a tty device.   Handles:    - hangup (both before and during)  - non blocking open  - rtsdtrdcd  - signals  - port flags and counts     The passed @port must implement the @port->ops->carrier_raised method if it   can do carrier detect and the @port->ops->dtr_rts method if it supports   software management of these lines. Note that the dtrrts raise is done each   iteration as a hangup may have previously dropped them while we wait.     Caller holds tty lock.     Note: May drop and reacquire tty lock when blocking, so @tty and @port may   have changed state (eg., may have been hung up). ", "int tty_port_close_start(struct tty_port *port,struct tty_struct *tty, struct file *filp)": "tty_port_close_end().     Locking: Caller holds tty lock.     Return: 1 if this is the last close, otherwise 0 ", "int tty_port_open(struct tty_port *port, struct tty_struct *tty,struct file *filp)": "tty_port_open - generic tty->ops->open handler   @port: tty_port of the device   @tty: tty to be opened   @filp: passed file pointer     It is a generic helper to be used in driver's @tty->ops->open. It activates   the devices using @port->ops->activate if not active already. And waits for   the device to be ready using tty_port_block_til_ready() (e.g.  raises   DTRCTS and waits for carrier).     Note that @port->ops->shutdown is not called when @port->ops->activate   returns an error (on the contrary, @tty->ops->close is).     Locking: Caller holds tty lock.     Note: may drop and reacquire tty lock (in tty_port_block_til_ready()) so   @tty and @port may have changed state (eg., may be hung up now). ", "int __tty_check_change(struct tty_struct *tty, int sig)": "tty_check_change-check for POSIX terminal changes  @tty: tty to check  @sig: signal to send    If we try to write to, or set the state of, a terminal and we're  not in the foreground, send a SIGTTOU.  If the signal is blocked or  ignored, go ahead and perform the operation.  (POSIX 7.2)    Locking: ctrl.lock ", ".c_iflag = ICRNL | IXON,.c_oflag = OPOST | ONLCR,.c_cflag = B38400 | CS8 | CREAD | HUPCL,.c_lflag = ISIG | ICANON | ECHO | ECHOE | ECHOK |   ECHOCTL | ECHOKE | IEXTEN,.c_cc = INIT_C_CC,.c_ispeed = 38400,.c_ospeed = 38400,/* .c_line = N_TTY, ": "tty_std_termios = {  for the benefit of tty drivers  ", "const char *tty_name(const struct tty_struct *tty)": "tty_name-return tty naming   @tty: tty structure     Convert a tty structure into a name. The name reflects the kernel naming   policy and if udev is in use may not reflect user space     Locking: none ", "static void __tty_hangup(struct tty_struct *tty, int exit_session)": "tty_hangup-actual handler for hangup events   @tty: tty device   @exit_session: if non-zero, signal all foreground group processes     This can be called by a \"kworker\" kernel thread. That is process synchronous   but doesn't hold any locks, so we need to make sure we have the appropriate   locks for what we're doing.     The hangup event clears any pending redirections onto the hung up device. It   ensures future writes will error and it does the needed line discipline   hangup and signal delivery. The tty object itself remains intact.     Locking:      BTM         redirect lock for undoing redirection       file list lock for manipulating list of ttys       tty_ldiscs_lock from called functions       termios_rwsem resetting termios data       tasklist_lock to walk task list for hangup event          ->siglock to protect ->signal->sighand   ", "void tty_vhangup(struct tty_struct *tty)": "tty_vhangup-process vhangup   @tty: tty to hangup     The user has asked via system call for the terminal to be hung up. We do   this synchronously so that when the syscall returns the process is complete.   That guarantee is necessary for security reasons. ", "int tty_hung_up_p(struct file *filp)": "tty_hung_up_p-was tty hung up   @filp: file pointer of tty     Return: true if the tty has been subject to a vhangup or a carrier loss ", "void stop_tty(struct tty_struct *tty)": "stop_tty(struct tty_struct  tty){if (tty->flow.stopped)return;tty->flow.stopped = true;if (tty->ops->stop)tty->ops->stop(tty);}     stop_tty-propagate flow control   @tty: tty to stop     Perform flow control to the driver. May be called on an already stopped   device and will not re-call the &tty_driver->stop() method.     This functionality is used by both the line disciplines for halting incoming   flow and by the driver. It may therefore be called from any context, may be   under the tty %atomic_write_lock but not always.     Locking:  flow.lock ", "void start_tty(struct tty_struct *tty)": "start_tty(struct tty_struct  tty){if (!tty->flow.stopped || tty->flow.tco_stopped)return;tty->flow.stopped = false;if (tty->ops->start)tty->ops->start(tty);tty_wakeup(tty);}     start_tty-propagate flow control   @tty: tty to start     Start a tty that has been stopped if at all possible. If @tty was previously   stopped and is now being started, the &tty_driver->start() method is invoked   and the line discipline woken.     Locking:  flow.lock ", "if (cons_filp) ": "tty_kref_put(tty);tty_ldisc_hangup(tty, cons_filp != NULL);spin_lock_irq(&tty->ctrl.lock);clear_bit(TTY_THROTTLED, &tty->flags);clear_bit(TTY_DO_WRITE_WAKEUP, &tty->flags);put_pid(tty->ctrl.session);put_pid(tty->ctrl.pgrp);tty->ctrl.session = NULL;tty->ctrl.pgrp = NULL;tty->ctrl.pktstatus = 0;spin_unlock_irq(&tty->ctrl.lock);    If one of the devices matches a console pointer, we   cannot just call hangup() because that will cause   tty->count and state->count to go out of sync.   So we just call close() the right number of times. ", "int tty_do_resize(struct tty_struct *tty, struct winsize *ws)": "tty_do_resize-resize event   @tty: tty being resized   @ws: new dimensions     Update the termios variables and send the necessary signals to peform a   terminal resize correctly. ", "#include <linux/types.h>#include <linux/major.h>#include <linux/errno.h>#include <linux/signal.h>#include <linux/fcntl.h>#include <linux/sched/signal.h>#include <linux/sched/task.h>#include <linux/interrupt.h>#include <linux/tty.h>#include <linux/tty_driver.h>#include <linux/tty_flip.h>#include <linux/devpts_fs.h>#include <linux/file.h>#include <linux/fdtable.h>#include <linux/console.h>#include <linux/timer.h>#include <linux/ctype.h>#include <linux/kd.h>#include <linux/mm.h>#include <linux/string.h>#include <linux/slab.h>#include <linux/poll.h>#include <linux/ppp-ioctl.h>#include <linux/proc_fs.h>#include <linux/init.h>#include <linux/module.h>#include <linux/device.h>#include <linux/wait.h>#include <linux/bitops.h>#include <linux/delay.h>#include <linux/seq_file.h>#include <linux/serial.h>#include <linux/ratelimit.h>#include <linux/compat.h>#include <linux/uaccess.h>#include <linux/termios_internal.h>#include <linux/fs.h>#include <linux/kbd_kern.h>#include <linux/vt_kern.h>#include <linux/selection.h>#include <linux/kmod.h>#include <linux/nsproxy.h>#include \"tty.h\"#undef TTY_DEBUG_HANGUP#ifdef TTY_DEBUG_HANGUP# define tty_debug_hangup(tty, f, args...)tty_debug(tty, f, ##args)#else# define tty_debug_hangup(tty, f, args...)do ": "do_SAK() into process context.  Less stack use in devfs functions.   alloc_tty_struct() always uses kmalloc()   -- Andrew Morton <andrewm@uow.edu.eu> 17Mar01 ", "struct device *tty_register_device(struct tty_driver *driver, unsigned index,   struct device *device)": "tty_register_device - register a tty device   @driver: the tty driver that describes the tty device   @index: the index in the tty driver for this tty device   @device: a struct device that is associated with this tty device.  This field is optional, if there is no known struct device  for this tty device it can be set to NULL safely.     This call is required to be made to register an individual tty device   if the tty driver's flags have the %TTY_DRIVER_DYNAMIC_DEV bit set.  If   that bit is not set, this function should not be called by a tty   driver.     Locking: ??     Return: A pointer to the struct device for this tty device (or   ERR_PTR(-EFOO) on error). ", "void tty_unregister_device(struct tty_driver *driver, unsigned index)": "tty_unregister_device - unregister a tty device   @driver: the tty driver that describes the tty device   @index: the index in the tty driver for this tty device     If a tty device is registered with a call to tty_register_device() then   this function must be called when the tty device is gone.     Locking: ?? ", "struct tty_driver *__tty_alloc_driver(unsigned int lines, struct module *owner,unsigned long flags)": "__tty_alloc_driver -- allocate tty driver   @lines: count of lines this driver can handle at most   @owner: module which is responsible for this driver   @flags: some of %TTY_DRIVER_ flags, will be set in driver->flags     This should not be called directly, some of the provided macros should be   used instead. Use IS_ERR() and friends on @retval. ", "INIT_WORK(&tty->hangup_work, release_one_tty);schedule_work(&tty->hangup_work);}/** * tty_kref_put-release a tty kref * @tty: tty device * * Release a reference to the @tty device and if need be let the kref layer * destruct the object for us. ": "tty_driver_kref_put(driver);module_put(owner);spin_lock(&tty->files_lock);list_del_init(&tty->tty_files);spin_unlock(&tty->files_lock);put_pid(tty->ctrl.pgrp);put_pid(tty->ctrl.session);free_tty_struct(tty);}static void queue_release_one_tty(struct kref  kref){struct tty_struct  tty = container_of(kref, struct tty_struct, kref);  The hangup queue is now free so we can reuse it rather than    waste a chunk of memory for each port. ", "int tty_register_driver(struct tty_driver *driver)": "tty_register_driver -- register a tty driver   @driver: driver to register     Called by a tty driver to register itself. ", "void tty_driver_kref_put(struct tty_driver *driver)": "tty_unregister_driver(driver);if (driver->flags & TTY_DRIVER_DYNAMIC_ALLOC)cdev_del(driver->cdevs[0]);}kfree(driver->cdevs);kfree(driver->ports);kfree(driver->termios);kfree(driver->ttys);kfree(driver);}     tty_driver_kref_put -- drop a reference to a tty driver   @driver: driver of which to drop the reference     The final put will destroy and free up the driver. ", "case TIOCSBRK:/* Turn break on, unconditionally ": "tty_devnum(real_tty));return put_user(ret, (unsigned int __user  )p);}    Break handling ", "speed_t tty_termios_baud_rate(const struct ktermios *termios)": "tty_termios_baud_rate  @termios: termios structure    Convert termios baud rate data into a speed. This should be called  with the termios lock held if this termios is a terminal termios  structure. Device drivers can call this function but should use  ->c_[io]speed directly as they are updated.    Locking: none ", "speed_t tty_termios_input_baud_rate(const struct ktermios *termios)": "tty_termios_input_baud_rate  @termios: termios structure    Convert termios baud rate data into a speed. This should be called  with the termios lock held if this termios is a terminal termios  structure. Device drivers can call this function but should use  ->c_[io]speed directly as they are updated.    Locking: none ", "unsigned int tty_chars_in_buffer(struct tty_struct *tty)": "tty_chars_in_buffer-characters pending  @tty: terminal    Return the number of bytes of data in the device private  output queue. If no private method is supplied there is assumed  to be no queue on the device. ", " unsigned int tty_write_room(struct tty_struct *tty)": "tty_write_room-write queue space  @tty: terminal    Return the number of bytes that can be queued to this device  at the present time. The result should be treated as a guarantee  and the driver cannot offer a value it later shrinks by more than  the number of bytes written. If no method is provided 2K is always  returned and data may be lost as there will be no flow control. ", "void tty_driver_flush_buffer(struct tty_struct *tty)": "tty_driver_flush_buffer-discard internal buffer  @tty: terminal    Discard the internal output buffer for this device. If no method  is provided then either the buffer cannot be hardware flushed or  there is no buffer driver side. ", "void tty_unthrottle(struct tty_struct *tty)": "tty_unthrottle-flow control  @tty: terminal    Indicate that a tty may continue transmitting data down the stack.  Takes the termios rwsem to protect against parallel throttleunthrottle  and also to ensure the driver can consistently reference its own  termios data at this point when implementing software flow control.    Drivers should however remember that the stack can issue a throttle,  then change flow control method, then unthrottle. ", "void tty_wait_until_sent(struct tty_struct *tty, long timeout)": "tty_wait_until_sent-wait for IO to finish  @tty: tty we are waiting for  @timeout: how long we will wait    Wait for characters pending in a tty driver to hit the wire, or  for a timeout to occur (eg due to flow control)    Locking: none ", "void tty_termios_copy_hw(struct ktermios *new, const struct ktermios *old)": "tty_termios_copy_hw-copy hardware settings  @new: New termios  @old: Old termios    Propagate the hardware specific terminal setting bits from  the old termios structure to the new one. This is used in cases  where the hardware does not support reconfiguration or as a helper  in some cases where only minimal reconfiguration is supported ", "bool tty_termios_hw_change(const struct ktermios *a, const struct ktermios *b)": "tty_termios_hw_change-check for setting change  @a: termios  @b: termios to compare    Check if any of the bits that affect a dumb device have changed  between the two termios structures, or a speed change is needed. ", "return tty_mode_ioctl(tty, cmd, arg);}}EXPORT_SYMBOL(n_tty_ioctl_helper": "n_tty_ioctl_helper(struct tty_struct  tty, unsigned int cmd,unsigned long arg){int retval;switch (cmd) {case TCXONC:retval = tty_check_change(tty);if (retval)return retval;switch (arg) {case TCOOFF:spin_lock_irq(&tty->flow.lock);if (!tty->flow.tco_stopped) {tty->flow.tco_stopped = true;__stop_tty(tty);}spin_unlock_irq(&tty->flow.lock);break;case TCOON:spin_lock_irq(&tty->flow.lock);if (tty->flow.tco_stopped) {tty->flow.tco_stopped = false;__start_tty(tty);}spin_unlock_irq(&tty->flow.lock);break;case TCIOFF:if (STOP_CHAR(tty) != __DISABLED_CHAR)retval = tty_send_xchar(tty, STOP_CHAR(tty));break;case TCION:if (START_CHAR(tty) != __DISABLED_CHAR)retval = tty_send_xchar(tty, START_CHAR(tty));break;default:return -EINVAL;}return retval;case TCFLSH:retval = tty_check_change(tty);if (retval)return retval;return __tty_perform_flush(tty, arg);default:  Try the mode commands ", "int tty_insert_flip_string_fixed_flag(struct tty_port *port,const unsigned char *chars, char flag, size_t size)": "tty_insert_flip_string_fixed_flag - add characters to the tty buffer   @port: tty port   @chars: characters   @flag: flag value for each character   @size: size     Queue a series of bytes to the tty buffering. All the characters passed are   marked with the supplied flag.     Returns: the number added. ", "int tty_insert_flip_string_flags(struct tty_port *port,const unsigned char *chars, const char *flags, size_t size)": "tty_insert_flip_string_flags-add characters to the tty buffer   @port: tty port   @chars: characters   @flags: flag bytes   @size: size     Queue a series of bytes to the tty buffering. For each character the flags   array indicates the status of the character.     Returns: the number added. ", "int __tty_insert_flip_char(struct tty_port *port, unsigned char ch, char flag)": "__tty_insert_flip_char   -add one character to the tty buffer   @port: tty port   @ch: character   @flag: flag byte     Queue a single byte @ch to the tty buffering, with an optional flag. This is   the slow path of tty_insert_flip_char(). ", "void tty_flip_buffer_push(struct tty_port *port)": "tty_flip_buffer_push-push terminal buffers   @port: tty port to push     Queue a push of the terminal flip buffers to the line discipline. Can be   called from IRQatomic context.     In the event of the queue being busy for flipping the work will be held off   and retried later. ", "orig_log_level = console_loglevel;console_loglevel = CONSOLE_LOGLEVEL_DEFAULT;op_p = __sysrq_get_key_op(key);if (op_p) ": "handle_sysrq(int key, bool check_mask){const struct sysrq_key_op  op_p;int orig_log_level;int orig_suppress_printk;int i;orig_suppress_printk = suppress_printk;suppress_printk = 0;rcu_sysrq_start();rcu_read_lock();    Raise the apparent loglevel to maximum so that the sysrq header   is shown to provide the user with positive feedback.  We do not   simply emit this at KERN_EMERG as that would change message   routing in the consumers of prockmsg. ", "int fg_console;EXPORT_SYMBOL(fg_console": "fg_console is the current virtual console,   last_console is the last used one,   want_console is the console we want to switch to,   saved_  variants are for saverestore around kernel debugger enterleave ", "/* printk(\"redraw_screen: tty %d not allocated ??\\n\", new_console+1); ": "redraw_screen(struct vc_data  vc, int is_switch){int redraw = 0;WARN_CONSOLE_UNLOCKED();if (!vc) {  strange ... ", "return resize_screen(vc, new_cols, new_rows, user);}if (new_screen_size > KMALLOC_MAX_SIZE || !new_screen_size)return -EINVAL;newscreen = kzalloc(new_screen_size, GFP_USER);if (!newscreen)return -ENOMEM;if (vc->vc_uni_lines) ": "vc_resize_user;vc->vc_resize_user = 0;if (cols > VC_MAXCOL || lines > VC_MAXROW)return -EINVAL;new_cols = (cols ? cols : vc->vc_cols);new_rows = (lines ? lines : vc->vc_rows);new_row_size = new_cols << 1;new_screen_size = new_row_size   new_rows;if (new_cols == vc->vc_cols && new_rows == vc->vc_rows) {    This function is being called here to cover the case   where the userspace calls the FBIOPUT_VSCREENINFO twice,   passing the same fb_var_screeninfo containing the fields   yresxres equal to a number non-multiple of vc_font.height   and yres_virtualxres_virtual equal to number lesser than the   vc_font.height and yresxres.   In the second call, the struct fb_var_screeninfo isn't   being modified by the underlying driver because of the   if above, and this causes the fbcon_display->vrows to become   negative and it eventually leads to out-of-bound   access by the imageblit function.   To give the correct values to the struct and to not have   to deal with possible errors from the code below, we call   the resize_screen here as well. ", "conswitchp = defconsw;}if (!con_is_bound(csw))con_driver->flag &= ~CON_DRIVER_FLAG_INIT;/* ignore return value, binding should not fail ": "con_is_bound(csw))goto err;first = max(first, con_driver->first);last = min(last, con_driver->last);for (i = first; i <= last; i++) {if (con_driver_map[i] == csw) {module_put(csw->owner);con_driver_map[i] = NULL;}}if (!con_is_bound(defcsw)) {const struct consw  defconsw = conswitchp;defcsw->con_startup();con_back->flag |= CON_DRIVER_FLAG_INIT;    vgacon may change the default driver to point   to dummycon, we restore it here... ", "static inline void scrolldelta(int lines)": "con_is_visible(vc) && !console_blanked;}static inline unsigned short  screenpos(const struct vc_data  vc, int offset,bool viewed){unsigned short  p;if (!viewed)p = (unsigned short  )(vc->vc_origin + offset);else if (!vc->vc_sw->con_screen_pos)p = (unsigned short  )(vc->vc_visible_origin + offset);elsep = vc->vc_sw->con_screen_pos(vc, offset);return p;}  Called  from the keyboard irq path.. ", "void give_up_console(const struct consw *csw)": "give_up_console is a wrapper to unregister_con_driver. It will only   work if driver is fully unbound. ", "return -EINVAL;}want_console = nr;schedule_console_callback();return 0;}struct tty_driver *console_driver;#ifdef CONFIG_VT_CONSOLE/** * vt_kmsg_redirect() - Sets/gets the kernel message console * @new:The new virtual terminal number or -1 if the console should stay * unchanged * * By default, the kernel messages are always printed on the current virtual * console. However, the user may modify that default with the * TIOCL_SETKMSGREDIRECT ioctl call. * * This function sets the kernel message console to be @new. It returns the old * virtual console number. The virtual terminal number 0 (both as parameter and * return value) means no redirection (i.e. always printed on the currently * active console). * * The parameter -1 means that only the current console is returned, but the * value is not modified. You may use the macro vt_get_kmsg_redirect() in that * case to make the code more understandable. * * When the kernel is compiled without CONFIG_VT_CONSOLE, this function ignores * the parameter and always returns 0. ": "do_blank_screen(0);blank_timer_expired = 0;}notify_update(vc_cons[fg_console].d);console_unlock();}int set_console(int nr){struct vc_data  vc = vc_cons[fg_console].d;if (!vc_cons_allocated(nr) || vt_dont_switch ||(vc->vt_mode.mode == VT_AUTO && vc->vc_mode == KD_GRAPHICS)) {    Console switch will fail in console_callback() or   change_console() so there is no point scheduling   the callback     Existing set_console() users don't check the return   value so this shouldn't break anything ", "if (!oops_in_progress)might_sleep();WARN_CONSOLE_UNLOCKED();ignore_poke = 0;if (!console_blanked)return;if (!vc_cons_allocated(fg_console)) ": "do_unblank_screen(int leaving_gfx){struct vc_data  vc;  This should now always be called from a \"sane\" (read: can schedule)   context for the sake of the low level drivers, except in the special   case of oops_in_progress ", "int con_set_default_unimap(struct vc_data *vc)": "con_set_default_unimap-set default unicode map  @vc: the console we are updating    Loads the unimap for the hardware font, as defined in uni_hash.tbl.  The representation used was the most compact I could come up  with.  This routine is executed at video setup, and when the  PIO_FONTRESET ioctl is called.    The caller must hold the console lock ", "int con_copy_unimap(struct vc_data *dst_vc, struct vc_data *src_vc)": "con_copy_unimap-copy unimap between two vts  @dst_vc: target  @src_vc: source    The caller must hold the console lock when invoking this method ", "if (drv->nr == count)err = uart_register_driver(drv);if (err == 0) ": "sunserial_register_minors(struct uart_driver  drv, int count){int err = 0;drv->minor = sunserial_current_minor;drv->nr += count;  Register the driver on the first call ", "if (mouse_got_break && ctr < 8)return 1;/* Ok, we need to try another baud. ": "suncore_mouse_baud_detection(unsigned char ch, int is_break){static int mouse_got_break = 0;static int ctr = 0;if (is_break) {  Let a few normal bytes go by before we jump the gun   and say we need to try another baud rate. ", "void uart_write_wakeup(struct uart_port *port)": "uart_write_wakeup - schedule write processing   @port: port to be processed     This routine is used by the interrupt handler to schedule processing in the   software interrupt portion of the driver. A driver is expected to call this   function when the number of characters in the transmit buffer have dropped   below a threshold.     Locking: @port->lock should be held ", "voiduart_update_timeout(struct uart_port *port, unsigned int cflag,    unsigned int baud)": "uart_update_timeout - update per-port frame timing information   @port: uart_port structure describing the port   @cflag: termios cflag value   @baud: speed of the port     Set the @port frame timing information from which the FIFO timeout value is   derived. The @cflag value should reflect the actual hardware settings as   number of bits, parity, stop bits and baud rate is taken into account here.     Locking: caller is expected to take @port->lock ", "unsigned intuart_get_baud_rate(struct uart_port *port, struct ktermios *termios,   const struct ktermios *old, unsigned int min, unsigned int max)": "uart_get_baud_rate - return baud rate for a particular port   @port: uart_port structure describing the port in question.   @termios: desired termios settings   @old: old termios (or %NULL)   @min: minimum acceptable baud rate   @max: maximum acceptable baud rate     Decode the termios structure into a numeric baud rate, taking account of the   magic 38400 baud rate (with spd_  flags), and mapping the %B0 rate to 9600   baud.     If the new baud rate is invalid, try the @old termios setting. If it's still   invalid, we try 9600 baud.     The @termios structure is updated to reflect the baud rate we're actually   going to be using. Don't do this for the case where B0 is requested (\"hang   up\").     Locking: caller dependent ", "unsigned intuart_get_divisor(struct uart_port *port, unsigned int baud)": "uart_get_divisor - return uart clock divisor   @port: uart_port structure describing the port   @baud: desired baud rate     Calculate the divisor (baud_base  baud) for the specified @baud,   appropriately rounded.     If 38400 baud and custom divisor is selected, return the custom divisor   instead.     Locking: caller dependent ", "if (!console_suspend_enabled && uart_console(uport)) ": "uart_suspend_port(struct uart_driver  drv, struct uart_port  uport){struct uart_state  state = drv->state + uport->line;struct tty_port  port = &state->port;struct device  tty_dev;struct uart_match match = {uport, drv};mutex_lock(&port->mutex);tty_dev = device_find_child(uport->dev, &match, serial_match_port);if (tty_dev && device_may_wakeup(tty_dev)) {enable_irq_wake(uport->irq);put_device(tty_dev);mutex_unlock(&port->mutex);return 0;}put_device(tty_dev);    Nothing to do if the console is not suspending   except stop_rx to prevent any asynchronous data   over RX line. However ensure that we will be   able to Re-start_rx later. ", "if (uart_console(uport)) ": "uart_resume_port(struct uart_driver  drv, struct uart_port  uport){struct uart_state  state = drv->state + uport->line;struct tty_port  port = &state->port;struct device  tty_dev;struct uart_match match = {uport, drv};struct ktermios termios;mutex_lock(&port->mutex);tty_dev = device_find_child(uport->dev, &match, serial_match_port);if (!uport->suspended && device_may_wakeup(tty_dev)) {if (irqd_is_wakeup_set(irq_get_irq_data((uport->irq))))disable_irq_wake(uport->irq);put_device(tty_dev);mutex_unlock(&port->mutex);return 0;}put_device(tty_dev);uport->suspended = 0;    Re-enable the console device after suspending. ", "int uart_register_driver(struct uart_driver *drv)": "uart_register_driver - register a driver with the uart core layer   @drv: low level driver structure     Register a uart driver with the core driver. We in turn register with the   tty layer, and initialise the core driver per-port state.     We have a proc file in procttydriver which is named after the normal   driver.     @drv->port should be %NULL, and the per-port structures should be registered   using uart_add_one_port() after this call has succeeded.     Locking: none, Interrupts: enabled ", "void uart_unregister_driver(struct uart_driver *drv)": "uart_unregister_driver - remove a driver from the uart core layer   @drv: low level driver structure     Remove all references to a driver from the core driver. The low level   driver must have removed all its ports via the uart_remove_one_port() if it   registered them with uart_add_one_port(). (I.e. @drv->port is %NULL.)     Locking: none, Interrupts: enabled ", "bool uart_match_port(const struct uart_port *port1,const struct uart_port *port2)": "uart_match_port - are the two ports equivalent?   @port1: first port   @port2: second port     This utility function can be used to determine whether two uart_port   structures describe the same port. ", "void serial8250_suspend_port(int line)": "serial8250_suspend_port - suspend one serial port  @line:  serial line number    Suspend one serial port. ", "void serial8250_resume_port(int line)": "serial8250_resume_port - resume one serial port  @line:  serial line number    Resume one serial port. ", "#include <linux/acpi.h>#include <linux/module.h>#include <linux/moduleparam.h>#include <linux/ioport.h>#include <linux/init.h>#include <linux/console.h>#include <linux/sysrq.h>#include <linux/delay.h>#include <linux/platform_device.h>#include <linux/pm_runtime.h>#include <linux/tty.h>#include <linux/ratelimit.h>#include <linux/tty_flip.h>#include <linux/serial.h>#include <linux/serial_8250.h>#include <linux/nmi.h>#include <linux/mutex.h>#include <linux/slab.h>#include <linux/string_helpers.h>#include <linux/uaccess.h>#include <linux/io.h>#ifdef CONFIG_SPARC#include <linux/sunserialcore.h>#endif#include <asm/irq.h>#include \"8250.h\"/* * Configuration: *   share_irqs - whether we pass IRQF_SHARED to request_irq().  This option *                is unsafe when used on edge-triggered interrupts. ": "serial8250_register_8250_port() ports ", "static struct platform_device *serial8250_isa_devs;/* * serial8250_register_8250_port and serial8250_unregister_port allows for * 16x50 serial ports to be configured at run-time, to support PCMCIA * modems and PCI multiport cards. ": "serial8250_unregister_port(i);}return 0;}static int serial8250_suspend(struct platform_device  dev, pm_message_t state){int i;for (i = 0; i < UART_NR; i++) {struct uart_8250_port  up = &serial8250_ports[i];if (up->port.type != PORT_UNKNOWN && up->port.dev == &dev->dev)uart_suspend_port(&serial8250_reg, &up->port);}return 0;}static int serial8250_resume(struct platform_device  dev){int i;for (i = 0; i < UART_NR; i++) {struct uart_8250_port  up = &serial8250_ports[i];if (up->port.type != PORT_UNKNOWN && up->port.dev == &dev->dev)serial8250_resume_port(i);}return 0;}static struct platform_driver serial8250_isa_driver = {.probe= serial8250_probe,.remove= serial8250_remove,.suspend= serial8250_suspend,.resume= serial8250_resume,.driver= {.name= \"serial8250\",},};    This \"device\" covers _all_ ISA 8250-compatible serial devices listed   in the table in includeasmserial.h ", "serial8250_rpm_get(up);spin_lock_irqsave(&port->lock, flags);up->lcr = cval;/* Save computed LCR ": "serial8250_do_set_termios(struct uart_port  port, struct ktermios  termios,          const struct ktermios  old){struct uart_8250_port  up = up_to_u8250p(port);unsigned char cval;unsigned long flags;unsigned int baud, quot, frac = 0;if (up->capabilities & UART_CAP_MINI) {termios->c_cflag &= ~(CSTOPB | PARENB | PARODD | CMSPAR);if ((termios->c_cflag & CSIZE) == CS5 ||    (termios->c_cflag & CSIZE) == CS6)termios->c_cflag = (termios->c_cflag & ~CSIZE) | CS7;}cval = serial8250_compute_lcr(up, termios->c_cflag);baud = serial8250_get_baud_rate(port, termios, old);quot = serial8250_get_divisor(port, baud, &frac);    Ok, we're now changing the port state.  Do it with   interrupts disabled.     Synchronize UART_IER access against the console. ", "unsigned int convert_ifc_address(phys_addr_t addr_base)": "convert_ifc_address - convert the base address   @addr_base:base address of the memory bank ", "int fsl_ifc_find(phys_addr_t addr_base)": "fsl_ifc_find - find IFC bank   @addr_base:base address of the memory bank     This function walks IFC banks comparing \"Base address\" field of the CSPR   registers with the supplied addr_base argument. When bases match this   function returns bank number (starting with 0), otherwise it returns   appropriate errno value. ", "const struct lpddr2_min_tck *of_get_min_tck(struct device_node *np,    struct device *dev)": "of_get_min_tck() - extract min timing values for ddr   @np: pointer to ddr device tree node   @dev: device requesting for min timing values     Populates the lpddr2_min_tck structure by extracting data   from device tree node. Returns a pointer to the populated   structure. If any error in populating the structure, returns   default min timings provided by JEDEC. ", "const struct lpddr2_timings *of_get_ddr_timings(struct device_node *np_ddr,struct device *dev,u32 device_type,u32 *nr_frequencies)": "of_get_ddr_timings() - extracts the ddr timings and updates no of   frequencies available.   @np_ddr: Pointer to ddr device tree node   @dev: Device requesting for ddr timings   @device_type: Type of ddr(LPDDR2 S2S4)   @nr_frequencies: No of frequencies available for ddr   (updated by this function)     Populates lpddr2_timings structure by extracting data from device   tree node. Returns pointer to populated structure. If any error   while populating, returns default timings provided by JEDEC. ", "const struct lpddr3_min_tck *of_lpddr3_get_min_tck(struct device_node *np,   struct device *dev)": "of_lpddr3_get_min_tck() - extract min timing values for lpddr3   @np: pointer to ddr device tree node   @dev: device requesting for min timing values     Populates the lpddr3_min_tck structure by extracting data   from device tree node. Returns a pointer to the populated   structure. If any error in populating the structure, returns NULL. ", "const struct lpddr3_timings*of_lpddr3_get_ddr_timings(struct device_node *np_ddr, struct device *dev,   u32 device_type, u32 *nr_frequencies)": "of_lpddr3_get_ddr_timings() - extracts the lpddr3 timings and updates no of   frequencies available.   @np_ddr: Pointer to ddr device tree node   @dev: Device requesting for ddr timings   @device_type: Type of ddr   @nr_frequencies: No of frequencies available for ddr   (updated by this function)     Populates lpddr3_timings structure by extracting data from device   tree node. Returns pointer to populated structure. If any error   while populating, returns NULL. ", "const struct lpddr2_info*of_lpddr2_get_info(struct device_node *np, struct device *dev)": "of_lpddr2_get_info() - extracts information about the lpddr2 chip.   @np: Pointer to device tree node containing lpddr2 info   @dev: Device requesting info     Populates lpddr2_info structure by extracting data from device   tree node. Returns pointer to populated structure. If error   happened while populating, returns NULL. If property is missing   in a device-tree, then the corresponding value is set to -ENOENT. ", "unsigned int jz4780_nemc_num_banks(struct device *dev)": "jz4780_nemc_num_banks() - count the number of banks referenced by a device   @dev: device to count banks for, must be a child of the NEMC.     Return: The number of unique NEMC banks referred to by the specified NEMC   child device. Unique here means that a device that references the same bank   multiple times in its \"reg\" property will only count once. ", "void jz4780_nemc_set_type(struct device *dev, unsigned int bank,  enum jz4780_nemc_bank_type type)": "jz4780_nemc_set_type() - set the type of device connected to a bank   @dev: child device of the NEMC.   @bank: bank number to configure.   @type: type of device connected to the bank. ", "void jz4780_nemc_assert(struct device *dev, unsigned int bank, bool assert)": "jz4780_nemc_assert() - (de-)assert a NAND device's chip enable pin   @dev: child device of the NEMC.   @bank: bank number of device.   @assert: whether the chip enable pin should be asserted.     (De-)asserts the chip enable pin for the NAND device connected to the   specified bank. ", "regmap_update_bits(rpc->regmap, RPCIF_PHYCNT, RPCIF_PHYCNT_HS, 0);regmap_update_bits(rpc->regmap, RPCIF_PHYCNT,   /* create mask with all affected bits set ": "rpcif_hw_init(struct device  dev, bool hyperflash){struct rpcif_priv  rpc = dev_get_drvdata(dev);u32 dummy;int ret;ret = pm_runtime_resume_and_get(dev);if (ret)return ret;if (rpc->info->type == RPCIF_RZ_G2L) {ret = reset_control_reset(rpc->rstc);if (ret)return ret;usleep_range(200, 300);rpcif_rzg2l_timing_adjust_sdr(rpc);}regmap_update_bits(rpc->regmap, RPCIF_PHYCNT, RPCIF_PHYCNT_PHYMEM_MASK,   RPCIF_PHYCNT_PHYMEM(hyperflash ? 3 : 0));  DMA Transfer is not supported ", "nbytes = bytes_left >= max ? max : (1 << ilog2(bytes_left));if (bytes_left > nbytes)smcr |= RPCIF_SMCR_SSLKP;smenr |= RPCIF_SMENR_SPIDE(rpcif_bits_set(rpc, nbytes));regmap_write(rpc->regmap, RPCIF_SMENR, smenr);rpc->xfer_size = nbytes;memcpy(data, rpc->buffer + pos, nbytes);if (nbytes == 8)regmap_write(rpc->regmap, RPCIF_SMWDR1, *p++);regmap_write(rpc->regmap, RPCIF_SMWDR0, *p);regmap_write(rpc->regmap, RPCIF_SMCR, smcr);ret = wait_msg_xfer_end(rpc);if (ret)goto err_out;pos += nbytes;smenr = rpc->enable &~RPCIF_SMENR_CDE & ~RPCIF_SMENR_ADE(0xF);}break;case RPCIF_DATA_IN:/* * RPC-IF spoils the data for the commands without an address * phase (like RDID) in the manual mode, so we'll have to work * around this issue by using the external address space read * mode instead. ": "rpcif_manual_xfer(struct device  dev){struct rpcif_priv  rpc = dev_get_drvdata(dev);u32 smenr, smcr, pos = 0, max = rpc->bus_size == 2 ? 8 : 4;int ret = 0;ret = pm_runtime_resume_and_get(dev);if (ret < 0)return ret;regmap_update_bits(rpc->regmap, RPCIF_PHYCNT,   RPCIF_PHYCNT_CAL, RPCIF_PHYCNT_CAL);regmap_update_bits(rpc->regmap, RPCIF_CMNCR,   RPCIF_CMNCR_MD, RPCIF_CMNCR_MD);regmap_write(rpc->regmap, RPCIF_SMCMR, rpc->command);regmap_write(rpc->regmap, RPCIF_SMOPR, rpc->option);regmap_write(rpc->regmap, RPCIF_SMDMCR, rpc->dummy);regmap_write(rpc->regmap, RPCIF_SMDRENR, rpc->ddr);regmap_write(rpc->regmap, RPCIF_SMADR, rpc->smadr);smenr = rpc->enable;switch (rpc->dir) {case RPCIF_DATA_OUT:while (pos < rpc->xferlen) {u32 bytes_left = rpc->xferlen - pos;u32 nbytes, data[2],  p = data;smcr = rpc->smcr | RPCIF_SMCR_SPIE;  nbytes may only be 1, 2, 4, or 8 ", "gpmc_cs_disable_mem(cs);r = gpmc_cs_set_memconf(cs, res->start, resource_size(res));if (r < 0) ": "gpmc_cs_request(int cs, unsigned long size, unsigned long  base){struct gpmc_cs_data  gpmc = &gpmc_cs[cs];struct resource  res = &gpmc->mem;int r = -1;if (cs >= gpmc_cs_num) {pr_err(\"%s: requested chip-select is disabled\\n\", __func__);return -ENODEV;}size = gpmc_mem_align(size);if (size > (1 << GPMC_SECTION_SHIFT))return -ENOMEM;spin_lock(&gpmc_mem_lock);if (gpmc_cs_reserved(cs)) {r = -EBUSY;goto out;}if (gpmc_cs_mem_enabled(cs))r = adjust_resource(res, res->start & ~(size - 1), size);if (r < 0)r = allocate_resource(&gpmc_mem_root, res, size, 0, ~0,      size, NULL, NULL);if (r < 0)goto out;  Disable CS while changing base address and size mask ", "int gpmc_configure(int cmd, int wval)": "gpmc_configure - write request to configure gpmc   @cmd: command type   @wval: value to write   @return status of the operation ", "struct vfio_info_cap_header *vfio_info_cap_add(struct vfio_info_cap *caps,       size_t size, u16 id, u16 version)": "vfio_info_cap_shift() should be called to fixup the   next offsets prior to copying to the user buffer. ", "if (!pages || !npage || WARN_ON(!vfio_assert_device_open(device)))return -EINVAL;if (vfio_device_has_container(device))return vfio_device_container_pin_pages(device, iova,       npage, prot, pages);if (device->iommufd_access) ": "vfio_pin_pages(struct vfio_device  device, dma_addr_t iova,   int npage, int prot, struct page   pages){  group->container cannot change while a vfio device is open ", "if (!current->mm)flags |= IOMMUFD_ACCESS_RW_KTHREAD;if (write)flags |= IOMMUFD_ACCESS_RW_WRITE;return iommufd_access_rw(device->iommufd_access, iova, data, len, flags);}return -EINVAL;}EXPORT_SYMBOL(vfio_dma_rw": "vfio_dma_rw(struct vfio_device  device, dma_addr_t iova, void  data,size_t len, bool write){if (!data || len <= 0 || !vfio_assert_device_open(device))return -EINVAL;if (vfio_device_has_container(device))return vfio_device_container_dma_rw(device, iova,    data, len, write);if (device->iommufd_access) {unsigned int flags = 0;if (iova > ULONG_MAX)return -EINVAL;  VFIO historically tries to auto-detect a kthread ", "int mdev_register_parent(struct mdev_parent *parent, struct device *dev,struct mdev_driver *mdev_driver, struct mdev_type **types,unsigned int nr_types)": "mdev_unregister_parent().     Returns a negative value on error, otherwise 0. ", "int mdev_register_driver(struct mdev_driver *drv)": "mdev_register_driver - register a new MDEV driver   @drv: the driver to register     Returns a negative value on error, otherwise 0.  ", "void mdev_unregister_driver(struct mdev_driver *drv)": "mdev_unregister_driver - unregister MDEV driver   @drv: the driver to unregister ", "struct pps_device *pps_register_source(struct pps_source_info *info,int default_params)": "pps_register_source - add a PPS source in the system   @info: the PPS info struct   @default_params: the default PPS parameters of the new source     This function is used to add a new PPS source in the system. The new   source is described by info's fields and it will have, as default PPS   parameters, the ones specified into default_params.     The function returns, in case of success, the PPS device. Otherwise   ERR_PTR(errno). ", "void pps_unregister_source(struct pps_device *pps)": "pps_unregister_source - remove a PPS source from the system   @pps: the PPS source     This function is used to remove a previously registered PPS source from   the system. ", "pps->params.api_version = PPS_API_VERS;pps->params.mode = default_params;pps->info = *info;/* check for default echo function ": "pps_event(). ", "wake_up_all(&glink->tx_avail_notify);for (;;) ": "qcom_glink_native_rx(struct qcom_glink  glink){struct glink_msg msg;unsigned int param1;unsigned int param2;unsigned int avail;unsigned int cmd;int ret = 0;  To wakeup any blocking writers ", "if (!eptdev->default_ept)rpmsg_destroy_ept(eptdev->ept);eptdev->ept = NULL;}mutex_unlock(&eptdev->ept_lock);/* wake up any blocked readers ": "rpmsg_chrdev_eptdev_destroy(struct device  dev, void  data){struct rpmsg_eptdev  eptdev = dev_to_eptdev(dev);mutex_lock(&eptdev->ept_lock);eptdev->rpdev = NULL;if (eptdev->ept) {  The default endpoint is released by the rpmsg core ", "struct qcom_smd_edge *qcom_smd_register_edge(struct device *parent,     struct device_node *node)": "qcom_smd_register_edge() - register an edge based on an device_node   @parent:    parent device for the edge   @node:      device_node describing the edge     Return: an edge reference, or negative ERR_PTR() on failure. ", "void qcom_smd_unregister_edge(struct qcom_smd_edge *edge)": "qcom_smd_unregister_edge() - release an edge and its children   @edge:      edge reference acquired from qcom_smd_register_edge ", "struct rpmsg_device *rpmsg_create_channel(struct rpmsg_device *rpdev,  struct rpmsg_channel_info *chinfo)": "rpmsg_create_channel() - create a new rpmsg channel   using its name and address info.   @rpdev: rpmsg device   @chinfo: channel_info to bind     Return: a pointer to the new rpmsg device on success, or NULL on error. ", "int rpmsg_release_channel(struct rpmsg_device *rpdev,  struct rpmsg_channel_info *chinfo)": "rpmsg_release_channel() - release a rpmsg channel   using its name and address info.   @rpdev: rpmsg device   @chinfo: channel_info to bind     Return: 0 on success or an appropriate error value. ", "struct rpmsg_endpoint *rpmsg_create_ept(struct rpmsg_device *rpdev,rpmsg_rx_cb_t cb, void *priv,struct rpmsg_channel_info chinfo)": "rpmsg_create_ept() - create a new rpmsg_endpoint   @rpdev: rpmsg channel device   @cb: rx callback handler   @priv: private data for the driver's use   @chinfo: channel_info with the local rpmsg address to bind with @cb     Every rpmsg address in the system is bound to an rx callback (so when   inbound messages arrive, they are dispatched by the rpmsg bus using the   appropriate callback handler) by means of an rpmsg_endpoint struct.     This function allows drivers to create such an endpoint, and by that,   bind a callback, and possibly some private data too, to an rpmsg address   (either one that is known in advance, or one that will be dynamically   assigned for them).     Simple rpmsg drivers need not call rpmsg_create_ept, because an endpoint   is already created for them when they are probed by the rpmsg bus   (using the rx callback provided when they registered to the rpmsg bus).     So things should just work for simple drivers: they already have an   endpoint, their rx callback is bound to their rpmsg address, and when   relevant inbound messages arrive (i.e. messages which their dst address   equals to the src address of their rpmsg channel), the driver's handler   is invoked to process it.     That said, more complicated drivers might need to allocate   additional rpmsg addresses, and bind them to different rx callbacks.   To accomplish that, those drivers need to call this function.     Drivers should provide their @rpdev channel (so the new endpoint would belong   to the same remote processor their channel belongs to), an rx callback   function, an optional private data (which is provided back when the   rx callback is invoked), and an address they want to bind with the   callback. If @addr is RPMSG_ADDR_ANY, then rpmsg_create_ept will   dynamically assign them an available rpmsg address (drivers should have   a very good reason why not to always use RPMSG_ADDR_ANY here).     Return: a pointer to the endpoint on success, or NULL on error. ", "void rpmsg_destroy_ept(struct rpmsg_endpoint *ept)": "rpmsg_destroy_ept() - destroy an existing rpmsg endpoint   @ept: endpoing to destroy     Should be used by drivers to destroy an rpmsg endpoint previously   created with rpmsg_create_ept(). As with other types of \"free\" NULL   is a valid parameter. ", "int rpmsg_send(struct rpmsg_endpoint *ept, void *data, int len)": "rpmsg_send() - send a message across to the remote processor   @ept: the rpmsg endpoint   @data: payload of message   @len: length of payload     This function sends @data of length @len on the @ept endpoint.   The message will be sent to the remote processor which the @ept   endpoint belongs to, using @ept's address and its associated rpmsg   device destination addresses.   In case there are no TX buffers available, the function will block until   one becomes available, or a timeout of 15 seconds elapses. When the latter   happens, -ERESTARTSYS is returned.     Can only be called from process context (for now).     Return: 0 on success and an appropriate error value on failure. ", "int rpmsg_sendto(struct rpmsg_endpoint *ept, void *data, int len, u32 dst)": "rpmsg_sendto() - send a message across to the remote processor, specify dst   @ept: the rpmsg endpoint   @data: payload of message   @len: length of payload   @dst: destination address     This function sends @data of length @len to the remote @dst address.   The message will be sent to the remote processor which the @ept   endpoint belongs to, using @ept's address as source.   In case there are no TX buffers available, the function will block until   one becomes available, or a timeout of 15 seconds elapses. When the latter   happens, -ERESTARTSYS is returned.     Can only be called from process context (for now).     Return: 0 on success and an appropriate error value on failure. ", "int rpmsg_send_offchannel(struct rpmsg_endpoint *ept, u32 src, u32 dst,  void *data, int len)": "rpmsg_send_offchannel() - send a message using explicit srcdst addresses   @ept: the rpmsg endpoint   @src: source address   @dst: destination address   @data: payload of message   @len: length of payload     This function sends @data of length @len to the remote @dst address,   and uses @src as the source address.   The message will be sent to the remote processor which the @ept   endpoint belongs to.   In case there are no TX buffers available, the function will block until   one becomes available, or a timeout of 15 seconds elapses. When the latter   happens, -ERESTARTSYS is returned.     Can only be called from process context (for now).     Return: 0 on success and an appropriate error value on failure. ", "int rpmsg_trysend(struct rpmsg_endpoint *ept, void *data, int len)": "rpmsg_trysend() - send a message across to the remote processor   @ept: the rpmsg endpoint   @data: payload of message   @len: length of payload     This function sends @data of length @len on the @ept endpoint.   The message will be sent to the remote processor which the @ept   endpoint belongs to, using @ept's address as source and its associated   rpdev's address as destination.   In case there are no TX buffers available, the function will immediately   return -ENOMEM without waiting until one becomes available.     Can only be called from process context (for now).     Return: 0 on success and an appropriate error value on failure. ", "int rpmsg_trysendto(struct rpmsg_endpoint *ept, void *data, int len, u32 dst)": "rpmsg_trysendto() - send a message across to the remote processor, specify dst   @ept: the rpmsg endpoint   @data: payload of message   @len: length of payload   @dst: destination address     This function sends @data of length @len to the remote @dst address.   The message will be sent to the remote processor which the @ept   endpoint belongs to, using @ept's address as source.   In case there are no TX buffers available, the function will immediately   return -ENOMEM without waiting until one becomes available.     Can only be called from process context (for now).     Return: 0 on success and an appropriate error value on failure. ", "__poll_t rpmsg_poll(struct rpmsg_endpoint *ept, struct file *filp,poll_table *wait)": "rpmsg_poll() - poll the endpoint's send buffers   @ept:the rpmsg endpoint   @filp:file for poll_wait()   @wait:poll_table for poll_wait()     Return: mask representing the current state of the endpoint's send buffers ", "int rpmsg_trysend_offchannel(struct rpmsg_endpoint *ept, u32 src, u32 dst,     void *data, int len)": "rpmsg_trysend_offchannel() - send a message using explicit srcdst addresses   @ept: the rpmsg endpoint   @src: source address   @dst: destination address   @data: payload of message   @len: length of payload     This function sends @data of length @len to the remote @dst address,   and uses @src as the source address.   The message will be sent to the remote processor which the @ept   endpoint belongs to.   In case there are no TX buffers available, the function will immediately   return -ENOMEM without waiting until one becomes available.     Can only be called from process context (for now).     Return: 0 on success and an appropriate error value on failure. ", "ssize_t rpmsg_get_mtu(struct rpmsg_endpoint *ept)": "rpmsg_get_mtu() - get maximum transmission buffer size for sending message.   @ept: the rpmsg endpoint     This function returns maximum buffer size available for a single outgoing message.     Return: the maximum transmission size on success and an appropriate error   value on failure. ", "int rpmsg_register_device_override(struct rpmsg_device *rpdev,   const char *driver_override)": "rpmsg_register_device(). ", "int __register_rpmsg_driver(struct rpmsg_driver *rpdrv, struct module *owner)": "__register_rpmsg_driver() - register an rpmsg driver with the rpmsg bus   @rpdrv: pointer to a struct rpmsg_driver   @owner: owning moduledriver     Return: 0 on success, and an appropriate error value on failure. ", "void unregister_rpmsg_driver(struct rpmsg_driver *rpdrv)": "unregister_rpmsg_driver() - unregister an rpmsg driver from the rpmsg bus   @rpdrv: pointer to a struct rpmsg_driver     Return: 0 on success, and an appropriate error value on failure. ", "int rpmsg_ns_register_device(struct rpmsg_device *rpdev)": "rpmsg_ns_register_device() - register name service device based on rpdev   @rpdev: prepared rpdev to be used for creating endpoints     This function wraps rpmsg_register_device() preparing the rpdev for use as   basis for the rpmsg name service device. ", "struct net_device *blackhole_netdev;EXPORT_SYMBOL(blackhole_netdev": "blackhole_netdev - a device used for dsts that are marked expired!   This is global device (instead of per-net-ns) since it's not needed   to be per-ns and gets initialized at boot time. ", "int mii_link_ok (struct mii_if_info *mii)": "mii_link_ok - is link status upok   @mii: the MII interface     Returns 1 if the MII reports link status upok, 0 otherwise. ", "int mii_nway_restart (struct mii_if_info *mii)": "mii_nway_restart - restart NWay (autonegotiation) for this interface   @mii: the MII interface     Returns 0 on success, negative on error. ", "void mii_ethtool_gset(struct mii_if_info *mii, struct ethtool_cmd *ecmd)": "mii_ethtool_gset - get settings that are specified in @ecmd   @mii: MII interface   @ecmd: requested ethtool_cmd     The @ecmd parameter is expected to have been cleared before calling   mii_ethtool_gset(). ", "void mii_ethtool_get_link_ksettings(struct mii_if_info *mii,    struct ethtool_link_ksettings *cmd)": "mii_ethtool_get_link_ksettings - get settings that are specified in @cmd   @mii: MII interface   @cmd: requested ethtool_link_ksettings     The @cmd parameter is expected to have been cleared before calling   mii_ethtool_get_link_ksettings(). ", "int mii_ethtool_sset(struct mii_if_info *mii, struct ethtool_cmd *ecmd)": "mii_ethtool_sset - set settings that are specified in @ecmd   @mii: MII interface   @ecmd: requested ethtool_cmd     Returns 0 for success, negative on error. ", "int mii_ethtool_set_link_ksettings(struct mii_if_info *mii,   const struct ethtool_link_ksettings *cmd)": "mii_ethtool_set_link_ksettings - set settings that are specified in @cmd   @mii: MII interfaces   @cmd: requested ethtool_link_ksettings     Returns 0 for success, negative on error. ", "void mii_check_link (struct mii_if_info *mii)": "mii_check_link - check MII link status   @mii: MII interface     If the link status changed (previous != current), call   netif_carrier_on() if current link status is Up or call   netif_carrier_off() if current link status is Down. ", "unsigned int mii_check_media (struct mii_if_info *mii,      unsigned int ok_to_print,      unsigned int init_media)": "mii_check_media - check the MII interface for a carrierspeedduplex change   @mii: the MII interface   @ok_to_print: OK to print link updown messages   @init_media: OK to save duplex mode in @mii     Returns 1 if the duplex mode changed, 0 if not.   If the media type is forced, always returns 0. ", "int mii_check_gmii_support(struct mii_if_info *mii)": "mii_check_gmii_support - check if the MII supports Gb interfaces   @mii: the MII interface ", "int generic_mii_ioctl(struct mii_if_info *mii_if,      struct mii_ioctl_data *mii_data, int cmd,      unsigned int *duplex_chg_out)": "generic_mii_ioctl - main MII ioctl interface   @mii_if: the MII interface   @mii_data: MII ioctl data structure   @cmd: MII ioctl command   @duplex_chg_out: pointer to @duplex_changed status if there was no  ioctl error     Returns 0 on success, negative on error. ", "int netdev_boot_setup_check(struct net_device *dev)": "netdev_boot_setup_check- check boot time settings   @dev: the netdevice     Check boot time settings for the device.   The found settings are set for the device to be used   later in the device probing.   Returns 0 if no settings found, 1 if they are. ", "int mdio45_probe(struct mdio_if_info *mdio, int prtad)": "mdio45_probe - probe for an MDIO (clause 45) device   @mdio: MDIO interface   @prtad: Expected PHY address     This sets @prtad and @mmds in the MDIO interface if successful.   Returns 0 on success, negative on error. ", "int mdio_set_flag(const struct mdio_if_info *mdio,  int prtad, int devad, u16 addr, int mask,  bool sense)": "mdio_set_flag - set or clear flag in an MDIO register   @mdio: MDIO interface   @prtad: PHY address   @devad: MMD address   @addr: Register address   @mask: Mask for flag (single bit set)   @sense: New value of flag     This debounces changes: it does not write the register if the flag   already has the proper value.  Returns 0 on success, negative on error. ", "int mdio45_links_ok(const struct mdio_if_info *mdio, u32 mmd_mask)": "mdio45_links_ok - is link status upOK   @mdio: MDIO interface   @mmd_mask: Mask for MMDs to check     Returns 1 if the PHY reports link status upOK, 0 otherwise.   @mmd_mask is normally @mdio->mmds, but if loopback is enabled   the MMDs being bypassed should be excluded from the mask. ", "int mdio45_nway_restart(const struct mdio_if_info *mdio)": "mdio45_nway_restart - restart auto-negotiation for this interface   @mdio: MDIO interface     Returns 0 on success, negative on error. ", "void mdio45_ethtool_gset_npage(const struct mdio_if_info *mdio,       struct ethtool_cmd *ecmd,       u32 npage_adv, u32 npage_lpa)": "mdio45_ethtool_gset_npage - get settings for ETHTOOL_GSET   @mdio: MDIO interface   @ecmd: Ethtool request structure   @npage_adv: Modes currently advertised on next pages   @npage_lpa: Modes advertised by link partner on next pages     The @ecmd parameter is expected to have been cleared before calling   mdio45_ethtool_gset_npage().     Since the CSRs for auto-negotiation using next pages are not fully   standardised, this function does not attempt to decode them.  The   caller must pass them in. ", "void mdio45_ethtool_ksettings_get_npage(const struct mdio_if_info *mdio,struct ethtool_link_ksettings *cmd,u32 npage_adv, u32 npage_lpa)": "mdio45_ethtool_ksettings_get_npage - get settings for ETHTOOL_GLINKSETTINGS   @mdio: MDIO interface   @cmd: Ethtool request structure   @npage_adv: Modes currently advertised on next pages   @npage_lpa: Modes advertised by link partner on next pages     The @cmd parameter is expected to have been cleared before calling   mdio45_ethtool_ksettings_get_npage().     Since the CSRs for auto-negotiation using next pages are not fully   standardised, this function does not attempt to decode them.  The   caller must pass them in. ", "int mdio_mii_ioctl(const struct mdio_if_info *mdio,   struct mii_ioctl_data *mii_data, int cmd)": "mdio_mii_ioctl - MII ioctl interface for MDIO (clause 22 or 45) PHYs   @mdio: MDIO interface   @mii_data: MII ioctl data structure   @cmd: MII ioctl command     Returns 0 on success, negative on error. ", "phy->mii_id = mii_id;/* Take PHY out of isloate mode and reset it. ": "sungem_phy_probe(struct mii_phy  phy, int mii_id){int rc;u32 id;struct mii_phy_def  def;int i;  We do not reset the mii_phy structure as the driver   may re-probe the PHY regulary ", "       s->hdlcrx.bitstream >>= 16;s->hdlcrx.bitstream |= word << 16;s->hdlcrx.bitbuf >>= 16;s->hdlcrx.bitbuf |= word << 16;s->hdlcrx.numbits += 16;for(i = 15, mask1 = 0x1fc00, mask2 = 0x1fe00, mask3 = 0x0fc00,    mask4 = 0x1f800, mask5 = 0xf800, mask6 = 0xffff;     i >= 0;     i--, mask1 <<= 1, mask2 <<= 1, mask3 <<= 1, mask4 <<= 1,     mask5 <<= 1, mask6 = (mask6 << 1) | 1) ": "hdlcdrv_receiver(struct net_device  dev, struct hdlcdrv_state  s){int i;unsigned int mask1, mask2, mask3, mask4, mask5, mask6, word;if (!s || s->magic != HDLCDRV_MAGIC) return;if (test_and_set_bit(0, &s->hdlcrx.in_hdlc_rx))return;while (!hdlcdrv_hbuf_empty(&s->hdlcrx.hbuf)) {word = hdlcdrv_hbuf_get(&s->hdlcrx.hbuf);#ifdef HDLCDRV_DEBUGhdlcdrv_add_bitbuffer_word(&s->bitbuf_hdlc, word);#endif   HDLCDRV_DEBUG ", "if (pkt_len >= HDLCDRV_MAXFLEN || pkt_len < 2) ": "hdlcdrv_transmitter(struct net_device  dev, struct hdlcdrv_state  s){unsigned int mask1, mask2, mask3;int i;struct sk_buff  skb;int pkt_len;if (!s || s->magic != HDLCDRV_MAGIC) return;if (test_and_set_bit(0, &s->hdlctx.in_hdlc_tx))return;for (;;) {if (s->hdlctx.numbits >= 16) {if (hdlcdrv_hbuf_full(&s->hdlctx.hbuf)) {clear_bit(0, &s->hdlctx.in_hdlc_tx);return;}hdlcdrv_hbuf_put(&s->hdlctx.hbuf, s->hdlctx.bitbuf);s->hdlctx.bitbuf >>= 16;s->hdlctx.numbits -= 16;}switch (s->hdlctx.tx_state) {default:clear_bit(0, &s->hdlctx.in_hdlc_tx);return;case 0:case 1:if (s->hdlctx.numflags) {s->hdlctx.numflags--;s->hdlctx.bitbuf |= 0x7e7e << s->hdlctx.numbits;s->hdlctx.numbits += 16;break;}if (s->hdlctx.tx_state == 1) {clear_bit(0, &s->hdlctx.in_hdlc_tx);return;}if (!(skb = s->skb)) {int flgs = tenms_to_2flags(s, s->ch_params.tx_tail);if (flgs < 2)flgs = 2;s->hdlctx.tx_state = 1;s->hdlctx.numflags = flgs;break;}s->skb = NULL;netif_wake_queue(dev);pkt_len = skb->len-1;   strip KISS byte ", "/* * ===================== network driver interface ========================= ": "hdlcdrv_arbitrate(struct net_device  dev, struct hdlcdrv_state  s){if (!s || s->magic != HDLCDRV_MAGIC || s->hdlctx.ptt || !s->skb) return;if (s->ch_params.fulldup) {start_tx(dev, s);return;}if (s->hdlcrx.dcd) {s->hdlctx.slotcnt = s->ch_params.slottime;return;}if ((--s->hdlctx.slotcnt) > 0)return;s->hdlctx.slotcnt = s->ch_params.slottime;if (get_random_u8() > s->ch_params.ppersist)return;start_tx(dev, s);}  --------------------------------------------------------------------- ", "s = netdev_priv(dev);s->magic = HDLCDRV_MAGIC;s->ops = ops;dev->base_addr = baseaddr;dev->irq = irq;dev->dma = dma;err = register_netdev(dev);if (err < 0) ": "hdlcdrv_register(const struct hdlcdrv_ops  ops,    unsigned int privsize, const char  ifname,    unsigned int baseaddr, unsigned int irq,     unsigned int dma) {struct net_device  dev;struct hdlcdrv_state  s;int err;if (privsize < sizeof(struct hdlcdrv_state))privsize = sizeof(struct hdlcdrv_state);dev = alloc_netdev(privsize, ifname, NET_NAME_UNKNOWN, hdlcdrv_setup);if (!dev)return ERR_PTR(-ENOMEM);    initialize part of the hdlcdrv_state struct ", "EXPORT_SYMBOL(hdlcdrv_receiver);EXPORT_SYMBOL(hdlcdrv_transmitter);EXPORT_SYMBOL(hdlcdrv_arbitrate);EXPORT_SYMBOL(hdlcdrv_register);EXPORT_SYMBOL(hdlcdrv_unregister": "hdlcdrv_unregister(struct net_device  dev) {struct hdlcdrv_state  s = netdev_priv(dev);BUG_ON(s->magic != HDLCDRV_MAGIC);if (s->opened && s->ops->close)s->ops->close(dev);unregister_netdev(dev);free_netdev(dev);}  --------------------------------------------------------------------- ", "int ppp_register_net_channel(struct net *net, struct ppp_channel *chan)": "ppp_register_channel(struct ppp_channel  chan){return ppp_register_net_channel(current->nsproxy->net_ns, chan);}  Create a new, unattached ppp channel for specified net. ", "chan->ppp = NULL;/* * This ensures that we have returned from any calls into * the channel's start_xmit or ioctl routine before we proceed. ": "ppp_unregister_channel(struct ppp_channel  chan){struct channel  pch = chan->ppp;struct ppp_net  pn;if (!pch)return;  should never happen ", "int ppp_unit_number(struct ppp_channel *chan)": "ppp_channel_index(struct ppp_channel  chan){struct channel  pch = chan->ppp;if (pch)return pch->file.index;return -1;}    Return the PPP unit number to which a channel is connected. ", "char *ppp_dev_name(struct ppp_channel *chan)": "ppp_unit_number(struct ppp_channel  chan){struct channel  pch = chan->ppp;int unit = -1;if (pch) {read_lock_bh(&pch->upl);if (pch->ppp)unit = pch->ppp->file.index;read_unlock_bh(&pch->upl);}return unit;}    Return the PPP device interface name of a channel. ", "voidppp_unregister_channel(struct ppp_channel *chan)": "ppp_dev_name(struct ppp_channel  chan){struct channel  pch = chan->ppp;char  name = NULL;if (pch) {read_lock_bh(&pch->upl);if (pch->ppp && pch->ppp->dev)name = pch->ppp->dev->name;read_unlock_bh(&pch->upl);}return name;}    Disconnect a channel from the generic layer.   This must be called in process context. ", "static int ppp_bridge_channels(struct channel *pch, struct channel *pchb)": "ppp_input on one channel is redirected to   the other's ops->start_xmit handler.   In order to safely bridge channels we must reject channels which are already   part of a bridge instance, or which form part of an existing unit.   Once successfully bridged, each channel holds a reference on the other   to prevent it being freed while the bridge is extant. ", "skb->cb[0] = code;ppp_do_recv(pch->ppp, skb, pch);}}read_unlock_bh(&pch->upl);}/* * We come in here to process a received frame. * The receive side of the ppp unit is locked. ": "ppp_input_error(struct ppp_channel  chan, int code){struct channel  pch = chan->ppp;struct sk_buff  skb;if (!pch)return;read_lock_bh(&pch->upl);if (pch->ppp) {skb = alloc_skb(0, GFP_ATOMIC);if (skb) {skb->len = 0;  probably unnecessary ", "/* Process the PPPIOCSCOMPRESS ioctl. ": "ppp_output_wakeup(struct ppp_channel  chan){struct channel  pch = chan->ppp;if (!pch)return;ppp_channel_push(pch);}    Compression control. ", "voidppp_unregister_compressor(struct compressor *cp)": "ppp_register_compressor(struct compressor  cp){struct compressor_entry  ce;int ret;spin_lock(&compressor_list_lock);ret = -EEXIST;if (find_comp_entry(cp->compress_proto))goto out;ret = -ENOMEM;ce = kmalloc(sizeof(struct compressor_entry), GFP_ATOMIC);if (!ce)goto out;ret = 0;ce->comp = cp;list_add(&ce->list, &compressor_list); out:spin_unlock(&compressor_list_lock);return ret;}  Unregister a compressor ", "static struct compressor *find_compressor(int type)": "ppp_unregister_compressor(struct compressor  cp){struct compressor_entry  ce;spin_lock(&compressor_list_lock);ce = find_comp_entry(cp->compress_proto);if (ce && ce->comp == cp) {list_del(&ce->list);kfree(ce);}spin_unlock(&compressor_list_lock);}  Find a compressor. ", "if (sk->sk_state & (PPPOX_BOUND | PPPOX_CONNECTED)) ": "pppox_unbind_sock(struct sock  sk){  Clear connection to ppp device, if attached. ", "vsc->reset = devm_gpiod_get_optional(dev, \"reset\", GPIOD_OUT_LOW);if (IS_ERR(vsc->reset)) ": "vsc73xx_probe(struct vsc73xx  vsc){struct device  dev = vsc->dev;int ret;  Release reset, if any ", "if (chip->reset_duration > 1000)chip->reset_duration = 1000;return 0;}int lan9303_probe(struct lan9303 *chip, struct device_node *np)": "lan9303_probe_reset_gpio(struct lan9303  chip,     struct device_node  np){chip->reset_gpio = devm_gpiod_get_optional(chip->dev, \"reset\",   GPIOD_OUT_HIGH);if (IS_ERR(chip->reset_gpio))return PTR_ERR(chip->reset_gpio);if (!chip->reset_gpio) {dev_dbg(chip->dev, \"No reset GPIO defined\\n\");return 0;}chip->reset_duration = 200;if (np) {of_property_read_u32(np, \"reset-duration\",     &chip->reset_duration);} else {dev_dbg(chip->dev, \"reset duration defaults to 200 ms\\n\");}  A sane reset duration should not be longer than 1s ", "gpiod_set_value_cansleep(chip->reset_gpio, 1);return 0;}EXPORT_SYMBOL(lan9303_remove": "lan9303_remove(struct lan9303  chip){int rc;rc = lan9303_disable_processing(chip);if (rc != 0)dev_warn(chip->dev, \"shutting down failed\\n\");dsa_unregister_switch(chip->ds);  assert reset to the whole device to prevent it from doing anything ", "dev->info = info;dev_info(dev->dev, \"found switch: %s, rev %i\\n\", dev->info->dev_name, dev->chip_rev);ret = ksz_check_device_id(dev);if (ret)return ret;dev->dev_ops = dev->info->ops;ret = dev->dev_ops->init(dev);if (ret)return ret;dev->ports = devm_kzalloc(dev->dev,  dev->info->port_cnt * sizeof(struct ksz_port),  GFP_KERNEL);if (!dev->ports)return -ENOMEM;for (i = 0; i < dev->info->port_cnt; i++) ": "ksz_switch_register(struct ksz_device  dev){const struct ksz_chip_data  info;struct device_node  port,  ports;phy_interface_t interface;unsigned int port_num;int ret;int i;if (dev->pdata)dev->chip_id = dev->pdata->chip_id;dev->reset_gpio = devm_gpiod_get_optional(dev->dev, \"reset\",  GPIOD_OUT_LOW);if (IS_ERR(dev->reset_gpio))return PTR_ERR(dev->reset_gpio);if (dev->reset_gpio) {gpiod_set_value_cansleep(dev->reset_gpio, 1);usleep_range(10000, 12000);gpiod_set_value_cansleep(dev->reset_gpio, 0);msleep(100);}mutex_init(&dev->dev_mutex);mutex_init(&dev->regmap_mutex);mutex_init(&dev->alu_mutex);mutex_init(&dev->vlan_mutex);ret = ksz_switch_detect(dev);if (ret)return ret;info = ksz_lookup_info(dev->chip_id);if (!info)return -ENODEV;  Update the compatible info with the probed one ", "if (dev->mib_read_interval) ": "ksz_switch_remove(struct ksz_device  dev){  timer started ", "b53_for_each_port(dev, i) ": "b53_imp_vlan_setup(struct dsa_switch  ds, int cpu_port){struct b53_device  dev = ds->priv;unsigned int i;u16 pvlan;  Enable the IMP port to be in the same VLAN as the other ports   on a per-port basis such that we only have Port i and IMP in   the same VLAN. ", "b53_write8(dev, B53_CTRL_PAGE, B53_PORT_CTRL(port), 0);/* Set this port, and only this one to be in the default VLAN, * if member of a bridge, restore its membership prior to * bringing down this port. ": "b53_enable_port(struct dsa_switch  ds, int port, struct phy_device  phy){struct b53_device  dev = ds->priv;unsigned int cpu_port;int ret = 0;u16 pvlan;if (!dsa_is_user_port(ds, port))return 0;cpu_port = dsa_to_port(ds, port)->cpu_dp->index;b53_port_set_ucast_flood(dev, port, true);b53_port_set_mcast_flood(dev, port, true);b53_port_set_learning(dev, port, false);if (dev->ops->irq_enable)ret = dev->ops->irq_enable(dev, port);if (ret)return ret;  Clear the Rx and Tx disable bits and set to no spanning tree ", "b53_read8(dev, B53_CTRL_PAGE, B53_PORT_CTRL(port), &reg);reg |= PORT_CTRL_RX_DISABLE | PORT_CTRL_TX_DISABLE;b53_write8(dev, B53_CTRL_PAGE, B53_PORT_CTRL(port), reg);if (dev->ops->irq_disable)dev->ops->irq_disable(dev, port);}EXPORT_SYMBOL(b53_disable_port": "b53_disable_port(struct dsa_switch  ds, int port){struct b53_device  dev = ds->priv;u8 reg;  Disable TxRx for the port ", "switch (port) ": "b53_brcm_hdr_setup(struct dsa_switch  ds, int port){struct b53_device  dev = ds->priv;bool tag_en = !(dev->tag_protocol == DSA_TAG_PROTO_NONE);u8 hdr_ctl, val;u16 reg;  Resolve which bit controls the Broadcom tag ", "if (is5325(dev) || is5365(dev)) ": "b53_configure_vlan(struct dsa_switch  ds){struct b53_device  dev = ds->priv;struct b53_vlan vl = { 0 };struct b53_vlan  v;int i, def_vid;u16 vid;def_vid = b53_default_pvid(dev);  clear all vlan entries ", "if (is5325(priv) || is5365(priv))return -EOPNOTSUPP;mutex_lock(&priv->arl_mutex);ret = b53_arl_op(priv, 0, port, addr, vid, true);mutex_unlock(&priv->arl_mutex);return ret;}EXPORT_SYMBOL(b53_fdb_add": "b53_fdb_add(struct dsa_switch  ds, int port,const unsigned char  addr, u16 vid,struct dsa_db db){struct b53_device  priv = ds->priv;int ret;  5325 and 5365 require some more massaging, but could   be supported eventually ", "reg = ARL_SRCH_STDN;b53_write8(priv, B53_ARLIO_PAGE, B53_ARL_SRCH_CTL, reg);do ": "b53_fdb_dump(struct dsa_switch  ds, int port, dsa_fdb_dump_cb_t  cb, void  data){struct b53_device  priv = ds->priv;struct b53_arl_entry results[2];unsigned int count = 0;int ret;u8 reg;mutex_lock(&priv->arl_mutex);  Start search operation ", "if (is5325(priv) || is5365(priv))return -EOPNOTSUPP;mutex_lock(&priv->arl_mutex);ret = b53_arl_op(priv, 0, port, mdb->addr, mdb->vid, true);mutex_unlock(&priv->arl_mutex);return ret;}EXPORT_SYMBOL(b53_mdb_add": "b53_mdb_add(struct dsa_switch  ds, int port,const struct switchdev_obj_port_mdb  mdb,struct dsa_db db){struct b53_device  priv = ds->priv;int ret;  5325 and 5365 require some more massaging, but could   be supported eventually ", "if (dev->chip_id == BCM7278_DEVICE_ID && port == 7)return -EINVAL;/* Make this port leave the all VLANs join since we will have proper * VLAN entries from now on ": "b53_br_join(struct dsa_switch  ds, int port, struct dsa_bridge bridge,bool  tx_fwd_offload, struct netlink_ext_ack  extack){struct b53_device  dev = ds->priv;s8 cpu_port = dsa_to_port(ds, port)->cpu_dp->index;u16 pvlan, reg;unsigned int i;  On 7278, port 7 which connects to the ASP should only receive   traffic from matching CFP rules. ", "if (!dsa_port_offloads_bridge(dsa_to_port(ds, i), &bridge))continue;b53_read16(dev, B53_PVLAN_PAGE, B53_PVLAN_PORT_MASK(i), &reg);reg &= ~BIT(port);b53_write16(dev, B53_PVLAN_PAGE, B53_PVLAN_PORT_MASK(i), reg);dev->ports[port].vlan_ctl_mask = reg;/* Prevent self removal to preserve isolation ": "b53_br_leave(struct dsa_switch  ds, int port, struct dsa_bridge bridge){struct b53_device  dev = ds->priv;struct b53_vlan  vl = &dev->vlans[0];s8 cpu_port = dsa_to_port(ds, port)->cpu_dp->index;unsigned int i;u16 pvlan, reg, pvid;b53_read16(dev, B53_PVLAN_PAGE, B53_PVLAN_PORT_MASK(port), &pvlan);b53_for_each_port(dev, i) {  Don't touch the remaining ports ", "if (is5325(dev) || is5365(dev) || is63xx(dev)) ": "b53_get_tag_protocol(struct dsa_switch  ds, int port,   enum dsa_tag_protocol mprot){struct b53_device  dev = ds->priv;if (!b53_can_enable_brcm_tags(ds, port, mprot)) {dev->tag_protocol = DSA_TAG_PROTO_NONE;goto out;}  Older models require a different 6 byte tag ", "b53_read16(dev, B53_MGMT_PAGE, loc, &reg);reg &= ~BIT(port);if (!(reg & MIRROR_MASK))loc_disable = true;b53_write16(dev, B53_MGMT_PAGE, loc, reg);/* Now look at the other one to know if we can disable mirroring * entirely ": "b53_mirror_del(struct dsa_switch  ds, int port,    struct dsa_mall_mirror_tc_entry  mirror){struct b53_device  dev = ds->priv;bool loc_disable = false, other_loc_disable = false;u16 reg, loc;if (mirror->ingress)loc = B53_IG_MIR_CTL;elseloc = B53_EG_MIR_CTL;  Update the desired ingressegress register ", "b53_read8(dev, B53_CTRL_PAGE, B53_PORT_CTRL(port), &reg);reg |= PORT_CTRL_RX_DISABLE | PORT_CTRL_TX_DISABLE;b53_write8(dev, B53_CTRL_PAGE, B53_PORT_CTRL(port), reg);if (dev->ops->irq_disable)dev->ops->irq_disable(dev, port);}EXPORT_SYMBOL(b53_disable_port);void b53_brcm_hdr_setup(struct dsa_switch *ds, int port)": "b53_eee_enable_set(ds, port, true);return 0;}EXPORT_SYMBOL(b53_enable_port);void b53_disable_port(struct dsa_switch  ds, int port){struct b53_device  dev = ds->priv;u8 reg;  Disable TxRx for the port ", "__set_bit(PHY_INTERFACE_MODE_GMII, config->supported_interfaces);/* These switches appear to support MII and RevMII too, but beyond * this, the code gives very few clues. FIXME: We probably need more * interface modes here. * * According to b53_srab_mux_init(), ports 3..5 can support: *  SGMII, MII, GMII, RGMII or INTERNAL depending on the MUX setting. * However, the interface mode read from the MUX configuration is * not passed back to DSA, so phylink uses NA. * DT can specify RGMII for ports 0, 1. * For MDIO, port 8 can be RGMII_TXID. ": "b53_eee_init(ds, port, phydev);}void b53_port_event(struct dsa_switch  ds, int port){struct b53_device  dev = ds->priv;bool link;u16 sts;b53_read16(dev, B53_STAT_PAGE, B53_LINK_STAT, &sts);link = !!(sts & BIT(port));dsa_port_phylink_mac_change(ds, port, link);}EXPORT_SYMBOL(b53_port_event);static void b53_phylink_get_caps(struct dsa_switch  ds, int port, struct phylink_config  config){struct b53_device  dev = ds->priv;  Internal ports need GMII for PHYLIB ", "ds->vlan_filtering_is_global = true;mutex_init(&dev->reg_mutex);mutex_init(&dev->stats_mutex);mutex_init(&dev->arl_mutex);return dev;}EXPORT_SYMBOL(b53_switch_alloc": "b53_switch_alloc(struct device  base,    const struct b53_io_ops  ops,    void  priv){struct dsa_switch  ds;struct b53_device  dev;ds = devm_kzalloc(base, sizeof( ds), GFP_KERNEL);if (!ds)return NULL;ds->dev = base;dev = devm_kzalloc(base, sizeof( dev), GFP_KERNEL);if (!dev)return NULL;ds->priv = dev;dev->dev = base;dev->ds = ds;dev->priv = priv;dev->ops = ops;ds->ops = &b53_switch_ops;dev->vlan_enabled = true;  Let DSA handle the case were multiple bridges span the same switch   device and different VLAN awareness settings are requested, which   would be breaking filtering semantics for any of the other bridge   devices. (not hardware supported) ", "b53_write16(dev, B53_VLAN_PAGE, B53_VLAN_TABLE_ACCESS_25, 0xf);b53_read16(dev, B53_VLAN_PAGE, B53_VLAN_TABLE_ACCESS_25, &tmp);if (tmp == 0xf)dev->chip_id = BCM5325_DEVICE_ID;elsedev->chip_id = BCM5365_DEVICE_ID;break;case BCM5389_DEVICE_ID:case BCM5395_DEVICE_ID:case BCM5397_DEVICE_ID:case BCM5398_DEVICE_ID:dev->chip_id = id8;break;default:ret = b53_read32(dev, B53_MGMT_PAGE, B53_DEVICE_ID, &id32);if (ret)return ret;switch (id32) ": "b53_switch_detect(struct b53_device  dev){u32 id32;u16 tmp;u8 id8;int ret;ret = b53_read8(dev, B53_MGMT_PAGE, B53_DEVICE_ID, &id8);if (ret)return ret;switch (id8) {case 0:  BCM5325 and BCM5365 do not have this register so reads   return 0. But the read operation did succeed, so assume this   is one of them.     Next check if we can write to the 5325's VTA register; for   5365 it is read only. ", "__set_bit(PHY_INTERFACE_MODE_2500BASEX,  config->supported_interfaces);config->mac_capabilities |= MAC_2500FD;fallthrough;case 1:/* It appears lane 1 only supports 1000base-X and SGMII ": "b53_serdes_phylink_get_caps(struct b53_device  dev, int port, struct phylink_config  config){u8 lane = b53_serdes_map_lane(dev, port);if (lane == B53_INVALID_LANE)return;switch (lane) {case 0:  It appears lane 0 supports 2500base-X and 1000base-X ", "for (i = 0; i < IEEE80211_NUM_TIDS; i++) ": "mt76_wcid_key_setup(struct mt76_dev  dev, struct mt76_wcid  wcid, struct ieee80211_key_conf  key){struct ieee80211_key_seq seq;int i;wcid->rx_check_pn = false;if (!key)return;if (key->cipher != WLAN_CIPHER_SUITE_CCMP)return;wcid->rx_check_pn = true;  data frame ", "rsi_send_vap_dynamic_update(common);rx_filter_word = (ALLOW_DATA_ASSOC_PEER | DISALLOW_BEACONS);rsi_send_rx_filter_frame(common, rx_filter_word);return 0;}EXPORT_SYMBOL(rsi_config_wowlan": "rsi_config_wowlan(struct rsi_hw  adapter, struct cfg80211_wowlan  wowlan){struct rsi_common  common = adapter->priv;struct ieee80211_vif  vif = adapter->vifs[0];u16 triggers = 0;u16 rx_filter_word = 0;rsi_dbg(INFO_ZONE, \"Config WoWLAN to device\\n\");if (!vif)return -EINVAL;if (WARN_ON(!wowlan)) {rsi_dbg(ERR_ZONE, \"WoW triggers not enabled\\n\");return -EINVAL;}common->wow_flags |= RSI_WOW_ENABLED;triggers = rsi_wow_map_triggers(common, wowlan);if (!triggers) {rsi_dbg(ERR_ZONE, \"%s:No valid WoW triggers\\n\", __func__);return -EINVAL;}if (!vif->cfg.assoc) {rsi_dbg(ERR_ZONE,\"Cannot configure WoWLAN (Station not connected)\\n\");common->wow_flags |= RSI_WOW_NO_CONNECTION;return 0;}rsi_dbg(INFO_ZONE, \"TRIGGERS %x\\n\", triggers);if (common->coex_mode > 1)rsi_disable_ps(adapter, adapter->vifs[0]);rsi_send_wowlan_request(common, triggers, 1);     Increase the beacon_miss threshold & keep-alive timers in   vap_update frame ", "memset(pq, 0,      offsetof(struct pktq, q) + (sizeof(struct pktq_prec) * num_prec));pq->num_prec = (u16) num_prec;pq->max = (u16) max_len;for (prec = 0; prec < num_prec; prec++) ": "brcmu_pktq_init(struct pktq  pq, int num_prec, int max_len){int prec;  pq is variable size; only zero out what's requested ", "void ath_hw_setbssidmask(struct ath_common *common)": "ath_hw_setbssidmask - filter out bssids we listen     @common: the ath_common struct for the device.     BSSID masking is a method used by AR5212 and newer hardware to inform PCU   which bits of the interface's MAC address should be looked at when trying   to decide which packets to ACK. In station mode and AP mode with a single   BSS every bit matters since we lock to only one BSS. In AP mode with   multiple BSSes (virtual interfaces) not every bit matters because hw must   accept frames for all BSSes and so we tweak some bits of our mac address   in order to have multiple BSSes.     NOTE: This is a simple filter and does  not  filter out all   relevant frames. Some frames that are not for us might get ACKed from us   by PCU because they just match the mask.     When handling multiple BSSes you can get the BSSID mask by computing the   set of  ~ ( MAC XOR BSSID ) for all bssids we handle.     When you do this you are essentially computing the common bits of all your   BSSes. Later it is assumed the hardware will \"and\" (&) the BSSID mask with   the MAC address to obtain the relevant bits and compare the result with   (frame's BSSID & mask) to see if they match.     Simple example: on your card you have two BSSes you have created with   BSSID-01 and BSSID-02. Lets assume BSSID-01 will not use the MAC address.   There is another BSSID-03 but you are not part of it. For simplicity's sake,   assuming only 4 bits for a mac address and for BSSIDs you can then have:                      \\   MAC:        0001 |   BSSID-01:   0100 | --> Belongs to us   BSSID-02:   1001 |                       -------------------   BSSID-03:   0110  | --> External   -------------------     Our bssid_mask would then be:                 On loop iteration for BSSID-01:               ~(0001 ^ 0100)  -> ~(0101)                               ->   1010               bssid_mask      =    1010                 On loop iteration for BSSID-02:               bssid_mask &= ~(0001   ^   1001)               bssid_mask =   (1010)  & ~(0001 ^ 1001)               bssid_mask =   (1010)  & ~(1000)               bssid_mask =   (1010)  &  (0111)               bssid_mask =   0010     A bssid_mask of 0010 means \"only pay attention to the second least   significant bit\". This is because its the only bit common   amongst the MAC and all BSSIDs we support. To findout what the real   common bit is we can simply \"&\" the bssid_mask now with any BSSID we have   or our MAC address (we assume the hardware uses the MAC address).     Now, suppose there's an incoming frame for BSSID-03:     IFRAME-01:  0110     An easy eye-inspeciton of this already should tell you that this frame   will not pass our check. This is because the bssid_mask tells the   hardware to only look at the second least significant bit and the   common bit amongst the MAC and BSSIDs is 0, this frame has the 2nd LSB   as 1, which does not match 0.     So with IFRAME-01 we  assume  the hardware will do:         allow = (IFRAME-01 & bssid_mask) == (bssid_mask & MAC) ? 1 : 0;    --> allow = (0110 & 0010) == (0010 & 0001) ? 1 : 0;    --> allow = (0010) == 0000 ? 1 : 0;    --> allow = 0      Lets now test a frame that should work:     IFRAME-02:  0001 (we should allow)         allow = (IFRAME-02 & bssid_mask) == (bssid_mask & MAC) ? 1 : 0;    --> allow = (0001 & 0010) ==  (0010 & 0001) ? 1 :0;    --> allow = (0000) == (0000)    --> allow = 1     Other examples:     IFRAME-03:  0100 --> allowed   IFRAME-04:  1001 --> allowed   IFRAME-05:  1101 --> allowed but its not for us!!!   ", "void ath_hw_cycle_counters_update(struct ath_common *common)": "ath_hw_cycle_counters_update - common function to update cycle counters     @common: the ath_common struct for the device.     This function is used to update all cycle counters in one place.   It has to be called while holding common->cc_lock! ", "if (mac[0] & 0x01)unicast_flag = 0;macLo = get_unaligned_le32(mac);macHi = get_unaligned_le16(mac + 4);macLo >>= 1;macLo |= (macHi & 1) << 31;macHi >>= 1;} else ": "ath_hw_keysetmac(struct ath_common  common, u16 entry, const u8  mac){u32 macHi, macLo;u32 unicast_flag = AR_KEYTABLE_VALID;void  ah = common->ah;if (entry >= common->keymax) {ath_err(common, \"keysetmac: keycache entry %u out of range\\n\",entry);return false;}if (mac != NULL) {    AR_KEYTABLE_VALID indicates that the address is a unicast   address, which must match the transmitter address for   decrypting frames.   Not setting this bit allows the hardware to use the key   for multicast frame decryption. ", "idx = key->keyidx;} elsereturn -EIO;} else ": "ath_key_config(struct ath_common  common,  struct ieee80211_vif  vif,  struct ieee80211_sta  sta,  struct ieee80211_key_conf  key){struct ath_keyval hk;const u8  mac = NULL;u8 gmac[ETH_ALEN];int ret = 0;int idx;memset(&hk, 0, sizeof(hk));switch (key->cipher) {case 0:hk.kv_type = ATH_CIPHER_CLR;break;case WLAN_CIPHER_SUITE_WEP40:case WLAN_CIPHER_SUITE_WEP104:hk.kv_type = ATH_CIPHER_WEP;break;case WLAN_CIPHER_SUITE_TKIP:hk.kv_type = ATH_CIPHER_TKIP;break;case WLAN_CIPHER_SUITE_CCMP:hk.kv_type = ATH_CIPHER_AES_CCM;break;default:return -EOPNOTSUPP;}hk.kv_len = key->keylen;if (key->keylen)memcpy(&hk.kv_values, key->key, key->keylen);if (!(key->flags & IEEE80211_KEY_FLAG_PAIRWISE)) {switch (vif->type) {case NL80211_IFTYPE_AP:memcpy(gmac, vif->addr, ETH_ALEN);gmac[0] |= 0x01;mac = gmac;idx = ath_reserve_key_cache_slot(common, key->cipher);break;case NL80211_IFTYPE_ADHOC:if (!sta) {idx = key->keyidx;break;}memcpy(gmac, sta->addr, ETH_ALEN);gmac[0] |= 0x01;mac = gmac;idx = ath_reserve_key_cache_slot(common, key->cipher);break;default:idx = key->keyidx;break;}} else if (key->keyidx) {if (WARN_ON(!sta))return -EOPNOTSUPP;mac = sta->addr;if (vif->type != NL80211_IFTYPE_AP) {  Only keyidx 0 should be used with unicast key, but   allow this for client mode for now. ", "if (test_bit(hw_key_idx, common->ccmp_keymap) ||    test_bit(hw_key_idx, common->tkip_keymap))ath_hw_keysetmac(common, hw_key_idx, NULL);elseath_hw_keyreset(common, hw_key_idx);if (hw_key_idx < IEEE80211_WEP_NKID)return;clear_bit(hw_key_idx, common->keymap);clear_bit(hw_key_idx, common->ccmp_keymap);if (!test_bit(hw_key_idx, common->tkip_keymap))return;clear_bit(hw_key_idx + 64, common->keymap);clear_bit(hw_key_idx, common->tkip_keymap);clear_bit(hw_key_idx + 64, common->tkip_keymap);if (!(common->crypt_caps & ATH_CRYPT_CAP_MIC_COMBINED)) ": "ath_key_delete(struct ath_common  common, u8 hw_key_idx){  Leave CCMP and TKIP (main key) configured to avoid disabling   encryption for potentially pending frames already in a TXQ with the   keyix pointing to this key entry. Instead, only clear the MAC address   to prevent RX processing from using this key cache entry. ", "return regdomain == MKK9_MKKC;}EXPORT_SYMBOL(ath_is_49ghz_allowed": "ath_is_49ghz_allowed(u16 regdomain){  possibly more ", "ath_reg_apply_radar_flags(wiphy, reg);/* * This would happen when we have sent a custom regulatory request * a world regulatory domain and the scheduler hasn't yet processed * any pending requests in the queue. ": "ath_reg_notifier_apply(struct wiphy  wiphy,    struct regulatory_request  request,    struct ath_regulatory  reg){struct ath_common  common = container_of(reg, struct ath_common, regulatory);  We always apply this ", "/* Only these channels all allow active scan on all world regulatory domains ": "ath_regd_init(struct ath_regulatory  reg);    This is a set of common rules used by our world regulatory domains.   We have 12 world regulatory domains. To save space we consolidate   the regulatory domains in 5 structures by frequency and change   the flags on our reg_notifier() on a case by case basis. ", "/* Note: the kernel can allocate a value greater than * what we ask it to give us. We really only need 4 KB as that * is this hardware supports and in fact we need at least 3849 * as that is the MAX AMSDU size this hardware supports. * Unfortunately this means we may get 8 KB here from the * kernel... and that is actually what is observed on some * systems :( ": "ath_rxbuf_alloc(struct ath_common  common,u32 len,gfp_t gfp_mask){struct sk_buff  skb;u32 off;    Cache-line-align.  This is important (for the   5210 at least) as not doing so causes bogus data   in rx'd frames. ", "ath6kl_wmi_shutdown(ar->wmi);clear_bit(WMI_ENABLED, &ar->flag);if (ar->htc_target) ": "ath6kl_stop_txrx(struct ath6kl  ar){struct ath6kl_vif  vif,  tmp_vif;int i;set_bit(DESTROY_IN_PROGRESS, &ar->flag);if (down_interruptible(&ar->sem)) {ath6kl_err(\"down_interruptible failed\\n\");return;}for (i = 0; i < AP_MAX_NUM_STA; i++)aggr_reset_state(ar->sta_list[i].aggr_conn);spin_lock_bh(&ar->list_lock);list_for_each_entry_safe(vif, tmp_vif, &ar->vif_list, list) {list_del(&vif->list);spin_unlock_bh(&ar->list_lock);ath6kl_cfg80211_vif_stop(vif, test_bit(WMI_READY, &ar->flag));rtnl_lock();wiphy_lock(ar->wiphy);ath6kl_cfg80211_vif_cleanup(vif);wiphy_unlock(ar->wiphy);rtnl_unlock();spin_lock_bh(&ar->list_lock);}spin_unlock_bh(&ar->list_lock);clear_bit(WMI_READY, &ar->flag);if (ar->fw_recovery.enable)del_timer_sync(&ar->fw_recovery.hb_timer);    After wmi_shudown all WMI events will be dropped. We   need to cleanup the buffers allocated in AP mode and   give disconnect notification to stack, which usually   happens in the disconnect_event. Simulate the disconnect   event by calling the function directly. Sometimes   disconnect_event will be received when the debug logs   are collected. ", "ret = ath6kl_hif_power_on(ar);if (ret)goto err_bmi_cleanup;ret = ath6kl_bmi_get_target_info(ar, &targ_info);if (ret)goto err_power_off;ar->version.target_ver = le32_to_cpu(targ_info.version);ar->target_type = le32_to_cpu(targ_info.type);ar->wiphy->hw_version = le32_to_cpu(targ_info.version);ret = ath6kl_init_hw_params(ar);if (ret)goto err_power_off;ar->htc_target = ath6kl_htc_create(ar);if (!ar->htc_target) ": "ath6kl_core_init(struct ath6kl  ar, enum ath6kl_htc_type htc_type){struct ath6kl_bmi_target_info targ_info;struct wireless_dev  wdev;int ret = 0, i;switch (htc_type) {case ATH6KL_HTC_TYPE_MBOX:ath6kl_htc_mbox_attach(ar);break;case ATH6KL_HTC_TYPE_PIPE:ath6kl_htc_pipe_attach(ar);break;default:WARN_ON(1);return -ENOMEM;}ar->ath6kl_wq = create_singlethread_workqueue(\"ath6kl\");if (!ar->ath6kl_wq)return -ENOMEM;ret = ath6kl_bmi_init(ar);if (ret)goto err_wq;    Turn on power to get hardware (target) version and leave power   on delibrately as we will boot the hardware anyway within few   seconds. ", "for (ctr = 0; ctr < AP_MAX_NUM_STA; ctr++) ": "ath6kl_core_create(struct device  dev){struct ath6kl  ar;u8 ctr;ar = ath6kl_cfg80211_create();if (!ar)return NULL;ar->p2p = !!ath6kl_p2p;ar->dev = dev;ar->vif_max = 1;ar->max_norm_iface = 1;spin_lock_init(&ar->lock);spin_lock_init(&ar->mcastpsq_lock);spin_lock_init(&ar->list_lock);init_waitqueue_head(&ar->event_wq);sema_init(&ar->sem, 1);INIT_LIST_HEAD(&ar->amsdu_rx_buffer_queue);INIT_LIST_HEAD(&ar->vif_list);clear_bit(WMI_ENABLED, &ar->flag);clear_bit(SKIP_SCAN, &ar->flag);clear_bit(DESTROY_IN_PROGRESS, &ar->flag);ar->tx_pwr = 0;ar->intra_bss = 1;ar->lrssi_roam_threshold = DEF_LRSSI_ROAM_THRESHOLD;ar->state = ATH6KL_STATE_OFF;memset((u8  )ar->sta_list, 0,       AP_MAX_NUM_STA   sizeof(struct ath6kl_sta));  Init the PS queues ", "trace_ath6kl_log_dbg_dump(msg ? msg : \"\", prefix ? prefix : \"\",  buf, len);}EXPORT_SYMBOL(ath6kl_dbg_dump": "ath6kl_dbg_dump(enum ATH6K_DEBUG_MASK mask,     const char  msg, const char  prefix,     const void  buf, size_t len){if (debug_mask & mask) {if (msg)ath6kl_dbg(mask, \"%s\\n\", msg);print_hex_dump_bytes(prefix, DUMP_PREFIX_OFFSET, buf, len);}  tracing code doesn't like null strings : ", "dev->htc_cnxt->chk_irq_status_cnt = 0;/* * IRQ processing is synchronous, interrupt status registers can be * re-read. ": "ath6kl_hif_intr_bh_handler(struct ath6kl  ar){struct ath6kl_device  dev = ar->htc_target->dev;unsigned long timeout;int status = 0;bool done = false;    Reset counter used to flag a re-scan of IRQ status registers on   the target. ", "ath6kl_tx_data_cleanup(ar);prev_state = ar->state;ret = ath6kl_wow_suspend(ar, wow);if (ret) ": "ath6kl_cfg80211_suspend(struct ath6kl  ar,    enum ath6kl_cfg_suspend_mode mode,    struct cfg80211_wowlan  wow){struct ath6kl_vif  vif;enum ath6kl_state prev_state;int ret;switch (mode) {case ATH6KL_CFG_SUSPEND_WOW:ath6kl_dbg(ATH6KL_DBG_SUSPEND, \"wow mode suspend\\n\");  Flush all non control pkts in TX path ", "if (!test_bit(ATH_OP_PRIM_STA_VIF, &common->op_flags)) ": "ath9k_cmn_beacon_config_sta(struct ath_hw  ah, struct ath_beacon_config  conf, struct ath9k_beacon_state  bs){struct ath_common  common = ath9k_hw_common(ah);int dtim_intval;u64 tsf;  No need to configure beacon if we are not associated ", "conf->intval = TU_TO_USEC(conf->beacon_interval);conf->intval /= bc_buf;conf->nexttbtt = ath9k_get_next_tbtt(ah, ath9k_hw_gettsf64(ah),       conf->beacon_interval);if (conf->enable_beacon)ah->imask |= ATH9K_INT_SWBA;elseah->imask &= ~ATH9K_INT_SWBA;ath_dbg(common, BEACON,\"AP (%s) nexttbtt: %u intval: %u conf_intval: %u\\n\",(conf->enable_beacon) ? \"Enable\" : \"Disable\",conf->nexttbtt, conf->intval, conf->beacon_interval);}EXPORT_SYMBOL(ath9k_cmn_beacon_config_ap": "ath9k_cmn_beacon_config_ap(struct ath_hw  ah,struct ath_beacon_config  conf,unsigned int bc_buf){struct ath_common  common = ath9k_hw_common(ah);  NB: the beacon interval is kept internally in TU's ", "if (!common->btcoex_enabled) ": "ath9k_hw_btcoex_init_scheme(struct ath_hw  ah){struct ath_common  common = ath9k_hw_common(ah);struct ath_btcoex_hw  btcoex_hw = &ah->btcoex_hw;    Check if BTCOEX is globally disabled. ", "REG_CLR_BIT(ah, AR_GPIO_INPUT_EN_VAL(ah),    (AR_GPIO_INPUT_EN_VAL_BT_PRIORITY_DEF |     AR_GPIO_INPUT_EN_VAL_BT_FREQUENCY_DEF));REG_SET_BIT(ah, AR_GPIO_INPUT_EN_VAL(ah),    AR_GPIO_INPUT_EN_VAL_BT_ACTIVE_BB);/* Set input mux for bt_active to gpio pin ": "ath9k_hw_btcoex_init_2wire(struct ath_hw  ah){struct ath_btcoex_hw  btcoex_hw = &ah->btcoex_hw;  connect bt_active to baseband ", "REG_SET_BIT(ah, AR_GPIO_INPUT_EN_VAL(ah),(AR_GPIO_INPUT_EN_VAL_BT_PRIORITY_BB | AR_GPIO_INPUT_EN_VAL_BT_ACTIVE_BB));/* Set input mux for bt_prority_async and *                  bt_active_async to GPIO pins ": "ath9k_hw_btcoex_init_3wire(struct ath_hw  ah){struct ath_btcoex_hw  btcoex_hw = &ah->btcoex_hw;  btcoex 3-wire ", "bool concur_tx = (mci_hw->concur_tx && btcoex_hw->tx_prio[stomp_type]);const u32 *weight = ar9003_wlan_weights[stomp_type];int i;if (!AR_SREV_9300_20_OR_LATER(ah)) ": "ath9k_hw_btcoex_set_weight(struct ath_hw  ah,u32 bt_weight,u32 wlan_weight,enum ath_stomp_type stomp_type){struct ath_btcoex_hw  btcoex_hw = &ah->btcoex_hw;struct ath9k_hw_mci  mci_hw = &ah->btcoex_hw.mci;u8 txprio_shift[] = { 24, 16, 16, 0 };   tx priority weight ", "ath9k_hw_gpio_request_out(ah, btcoex_hw->wlanactive_gpio,  \"ath9k-wlanactive\",  AR_GPIO_OUTPUT_MUX_AS_TX_FRAME);}/* * For AR9002, bt_weight/wlan_weight are used. * For AR9003 and above, stomp_type is used. ": "ath9k_hw_btcoex_enable_2wire(struct ath_hw  ah){struct ath_btcoex_hw  btcoex_hw = &ah->btcoex_hw;  Configure the desired GPIO port for TX_FRAME output ", "void ath9k_hw_btcoex_bt_stomp(struct ath_hw *ah,      enum ath_stomp_type stomp_type)": "ath9k_hw_btcoex_bt_stomp(ah, ATH_BTCOEX_STOMP_NONE);for (i = 0; i < AR9300_NUM_BT_WEIGHTS; i++)REG_WRITE(ah, AR_MCI_COEX_WL_WEIGHTS(i),  btcoex_hw->wlan_weight[i]);}void ath9k_hw_btcoex_enable(struct ath_hw  ah){struct ath_btcoex_hw  btcoex_hw = &ah->btcoex_hw;switch (ath9k_hw_get_btcoex_scheme(ah)) {case ATH_BTCOEX_CFG_NONE:return;case ATH_BTCOEX_CFG_2WIRE:ath9k_hw_btcoex_enable_2wire(ah);break;case ATH_BTCOEX_CFG_3WIRE:ath9k_hw_btcoex_enable_3wire(ah);break;case ATH_BTCOEX_CFG_MCI:ath9k_hw_btcoex_enable_mci(ah);break;}if (ath9k_hw_get_btcoex_scheme(ah) != ATH_BTCOEX_CFG_MCI &&    !AR_SREV_SOC(ah)) {REG_RMW(ah, AR_GPIO_PDPU(ah),(0x2 << (btcoex_hw->btactive_gpio   2)),(0x3 << (btcoex_hw->btactive_gpio   2)));}ah->btcoex_hw.enabled = true;}EXPORT_SYMBOL(ath9k_hw_btcoex_enable);void ath9k_hw_btcoex_disable(struct ath_hw  ah){struct ath_btcoex_hw  btcoex_hw = &ah->btcoex_hw;int i;btcoex_hw->enabled = false;if (ath9k_hw_get_btcoex_scheme(ah) == ATH_BTCOEX_CFG_MCI) {ath9k_hw_btcoex_disable_mci(ah);return;}if (!AR_SREV_9300_20_OR_LATER(ah))ath9k_hw_set_gpio(ah, btcoex_hw->wlanactive_gpio, 0);ath9k_hw_gpio_request_out(ah, btcoex_hw->wlanactive_gpio,  NULL, AR_GPIO_OUTPUT_MUX_AS_OUTPUT);if (btcoex_hw->scheme == ATH_BTCOEX_CFG_3WIRE) {REG_WRITE(ah, AR_BT_COEX_MODE, AR_BT_QUIET | AR_BT_MODE);REG_WRITE(ah, AR_BT_COEX_MODE2, 0);if (AR_SREV_9300_20_OR_LATER(ah)) {REG_WRITE(ah, AR_BT_COEX_WL_WEIGHTS0, 0);REG_WRITE(ah, AR_BT_COEX_WL_WEIGHTS1, 0);for (i = 0; i < AR9300_NUM_BT_WEIGHTS; i++)REG_WRITE(ah, AR_BT_COEX_BT_WEIGHTS(i), 0);} elseREG_WRITE(ah, AR_BT_COEX_WEIGHT, 0);}}EXPORT_SYMBOL(ath9k_hw_btcoex_disable);    Configures appropriate weight based on stomp type. ", "if (!is2ghz) ": "ar9003_paprd_enable(struct ath_hw  ah, bool val){struct ath9k_channel  chan = ah->curchan;bool is2ghz = IS_CHAN_2GHZ(chan);    3 bits for modalHeader5G.papdRateMaskHt20   is used for sub-band disabling of PAPRD.   5G band is divided into 3 sub-bands -- upper,   middle, lower.   if bit 30 of modalHeader5G.papdRateMaskHt20 is set   -- disable PAPRD for upper band 5GHz   if bit 29 of modalHeader5G.papdRateMaskHt20 is set   -- disable PAPRD for middle band 5GHz   if bit 28 of modalHeader5G.papdRateMaskHt20 is set   -- disable PAPRD for lower band 5GHz ", "REG_RMW_FIELD(ah, AR_PHY_PAPRD_CTRL1_B2,      AR_PHY_PAPRD_CTRL1_PAPRD_POWER_AT_AM2AM_CAL,      training_power);}EXPORT_SYMBOL(ar9003_paprd_populate_single_table": "ar9003_paprd_populate_single_table(struct ath_hw  ah,struct ath9k_hw_cal_data  caldata,int chain){u32  paprd_table_val = caldata->pa_table[chain];u32 small_signal_gain = caldata->small_signal_gain[chain];u32 training_power = ah->paprd_training_power;u32 reg = 0;int i;if (chain == 0)reg = AR_PHY_PAPRD_MEM_TAB_B0;else if (chain == 1)reg = AR_PHY_PAPRD_MEM_TAB_B1;else if (chain == 2)reg = AR_PHY_PAPRD_MEM_TAB_B2;for (i = 0; i < PAPRD_TABLE_SZ; i++) {REG_WRITE(ah, reg, paprd_table_val[i]);reg = reg + 4;}if (chain == 0)reg = AR_PHY_PA_GAIN123_B0;else if (chain == 1)reg = AR_PHY_PA_GAIN123_B1;elsereg = AR_PHY_PA_GAIN123_B2;REG_RMW_FIELD(ah, reg, AR_PHY_PA_GAIN123_PA_GAIN1, small_signal_gain);REG_RMW_FIELD(ah, AR_PHY_PAPRD_CTRL1_B0,      AR_PHY_PAPRD_CTRL1_PAPRD_POWER_AT_AM2AM_CAL,      training_power);if (ah->caps.tx_chainmask & BIT(1))REG_RMW_FIELD(ah, AR_PHY_PAPRD_CTRL1_B1,      AR_PHY_PAPRD_CTRL1_PAPRD_POWER_AT_AM2AM_CAL,      training_power);if (ah->caps.tx_chainmask & BIT(2))  val AR_PHY_PAPRD_CTRL1_PAPRD_POWER_AT_AM2AM_CAL correct? ", "if (agc2_pwr <= PAPRD_IDEAL_AGC2_PWR_RANGE)paprd_done = 0;}exit:return !!paprd_done;}EXPORT_SYMBOL(ar9003_paprd_is_done": "ar9003_paprd_is_done(struct ath_hw  ah){int paprd_done, agc2_pwr;paprd_done = REG_READ_FIELD(ah, AR_PHY_PAPRD_TRAINER_STAT1(ah),AR_PHY_PAPRD_TRAINER_STAT1_PAPRD_TRAIN_DONE);if (AR_SREV_9485(ah))goto exit;if (paprd_done == 0x1) {agc2_pwr = REG_READ_FIELD(ah, AR_PHY_PAPRD_TRAINER_STAT1(ah),AR_PHY_PAPRD_TRAINER_STAT1_PAPRD_AGC2_PWR);ath_dbg(ath9k_hw_common(ah), CALIBRATE,\"AGC2_PWR = 0x%x training done = 0x%x\\n\",agc2_pwr, paprd_done);    agc2_pwr range should not be less than 'IDEAL_AGC2_PWR_CHANGE'   when the training is completely done, otherwise retraining is   done to make sure the value is in ideal range ", "nfval =    ath9k_hw_get_nf_limits(ah, chan)->cal[i];if (nfval > -60 || nfval < -127)nfval = default_nf;}REG_RMW(ah, ah->nf_regs[i],(((u32) nfval << 1) & 0x1ff), 0x1ff);}}/* * stop NF cal if ongoing to ensure NF load completes immediately * (or after end rx/tx frame if ongoing) ": "ath9k_hw_loadnf(struct ath_hw  ah, struct ath9k_channel  chan){struct ath9k_nfcal_hist  h = NULL;unsigned i, j;u8 chainmask = (ah->rxchainmask << 3) | ah->rxchainmask;struct ath_common  common = ath9k_hw_common(ah);s16 default_nf = ath9k_hw_get_nf_limits(ah, chan)->nominal;u32 bb_agc_ctl = REG_READ(ah, AR_PHY_AGC_CONTROL(ah));if (ah->caldata)h = ah->caldata->nfCalHist;ENABLE_REG_RMW_BUFFER(ah);for (i = 0; i < NUM_NF_READINGS; i++) {if (chainmask & (1 << i)) {s16 nfval;if ((i >= AR5416_MAX_CHAINS) && !IS_CHAN_HT40(chan))continue;if (ah->nf_override)nfval = ah->nf_override;else if (h)nfval = h[i].privNF;else {  Try to get calibrated noise floor value ", "if (!test_bit(NFCAL_PENDING, &caldata->cal_flags))ath9k_hw_start_nfcal(ah, true);else if (!(REG_READ(ah, AR_PHY_AGC_CONTROL(ah)) & AR_PHY_AGC_CONTROL_NF))ath9k_hw_getnf(ah, ah->curchan);set_bit(NFCAL_INTF, &caldata->cal_flags);}EXPORT_SYMBOL(ath9k_hw_bstuck_nfcal": "ath9k_hw_bstuck_nfcal(struct ath_hw  ah){struct ath9k_hw_cal_data  caldata = ah->caldata;if (unlikely(!caldata))return;    If beacons are stuck, the most likely cause is interference.   Triggering a noise floor calibration at this point helps the   hardware adapt to a noisy environment much faster.   To ensure that we recover from stuck beacons quickly, let   the baseband update the internal NF value itself, similar to   what is being done after a full reset. ", "bool ar9003_hw_bb_watchdog_check(struct ath_hw *ah)": "ar9003_hw_bb_watchdog_check(): Returns true if a chip reset is required. ", "result = MS(ah->bb_watchdog_last_status, AR_PHY_WATCHDOG_RX_OFDM_SM);if ((result == 0xb) || ah->bb_hang_rx_ofdm) ": "ar9003_hw_disable_phy_restart(struct ath_hw  ah){u8 result;u32 val;  While receiving unsupported rate frame rx state machine   gets into a state 0xb and if phy_restart happens in that   state, BB would go hang. If RXSM is in 0xb state after   first bb panic, ensure to disable the phy_restart. ", "bool ath9k_hw_updatetxtriglevel(struct ath_hw *ah, bool bIncTrigLevel)": "ath9k_hw_updatetxtriglevel - adjusts the frame trigger level     @ah: atheros hardware struct   @bIncTrigLevel: whether or not the frame trigger level should be updated     The frame trigger level specifies the minimum number of bytes,   in units of 64 bytes, that must be DMA'ed into the PCU TX FIFO   before the PCU will initiate sending the frame on the air. This can   mean we initiate transmit before a full frame is on the PCU TX FIFO.   Resets to 0x1 (meaning 64 bytes or a full frame, whichever occurs   first)     Caution must be taken to ensure to set the frame trigger level based   on the DMA request size. For example if the DMA request size is set to   128 bytes the trigger level cannot exceed 6   64 = 384. This is because   there need to be enough space in the tx FIFO for the requested transfer   size. Hence the tx FIFO will stop with 512 - 128 = 384 bytes. If we set   the threshold to a value beyond 6, then the transmit will hang.     Current dual   stream devices have a PCU TX FIFO size of 8 KB.   Current single stream devices have a PCU TX FIFO size of 4 KB, however,   there is a hardware issue which forces us to use 2 KB instead so the   frame trigger level must not exceed 2 KB for these chipsets. ", "#define ATH9K_TIME_QUANTUM100     /* usec ": "ath9k_hw_enable_interrupts(ah);ah->tx_trig_level = newLevel;return newLevel != curLevel;}EXPORT_SYMBOL(ath9k_hw_updatetxtriglevel);void ath9k_hw_abort_tx_dma(struct ath_hw  ah){int maxdelay = 1000;int i, q;if (ah->curchan) {if (IS_CHAN_HALF_RATE(ah->curchan))maxdelay  = 2;else if (IS_CHAN_QUARTER_RATE(ah->curchan))maxdelay  = 4;}REG_WRITE(ah, AR_Q_TXD, AR_Q_TXD_M);REG_SET_BIT(ah, AR_PCU_MISC, AR_PCU_FORCE_QUIET_COLL | AR_PCU_CLEAR_VMF);REG_SET_BIT(ah, AR_DIAG_SW, AR_DIAG_FORCE_CH_IDLE_HIGH);REG_SET_BIT(ah, AR_D_GBL_IFS_MISC, AR_D_GBL_IFS_MISC_IGNORE_BACKOFF);for (q = 0; q < AR_NUM_QCU; q++) {for (i = 0; i < maxdelay; i++) {if (i)udelay(5);if (!ath9k_hw_numtxpending(ah, q))break;}}REG_CLR_BIT(ah, AR_PCU_MISC, AR_PCU_FORCE_QUIET_COLL | AR_PCU_CLEAR_VMF);REG_CLR_BIT(ah, AR_DIAG_SW, AR_DIAG_FORCE_CH_IDLE_HIGH);REG_CLR_BIT(ah, AR_D_GBL_IFS_MISC, AR_D_GBL_IFS_MISC_IGNORE_BACKOFF);REG_WRITE(ah, AR_Q_TXD, 0);}EXPORT_SYMBOL(ath9k_hw_abort_tx_dma);bool ath9k_hw_stop_dma_queue(struct ath_hw  ah, u32 q){#define ATH9K_TX_STOP_DMA_TIMEOUT1000      usec ", "if (AR_SREV_9300_20_OR_LATER(ah) &&    ah->opmode != NL80211_IFTYPE_ADHOC) ": "ath9k_hw_resettxqueue(struct ath_hw  ah, u32 q){struct ath_common  common = ath9k_hw_common(ah);struct ath9k_tx_queue_info  qi;u32 cwMin, chanCwMin, value;qi = &ah->txq[q];if (qi->tqi_type == ATH9K_TX_QUEUE_INACTIVE) {ath_dbg(common, QUEUE, \"Reset TXQ, inactive queue: %u\\n\", q);return true;}ath_dbg(common, QUEUE, \"Reset TX queue: %u\\n\", q);if (qi->tqi_cwmin == ATH9K_TXQ_USEDEFAULT) {chanCwMin = INIT_CWMIN;for (cwMin = 1; cwMin < chanCwMin; cwMin = (cwMin << 1) | 1);} elsecwMin = qi->tqi_cwmin;ENABLE_REGWRITE_BUFFER(ah);REG_WRITE(ah, AR_DLCL_IFS(q),  SM(cwMin, AR_D_LCL_IFS_CWMIN) |  SM(qi->tqi_cwmax, AR_D_LCL_IFS_CWMAX) |  SM(qi->tqi_aifs, AR_D_LCL_IFS_AIFS));REG_WRITE(ah, AR_DRETRY_LIMIT(q),  SM(INIT_SSH_RETRY, AR_D_RETRY_LIMIT_STA_SH) |  SM(INIT_SLG_RETRY, AR_D_RETRY_LIMIT_STA_LG) |  SM(qi->tqi_shretry, AR_D_RETRY_LIMIT_FR_SH));REG_WRITE(ah, AR_QMISC(q), AR_Q_MISC_DCU_EARLY_TERM_REQ);if (AR_SREV_9340(ah) && !AR_SREV_9340_13_OR_LATER(ah))REG_WRITE(ah, AR_DMISC(q),  AR_D_MISC_CW_BKOFF_EN | AR_D_MISC_FRAG_WAIT_EN | 0x1);elseREG_WRITE(ah, AR_DMISC(q),  AR_D_MISC_CW_BKOFF_EN | AR_D_MISC_FRAG_WAIT_EN | 0x2);if (qi->tqi_cbrPeriod) {REG_WRITE(ah, AR_QCBRCFG(q),  SM(qi->tqi_cbrPeriod, AR_Q_CBRCFG_INTERVAL) |  SM(qi->tqi_cbrOverflowLimit, AR_Q_CBRCFG_OVF_THRESH));REG_SET_BIT(ah, AR_QMISC(q), AR_Q_MISC_FSP_CBR |    (qi->tqi_cbrOverflowLimit ?     AR_Q_MISC_CBR_EXP_CNTR_LIMIT_EN : 0));}if (qi->tqi_readyTime && (qi->tqi_type != ATH9K_TX_QUEUE_CAB)) {REG_WRITE(ah, AR_QRDYTIMECFG(q),  SM(qi->tqi_readyTime, AR_Q_RDYTIMECFG_DURATION) |  AR_Q_RDYTIMECFG_EN);}REG_WRITE(ah, AR_DCHNTIME(q),  SM(qi->tqi_burstTime, AR_D_CHNTIME_DUR) |  (qi->tqi_burstTime ? AR_D_CHNTIME_EN : 0));if (qi->tqi_burstTime    && (qi->tqi_qflags & TXQ_FLAG_RDYTIME_EXP_POLICY_ENABLE))REG_SET_BIT(ah, AR_QMISC(q), AR_Q_MISC_RDYTIME_EXP_POLICY);if (qi->tqi_qflags & TXQ_FLAG_BACKOFF_DISABLE)REG_SET_BIT(ah, AR_DMISC(q), AR_D_MISC_POST_FR_BKOFF_DIS);REGWRITE_BUFFER_FLUSH(ah);if (qi->tqi_qflags & TXQ_FLAG_FRAG_BURST_BACKOFF_ENABLE)REG_SET_BIT(ah, AR_DMISC(q), AR_D_MISC_FRAG_BKOFF_EN);switch (qi->tqi_type) {case ATH9K_TX_QUEUE_BEACON:ENABLE_REGWRITE_BUFFER(ah);REG_SET_BIT(ah, AR_QMISC(q),    AR_Q_MISC_FSP_DBA_GATED    | AR_Q_MISC_BEACON_USE    | AR_Q_MISC_CBR_INCR_DIS1);REG_SET_BIT(ah, AR_DMISC(q),    (AR_D_MISC_ARB_LOCKOUT_CNTRL_GLOBAL <<     AR_D_MISC_ARB_LOCKOUT_CNTRL_S)    | AR_D_MISC_BEACON_USE    | AR_D_MISC_POST_FR_BKOFF_DIS);REGWRITE_BUFFER_FLUSH(ah);    cwmin and cwmax should be 0 for beacon queue   but not for IBSS as we would create an imbalance   on beaconing fairness for participating nodes. ", "rs->enc_flags |=(ads.ds_rxstatus3 & AR_GI) ? RX_ENC_FLAG_SHORT_GI : 0;rs->bw = (ads.ds_rxstatus3 & AR_2040) ? RATE_INFO_BW_40 :RATE_INFO_BW_20;if (AR_SREV_9280_20_OR_LATER(ah))rs->enc_flags |=(ads.ds_rxstatus3 & AR_STBC) ?/* we can only Nss=1 STBC ": "ath9k_hw_rxprocdesc(struct ath_hw  ah, struct ath_desc  ds,struct ath_rx_status  rs){struct ar5416_desc ads;struct ar5416_desc  adsp = AR5416DESC(ds);u32 phyerr;if ((adsp->ds_rxstatus8 & AR_RxDone) == 0)return -EINPROGRESS;ads.u.rx = adsp->u.rx;rs->rs_status = 0;rs->rs_flags = 0;rs->enc_flags = 0;rs->bw = RATE_INFO_BW_20;rs->rs_datalen = ads.ds_rxstatus1 & AR_DataLen;rs->rs_tstamp = ads.AR_RcvTimestamp;if (ads.ds_rxstatus8 & AR_PostDelimCRCErr) {rs->rs_rssi = ATH9K_RSSI_BAD;rs->rs_rssi_ctl[0] = ATH9K_RSSI_BAD;rs->rs_rssi_ctl[1] = ATH9K_RSSI_BAD;rs->rs_rssi_ctl[2] = ATH9K_RSSI_BAD;rs->rs_rssi_ext[0] = ATH9K_RSSI_BAD;rs->rs_rssi_ext[1] = ATH9K_RSSI_BAD;rs->rs_rssi_ext[2] = ATH9K_RSSI_BAD;} else {rs->rs_rssi = MS(ads.ds_rxstatus4, AR_RxRSSICombined);rs->rs_rssi_ctl[0] = MS(ads.ds_rxstatus0,AR_RxRSSIAnt00);rs->rs_rssi_ctl[1] = MS(ads.ds_rxstatus0,AR_RxRSSIAnt01);rs->rs_rssi_ctl[2] = MS(ads.ds_rxstatus0,AR_RxRSSIAnt02);rs->rs_rssi_ext[0] = MS(ads.ds_rxstatus4,AR_RxRSSIAnt10);rs->rs_rssi_ext[1] = MS(ads.ds_rxstatus4,AR_RxRSSIAnt11);rs->rs_rssi_ext[2] = MS(ads.ds_rxstatus4,AR_RxRSSIAnt12);}if (ads.ds_rxstatus8 & AR_RxKeyIdxValid)rs->rs_keyix = MS(ads.ds_rxstatus8, AR_KeyIdx);elsers->rs_keyix = ATH9K_RXKEYIX_INVALID;rs->rs_rate = MS(ads.ds_rxstatus0, AR_RxRate);rs->rs_more = (ads.ds_rxstatus1 & AR_RxMore) ? 1 : 0;rs->rs_firstaggr = (ads.ds_rxstatus8 & AR_RxFirstAggr) ? 1 : 0;rs->rs_isaggr = (ads.ds_rxstatus8 & AR_RxAggr) ? 1 : 0;rs->rs_moreaggr = (ads.ds_rxstatus8 & AR_RxMoreAggr) ? 1 : 0;rs->rs_antenna = MS(ads.ds_rxstatus3, AR_RxAntenna);  directly mapped flags for ieee80211_rx_status ", "struct ath_common *common = ath9k_hw_common(ah);u32 mac_status, last_mac_status = 0;int i;/* Enable access to the DMA observation bus ": "ath9k_hw_stopdmarecv(struct ath_hw  ah, bool  reset){#define AH_RX_STOP_DMA_TIMEOUT 10000     usec ", "if (num_possible_cpus() > 1)ah->config.serialize_regmode = SER_REG_MODE_AUTO;if (NR_CPUS > 1 && ah->config.serialize_regmode == SER_REG_MODE_AUTO) ": "ath9k_hw_init_config(struct ath_hw  ah){struct ath_common  common = ath9k_hw_common(ah);ah->config.dma_beacon_response_time = 1;ah->config.sw_beacon_response_time = 6;ah->config.cwm_ignore_extcca = false;ah->config.analog_shiftreg = 1;ah->config.rx_intr_mitigation = true;if (AR_SREV_9300_20_OR_LATER(ah)) {ah->config.rimt_last = 500;ah->config.rimt_first = 2000;} else {ah->config.rimt_last = 250;ah->config.rimt_first = 700;}if (AR_SREV_9462(ah) || AR_SREV_9565(ah))ah->config.pll_pwrsave = 7;    We need this for PCI devices only (Cardbus, PCI, miniPCI)   _and_ if on non-uniprocessor systems (MultiprocessorHT).   This means we use it for all AR5416 devices, and the few   minor PCI AR9280 devices out there.     Serialization is required because these devices do not handle   well the case of two concurrent readswrites due to the latency   involved. During one readwrite another readwrite can be issued   on another CPU while the previous readwrite may still be working   on our hardware, if we hit this case the hardware poops in a loop.   We prevent this by serializing reads and writes.     This issue is not present on PCI-Express devices or pre-AR5416   devices (legacy, 802.11abg). ", "slottime += 3 * ah->coverage_class;acktimeout = slottime + sifstime + ack_offset;ctstimeout = acktimeout;/* * Workaround for early ACK timeouts, add an offset to match the * initval's 64us ack timeout value. Use 48us for the CTS timeout. * This was initially only meant to work around an issue with delayed * BA frames in some implementations, but it has been found to fix ACK * timeout issues in other cases as well. ": "ath9k_hw_init_global_settings(struct ath_hw  ah){struct ath_common  common = ath9k_hw_common(ah);const struct ath9k_channel  chan = ah->curchan;int acktimeout, ctstimeout, ack_offset = 0;int slottime;int sifstime;int rx_lat = 0, tx_lat = 0, eifs = 0, ack_shift = 0;u32 reg;ath_dbg(ath9k_hw_common(ah), RESET, \"ah->misc_mode 0x%x\\n\",ah->misc_mode);if (!chan)return;if (ah->misc_mode != 0)REG_SET_BIT(ah, AR_PCU_MISC, ah->misc_mode);if (IS_CHAN_A_FAST_CLOCK(ah, chan))rx_lat = 41;elserx_lat = 37;tx_lat = 54;if (IS_CHAN_5GHZ(chan))sifstime = 16;elsesifstime = 10;if (IS_CHAN_HALF_RATE(chan)) {eifs = 175;rx_lat  = 2;tx_lat  = 2;if (IS_CHAN_A_FAST_CLOCK(ah, chan))    tx_lat += 11;sifstime = 32;ack_offset = 16;ack_shift = 3;slottime = 13;} else if (IS_CHAN_QUARTER_RATE(chan)) {eifs = 340;rx_lat = (rx_lat   4) - 1;tx_lat  = 4;if (IS_CHAN_A_FAST_CLOCK(ah, chan))    tx_lat += 22;sifstime = 64;ack_offset = 32;ack_shift = 1;slottime = 21;} else {if (AR_SREV_9287(ah) && AR_SREV_9287_13_OR_LATER(ah)) {eifs = AR_D_GBL_IFS_EIFS_ASYNC_FIFO;reg = AR_USEC_ASYNC_FIFO;} else {eifs = REG_READ(ah, AR_D_GBL_IFS_EIFS)common->clockrate;reg = REG_READ(ah, AR_USEC);}rx_lat = MS(reg, AR_USEC_RX_LAT);tx_lat = MS(reg, AR_USEC_TX_LAT);slottime = ah->slottime;}  As defined by IEEE 802.11-2007 17.3.8.6 ", "if (REG_READ(ah, AR_CFG) == 0xdeadbeef)return false;if (AR_SREV_9300(ah))return !ath9k_hw_detect_mac_hang(ah);if (AR_SREV_9285_12_OR_LATER(ah))return true;last_val = REG_READ(ah, AR_OBS_BUS_1);do ": "ath9k_hw_set_gpio(ah, i, !!(ah->gpio_val & BIT(i)));}}void ath9k_hw_check_nav(struct ath_hw  ah){struct ath_common  common = ath9k_hw_common(ah);u32 val;val = REG_READ(ah, AR_NAV);if (val != 0xdeadbeef && val > 0x7fff) {ath_dbg(common, BSTUCK, \"Abnormal NAV: 0x%x\\n\", val);REG_WRITE(ah, AR_NAV, 0);}}EXPORT_SYMBOL(ath9k_hw_check_nav);bool ath9k_hw_check_alive(struct ath_hw  ah){int count = 50;u32 reg, last_val;  Check if chip failed to wake up ", "static bool ath9k_hw_ar9330_reset_war(struct ath_hw *ah, int type)": "ath9k_hw_reset_txstatus_ring(ah);}static void ath9k_hw_set_operating_mode(struct ath_hw  ah, int opmode){u32 mask = AR_STA_ID1_STA_AP | AR_STA_ID1_ADHOC;u32 set = AR_STA_ID1_KSRCH_MODE;ENABLE_REG_RMW_BUFFER(ah);switch (opmode) {case NL80211_IFTYPE_ADHOC:if (!AR_SREV_9340_13(ah)) {set |= AR_STA_ID1_ADHOC;REG_SET_BIT(ah, AR_CFG, AR_CFG_AP_ADHOC_INDICATION);break;}fallthrough;case NL80211_IFTYPE_OCB:case NL80211_IFTYPE_MESH_POINT:case NL80211_IFTYPE_AP:set |= AR_STA_ID1_STA_AP;fallthrough;case NL80211_IFTYPE_STATION:REG_CLR_BIT(ah, AR_CFG, AR_CFG_AP_ADHOC_INDICATION);break;default:if (!ah->is_monitoring)set = 0;break;}REG_RMW(ah, AR_STA_ID1, set, mask);REG_RMW_BUFFER_FLUSH(ah);}void ath9k_hw_get_delta_slope_vals(struct ath_hw  ah, u32 coef_scaled,   u32  coef_mantissa, u32  coef_exponent){u32 coef_exp, coef_man;for (coef_exp = 31; coef_exp > 0; coef_exp--)if ((coef_scaled >> coef_exp) & 0x1)break;coef_exp = 14 - (coef_exp - COEF_SCALE_S);coef_man = coef_scaled + (1 << (COEF_SCALE_S - coef_exp - 1)); coef_mantissa = coef_man >> (COEF_SCALE_S - coef_exp); coef_exponent = coef_exp - 16;}  AR9330 WAR:   call external reset function to reset WMAC if:   - doing a cold reset   - we have pending frames in the TX queues. ", "switch (ah->hw_version.devid) ": "ath9k_hw_setpower(ah, ATH9K_PM_AWAKE)) {ath_err(common, \"Couldn't wakeup chip\\n\");return -EIO;}if (AR_SREV_9271(ah) || AR_SREV_9100(ah) || AR_SREV_9340(ah) ||    AR_SREV_9330(ah) || AR_SREV_9550(ah))ah->is_pciexpress = false;ah->hw_version.phyRev = REG_READ(ah, AR_PHY_CHIP_ID);ath9k_hw_init_cal_settings(ah);if (!ah->is_pciexpress)ath9k_hw_disablepcie(ah);r = ath9k_hw_post_init(ah);if (r)return r;ath9k_hw_init_mode_gain_regs(ah);r = ath9k_hw_fill_cap_info(ah);if (r)return r;ath9k_hw_init_macaddr(ah);ath9k_hw_init_hang_checks(ah);common->state = ATH_HW_INITIALIZED;return 0;}int ath9k_hw_init(struct ath_hw  ah){int ret;struct ath_common  common = ath9k_hw_common(ah);  These are all the AR5008AR9001AR9002AR9003 hardware family of chipsets ", "REG_WRITE(ah, AR_TSFOOR_THRESHOLD, bs->bs_tsfoor_threshold);}EXPORT_SYMBOL(ath9k_hw_set_sta_beacon_timers": "ath9k_hw_set_sta_beacon_timers(struct ath_hw  ah,    const struct ath9k_beacon_state  bs){u32 nextTbtt, beaconintval, dtimperiod, beacontimeout;struct ath9k_hw_capabilities  pCap = &ah->caps;struct ath_common  common = ath9k_hw_common(ah);ENABLE_REGWRITE_BUFFER(ah);REG_WRITE(ah, AR_NEXT_TBTT_TIMER, bs->bs_nexttbtt);REG_WRITE(ah, AR_BEACON_PERIOD, bs->bs_intval);REG_WRITE(ah, AR_DMA_BEACON_PERIOD, bs->bs_intval);REGWRITE_BUFFER_FLUSH(ah);REG_RMW_FIELD(ah, AR_RSSI_THR,      AR_RSSI_THR_BM_THR, bs->bs_bmissthreshold);beaconintval = bs->bs_intval;if (bs->bs_sleepduration > beaconintval)beaconintval = bs->bs_sleepduration;dtimperiod = bs->bs_dtimperiod;if (bs->bs_sleepduration > dtimperiod)dtimperiod = bs->bs_sleepduration;if (beaconintval == dtimperiod)nextTbtt = bs->bs_nextdtim;elsenextTbtt = bs->bs_nexttbtt;ath_dbg(common, BEACON, \"next DTIM %u\\n\", bs->bs_nextdtim);ath_dbg(common, BEACON, \"next beacon %u\\n\", nextTbtt);ath_dbg(common, BEACON, \"beacon period %u\\n\", beaconintval);ath_dbg(common, BEACON, \"DTIM period %u\\n\", dtimperiod);ENABLE_REGWRITE_BUFFER(ah);REG_WRITE(ah, AR_NEXT_DTIM, bs->bs_nextdtim - SLEEP_SLOP);REG_WRITE(ah, AR_NEXT_TIM, nextTbtt - SLEEP_SLOP);REG_WRITE(ah, AR_SLEEP1,  SM((CAB_TIMEOUT_VAL << 3), AR_SLEEP1_CAB_TIMEOUT)  | AR_SLEEP1_ASSUME_DTIM);if (pCap->hw_caps & ATH9K_HW_CAP_AUTOSLEEP)beacontimeout = (BEACON_TIMEOUT_VAL << 3);elsebeacontimeout = MIN_BEACON_TIMEOUT_VAL;REG_WRITE(ah, AR_SLEEP2,  SM(beacontimeout, AR_SLEEP2_BEACON_TIMEOUT));REG_WRITE(ah, AR_TIM_PERIOD, beaconintval);REG_WRITE(ah, AR_DTIM_PERIOD, dtimperiod);REGWRITE_BUFFER_FLUSH(ah);REG_SET_BIT(ah, AR_TIMER_MODE,    AR_TBTT_TIMER_EN | AR_TIM_TIMER_EN |    AR_DTIM_TIMER_EN);  TSF Out of Range Threshold ", "ah->radar_conf.ext_channel = IS_CHAN_HT40(chan);ath9k_hw_set_radar_params(ah);}return 0;}EXPORT_SYMBOL(ath9k_hw_reset);/*****************************": "ath9k_hw_gen_timer_start_tsf2(ah);ath9k_hw_init_desc(ah);if (ath9k_hw_btcoex_is_enabled(ah))ath9k_hw_btcoex_enable(ah);if (ath9k_hw_mci_is_enabled(ah))ar9003_mci_check_bt(ah);if (AR_SREV_9300_20_OR_LATER(ah)) {ath9k_hw_loadnf(ah, chan);ath9k_hw_start_nfcal(ah, true);}if (AR_SREV_9300_20_OR_LATER(ah))ar9003_hw_bb_watchdog_config(ah);if (ah->config.hw_hang_checks & HW_PHYRESTART_CLC_WAR)ar9003_hw_disable_phy_restart(ah);ath9k_hw_apply_gpio_override(ah);if (AR_SREV_9565(ah) && common->bt_ant_diversity)REG_SET_BIT(ah, AR_BTCOEX_WL_LNADIV, AR_BTCOEX_WL_LNADIV_FORCE_ON);if (ah->hw->conf.radar_enabled) {  set HW specific DFS configuration ", "pCap->rx_chainmask = ah->eep_ops->get_eeprom(ah, EEP_RX_MASK);pCap->tx_chainmask = fixup_chainmask(pCap->chip_chainmask, pCap->tx_chainmask);pCap->rx_chainmask = fixup_chainmask(pCap->chip_chainmask, pCap->rx_chainmask);ah->txchainmask = pCap->tx_chainmask;ah->rxchainmask = pCap->rx_chainmask;ah->misc_mode |= AR_PCU_MIC_NEW_LOC_ENA;/* enable key search for every frame in an aggregate ": "ath9k_hw_gpio_get(ah, 0) ? 0x5 : 0x7;else if (AR_SREV_9100(ah))pCap->rx_chainmask = 0x7;else  Use rx_chainmask from EEPROM. ", "static bool ath9k_hw_chip_test(struct ath_hw *ah)": "ath9k_hw_disablepcie(struct ath_hw  ah){if (!AR_SREV_5416(ah))return;REG_WRITE(ah, AR_PCIE_SERDES, 0x9248fc00);REG_WRITE(ah, AR_PCIE_SERDES, 0x24924924);REG_WRITE(ah, AR_PCIE_SERDES, 0x28000029);REG_WRITE(ah, AR_PCIE_SERDES, 0x57160824);REG_WRITE(ah, AR_PCIE_SERDES, 0x25980579);REG_WRITE(ah, AR_PCIE_SERDES, 0x00000000);REG_WRITE(ah, AR_PCIE_SERDES, 0x1aaabe40);REG_WRITE(ah, AR_PCIE_SERDES, 0xbe105554);REG_WRITE(ah, AR_PCIE_SERDES, 0x000e1007);REG_WRITE(ah, AR_PCIE_SERDES2, 0x00000000);}  This should work for all families including legacy ", "static void ath9k_hw_init_desc(struct ath_hw *ah)": "ath9k_hw_write_associd(ah);REG_WRITE(ah, AR_ISR, ~0);REG_WRITE(ah, AR_RSSI_THR, INIT_RSSI_THR);REGWRITE_BUFFER_FLUSH(ah);ath9k_hw_set_operating_mode(ah, ah->opmode);}static void ath9k_hw_init_queues(struct ath_hw  ah){int i;ENABLE_REGWRITE_BUFFER(ah);for (i = 0; i < AR_NUM_DCU; i++)REG_WRITE(ah, AR_DQCUMASK(i), 1 << i);REGWRITE_BUFFER_FLUSH(ah);ah->intr_txqs = 0;for (i = 0; i < ATH9K_NUM_TX_QUEUES; i++)ath9k_hw_resettxqueue(ah, i);}    For big endian systems turn on swapping for descriptors ", "if (AR_SREV_9271(ah) && ah->htc_reset_init) ": "ath9k_hw_gettsf64(ah);saveLedState = REG_READ(ah, AR_CFG_LED) &(AR_CFG_LED_ASSOC_CTL | AR_CFG_LED_MODE_SEL | AR_CFG_LED_BLINK_THRESH_SEL | AR_CFG_LED_BLINK_SLOW);ath9k_hw_mark_phy_inactive(ah);ah->paprd_table_write_done = false;  Only required on the first reset ", "if (AR_SREV_9100(ah) && (ath9k_hw_gettsf64(ah) < tsf)) ": "ath9k_hw_settsf64(ah, tsf + tsf_offset);if (AR_SREV_9280_20_OR_LATER(ah))REG_SET_BIT(ah, AR_GPIO_INPUT_EN_VAL(ah), AR_GPIO_JTAG_DISABLE);if (!AR_SREV_9300_20_OR_LATER(ah))ar9002_hw_enable_async_fifo(ah);r = ath9k_hw_process_ini(ah, chan);if (r)return r;ath9k_hw_set_rfmode(ah, chan);if (ath9k_hw_mci_is_enabled(ah))ar9003_mci_reset(ah, false, IS_CHAN_2GHZ(chan), save_fullsleep);    Some AR91xx SoC devices frequently fail to accept TSF writes   right after the chip reset. When that happens, write a new   value after the initvals have been applied. ", "timer_table->timers[timer_index] = timer;timer->index = timer_index;timer->trigger = trigger;timer->overflow = overflow;timer->arg = arg;if ((timer_index > AR_FIRST_NDP_TIMER) && !timer_table->tsf2_enabled) ": "ath_gen_timer_alloc(struct ath_hw  ah,  void ( trigger)(void  ),  void ( overflow)(void  ),  void  arg,  u8 timer_index){struct ath_gen_timer_table  timer_table = &ah->hw_gen_timers;struct ath_gen_timer  timer;if ((timer_index < AR_FIRST_NDP_TIMER) ||    (timer_index >= ATH_MAX_GEN_TIMER))return NULL;if ((timer_index > AR_FIRST_NDP_TIMER) &&    !AR_SREV_9300_20_OR_LATER(ah))return NULL;timer = kzalloc(sizeof(struct ath_gen_timer), GFP_KERNEL);if (timer == NULL)return NULL;  allocate a hardware generic timer slot ", "REG_CLR_BIT(ah, gen_tmr_configuration[timer->index].mode_addr,gen_tmr_configuration[timer->index].mode_mask);if (AR_SREV_9462(ah) || AR_SREV_9565(ah)) ": "ath9k_hw_gen_timer_stop(struct ath_hw  ah, struct ath_gen_timer  timer){struct ath_gen_timer_table  timer_table = &ah->hw_gen_timers;  Clear generic timer enable bits. ", "timer_table->timers[timer->index] = NULL;kfree(timer);}EXPORT_SYMBOL(ath_gen_timer_free": "ath_gen_timer_free(struct ath_hw  ah, struct ath_gen_timer  timer){struct ath_gen_timer_table  timer_table = &ah->hw_gen_timers;  free the hardware generic timer slot ", "trigger_mask = ah->intr_gen_timer_trigger;thresh_mask = ah->intr_gen_timer_thresh;trigger_mask &= timer_table->timer_mask;thresh_mask &= timer_table->timer_mask;for_each_set_bit(index, &thresh_mask, ARRAY_SIZE(timer_table->timers)) ": "ath_gen_timer_isr(struct ath_hw  ah){struct ath_gen_timer_table  timer_table = &ah->hw_gen_timers;struct ath_gen_timer  timer;unsigned long trigger_mask, thresh_mask;unsigned int index;  get hardware generic timer interrupt status ", "if (AR_SREV_9280_20_OR_LATER(ah)) ": "ath9k_hw_name(struct ath_hw  ah, char  hw_name, size_t len){int used;  chipsets >= AR9280 are single-chip ", "memset(&ht_info->mcs, 0, sizeof(ht_info->mcs));tx_streams = ath9k_cmn_count_streams(ah->txchainmask, max_streams);rx_streams = ath9k_cmn_count_streams(ah->rxchainmask, max_streams);ath_dbg(common, CONFIG, \"TX streams %d, RX streams: %d\\n\",tx_streams, rx_streams);if (tx_streams != rx_streams) ": "ath9k_cmn_setup_ht_cap(struct ath_hw  ah,    struct ieee80211_sta_ht_cap  ht_info){struct ath_common  common = ath9k_hw_common(ah);u8 tx_streams, rx_streams;int i, max_streams;ht_info->ht_supported = true;ht_info->cap = IEEE80211_HT_CAP_SUP_WIDTH_20_40 |       IEEE80211_HT_CAP_SM_PS |       IEEE80211_HT_CAP_SGI_40 |       IEEE80211_HT_CAP_DSSSCCK40;if (ah->caps.hw_caps & ATH9K_HW_CAP_LDPC)ht_info->cap |= IEEE80211_HT_CAP_LDPC_CODING;if (ah->caps.hw_caps & ATH9K_HW_CAP_SGI_20)ht_info->cap |= IEEE80211_HT_CAP_SGI_20;ht_info->ampdu_factor = IEEE80211_HT_MAX_AMPDU_64K;ht_info->ampdu_density = IEEE80211_HT_MPDU_DENSITY_8;if (AR_SREV_9271(ah) || AR_SREV_9330(ah) || AR_SREV_9485(ah) || AR_SREV_9565(ah))max_streams = 1;else if (AR_SREV_9462(ah))max_streams = 2;else if (AR_SREV_9300_20_OR_LATER(ah))max_streams = 3;elsemax_streams = 2;if (AR_SREV_9280_20_OR_LATER(ah)) {if (max_streams >= 2)ht_info->cap |= IEEE80211_HT_CAP_TX_STBC;ht_info->cap |= (1 << IEEE80211_HT_CAP_RX_STBC_SHIFT);}  set up supported mcs set ", "rxs->rs_rssi = MS(rxsp->status5, AR_RxRSSICombined);rxs->rs_rssi_ctl[0] = MS(rxsp->status1, AR_RxRSSIAnt00);rxs->rs_rssi_ctl[1] = MS(rxsp->status1, AR_RxRSSIAnt01);rxs->rs_rssi_ctl[2] = MS(rxsp->status1, AR_RxRSSIAnt02);rxs->rs_rssi_ext[0] = MS(rxsp->status5, AR_RxRSSIAnt10);rxs->rs_rssi_ext[1] = MS(rxsp->status5, AR_RxRSSIAnt11);rxs->rs_rssi_ext[2] = MS(rxsp->status5, AR_RxRSSIAnt12);if (rxsp->status11 & AR_RxKeyIdxValid)rxs->rs_keyix = MS(rxsp->status11, AR_KeyIdx);elserxs->rs_keyix = ATH9K_RXKEYIX_INVALID;rxs->rs_rate = MS(rxsp->status1, AR_RxRate);rxs->rs_more = (rxsp->status2 & AR_RxMore) ? 1 : 0;rxs->rs_firstaggr = (rxsp->status11 & AR_RxFirstAggr) ? 1 : 0;rxs->rs_isaggr = (rxsp->status11 & AR_RxAggr) ? 1 : 0;rxs->rs_moreaggr = (rxsp->status11 & AR_RxMoreAggr) ? 1 : 0;rxs->rs_antenna = (MS(rxsp->status4, AR_RxAntenna) & 0x7);rxs->enc_flags |= (rxsp->status4 & AR_GI) ? RX_ENC_FLAG_SHORT_GI : 0;rxs->enc_flags |=(rxsp->status4 & AR_STBC) ? (1 << RX_ENC_FLAG_STBC_SHIFT) : 0;rxs->bw = (rxsp->status4 & AR_2040) ? RATE_INFO_BW_40 : RATE_INFO_BW_20;rxs->evm0 = rxsp->status6;rxs->evm1 = rxsp->status7;rxs->evm2 = rxsp->status8;rxs->evm3 = rxsp->status9;rxs->evm4 = (rxsp->status10 & 0xffff);if (rxsp->status11 & AR_PreDelimCRCErr)rxs->rs_flags |= ATH9K_RX_DELIM_CRC_PRE;if (rxsp->status11 & AR_PostDelimCRCErr)rxs->rs_flags |= ATH9K_RX_DELIM_CRC_POST;if (rxsp->status11 & AR_DecryptBusyErr)rxs->rs_flags |= ATH9K_RX_DECRYPT_BUSY;if ((rxsp->status11 & AR_RxFrameOK) == 0) ": "ath9k_hw_process_rxdesc_edma(struct ath_hw  ah, struct ath_rx_status  rxs, void  buf_addr){struct ar9003_rxs  rxsp = buf_addr;unsigned int phyerr;if ((rxsp->status11 & AR_RxDone) == 0)return -EINPROGRESS;if (MS(rxsp->ds_info, AR_DescId) != 0x168c)return -EINVAL;if ((rxsp->ds_info & (AR_TxRxDesc | AR_CtrlStat)) != 0)return -EINPROGRESS;rxs->rs_status = 0;rxs->rs_flags =  0;rxs->enc_flags = 0;rxs->bw = RATE_INFO_BW_20;rxs->rs_datalen = rxsp->status2 & AR_DataLen;rxs->rs_tstamp =  rxsp->status3;  XXX: Keycache ", "if (rx_stats->rs_keyix == ATH9K_RXKEYIX_INVALID ||    !test_bit(rx_stats->rs_keyix, common->ccmp_keymap))rx_stats->rs_status &= ~ATH9K_RXERR_KEYMISS;mic_error = is_valid_tkip && !ieee80211_is_ctl(fc) &&!ieee80211_has_morefrags(fc) &&!(le16_to_cpu(hdr->seq_ctrl) & IEEE80211_SCTL_FRAG) &&(rx_stats->rs_status & ATH9K_RXERR_MIC);/* * The rx_stats->rs_status will not be set until the end of the * chained descriptors so it can be ignored if rs_more is set. The * rs_more will be false at the last element of the chained * descriptors. ": "ath9k_cmn_rx_accept(struct ath_common  common, struct ieee80211_hdr  hdr, struct ieee80211_rx_status  rxs, struct ath_rx_status  rx_stats, bool  decrypt_error, unsigned int rxfilter){struct ath_hw  ah = common->ah;bool is_mc, is_valid_tkip, strip_mic, mic_error;__le16 fc;fc = hdr->frame_control;is_mc = !!is_multicast_ether_addr(hdr->addr1);is_valid_tkip = rx_stats->rs_keyix != ATH9K_RXKEYIX_INVALID &&test_bit(rx_stats->rs_keyix, common->tkip_keymap);strip_mic = is_valid_tkip && ieee80211_is_data(fc) &&ieee80211_has_protected(fc) &&!(rx_stats->rs_status &(ATH9K_RXERR_DECRYPT | ATH9K_RXERR_CRC | ATH9K_RXERR_MIC | ATH9K_RXERR_KEYMISS));    Key miss events are only relevant for pairwise keys where the   descriptor does contain a valid key index. This has been observed   mostly with CCMP encryption. ", "hdr = (struct ieee80211_hdr *) skb->data;hdrlen = ieee80211_get_hdrlen_from_skb(skb);fc = hdr->frame_control;padpos = ieee80211_hdrlen(fc);/* The MAC header is padded to have 32-bit boundary if the * packet payload is non-zero. The general calculation for * padsize would take into account odd header lengths: * padsize = (4 - padpos % 4) % 4; However, since only * even-length headers are used, padding can only be 0 or 2 * bytes and we can optimize this a bit. In addition, we must * not try to remove padding from short control frames that do * not have payload. ": "ath9k_cmn_rx_skb_postprocess(struct ath_common  common,  struct sk_buff  skb,  struct ath_rx_status  rx_stats,  struct ieee80211_rx_status  rxs,  bool decrypt_error){struct ath_hw  ah = common->ah;struct ieee80211_hdr  hdr;int hdrlen, padpos, padsize;u8 keyix;__le16 fc;  see if any padding is done by the hw and remove it ", "rxs->encoding = RX_ENC_HT;rxs->enc_flags |= rx_stats->enc_flags;rxs->bw = rx_stats->bw;rxs->rate_idx = rx_stats->rs_rate & 0x7f;return 0;}for (i = 0; i < sband->n_bitrates; i++) ": "ath9k_cmn_process_rate(struct ath_common  common,   struct ieee80211_hw  hw,   struct ath_rx_status  rx_stats,   struct ieee80211_rx_status  rxs){struct ieee80211_supported_band  sband;enum nl80211_band band;unsigned int i = 0;struct ath_hw  ah = common->ah;band = ah->curchan->chan->band;sband = hw->wiphy->bands[band];if (IS_CHAN_QUARTER_RATE(ah->curchan))rxs->bw = RATE_INFO_BW_5;else if (IS_CHAN_HALF_RATE(ah->curchan))rxs->bw = RATE_INFO_BW_10;if (rx_stats->rs_rate & 0x80) {  HT rate ", "if (rx_stats->rs_moreaggr) ": "ath9k_cmn_process_rssi(struct ath_common  common,    struct ieee80211_hw  hw,    struct ath_rx_status  rx_stats,    struct ieee80211_rx_status  rxs){struct ath_hw  ah = common->ah;int last_rssi;int rssi = rx_stats->rs_rssi;int i, j;    RSSI is not available for subframes in an A-MPDU. ", "*txpower = reg->max_power_level;}EXPORT_SYMBOL(ath9k_cmn_update_txpow": "ath9k_cmn_update_txpow(struct ath_hw  ah, u16 cur_txpow,    u16 new_txpow, u16  txpower){struct ath_regulatory  reg = ath9k_hw_regulatory(ah);if (ah->curchan && reg->power_limit != new_txpow)ath9k_hw_set_txpowerlimit(ah, new_txpow, false);  read back in case value is clamped ", "common->keymax = AR_KEYTABLE_SIZE;/* * Check whether the separate key cache entries * are required to handle both tx+rx MIC keys. * With split mic keys the number of stations is limited * to 27 otherwise 59. ": "ath9k_cmn_init_crypto(struct ath_hw  ah){struct ath_common  common = ath9k_hw_common(ah);int i = 0;  Get the hardware key cache size. ", "rval = REG_READ(ah, AR_WOW_PATTERN);val = AR_WOW_STATUS(rval);/* * Mask only the WoW events that we have enabled. Sometimes * we have spurious WoW events from the AR_WOW_PATTERN * register. This mask will clean it up. ": "ath9k_hw_wow_wakeup(struct ath_hw  ah){u32 wow_status = 0;u32 val = 0, rval;    Read the WoW status register to know   the wakeup reason. ", "REG_SET_BIT(ah, AR_PCIE_PM_CTRL(ah), AR_PMCTRL_HOST_PME_EN |     AR_PMCTRL_PWR_PM_CTRL_ENA |     AR_PMCTRL_AUX_PWR_DET |     AR_PMCTRL_WOW_PME_CLR);REG_CLR_BIT(ah, AR_PCIE_PM_CTRL(ah), AR_PMCTRL_WOW_PME_CLR);/* * Random Backoff. * * 31:28 in AR_WOW_PATTERN : Indicates the number of bits used in the *                           contention window. For value N, *                           the random backoff will be selected between *                           0 and (2 ^ N) - 1. ": "ath9k_hw_wow_enable(struct ath_hw  ah, u32 pattern_enable){u32 wow_event_mask;u32 keep_alive, magic_pattern, host_pm_ctrl;wow_event_mask = ah->wow.wow_event_mask;    AR_PMCTRL_HOST_PME_EN - Override PME enable in configuration                           space and allow MAC to generate WoW anyway.     AR_PMCTRL_PWR_PM_CTRL_ENA - ???     AR_PMCTRL_AUX_PWR_DET - PCI core SYS_AUX_PWR_DET signal,                           needs to be set for WoW in PCI mode.     AR_PMCTRL_WOW_PME_CLR - WoW Clear Signal going to the MAC.     Set the power states appropriately and enable PME.     Set and clear WOW_PME_CLEAR for the chip   to generate next wow signal. ", "void ath_dynack_sample_tx_ts(struct ath_hw *ah, struct sk_buff *skb,     struct ath_tx_status *ts,     struct ieee80211_sta *sta)": "ath_dynack_sample_tx_ts - status timestamp sampling method   @ah: ath hw   @skb: socket buffer   @ts: tx status info   @sta: station pointer   ", "void ath_dynack_sample_ack_ts(struct ath_hw *ah, struct sk_buff *skb,      u32 ts)": "ath_dynack_sample_ack_ts - ACK timestamp sampling method   @ah: ath hw   @skb: socket buffer   @ts: rx timestamp   ", "void ath_dynack_node_init(struct ath_hw *ah, struct ath_node *an)": "ath_dynack_node_init - init ath_node related info   @ah: ath hw   @an: ath node   ", "void ath_dynack_node_deinit(struct ath_hw *ah, struct ath_node *an)": "ath_dynack_node_deinit - deinit ath_node related info   @ah: ath hw   @an: ath node   ", "void ath_dynack_reset(struct ath_hw *ah)": "ath_dynack_reset - reset dynack processing   @ah: ath hw   ", "mci->raw_intr = 0;mci->rx_msg_intr = 0;}EXPORT_SYMBOL(ar9003_mci_get_interrupt": "ar9003_mci_get_interrupt(struct ath_hw  ah, u32  raw_intr,      u32  rx_msg_intr){struct ath9k_hw_mci  mci = &ah->btcoex_hw.mci; raw_intr = mci->raw_intr; rx_msg_intr = mci->rx_msg_intr;  Clean int bits after the values are read. ", "if (!ar9003_mci_send_message(ah, MCI_GPM, 0, payload, 16,wait_done, true)) ": "ar9003_mci_send_message(ah, MCI_REMOTE_RESET, 0, payload, 16,wait_done, false);udelay(5);}static void ar9003_mci_send_lna_transfer(struct ath_hw  ah, bool wait_done){u32 payload = 0x00000000;ar9003_mci_send_message(ah, MCI_LNA_TRANS, 0, &payload, 1,wait_done, false);}static void ar9003_mci_send_req_wake(struct ath_hw  ah, bool wait_done){ar9003_mci_send_message(ah, MCI_REQ_WAKE, MCI_FLAG_DISABLE_TIMESTAMP,NULL, 0, wait_done, false);udelay(5);}static void ar9003_mci_send_sys_waking(struct ath_hw  ah, bool wait_done){ar9003_mci_send_message(ah, MCI_SYS_WAKING, MCI_FLAG_DISABLE_TIMESTAMP,NULL, 0, wait_done, false);}static void ar9003_mci_send_lna_take(struct ath_hw  ah, bool wait_done){u32 payload = 0x70000000;ar9003_mci_send_message(ah, MCI_LNA_TAKE, 0, &payload, 1,wait_done, false);}static void ar9003_mci_send_sys_sleeping(struct ath_hw  ah, bool wait_done){ar9003_mci_send_message(ah, MCI_SYS_SLEEPING,MCI_FLAG_DISABLE_TIMESTAMP,NULL, 0, wait_done, false);}static void ar9003_mci_send_coex_version_query(struct ath_hw  ah,       bool wait_done){struct ath9k_hw_mci  mci = &ah->btcoex_hw.mci;u32 payload[4] = {0, 0, 0, 0};if (mci->bt_version_known ||    (mci->bt_state == MCI_BT_SLEEP))return;MCI_GPM_SET_TYPE_OPCODE(payload, MCI_GPM_COEX_AGENT,MCI_GPM_COEX_VERSION_QUERY);ar9003_mci_send_message(ah, MCI_GPM, 0, payload, 16, wait_done, true);}static void ar9003_mci_send_coex_version_response(struct ath_hw  ah,  bool wait_done){struct ath9k_hw_mci  mci = &ah->btcoex_hw.mci;u32 payload[4] = {0, 0, 0, 0};MCI_GPM_SET_TYPE_OPCODE(payload, MCI_GPM_COEX_AGENT,MCI_GPM_COEX_VERSION_RESPONSE); (((u8  )payload) + MCI_GPM_COEX_B_MAJOR_VERSION) =mci->wlan_ver_major; (((u8  )payload) + MCI_GPM_COEX_B_MINOR_VERSION) =mci->wlan_ver_minor;ar9003_mci_send_message(ah, MCI_GPM, 0, payload, 16, wait_done, true);}static void ar9003_mci_send_coex_wlan_channels(struct ath_hw  ah,       bool wait_done){struct ath9k_hw_mci  mci = &ah->btcoex_hw.mci;u32  payload = &mci->wlan_channels[0];if (!mci->wlan_channels_update ||    (mci->bt_state == MCI_BT_SLEEP))return;MCI_GPM_SET_TYPE_OPCODE(payload, MCI_GPM_COEX_AGENT,MCI_GPM_COEX_WLAN_CHANNELS);ar9003_mci_send_message(ah, MCI_GPM, 0, payload, 16, wait_done, true);MCI_GPM_SET_TYPE_OPCODE(payload, 0xff, 0xff);}static void ar9003_mci_send_coex_bt_status_query(struct ath_hw  ah,bool wait_done, u8 query_type){struct ath9k_hw_mci  mci = &ah->btcoex_hw.mci;u32 payload[4] = {0, 0, 0, 0};bool query_btinfo;if (mci->bt_state == MCI_BT_SLEEP)return;query_btinfo = !!(query_type & (MCI_GPM_COEX_QUERY_BT_ALL_INFO |MCI_GPM_COEX_QUERY_BT_TOPOLOGY));MCI_GPM_SET_TYPE_OPCODE(payload, MCI_GPM_COEX_AGENT,MCI_GPM_COEX_STATUS_QUERY); (((u8  )payload) + MCI_GPM_COEX_B_BT_BITMAP) = query_type;    If bt_status_query message is  not sent successfully,   then need_flush_btinfo should be set again. ", "REG_WRITE(ah, AR_BTCOEX_CTRL, 0x00);ar9003_mci_disable_interrupt(ah);}EXPORT_SYMBOL(ar9003_mci_cleanup": "ar9003_mci_cleanup(struct ath_hw  ah){  Turn off MCI and Jupiter mode. ", "mci->raw_intr = 0;mci->rx_msg_intr = 0;}EXPORT_SYMBOL(ar9003_mci_get_interrupt);void ar9003_mci_get_isr(struct ath_hw *ah, enum ath9k_int *masked)": "ar9003_mci_state(ah, MCI_STATE_ENABLE) &&    (mci->bt_state != MCI_BT_SLEEP) &&    !mci->halted_bt_gpm) {ar9003_mci_send_coex_halt_bt_gpm(ah, true, true);}mci->ready = false;}static void ar9003_mci_disable_interrupt(struct ath_hw  ah){REG_WRITE(ah, AR_MCI_INTERRUPT_EN, 0);REG_WRITE(ah, AR_MCI_INTERRUPT_RX_MSG_EN, 0);}static void ar9003_mci_enable_interrupt(struct ath_hw  ah){REG_WRITE(ah, AR_MCI_INTERRUPT_EN, AR_MCI_INTERRUPT_DEFAULT);REG_WRITE(ah, AR_MCI_INTERRUPT_RX_MSG_EN,  AR_MCI_INTERRUPT_RX_MSG_DEFAULT);}static bool ar9003_mci_check_int(struct ath_hw  ah, u32 ints){u32 intr;intr = REG_READ(ah, AR_MCI_INTERRUPT_RX_MSG_RAW);return ((intr & ints) == ints);}void ar9003_mci_get_interrupt(struct ath_hw  ah, u32  raw_intr,      u32  rx_msg_intr){struct ath9k_hw_mci  mci = &ah->btcoex_hw.mci; raw_intr = mci->raw_intr; rx_msg_intr = mci->rx_msg_intr;  Clean int bits after the values are read. ", "if ((gpm_type == MCI_GPM_BT_CAL_GRANT) &&    (recv_type == MCI_GPM_BT_CAL_REQ)) ": "ar9003_mci_get_next_gpm_offset(ah, &more_data);if (offset == MCI_GPM_INVALID)continue;p_gpm = (u32  ) (mci->gpm_buf + offset);recv_type = MCI_GPM_TYPE(p_gpm);recv_opcode = MCI_GPM_OPCODE(p_gpm);if (MCI_GPM_IS_CAL_TYPE(recv_type)) {if (recv_type == gpm_type) {if ((gpm_type == MCI_GPM_BT_CAL_DONE) &&    !b_is_bt_cal_done) {gpm_type = MCI_GPM_BT_CAL_GRANT;continue;}break;}} else if ((recv_type == gpm_type) &&   (recv_opcode == gpm_opcode))break;    check if it's cal_grant     When we're waiting for cal_grant in reset routine,   it's possible that BT sends out cal_request at the   same time. Since BT's calibration doesn't happen   that often, we'll let BT completes calibration then   we continue to wait for cal_grant from BT.   Orginal: Wait BT_CAL_GRANT.   New: Receive BT_CAL_REQ -> send WLAN_CAL_GRANT->wait   BT_CAL_DONE -> Wait BT_CAL_GRANT. ", "if (rs->rs_phyerr != ATH9K_PHYERR_RADAR &&    rs->rs_phyerr != ATH9K_PHYERR_FALSE_RADAR_EXT &&    rs->rs_phyerr != ATH9K_PHYERR_SPECTRAL)return 0;/* check if spectral scan bit is set. This does not have to be checked * if received through a SPECTRAL phy error, but shouldn't hurt. ": "ath_cmn_process_fft(struct ath_spec_scan_priv  spec_priv, struct ieee80211_hdr  hdr,    struct ath_rx_status  rs, u64 tsf){u8 sample_buf[SPECTRAL_SAMPLE_MAX_LEN] = {0};struct ath_hw  ah = spec_priv->ah;struct ath_common  common = ath9k_hw_common(spec_priv->ah);struct ath_softc  sc = (struct ath_softc  )common->priv;u8 num_bins,  vdata = (u8  )hdr;struct ath_radar_info  radar_info;int len = rs->rs_datalen;int i;int got_slen = 0;u8   sample_start;int sample_bytes = 0;int ret = 0;u16 fft_len, sample_len, freq = ah->curchan->chan->center_freq;enum nl80211_channel_type chan_type;ath_cmn_fft_idx_validator  fft_idx_validator;ath_cmn_fft_sample_handler  fft_handler;  AR9280 and before report via ATH9K_PHYERR_RADAR, AR93xx and newer   via ATH9K_PHYERR_SPECTRAL. Haven't seen ATH9K_PHYERR_FALSE_RADAR_EXT   yet, but this is supposed to be possible as well. ", "ath9k_cmn_spectral_scan_config(common, spec_priv, spec_priv->spectral_mode);ath9k_hw_ops(ah)->spectral_scan_trigger(ah);ath_ps_ops(common)->restore(common);}EXPORT_SYMBOL(ath9k_cmn_spectral_scan_trigger": "ath9k_cmn_spectral_scan_trigger(struct ath_common  common, struct ath_spec_scan_priv  spec_priv){struct ath_hw  ah = spec_priv->ah;u32 rxfilter;if (IS_ENABLED(CONFIG_ATH9K_TX99))return;if (!ath9k_hw_ops(ah)->spectral_scan_trigger) {ath_err(common, \"spectrum analyzer not implemented on this hardware\\n\");return;}if (!spec_priv->spec_config.enabled)return;ath_ps_ops(common)->wakeup(common);rxfilter = ath9k_hw_getrxfilter(ah);ath9k_hw_setrxfilter(ah, rxfilter | ATH9K_RX_FILTER_PHYRADAR | ATH9K_RX_FILTER_PHYERR);  TODO: usually this should not be neccesary, but for some reason   (or in some mode?) the trigger must be called after the   configuration, otherwise the register will have its values reset   (on my ar9220 to value 0x01002310) ", "spec_priv->spec_config.endless = 1;spec_priv->spec_config.enabled = 1;break;case SPECTRAL_CHANSCAN:case SPECTRAL_MANUAL:spec_priv->spec_config.endless = 0;spec_priv->spec_config.enabled = 1;break;default:return -1;}ath_ps_ops(common)->wakeup(common);ath9k_hw_ops(ah)->spectral_scan_config(ah, &spec_priv->spec_config);ath_ps_ops(common)->restore(common);spec_priv->spectral_mode = spectral_mode;return 0;}EXPORT_SYMBOL(ath9k_cmn_spectral_scan_config": "ath9k_cmn_spectral_scan_config(common, spec_priv, spec_priv->spectral_mode);ath9k_hw_ops(ah)->spectral_scan_trigger(ah);ath_ps_ops(common)->restore(common);}EXPORT_SYMBOL(ath9k_cmn_spectral_scan_trigger);int ath9k_cmn_spectral_scan_config(struct ath_common  common,       struct ath_spec_scan_priv  spec_priv,       enum spectral_mode spectral_mode){struct ath_hw  ah = spec_priv->ah;if (!ath9k_hw_ops(ah)->spectral_scan_trigger) {ath_err(common, \"spectrum analyzer not implemented on this hardware\\n\");return -1;}switch (spectral_mode) {case SPECTRAL_DISABLED:spec_priv->spec_config.enabled = 0;break;case SPECTRAL_BACKGROUND:  send endless samples.   TODO: is this really useful for \"background\"? ", "if (*shadow_cfg_len)return;/* shadow isn't configured yet, configure now. * non-CE srngs are configured firstly, then * all CE srngs. ": "ath11k_ce_get_shadow_config(struct ath11k_base  ab, u32   shadow_cfg, u32  shadow_cfg_len){if (!ab->hw_params.supports_shadow_regs)return;ath11k_hal_srng_get_shadow_config(ab, shadow_cfg, shadow_cfg_len);  shadow is already configured ", "ath11k_ce_poll_send_completed(ab, pipe_num);/* NOTE: Should we also clean up tx buffer in all pipes? ": "ath11k_ce_cleanup_pipes(struct ath11k_base  ab){struct ath11k_ce_pipe  pipe;int pipe_num;ath11k_ce_stop_shadow_timers(ab);for (pipe_num = 0; pipe_num < ab->hw_params.ce_count; pipe_num++) {pipe = &ab->ce.ce_pipe[pipe_num];ath11k_ce_rx_pipe_cleanup(pipe);  Cleanup any src CE's which have interrupts disabled ", "ath11k_ce_free_pipes(ab);return ret;}}return 0;}EXPORT_SYMBOL(ath11k_ce_alloc_pipes": "ath11k_ce_alloc_pipes(struct ath11k_base  ab){struct ath11k_ce_pipe  pipe;int i;int ret;const struct ce_attr  attr;spin_lock_init(&ab->ce.ce_lock);for (i = 0; i < ab->hw_params.ce_count; i++) {attr = &ab->hw_params.host_ce_config[i];pipe = &ab->ce.ce_pipe[i];pipe->pipe_num = i;pipe->ab = ab;pipe->buf_sz = attr->src_sz_max;ret = ath11k_ce_alloc_pipe(ab, i);if (ret) {  Free any partial successful allocation ", "pdev = ath11k_core_get_single_pdev(ab);ar = pdev->ar;if (!ar || ar->state != ATH11K_STATE_OFF)return 0;ret = ath11k_dp_rx_pktlog_stop(ab, true);if (ret) ": "ath11k_core_suspend(struct ath11k_base  ab){int ret;struct ath11k_pdev  pdev;struct ath11k  ar;if (!ab->hw_params.supports_suspend)return -EOPNOTSUPP;  so far single_pdev_only chips have supports_suspend as true   and only the first pdev is valid. ", "pdev = ath11k_core_get_single_pdev(ab);ar = pdev->ar;if (!ar || ar->state != ATH11K_STATE_OFF)return 0;ret = ath11k_hif_resume(ab);if (ret) ": "ath11k_core_resume(struct ath11k_base  ab){int ret;struct ath11k_pdev  pdev;struct ath11k  ar;if (!ab->hw_params.supports_suspend)return -EOPNOTSUPP;  so far signle_pdev_only chips have supports_suspend as true   and only the first pdev is valid. ", "while (buf_len > sizeof(struct ath11k_fw_ie)) ": "ath11k_core_free_bdf(struct ath11k_base  ab, struct ath11k_board_data  bd){if (!IS_ERR(bd->fw))release_firmware(bd->fw);memset(bd, 0, sizeof( bd));}static int ath11k_core_parse_bd_ie_board(struct ath11k_base  ab, struct ath11k_board_data  bd, const void  buf, size_t buf_len, const char  boardname, int ie_id, int name_id, int data_id){const struct ath11k_fw_ie  hdr;bool name_match_found;int ret, board_ie_id;size_t board_ie_len;const void  board_ie_data;name_match_found = false;  go through ATH11K_BD_IE_BOARD_ATH11K_BD_IE_REGDB_ elements ", "done:return tot_work_done;}EXPORT_SYMBOL(ath11k_dp_service_srng": "ath11k_dp_service_srng(struct ath11k_base  ab,   struct ath11k_ext_irq_grp  irq_grp,   int budget){struct napi_struct  napi = &irq_grp->napi;const struct ath11k_hw_hal_params  hal_params;int grp_id = irq_grp->grp_id;int work_done = 0;int i, j;int tot_work_done = 0;for (i = 0; i < ab->hw_params.max_tx_ring; i++) {if (BIT(ab->hw_params.hal_params->tcl2wbm_rbm_map[i].wbm_ring_num) &    ab->hw_params.ring_mask->tx[grp_id])ath11k_dp_tx_completion_handler(ab, i);}if (ab->hw_params.ring_mask->rx_err[grp_id]) {work_done = ath11k_dp_process_rx_err(ab, napi, budget);budget -= work_done;tot_work_done += work_done;if (budget <= 0)goto done;}if (ab->hw_params.ring_mask->rx_wbm_rel[grp_id]) {work_done = ath11k_dp_rx_process_wbm_err(ab, napi, budget);budget -= work_done;tot_work_done += work_done;if (budget <= 0)goto done;}if (ab->hw_params.ring_mask->rx[grp_id]) {i =  fls(ab->hw_params.ring_mask->rx[grp_id]) - 1;work_done = ath11k_dp_process_rx(ab, i, napi, budget);budget -= work_done;tot_work_done += work_done;if (budget <= 0)goto done;}if (ab->hw_params.ring_mask->rx_mon_status[grp_id]) {for (i = 0; i < ab->num_radios; i++) {for (j = 0; j < ab->hw_params.num_rxmda_per_pdev; j++) {int id = i   ab->hw_params.num_rxmda_per_pdev + j;if (ab->hw_params.ring_mask->rx_mon_status[grp_id] &BIT(id)) {work_done =ath11k_dp_rx_process_mon_rings(ab,       id,       napi, budget);budget -= work_done;tot_work_done += work_done;if (budget <= 0)goto done;}}}}if (ab->hw_params.ring_mask->reo_status[grp_id])ath11k_dp_process_reo_status(ab);for (i = 0; i < ab->num_radios; i++) {for (j = 0; j < ab->hw_params.num_rxmda_per_pdev; j++) {int id = i   ab->hw_params.num_rxmda_per_pdev + j;if (ab->hw_params.ring_mask->rxdma2host[grp_id] & BIT(id)) {work_done = ath11k_dp_process_rxdma_err(ab, id, budget);budget -= work_done;tot_work_done += work_done;}if (budget <= 0)goto done;if (ab->hw_params.ring_mask->host2rxdma[grp_id] & BIT(id)) {struct ath11k  ar = ath11k_ab_to_ar(ab, id);struct ath11k_pdev_dp  dp = &ar->dp;struct dp_rxdma_ring  rx_ring = &dp->rx_refill_buf_ring;hal_params = ab->hw_params.hal_params;ath11k_dp_rxbufs_replenish(ab, id, rx_ring, 0,   hal_params->rx_buf_rbm);}}}  TODO: Implement handler for other interrupts ", "}EXPORT_SYMBOL(ath11k_debugfs_soc_destroy": "ath11k_debugfs_soc_destroy(struct ath11k_base  ab){debugfs_remove_recursive(ab->debugfs_soc);ab->debugfs_soc = NULL;  We are not removing ath11k directory on purpose, even if it   would be empty. This simplifies the directory handling and it's   a minor cosmetic issue to leave an empty ath11k directory to   debugfs. ", "trace_ath11k_log_dbg_dump(ab, msg ? msg : \"\", prefix ? prefix : \"\",  buf, len);}EXPORT_SYMBOL(ath11k_dbg_dump": "ath11k_dbg_dump(struct ath11k_base  ab,     enum ath11k_debug_mask mask,     const char  msg, const char  prefix,     const void  buf, size_t len){char linebuf[256];size_t linebuflen;const void  ptr;if (ath11k_debug_mask & mask) {if (msg)__ath11k_dbg(ab, mask, \"%s\\n\", msg);for (ptr = buf; (ptr - buf) < len; ptr += 16) {linebuflen = 0;linebuflen += scnprintf(linebuf + linebuflen,sizeof(linebuf) - linebuflen,\"%s%08x: \",(prefix ? prefix : \"\"),(unsigned int)(ptr - buf));hex_dump_to_buffer(ptr, len - (ptr - buf), 16, 1,   linebuf + linebuflen,   sizeof(linebuf) - linebuflen, true);dev_printk(KERN_DEBUG, ab->dev, \"%s\\n\", linebuf);}}  tracing code doesn't like null strings ", "wakeup_required = test_bit(ATH11K_FLAG_DEVICE_INIT_DONE, &ab->dev_flags) &&  offset >= ATH11K_PCI_ACCESS_ALWAYS_OFF;if (wakeup_required && ab->pci.ops->wakeup)ret = ab->pci.ops->wakeup(ab);__ath11k_pcic_write32(ab, offset, value);if (wakeup_required && !ret && ab->pci.ops->release)ab->pci.ops->release(ab);}EXPORT_SYMBOL(ath11k_pcic_write32": "ath11k_pcic_write32(struct ath11k_base  ab, u32 offset, u32 value){if (offset < ATH11K_PCI_WINDOW_START)iowrite32(value, ab->mem  + offset);elseab->pci.ops->window_write32(ab, offset, value);}void ath11k_pcic_write32(struct ath11k_base  ab, u32 offset, u32 value){int ret = 0;bool wakeup_required;  for offset beyond BAR + 4K - 32, may   need to wakeup the device to access. ", "wakeup_required = test_bit(ATH11K_FLAG_DEVICE_INIT_DONE, &ab->dev_flags) &&  offset >= ATH11K_PCI_ACCESS_ALWAYS_OFF;if (wakeup_required && ab->pci.ops->wakeup)ret = ab->pci.ops->wakeup(ab);val = __ath11k_pcic_read32(ab, offset);if (wakeup_required && !ret && ab->pci.ops->release)ab->pci.ops->release(ab);return val;}EXPORT_SYMBOL(ath11k_pcic_read32": "ath11k_pcic_read32(struct ath11k_base  ab, u32 offset){u32 val;if (offset < ATH11K_PCI_WINDOW_START)val = ioread32(ab->mem + offset);elseval = ab->pci.ops->window_read32(ab, offset);return val;}u32 ath11k_pcic_read32(struct ath11k_base  ab, u32 offset){int ret = 0;u32 val;bool wakeup_required;  for offset beyond BAR + 4K - 32, may   need to wakeup the device to access. ", "wakeup_required = test_bit(ATH11K_FLAG_DEVICE_INIT_DONE, &ab->dev_flags) &&  offset >= ATH11K_PCI_ACCESS_ALWAYS_OFF;if (wakeup_required && ab->pci.ops->wakeup)ret = ab->pci.ops->wakeup(ab);val = __ath11k_pcic_read32(ab, offset);if (wakeup_required && !ret && ab->pci.ops->release)ab->pci.ops->release(ab);return val;}EXPORT_SYMBOL(ath11k_pcic_read": "ath11k_pcic_read32(struct ath11k_base  ab, u32 offset){u32 val;if (offset < ATH11K_PCI_WINDOW_START)val = ioread32(ab->mem + offset);elseval = ab->pci.ops->window_read32(ab, offset);return val;}u32 ath11k_pcic_read32(struct ath11k_base  ab, u32 offset){int ret = 0;u32 val;bool wakeup_required;  for offset beyond BAR + 4K - 32, may   need to wakeup the device to access. ", "if (!test_bit(ATH11K_FLAG_MULTI_MSI_VECTORS, &ab->dev_flags))return;for (i = 0; i < irq_grp->num_irq; i++)enable_irq(irq_grp->ab->irq_num[irq_grp->irqs[i]]);}void ath11k_pcic_ext_irq_enable(struct ath11k_base *ab)": "ath11k_pcic_ext_irq_disable(struct ath11k_base  sc){int i;clear_bit(ATH11K_FLAG_EXT_IRQ_ENABLED, &sc->dev_flags);for (i = 0; i < ATH11K_EXT_IRQ_GRP_NUM_MAX; i++) {struct ath11k_ext_irq_grp  irq_grp = &sc->ext_irq_grp[i];ath11k_pcic_ext_grp_disable(irq_grp);if (irq_grp->napi_enabled) {napi_synchronize(&irq_grp->napi);napi_disable(&irq_grp->napi);irq_grp->napi_enabled = false;}}}static void ath11k_pcic_ext_grp_enable(struct ath11k_ext_irq_grp  irq_grp){struct ath11k_base  ab = irq_grp->ab;int i;  In case of one MSI vector, we handle irq enabledisable in a   uniform way since we only have one irq ", "for (i = 0, msi_data_idx = 0; i < ab->hw_params.ce_count; i++) ": "ath11k_pcic_config_irq(struct ath11k_base  ab){struct ath11k_ce_pipe  ce_pipe;u32 msi_data_start;u32 msi_data_count, msi_data_idx;u32 msi_irq_start;unsigned int msi_data;int irq, i, ret, irq_idx;unsigned long irq_flags;ret = ath11k_pcic_get_user_msi_assignment(ab, \"CE\", &msi_data_count,  &msi_data_start, &msi_irq_start);if (ret)return ret;irq_flags = IRQF_SHARED;if (!test_bit(ATH11K_FLAG_MULTI_MSI_VECTORS, &ab->dev_flags))irq_flags |= IRQF_NOBALANCING;  Configure CE irqs ", "if (!pci_ops->get_msi_irq || !pci_ops->window_write32 ||    !pci_ops->window_read32)return -EINVAL;ab->pci.ops = pci_ops;return 0;}EXPORT_SYMBOL(ath11k_pcic_register_pci_ops": "ath11k_pcic_register_pci_ops(struct ath11k_base  ab, const struct ath11k_pci_ops  pci_ops){if (!pci_ops)return 0;  Return error if mandatory pci_ops callbacks are missing ", "write_index = CE_RING_IDX_INCR(nentries_mask, write_index);/* WORKAROUND ": "ath10k_ce_send_nolock(struct ath10k_ce_pipe  ce_state,  void  per_transfer_context,  dma_addr_t buffer,  unsigned int nbytes,  unsigned int transfer_id,  unsigned int flags){struct ath10k  ar = ce_state->ar;struct ath10k_ce_ring  src_ring = ce_state->src_ring;struct ce_desc  desc, sdesc;unsigned int nentries_mask = src_ring->nentries_mask;unsigned int sw_index = src_ring->sw_index;unsigned int write_index = src_ring->write_index;u32 ctrl_addr = ce_state->ctrl_addr;u32 desc_flags = 0;int ret = 0;if (nbytes > ce_state->src_sz_max)ath10k_warn(ar, \"%s: send more we can (nbytes: %d, max: %d)\\n\",    __func__, nbytes, ce_state->src_sz_max);if (unlikely(CE_RING_DELTA(nentries_mask,   write_index, sw_index - 1) <= 0)) {ret = -ENOSR;goto exit;}desc = CE_SRC_RING_TO_DESC(src_ring->base_addr_owner_space,   write_index);desc_flags |= SM(transfer_id, CE_DESC_FLAGS_META_DATA);if (flags & CE_SEND_FLAG_GATHER)desc_flags |= CE_DESC_FLAGS_GATHER;if (flags & CE_SEND_FLAG_BYTE_SWAP)desc_flags |= CE_DESC_FLAGS_BYTE_SWAP;sdesc.addr   = __cpu_to_le32(buffer);sdesc.nbytes = __cpu_to_le16(nbytes);sdesc.flags  = __cpu_to_le16(desc_flags); desc = sdesc;src_ring->per_transfer_context[write_index] = per_transfer_context;  Update Source Ring Write Index ", "if (WARN_ON_ONCE(src_ring->write_index == src_ring->sw_index))return;if (WARN_ON_ONCE(src_ring->write_index == ath10k_ce_src_ring_write_index_get(ar, ctrl_addr)))return;src_ring->write_index--;src_ring->write_index &= src_ring->nentries_mask;src_ring->per_transfer_context[src_ring->write_index] = NULL;}EXPORT_SYMBOL(__ath10k_ce_send_revert": "__ath10k_ce_send_revert(struct ath10k_ce_pipe  pipe){struct ath10k  ar = pipe->ar;struct ath10k_ce  ce = ath10k_ce_priv(ar);struct ath10k_ce_ring  src_ring = pipe->src_ring;u32 ctrl_addr = pipe->ctrl_addr;lockdep_assert_held(&ce->ce_lock);    This function must be called only if there is an incomplete   scatter-gather transfer (before index register is updated)   that needs to be cleaned up. ", "static int _ath10k_ce_send_nolock(struct ath10k_ce_pipe *ce_state,  void *per_transfer_context,  dma_addr_t buffer,  unsigned int nbytes,  unsigned int transfer_id,  unsigned int flags)": "ath10k_ce_send.   The caller takes responsibility for any needed locking. ", "if (((cur_write_idx + nentries) & nentries_mask) == dest_ring->sw_index)nentries -= 1;write_index = CE_RING_IDX_ADD(nentries_mask, write_index, nentries);ath10k_ce_dest_ring_write_index_set(ar, ctrl_addr, write_index);dest_ring->write_index = write_index;}EXPORT_SYMBOL(ath10k_ce_rx_update_write_idx": "ath10k_ce_rx_update_write_idx(struct ath10k_ce_pipe  pipe, u32 nentries){struct ath10k  ar = pipe->ar;struct ath10k_ce_ring  dest_ring = pipe->dest_ring;unsigned int nentries_mask = dest_ring->nentries_mask;unsigned int write_index = dest_ring->write_index;u32 ctrl_addr = pipe->ctrl_addr;u32 cur_write_idx = ath10k_ce_dest_ring_write_index_get(ar, ctrl_addr);  Prevent CE ring stuck issue that will occur when ring is full.   Make sure that write index is 1 less than read index. ", "if (((cur_write_idx + nentries) & nentries_mask) == dest_ring->sw_index)nentries -= 1;write_index = CE_RING_IDX_ADD(nentries_mask, write_index, nentries);ath10k_ce_dest_ring_write_index_set(ar, ctrl_addr, write_index);dest_ring->write_index = write_index;}EXPORT_SYMBOL(ath10k_ce_rx_update_write_idx);int ath10k_ce_rx_post_buf(struct ath10k_ce_pipe *pipe, void *ctx,  dma_addr_t paddr)": "ath10k_ce_rx_post_buf(struct ath10k_ce_pipe  pipe, void  ctx,   dma_addr_t paddr){struct ath10k  ar = pipe->ar;struct ath10k_ce  ce = ath10k_ce_priv(ar);struct ath10k_ce_ring  dest_ring = pipe->dest_ring;unsigned int nentries_mask = dest_ring->nentries_mask;unsigned int write_index = dest_ring->write_index;unsigned int sw_index = dest_ring->sw_index;struct ce_desc  base = dest_ring->base_addr_owner_space;struct ce_desc  desc = CE_DEST_RING_TO_DESC(base, write_index);u32 ctrl_addr = pipe->ctrl_addr;lockdep_assert_held(&ce->ce_lock);if ((pipe->id != 5) &&    CE_RING_DELTA(nentries_mask, write_index, sw_index - 1) == 0)return -ENOSPC;desc->addr = __cpu_to_le32(paddr);desc->nbytes = 0;dest_ring->per_transfer_context[write_index] = ctx;write_index = CE_RING_IDX_INCR(nentries_mask, write_index);ath10k_ce_dest_ring_write_index_set(ar, ctrl_addr, write_index);dest_ring->write_index = write_index;return 0;}static int __ath10k_ce_rx_post_buf_64(struct ath10k_ce_pipe  pipe,      void  ctx,      dma_addr_t paddr){struct ath10k  ar = pipe->ar;struct ath10k_ce  ce = ath10k_ce_priv(ar);struct ath10k_ce_ring  dest_ring = pipe->dest_ring;unsigned int nentries_mask = dest_ring->nentries_mask;unsigned int write_index = dest_ring->write_index;unsigned int sw_index = dest_ring->sw_index;struct ce_desc_64  base = dest_ring->base_addr_owner_space;struct ce_desc_64  desc =CE_DEST_RING_TO_DESC_64(base, write_index);u32 ctrl_addr = pipe->ctrl_addr;lockdep_assert_held(&ce->ce_lock);if (CE_RING_DELTA(nentries_mask, write_index, sw_index - 1) == 0)return -ENOSPC;desc->addr = __cpu_to_le64(paddr);desc->addr &= __cpu_to_le64(CE_DESC_ADDR_MASK);desc->nbytes = 0;dest_ring->per_transfer_context[write_index] = ctx;write_index = CE_RING_IDX_INCR(nentries_mask, write_index);ath10k_ce_dest_ring_write_index_set(ar, ctrl_addr, write_index);dest_ring->write_index = write_index;return 0;}void ath10k_ce_rx_update_write_idx(struct ath10k_ce_pipe  pipe, u32 nentries){struct ath10k  ar = pipe->ar;struct ath10k_ce_ring  dest_ring = pipe->dest_ring;unsigned int nentries_mask = dest_ring->nentries_mask;unsigned int write_index = dest_ring->write_index;u32 ctrl_addr = pipe->ctrl_addr;u32 cur_write_idx = ath10k_ce_dest_ring_write_index_get(ar, ctrl_addr);  Prevent CE ring stuck issue that will occur when ring is full.   Make sure that write index is 1 less than read index. ", "sdesc = *desc;nbytes = __le16_to_cpu(sdesc.nbytes);if (nbytes == 0) ": "ath10k_ce_completed_recv_next_nolock(struct ath10k_ce_pipe  ce_state,       void   per_transfer_contextp,       unsigned int  nbytesp){struct ath10k_ce_ring  dest_ring = ce_state->dest_ring;unsigned int nentries_mask = dest_ring->nentries_mask;unsigned int sw_index = dest_ring->sw_index;struct ce_desc  base = dest_ring->base_addr_owner_space;struct ce_desc  desc = CE_DEST_RING_TO_DESC(base, sw_index);struct ce_desc sdesc;u16 nbytes;  Copy in one go for performance reasons ", "static int _ath10k_ce_completed_recv_next_nolock(struct ath10k_ce_pipe *ce_state,       void **per_transfer_contextp,       unsigned int *nbytesp)": "ath10k_ce_completed_recv_next.   The caller takes responsibility for any necessary locking. ", "*bufferp = __le32_to_cpu(desc->addr);if (per_transfer_contextp)*per_transfer_contextp =dest_ring->per_transfer_context[sw_index];/* sanity ": "ath10k_ce_revoke_recv_next(struct ath10k_ce_pipe  ce_state,       void   per_transfer_contextp,       dma_addr_t  bufferp){struct ath10k_ce_ring  dest_ring;unsigned int nentries_mask;unsigned int sw_index;unsigned int write_index;int ret;struct ath10k  ar;struct ath10k_ce  ce;dest_ring = ce_state->dest_ring;if (!dest_ring)return -EIO;ar = ce_state->ar;ce = ath10k_ce_priv(ar);spin_lock_bh(&ce->ce_lock);nentries_mask = dest_ring->nentries_mask;sw_index = dest_ring->sw_index;write_index = dest_ring->write_index;if (write_index != sw_index) {struct ce_desc  base = dest_ring->base_addr_owner_space;struct ce_desc  desc = CE_DEST_RING_TO_DESC(base, sw_index);  Return data from completed destination descriptor ", "read_index = ath10k_ce_src_ring_read_index_get(ar, ctrl_addr);if (read_index == 0xffffffff)return -ENODEV;read_index &= nentries_mask;src_ring->hw_index = read_index;}if (ar->hw_params.rri_on_ddr)read_index = ath10k_ce_src_ring_read_index_get(ar, ctrl_addr);elseread_index = src_ring->hw_index;if (read_index == sw_index)return -EIO;if (per_transfer_contextp)*per_transfer_contextp =src_ring->per_transfer_context[sw_index];/* sanity ": "ath10k_ce_completed_send_next_nolock(struct ath10k_ce_pipe  ce_state, void   per_transfer_contextp){struct ath10k_ce_ring  src_ring = ce_state->src_ring;u32 ctrl_addr = ce_state->ctrl_addr;struct ath10k  ar = ce_state->ar;unsigned int nentries_mask = src_ring->nentries_mask;unsigned int sw_index = src_ring->sw_index;unsigned int read_index;struct ce_desc  desc;if (src_ring->hw_index == sw_index) {    The SW completion index has caught up with the cached   version of the HW completion index.   Update the cached HW completion index to see whether   the SW has really caught up to the HW, or if the cached   value of the HW index has become stale. ", "src_ring->per_transfer_context[sw_index] = NULL;/* Update sw_index ": "ath10k_ce_cancel_send_next(struct ath10k_ce_pipe  ce_state,       void   per_transfer_contextp,       dma_addr_t  bufferp,       unsigned int  nbytesp,       unsigned int  transfer_idp){struct ath10k_ce_ring  src_ring;unsigned int nentries_mask;unsigned int sw_index;unsigned int write_index;int ret;struct ath10k  ar;struct ath10k_ce  ce;src_ring = ce_state->src_ring;if (!src_ring)return -EIO;ar = ce_state->ar;ce = ath10k_ce_priv(ar);spin_lock_bh(&ce->ce_lock);nentries_mask = src_ring->nentries_mask;sw_index = src_ring->sw_index;write_index = src_ring->write_index;if (write_index != sw_index) {ce_state->ops->ce_extract_desc_data(ar, src_ring, sw_index,    bufferp, nbytesp,    transfer_idp);if (per_transfer_contextp) per_transfer_contextp =src_ring->per_transfer_context[sw_index];  sanity ", "static int _ath10k_ce_completed_send_next_nolock(struct ath10k_ce_pipe *ce_state, void **per_transfer_contextp)": "ath10k_ce_completed_send_next.   The caller takes responsibility for any necessary locking. ", "ath10k_ce_engine_int_status_clear(ar, ctrl_addr,  wm_regs->cc_mask | wm_regs->wm_mask);if (ce_state->recv_cb)ce_state->recv_cb(ce_state);if (ce_state->send_cb)ce_state->send_cb(ce_state);}EXPORT_SYMBOL(ath10k_ce_per_engine_service": "ath10k_ce_per_engine_service(struct ath10k  ar, unsigned int ce_id){struct ath10k_ce  ce = ath10k_ce_priv(ar);struct ath10k_ce_pipe  ce_state = &ce->ce_states[ce_id];struct ath10k_hw_ce_host_wm_regs  wm_regs = ar->hw_ce_regs->wm_regs;u32 ctrl_addr = ce_state->ctrl_addr;    Clear before handling     Misc CE interrupts are not being handled, but still need   to be cleared.     NOTE: When the last copy engine interrupt is cleared the   hardware will go to sleep.  Once this happens any access to   the CE registers can cause a hardware fault. ", "continue;ath10k_ce_per_engine_service(ar, ce_id);}}EXPORT_SYMBOL(ath10k_ce_per_engine_service_any": "ath10k_ce_per_engine_service_any(struct ath10k  ar){int ce_id;u32 intr_summary;intr_summary = ath10k_ce_interrupt_summary(ar);for (ce_id = 0; intr_summary && (ce_id < CE_COUNT); ce_id++) {if (intr_summary & (1 << ce_id))intr_summary &= ~(1 << ce_id);else  no intr pending on this CE ", "for (ce_id = 0; ce_id < CE_COUNT; ce_id++)ath10k_ce_enable_interrupt(ar, ce_id);}EXPORT_SYMBOL(ath10k_ce_enable_interrupts": "ath10k_ce_enable_interrupts(struct ath10k  ar){int ce_id;  Enable interrupts for copy engine that   are not using polling mode. ", "BUILD_BUG_ON(2 * TARGET_NUM_MSDU_DESC >     (CE_HTT_H2T_MSG_SRC_NENTRIES - 1));BUILD_BUG_ON(2 * TARGET_10_4_NUM_MSDU_DESC_PFC >     (CE_HTT_H2T_MSG_SRC_NENTRIES - 1));BUILD_BUG_ON(2 * TARGET_TLV_NUM_MSDU_DESC >     (CE_HTT_H2T_MSG_SRC_NENTRIES - 1));ce_state->ar = ar;ce_state->id = ce_id;ce_state->ctrl_addr = ath10k_ce_base_address(ar, ce_id);ce_state->attr_flags = attr->flags;ce_state->src_sz_max = attr->src_sz_max;if (attr->src_nentries)ce_state->send_cb = attr->send_cb;if (attr->dest_nentries)ce_state->recv_cb = attr->recv_cb;if (attr->src_nentries) ": "ath10k_ce_alloc_pipe(struct ath10k  ar, int ce_id, const struct ce_attr  attr){struct ath10k_ce  ce = ath10k_ce_priv(ar);struct ath10k_ce_pipe  ce_state = &ce->ce_states[ce_id];int ret;ath10k_ce_set_ops(ar, ce_state);  Make sure there's enough CE ringbuffer entries for HTT TX to avoid   additional TX locking checks.     For the lack of a better place do the check here. ", "if (ar->hw_params.delay_unmap_buffer &&    ep->ul_pipe_id == 3)mdelay(2);hdr = (struct ath10k_htc_hdr *)skb->data;ath10k_htc_restore_tx_skb(ep->htc, skb);if (!ep->ep_ops.ep_tx_complete) ": "ath10k_htc_notify_tx_completion(struct ath10k_htc_ep  ep,     struct sk_buff  skb){struct ath10k  ar = ep->htc->ar;struct ath10k_htc_hdr  hdr;ath10k_dbg(ar, ATH10K_DBG_HTC, \"%s: ep %d skb %pK\\n\", __func__,   ep->eid, skb);  A corner case where the copy completion is reaching to host but still   copy engine is processing it due to which host unmaps corresponding   memory and causes SMMU fault, hence as workaround adding delay   the unmapping memory to avoid SMMU faults. ", "}EXPORT_SYMBOL(ath10k_htc_tx_completion_handler": "ath10k_htc_tx_completion_handler(struct ath10k  ar, struct sk_buff  skb){struct ath10k_htc  htc = &ar->htc;struct ath10k_skb_cb  skb_cb;struct ath10k_htc_ep  ep;if (WARN_ON_ONCE(!skb))return;skb_cb = ATH10K_SKB_CB(skb);ep = &htc->endpoint[skb_cb->eid];ath10k_htc_notify_tx_completion(ep, skb);  the skb now belongs to the completion handler ", "ath10k_warn(ar, \"Invalid record length: %d\\n\",    record->hdr.len);status = -EINVAL;break;}switch (record->hdr.id) ": "ath10k_htc_process_trailer(struct ath10k_htc  htc,       u8  buffer,       int length,       enum ath10k_htc_ep_id src_eid,       void  next_lookaheads,       int  next_lookaheads_len){struct ath10k_htc_lookahead_bundle  bundle;struct ath10k  ar = htc->ar;int status = 0;struct ath10k_htc_record  record;u8  orig_buffer;int orig_length;size_t len;orig_buffer = buffer;orig_length = length;while (length > 0) {record = (struct ath10k_htc_record  )buffer;if (length < sizeof(record->hdr)) {status = -EINVAL;break;}if (record->hdr.len > length) {  no room left in buffer for record ", "trailer_present = hdr->flags & ATH10K_HTC_FLAG_TRAILER_PRESENT;if (trailer_present) ": "ath10k_htc_rx_completion_handler(struct ath10k  ar, struct sk_buff  skb){int status = 0;struct ath10k_htc  htc = &ar->htc;struct ath10k_htc_hdr  hdr;struct ath10k_htc_ep  ep;u16 payload_len;u32 trailer_len = 0;size_t min_len;u8 eid;bool trailer_present;hdr = (struct ath10k_htc_hdr  )skb->data;skb_pull(skb, sizeof( hdr));eid = hdr->eid;if (eid >= ATH10K_HTC_EP_COUNT) {ath10k_warn(ar, \"HTC Rx: invalid eid %d\\n\", eid);ath10k_dbg_dump(ar, ATH10K_DBG_HTC, \"htc bad header\", \"\",hdr, sizeof( hdr));goto out;}ep = &htc->endpoint[eid];if (ep->service_id == ATH10K_HTC_SVC_ID_UNUSED) {ath10k_warn(ar, \"htc rx endpoint %d is not connected\\n\", eid);goto out;}payload_len = __le16_to_cpu(hdr->len);if (payload_len + sizeof( hdr) > ATH10K_HTC_MAX_LEN) {ath10k_warn(ar, \"HTC rx frame too long, len: %zu\\n\",    payload_len + sizeof( hdr));ath10k_dbg_dump(ar, ATH10K_DBG_HTC, \"htc bad rx pkt len\", \"\",hdr, sizeof( hdr));goto out;}if (skb->len < payload_len) {ath10k_dbg(ar, ATH10K_DBG_HTC,   \"HTC Rx: insufficient length, got %d, expected %d\\n\",   skb->len, payload_len);ath10k_dbg_dump(ar, ATH10K_DBG_HTC, \"htc bad rx pkt len\",\"\", hdr, sizeof( hdr));goto out;}  get flags to check for trailer ", "ret = ath10k_core_create_board_name(ar, boardname,    sizeof(boardname), true,    true);if (ret) ": "ath10k_core_fetch_board_file(struct ath10k  ar, int bd_ie_type){char boardname[100], fallback_boardname1[100], fallback_boardname2[100];int ret;if (bd_ie_type == ATH10K_BD_IE_BOARD) {  With variant and chip id ", "if (ar->state != ATH10K_STATE_RESTARTING &&    ar->state != ATH10K_STATE_UTF)ath10k_wait_for_suspend(ar, WMI_PDEV_SUSPEND_AND_DISABLE_INTR);ath10k_hif_stop(ar);ath10k_htt_tx_stop(&ar->htt);ath10k_htt_rx_free(&ar->htt);ath10k_wmi_detach(ar);ar->id.bmi_ids_valid = false;}EXPORT_SYMBOL(ath10k_core_stop": "ath10k_core_stop(struct ath10k  ar){lockdep_assert_held(&ar->conf_mutex);ath10k_debug_stop(ar);  try to suspend target ", "set_bit(ATH10K_FLAG_PEER_STATS, &ar->dev_flags);status = ath10k_core_probe_fw(ar);if (status) ": "ath10k_core_register_work(struct work_struct  work){struct ath10k  ar = container_of(work, struct ath10k, register_work);int status;  peer stats are enabled by default ", "ath10k_spectral_destroy(ar);/* We must unregister from mac80211 before we stop HTC and HIF. * Otherwise we will fail to submit commands to FW and mac80211 will be * unhappy about callback failures. ": "ath10k_core_unregister(struct ath10k  ar){cancel_work_sync(&ar->register_work);if (!test_bit(ATH10K_FLAG_CORE_REGISTERED, &ar->dev_flags))return;ath10k_thermal_unregister(ar);  Stop spectral before unregistering from mac80211 to remove the   relayfs debugfs file cleanly. Otherwise the parent debugfs tree   would be already be free'd recursively, leading to a double free. ", "char variant[9 + ATH10K_SMBIOS_BDF_EXT_STR_LENGTH] = ": "ath10k_core_create_board_name(struct ath10k  ar, char  name, size_t name_len, bool with_variant, bool with_chip_id){  strlen(',variant=') + strlen(ar->id.bdf_ext) ", "trace_ath10k_log_dbg_dump(ar, msg ? msg : \"\", prefix ? prefix : \"\",  buf, len);}EXPORT_SYMBOL(ath10k_dbg_dump": "ath10k_dbg_dump(struct ath10k  ar,     enum ath10k_debug_mask mask,     const char  msg, const char  prefix,     const void  buf, size_t len){char linebuf[256];size_t linebuflen;const void  ptr;if (ath10k_debug_mask & mask) {if (msg)__ath10k_dbg(ar, mask, \"%s\\n\", msg);for (ptr = buf; (ptr - buf) < len; ptr += 16) {linebuflen = 0;linebuflen += scnprintf(linebuf + linebuflen,sizeof(linebuf) - linebuflen,\"%s%08x: \",(prefix ? prefix : \"\"),(unsigned int)(ptr - buf));hex_dump_to_buffer(ptr, len - (ptr - buf), 16, 1,   linebuf + linebuflen,   sizeof(linebuf) - linebuflen, true);dev_printk(KERN_DEBUG, ar->dev, \"%s\\n\", linebuf);}}  tracing code doesn't like null strings : ", "size += hw->region_table.size * sizeof(struct ath10k_dump_ram_data_hdr);/* make sure it is aligned 16 bytes for debug message print out ": "ath10k_coredump_get_mem_layout(ar);if (!hw)return 0;mem_region = &hw->region_table.regions[0];for (i = 0; i < hw->region_table.size; i++) {size += mem_region->len;mem_region++;}  reserve space for the headers ", "return NULL;guid_gen(&crash_data->guid);ktime_get_real_ts64(&crash_data->timestamp);return crash_data;}EXPORT_SYMBOL(ath10k_coredump_new": "ath10k_coredump_new(struct ath10k  ar){struct ath10k_fw_crash_data  crash_data = ar->coredump.fw_crash_data;lockdep_assert_held(&ar->dump_mutex);if (ath10k_coredump_mask == 0)  coredump disabled ", "if (release)dev_kfree_skb_any(skb);}static inline s8 ath10k_get_legacy_rate_idx(struct ath10k *ar, u8 rate)": "ath10k_htt_t2h_msg_handler(ar, skb);  Free the indication buffer ", "quota = ath10k_htt_rx_deliver_msdu(ar, quota, budget);if (quota == budget) ": "ath10k_htt_txrx_compl_task(struct ath10k  ar, int budget){struct ath10k_htt  htt = &ar->htt;struct htt_tx_done tx_done = {};struct sk_buff_head tx_ind_q;struct sk_buff  skb;unsigned long flags;int quota = 0, done, ret;bool resched_napi = false;__skb_queue_head_init(&tx_ind_q);  Process pending frames before dequeuing more data   from hardware. ", "if (!try_module_get(THIS_MODULE)) ": "iwl_trans_pcie_remove(struct iwl_trans  trans, bool rescan){struct iwl_trans_pcie_removal  removal;if (test_bit(STATUS_TRANS_DEAD, &trans->status))return;IWL_ERR(trans, \"Device gone - scheduling removal!\\n\");    get a module reference to avoid doing this   while unloading anyway and to avoid   scheduling a work with code that's being   removed. ", "voidil_update_stats(struct il_priv *il, bool is_tx, __le16 fc, u16 len)": "il_update_stats function record all the MGMT, CTRL and DATA pkt for   both TX and Rx . Use debugfs to display the rxrx_stats ", "int t = 0;do ": "il_poll_bit(struct il_priv  il, u32 addr, u32 bits, u32 mask, int timeout){const int interval = 10;   microseconds ", "BUG_ON(cmd->callback);D_INFO(\"Attempting to send sync command %s\\n\",       il_get_cmd_string(cmd->id));set_bit(S_HCMD_ACTIVE, &il->status);D_INFO(\"Setting HCMD_ACTIVE for command %s\\n\",       il_get_cmd_string(cmd->id));cmd_idx = il_enqueue_hcmd(il, cmd);if (cmd_idx < 0) ": "il_send_cmd_sync(struct il_priv  il, struct il_host_cmd  cmd){int cmd_idx;int ret;lockdep_assert_held(&il->mutex);BUG_ON(cmd->flags & CMD_ASYNC);  A synchronous command can not have a callback set. ", "BUG_ON(cmd->flags & CMD_WANT_SKB);/* Assign a generic callback if one is not provided ": "il_send_cmd_async(struct il_priv  il, struct il_host_cmd  cmd){int ret;BUG_ON(!(cmd->flags & CMD_ASYNC));  An asynchronous command can not expect an SKB to be set. ", "intil_eeprom_init(struct il_priv *il)": "il_eeprom_init - read EEPROM contents     Load the EEPROM contents from adapter into il->eeprom     NOTE:  This routine uses the non-debug IO access functions. ", "il_apm_stop(il);return ret;}EXPORT_SYMBOL(il_eeprom_init);voidil_eeprom_free(struct il_priv *il)": "il_eeprom_free(il);  Reset chip to save power until we load uCode during \"up\". ", "intil_init_channel_map(struct il_priv *il)": "il_get_channel_info(il, band, channel);if (!il_is_channel_valid(ch_info))return -1;D_EEPROM(\"HT40 Ch. %d [%sGHz] %s%s%s%s%s(0x%02x %ddBm):\" \" Ad-Hoc %ssupported\\n\", ch_info->channel, il_is_channel_a_band(ch_info) ? \"5.2\" : \"2.4\", CHECK_AND_PRINT(IBSS), CHECK_AND_PRINT(ACTIVE), CHECK_AND_PRINT(RADAR), CHECK_AND_PRINT(WIDE), CHECK_AND_PRINT(DFS), eeprom_ch->flags, eeprom_ch->max_power_avg, ((eeprom_ch->flags & EEPROM_CHANNEL_IBSS) &&  !(eeprom_ch->flags & EEPROM_CHANNEL_RADAR)) ? \"\" : \"not \");ch_info->ht40_eeprom =  eeprom_ch;ch_info->ht40_max_power_avg = eeprom_ch->max_power_avg;ch_info->ht40_flags = eeprom_ch->flags;if (eeprom_ch->flags & EEPROM_CHANNEL_VALID)ch_info->ht40_extension_channel &=    ~clear_ht40_extension_channel;return 0;}#define CHECK_AND_PRINT_I(x) ((eeprom_ch_info[ch].flags & EEPROM_CHANNEL_##x) \\    ? # x \" \" : \"\")    il_init_channel_map - Set up driver's info for all possible channels ", "voidil_free_channel_map(struct il_priv *il)": "il_free_channel_map - undo allocations in il_init_channel_map ", "intil_scan_cancel(struct il_priv *il)": "il_scan_cancel - Cancel any currently executing HW scan ", "intil_scan_cancel_timeout(struct il_priv *il, unsigned long ms)": "il_scan_cancel_timeout - Cancel any currently executing HW scan   @ms: amount of time to wait (in milliseconds) for scan to abort   ", "il->handlers[C_SCAN] = il_hdl_scan;il->handlers[N_SCAN_START] = il_hdl_scan_start;il->handlers[N_SCAN_RESULTS] = il_hdl_scan_results;il->handlers[N_SCAN_COMPLETE] = il_hdl_scan_complete;}EXPORT_SYMBOL(il_setup_rx_scan_handlers": "il_setup_rx_scan_handlers(struct il_priv  il){  scan handlers ", "value = il->vif ? il->vif->bss_conf.beacon_int : 0;if (value > IL_PASSIVE_DWELL_BASE || !value)value = IL_PASSIVE_DWELL_BASE;value = (value * 98) / 100 - IL_CHANNEL_TUNE_TIME * 2;passive = min(value, passive);}return passive;}EXPORT_SYMBOL(il_get_passive_dwell_time": "il_get_passive_dwell_time(struct il_priv  il, enum nl80211_band band,  struct ieee80211_vif  vif){u16 value;u16 passive =    (band ==     NL80211_BAND_2GHZ) ? IL_PASSIVE_DWELL_BASE +    IL_PASSIVE_DWELL_TIME_24 : IL_PASSIVE_DWELL_BASE +    IL_PASSIVE_DWELL_TIME_52;if (il_is_any_associated(il)) {    If we're associated, we clamp the maximum passive   dwell time to be 98% of the smallest beacon interval   (minus 2   channel tune time) ", "il->scan_request = req;il->scan_vif = vif;il->scan_band = req->channels[0]->band;ret = il_scan_initiate(il, vif);out_unlock:D_MAC80211(\"leave ret %d\\n\", ret);mutex_unlock(&il->mutex);return ret;}EXPORT_SYMBOL(il_mac_hw_scan": "il_mac_hw_scan(struct ieee80211_hw  hw, struct ieee80211_vif  vif,       struct ieee80211_scan_request  hw_req){struct cfg80211_scan_request  req = &hw_req->req;struct il_priv  il = hw->priv;int ret;if (req->n_channels == 0) {IL_ERR(\"Can not scan on no channels.\\n\");return -EINVAL;}mutex_lock(&il->mutex);D_MAC80211(\"enter\\n\");if (test_bit(S_SCANNING, &il->status)) {D_SCAN(\"Scan already in progress.\\n\");ret = -EAGAIN;goto out_unlock;}  mac80211 will only ask for one band at a time ", "u16il_fill_probe_req(struct il_priv *il, struct ieee80211_mgmt *frame,  const u8 *ta, const u8 *ies, int ie_len, int left)": "il_fill_probe_req - fill in all required fields and IE for probe request ", "intil_add_station_common(struct il_priv *il, const u8 *addr, bool is_ap,      struct ieee80211_sta *sta, u8 *sta_id_r)": "il_add_station_common - ", "voidil_clear_ucode_stations(struct il_priv *il)": "il_clear_ucode_stations - clear ucode station table bits     This function clears all the bits in the driver indicating   which stations are active in the ucode. Call when something   other than explicit station management would cause this in   the ucode, e.g. unassociated RXON. ", "voidil_restore_stations(struct il_priv *il)": "il_restore_stations() - Restore driver known stations to device     All stations considered active by driver, but not present in ucode, is   restored.     Function sleeps. ", "static boolil_is_lq_table_valid(struct il_priv *il, struct il_link_quality_cmd *lq)": "il_send_lq_cmd(il, &lq, CMD_SYNC, true);spin_lock_irqsave(&il->sta_lock, flags_spin);il->stations[i].used &= ~IL_STA_UCODE_INPROGRESS;}}spin_unlock_irqrestore(&il->sta_lock, flags_spin);if (!found)D_INFO(\"Restoring all known stations\"       \" .... no stations to be restored.\\n\");elseD_INFO(\"Restoring all known stations\" \" .... complete.\\n\");}EXPORT_SYMBOL(il_restore_stations);intil_get_free_ucode_key_idx(struct il_priv  il){int i;for (i = 0; i < il->sta_key_max_num; i++)if (!test_and_set_bit(i, &il->ucode_key_table))return i;return WEP_INVALID_OFFSET;}EXPORT_SYMBOL(il_get_free_ucode_key_idx);voidil_dealloc_bcast_stations(struct il_priv  il){unsigned long flags;int i;spin_lock_irqsave(&il->sta_lock, flags);for (i = 0; i < il->hw_params.max_stations; i++) {if (!(il->stations[i].used & IL_STA_BCAST))continue;il->stations[i].used &= ~IL_STA_UCODE_ACTIVE;il->num_stations--;BUG_ON(il->num_stations < 0);kfree(il->stations[i].lq);il->stations[i].lq = NULL;}spin_unlock_irqrestore(&il->sta_lock, flags);}EXPORT_SYMBOL_GPL(il_dealloc_bcast_stations);#ifdef CONFIG_IWLEGACY_DEBUGstatic voidil_dump_lq_cmd(struct il_priv  il, struct il_link_quality_cmd  lq){int i;D_RATE(\"lq station id 0x%x\\n\", lq->sta_id);D_RATE(\"lq ant 0x%X 0x%X\\n\", lq->general_params.single_stream_ant_msk,       lq->general_params.dual_stream_ant_msk);for (i = 0; i < LINK_QUAL_MAX_RETRY_NUM; i++)D_RATE(\"lq idx %d 0x%X\\n\", i, lq->rs_table[i].rate_n_flags);}#elsestatic inline voidil_dump_lq_cmd(struct il_priv  il, struct il_link_quality_cmd  lq){}#endif    il_is_lq_table_valid() - Test one aspect of LQ cmd for validity     It sometimes happens when a HT rate has been in use and we   loose connectivity with AP then mac80211 will first tell us that the   current channel is not HT anymore before removing the station. In such a   scenario the RXON flags will be updated to indicate we are not   communicating HT anymore, but the LQ command may still contain HT rates.   Test for this to prevent driver from sending LQ command between the time   RXON flags are updated and when LQ command is updated. ", "intil_rx_queue_space(const struct il_rx_queue *q)": "il_rx_queue_space - Return number of free slots available in queue. ", "voidil_rx_queue_update_write_ptr(struct il_priv *il, struct il_rx_queue *q)": "il_rx_queue_update_write_ptr - Update the write pointer for the RX queue ", "/* * il_rx_queue_space - Return number of free slots available in queue. ": "il_rx_queue_alloc()   Allocates rx_free   il_rx_replenish()     Replenishes rx_free list from rx_used, and calls                              il_rx_queue_restock   il_rx_queue_restock() Moves available buffers from rx_free into Rx                              queue, updates firmware pointers, and updates                              the WRITE idx.  If insufficient rx_free buffers                              are available, schedules il_rx_replenish     -- enable interrupts --   ISR - il_rx()         Detach il_rx_bufs from pool up to the                              READ IDX, detaching the SKB from the pool.                              Moves the packet buffer from queue to rx_used.                              Calls il_rx_queue_restock to refill any empty                              slots.   ...   ", "if (il->active.filter_flags & RXON_FILTER_DIS_DECRYPT_MSK)return 0;if (!(fc & IEEE80211_FCTL_PROTECTED))return 0;D_RX(\"decrypt_res:0x%x\\n\", decrypt_res);switch (decrypt_res & RX_RES_STATUS_SEC_TYPE_MSK) ": "il_set_decrypted_flag(struct il_priv  il, struct ieee80211_hdr  hdr,      u32 decrypt_res, struct ieee80211_rx_status  stats){u16 fc = le16_to_cpu(hdr->frame_control);    All contexts have the same setting here due to it being   a module parameter, so OK to check any context. ", "voidil_txq_update_write_ptr(struct il_priv *il, struct il_tx_queue *txq)": "il_txq_update_write_ptr - Send new write idx to hardware ", "voidil_tx_queue_unmap(struct il_priv *il, int txq_id)": "il_tx_queue_unmap -  Unmap any remaining DMA mappings and free skb's ", "voidil_tx_queue_free(struct il_priv *il, int txq_id)": "il_tx_queue_free - Deallocate DMA queue.   @txq: Transmit queue to deallocate.     Empty queue by removing and destroying all BD's.   Free all buffers.   0-fill, but do not free \"txq\" descriptor structure. ", "voidil_cmd_queue_unmap(struct il_priv *il)": "il_cmd_queue_unmap - Unmap any remaining DMA mappings from command queue ", "voidil_cmd_queue_free(struct il_priv *il)": "il_cmd_queue_free - Deallocate DMA queue.     Empty queue by removing and destroying all BD's.   Free all buffers.   0-fill, but do not free \"txq\" descriptor structure. ", "s -= 2;if (s < 0)s = 0;return s;}EXPORT_SYMBOL(il_queue_space": "il_queue_space(const struct il_queue  q){int s = q->read_ptr - q->write_ptr;if (q->read_ptr > q->write_ptr)s -= q->n_bd;if (s <= 0)s += q->n_win;  keep some reserve to not confuse empty and full situations ", "intil_tx_queue_init(struct il_priv *il, u32 txq_id)": "il_tx_queue_init - Allocate and initialize one txcmd queue ", "il_queue_init(il, &txq->q, slots, txq_id);/* Tell device where to find queue ": "il_tx_queue_reset(struct il_priv  il, u32 txq_id){int slots, actual_slots;struct il_tx_queue  txq = &il->txq[txq_id];if (txq_id == il->cmd_queue) {slots = TFD_CMD_SLOTS;actual_slots = TFD_CMD_SLOTS + 1;} else {slots = TFD_TX_CMD_SLOTS;actual_slots = TFD_TX_CMD_SLOTS;}memset(txq->meta, 0, sizeof(struct il_cmd_meta)   actual_slots);txq->need_update = 0;  Initialize queue's highlow-water marks, and headtail idxes ", "voidil_tx_cmd_complete(struct il_priv *il, struct il_rx_buf *rxb)": "il_tx_cmd_complete - Pull unused buffers off the queue and reclaim them   @rxb: Rx buffer to reclaim     If an Rx buffer has an async callback associated with it the callback   will be executed.  The attached skb (if present) will only be freed   if the callback returns 1 ", "intil_init_geos(struct il_priv *il)": "il_init_geos - Initialize mac80211's geochannel info based from eeprom ", "voidil_free_geos(struct il_priv *il)": "il_free_geos - undo allocations in il_init_geos ", "u8il_prep_station(struct il_priv *il, const u8 *addr, bool is_ap,struct ieee80211_sta *sta)": "il_is_ht40_tx_allowed(il, &sta->deflink.ht_cap))sta_flags |= STA_FLG_HT40_EN_MSK;elsesta_flags &= ~STA_FLG_HT40_EN_MSK;il->stations[idx].sta.station_flags = sta_flags;done:return;}    il_prep_station - Prepare station information for addition     should be called with sta_lock held ", "il->timing.atim_win = 0;beacon_int =    il_adjust_beacon_interval(beacon_int,      il->hw_params.max_beacon_itrvl *      TIME_UNIT);il->timing.beacon_interval = cpu_to_le16(beacon_int);tsf = il->timestamp;/* tsf is modifed by do_div: copy it ": "il_send_rxon_timing(struct il_priv  il){u64 tsf;s32 interval_tm, rem;struct ieee80211_conf  conf = NULL;u16 beacon_int;struct ieee80211_vif  vif = il->vif;conf = &il->hw->conf;lockdep_assert_held(&il->mutex);memset(&il->timing, 0, sizeof(struct il_rxon_time_cmd));il->timing.timestamp = cpu_to_le64(il->timestamp);il->timing.listen_interval = cpu_to_le16(conf->listen_interval);beacon_int = vif ? vif->bss_conf.beacon_int : 0;    TODO: For IBSS we need to get atim_win from mac80211,         for now just always use 0 ", "if ((rxon->ofdm_basic_rates & RATE_6M_MASK) == 0 &&    (rxon->cck_basic_rates & RATE_1M_MASK) == 0) ": "il_check_rxon_cmd(struct il_priv  il){struct il_rxon_cmd  rxon = &il->staging;bool error = false;if (rxon->flags & RXON_FLG_BAND_24G_MSK) {if (rxon->flags & RXON_FLG_TGJ_NARROW_BAND_MSK) {IL_WARN(\"check 2.4G: wrong narrow\\n\");error = true;}if (rxon->flags & RXON_FLG_RADAR_DETECT_MSK) {IL_WARN(\"check 2.4G: wrong radar\\n\");error = true;}} else {if (!(rxon->flags & RXON_FLG_SHORT_SLOT_MSK)) {IL_WARN(\"check 5.2G: not short slot!\\n\");error = true;}if (rxon->flags & RXON_FLG_CCK_MSK) {IL_WARN(\"check 5.2G: CCK!\\n\");error = true;}}if ((rxon->node_addr[0] | rxon->bssid_addr[0]) & 0x1) {IL_WARN(\"macbssid mcast!\\n\");error = true;}  make sure basic rates 6Mbps and 1Mbps are supported ", "intil_full_rxon_required(struct il_priv *il)": "il_full_rxon_required - check if full RXON (vs RXON_ASSOC) cmd is needed   @il: staging_rxon is compared to active_rxon     If the RXON structure is changing enough to require a new tune,   or is clearing the RXON_FILTER_ASSOC_MSK, then return 1 to indicate that   a new tune (full RXON command, rather than RXON_ASSOC cmd) is required. ", "if (il->staging.flags & RXON_FLG_BAND_24G_MSK)return RATE_1M_PLCP;elsereturn RATE_6M_PLCP;}EXPORT_SYMBOL(il_get_lowest_plcp": "il_get_lowest_plcp(struct il_priv  il){    Assign the lowest rate -- should really get this from   the beacon skb from mac80211. ", "/* clear the HT channel mode before set the mode ": "il_set_rxon_ht(struct il_priv  il, struct il_ht_config  ht_conf){struct il_rxon_cmd  rxon = &il->staging;if (!il->ht.enabled) {rxon->flags &=    ~(RXON_FLG_CHANNEL_MODE_MSK |      RXON_FLG_CTRL_CHANNEL_LOC_HI_MSK | RXON_FLG_HT40_PROT_MSK      | RXON_FLG_HT_PROT_MSK);return;}rxon->flags |=    cpu_to_le32(il->ht.protection << RXON_FLG_HT_OPERATING_MODE_POS);  Set up channel bandwidth:   20 MHz only, 2040 mixed or pure 40 if ht40 ok ", "intil_set_rxon_channel(struct il_priv *il, struct ieee80211_channel *ch)": "il_set_rxon_channel - Set the band and channel values in staging RXON   @ch: requested channel as a pointer to struct ieee80211_channel   NOTE:  Does not commit to the hardware; it sets appropriate bit fields   in the staging RXON flag structure based on the ch->band ", "if (vif && vif->bss_conf.use_short_slot)il->staging.flags |= RXON_FLG_SHORT_SLOT_MSK;elseil->staging.flags &= ~RXON_FLG_SHORT_SLOT_MSK;il->staging.flags |= RXON_FLG_BAND_24G_MSK;il->staging.flags |= RXON_FLG_AUTO_DETECT_MSK;il->staging.flags &= ~RXON_FLG_CCK_MSK;}}EXPORT_SYMBOL(il_set_flags_for_band": "il_set_flags_for_band(struct il_priv  il, enum nl80211_band band,      struct ieee80211_vif  vif){if (band == NL80211_BAND_5GHZ) {il->staging.flags &=    ~(RXON_FLG_BAND_24G_MSK | RXON_FLG_AUTO_DETECT_MSK |      RXON_FLG_CCK_MSK);il->staging.flags |= RXON_FLG_SHORT_SLOT_MSK;} else {  Copied from il_post_associate() ", "if (!hw_to_local(il->hw)->short_preamble)il->staging.flags &= ~RXON_FLG_SHORT_PREAMBLE_MSK;elseil->staging.flags |= RXON_FLG_SHORT_PREAMBLE_MSK;#endifch_info =    il_get_channel_info(il, il->band, le16_to_cpu(il->active.channel));if (!ch_info)ch_info = &il->channel_info[0];il->staging.channel = cpu_to_le16(ch_info->channel);il->band = ch_info->band;il_set_flags_for_band(il, il->band, il->vif);il->staging.ofdm_basic_rates =    (IL_OFDM_RATES_MASK >> IL_FIRST_OFDM_RATE) & 0xFF;il->staging.cck_basic_rates =    (IL_CCK_RATES_MASK >> IL_FIRST_CCK_RATE) & 0xF;/* clear both MIX and PURE40 mode flag ": "il_connection_init_rx_config(struct il_priv  il){const struct il_channel_info  ch_info;memset(&il->staging, 0, sizeof(il->staging));switch (il->iw_mode) {case NL80211_IFTYPE_UNSPECIFIED:il->staging.dev_type = RXON_DEV_TYPE_ESS;break;case NL80211_IFTYPE_STATION:il->staging.dev_type = RXON_DEV_TYPE_ESS;il->staging.filter_flags = RXON_FILTER_ACCEPT_GRP_MSK;break;case NL80211_IFTYPE_ADHOC:il->staging.dev_type = RXON_DEV_TYPE_IBSS;il->staging.flags = RXON_FLG_SHORT_PREAMBLE_MSK;il->staging.filter_flags =    RXON_FILTER_BCON_AWARE_MSK | RXON_FILTER_ACCEPT_GRP_MSK;break;default:IL_ERR(\"Unsupported interface type %d\\n\", il->vif->type);return;}#if 0  TODO:  Figure out when short_preamble would be set and cache from   that ", "voidil_irq_handle_error(struct il_priv *il)": "il_irq_handle_error - called for HW or SW error interrupt from card ", "_il_set_bit(il, CSR_RESET, CSR_RESET_REG_FLAG_STOP_MASTER);ret =    _il_poll_bit(il, CSR_RESET, CSR_RESET_REG_FLAG_MASTER_DISABLED, CSR_RESET_REG_FLAG_MASTER_DISABLED, 100);if (ret < 0)IL_WARN(\"Master Disable Timed Out, 100 usec\\n\");D_INFO(\"stop master\\n\");return ret;}void_il_apm_stop(struct il_priv *il)": "_il_apm_stop_master(struct il_priv  il){int ret = 0;  stop device's busmaster DMA activity ", "*eeprom_ch_count = ARRAY_SIZE(il_eeprom_band_1);*eeprom_ch_info =    (struct il_eeprom_channel *)il_eeprom_query_addr(il,     offset);*eeprom_ch_idx = il_eeprom_band_1;break;case 2:/* 4.9GHz band ": "il_apm_stop(il);return ret;}EXPORT_SYMBOL(il_eeprom_init);voidil_eeprom_free(struct il_priv  il){kfree(il->eeprom);il->eeprom = NULL;}EXPORT_SYMBOL(il_eeprom_free);static voidil_init_band_reference(const struct il_priv  il, int eep_band,       int  eeprom_ch_count,       const struct il_eeprom_channel   eeprom_ch_info,       const u8   eeprom_ch_idx){u32 offset = il->cfg->regulatory_bands[eep_band - 1];switch (eep_band) {case 1:  2.4GHz band ", "/* Disable L0S exit timer (platform NMI Work/Around) ": "il_apm_init(struct il_priv  il){int ret = 0;u16 lctl;D_INFO(\"Init card's basic functions\\n\");    Use \"set_bit\" below rather than \"write\", to preserve any hardware   bits already set by default after reset. ", "static voidil_sta_ucode_activate(struct il_priv *il, u8 sta_id)": "il_set_tx_power(il, il->tx_power_next, false);il->ops->post_scan(il);out:mutex_unlock(&il->mutex);}voidil_setup_scan_deferred_work(struct il_priv  il){INIT_WORK(&il->scan_completed, il_bg_scan_completed);INIT_WORK(&il->abort_scan, il_bg_abort_scan);INIT_DELAYED_WORK(&il->scan_check, il_bg_scan_check);}EXPORT_SYMBOL(il_setup_scan_deferred_work);voidil_cancel_scan_deferred_work(struct il_priv  il){cancel_work_sync(&il->abort_scan);cancel_work_sync(&il->scan_completed);if (cancel_delayed_work_sync(&il->scan_check)) {mutex_lock(&il->mutex);il_force_scan_end(il);mutex_unlock(&il->mutex);}}EXPORT_SYMBOL(il_cancel_scan_deferred_work);  il->sta_lock must be held ", "reset = (il->vif == vif);if (il->vif && !reset) ": "il_mac_add_interface(struct ieee80211_hw  hw, struct ieee80211_vif  vif){struct il_priv  il = hw->priv;int err;bool reset;mutex_lock(&il->mutex);D_MAC80211(\"enter: type %d, addr %pM\\n\", vif->type, vif->addr);if (!il_is_ready_rf(il)) {IL_WARN(\"Try to add interface when device not ready\\n\");err = -EINVAL;goto out;}    We do not support multiple virtual interfaces, but on hardware reset   we have to add the same interface again. ", "if (!external && !il->cfg->mod_params->restart_fw) ": "il_force_reset(struct il_priv  il, bool external){struct il_force_reset  force_reset;if (test_bit(S_EXIT_PENDING, &il->status))return -EINVAL;force_reset = &il->force_reset;force_reset->reset_request_count++;if (!external) {if (force_reset->last_force_reset_jiffies &&    time_after(force_reset->last_force_reset_jiffies +       force_reset->reset_duration, jiffies)) {D_INFO(\"force reset rejected\\n\");force_reset->reset_reject_count++;return -EAGAIN;}}force_reset->reset_success_count++;force_reset->last_force_reset_jiffies = jiffies;    if the request is from external(ex: debugfs),   then always perform the request in regardless the module   parameter setting   if the request is from internal (uCode error or driver   detect failure), then fw_restart module parameter   need to be check before performing firmware reload ", "err = -EBUSY;goto out;}/* success ": "il_mac_change_interface(struct ieee80211_hw  hw, struct ieee80211_vif  vif,enum nl80211_iftype newtype, bool newp2p){struct il_priv  il = hw->priv;int err;mutex_lock(&il->mutex);D_MAC80211(\"enter: type %d, addr %pM newtype %d newp2p %d\\n\",    vif->type, vif->addr, newtype, newp2p);if (newp2p) {err = -EOPNOTSUPP;goto out;}if (!il->vif || !il_is_ready_rf(il)) {    Huh? But wait ... this can maybe happen when   we're in the middle of a firmware restart! ", "if (il_check_stuck_queue(il, il->cmd_queue))return;/* monitor and check for other stuck queues ": "il_bg_watchdog(struct timer_list  t){struct il_priv  il = from_timer(il, t, watchdog);int cnt;unsigned long timeout;if (test_bit(S_EXIT_PENDING, &il->status))return;timeout = il->cfg->wd_timeout;if (timeout == 0)return;  monitor and check for stuck cmd queue ", "intil_mac_config(struct ieee80211_hw *hw, u32 changed)": "il_mac_config - mac80211 config callback ", "dev_consume_skb_irq(il->beacon_skb);il->beacon_skb = NULL;il->timestamp = 0;spin_unlock_irqrestore(&il->lock, flags);il_scan_cancel_timeout(il, 100);if (!il_is_ready_rf(il)) ": "il_mac_reset_tsf(struct ieee80211_hw  hw, struct ieee80211_vif  vif){struct il_priv  il = hw->priv;unsigned long flags;mutex_lock(&il->mutex);D_MAC80211(\"enter: type %d, addr %pM\\n\", vif->type, vif->addr);spin_lock_irqsave(&il->lock, flags);memset(&il->current_ht_config, 0, sizeof(struct il_ht_config));  new association get rid of ibss beacon skb ", "if (vif->bss_conf.enable_beacon)il->beacon_enabled = true;elseil->beacon_enabled = false;}if (changes & BSS_CHANGED_BSSID) ": "il_mac_bss_info_changed(struct ieee80211_hw  hw, struct ieee80211_vif  vif,struct ieee80211_bss_conf  bss_conf, u64 changes){struct il_priv  il = hw->priv;int ret;mutex_lock(&il->mutex);D_MAC80211(\"enter: changes 0x%llx\\n\", changes);if (!il_is_alive(il)) {D_MAC80211(\"leave - not alive\\n\");mutex_unlock(&il->mutex);return;}if (changes & BSS_CHANGED_QOS) {unsigned long flags;spin_lock_irqsave(&il->lock, flags);il->qos_data.qos_active = bss_conf->qos;il_update_qos(il);spin_unlock_irqrestore(&il->lock, flags);}if (changes & BSS_CHANGED_BEACON_ENABLED) {  FIXME: can we remove beacon_enabled ? ", "inta_mask = _il_rd(il, CSR_INT_MASK);/* just for debug ": "il_isr(int irq, void  data){struct il_priv  il = data;u32 inta, inta_mask;u32 inta_fh;unsigned long flags;if (!il)return IRQ_NONE;spin_lock_irqsave(&il->lock, flags);  Disable (but don't clear!) interrupts here to avoid      back-to-back ISRs and sporadic interrupts from our NIC.   If we have something to service, the tasklet will re-enable ints.   If we  don't  have something, we'll re-enable before leaving here. ", "voidil_tx_cmd_protection(struct il_priv *il, struct ieee80211_tx_info *info,     __le16 fc, __le32 *tx_flags)": "il_tx_cmd_protection: Set rtscts. 3945 and 4965 only share this    function. ", "if (idx != 0 && ext->alg != IW_ENCODE_ALG_WEP)return -EINVAL;if (ieee->iw_mode == IW_MODE_INFRA)crypt = &ieee->crypt_info.crypt[idx];elsereturn -EINVAL;}sec.flags |= SEC_ENABLED | SEC_ENCRYPT;if ((encoding->flags & IW_ENCODE_DISABLED) ||    ext->alg == IW_ENCODE_ALG_NONE) ": "libipw_wx_get_encode(struct libipw_device  ieee,    struct iw_request_info  info,    union iwreq_data  wrqu, char  keybuf){struct iw_point  erq = &(wrqu->encoding);int len, key;struct libipw_security  sec = &ieee->sec;LIBIPW_DEBUG_WX(\"GET_ENCODE\\n\");key = erq->flags & IW_ENCODE_INDEX;if (key) {if (key > WEP_KEYS)return -EINVAL;key--;} elsekey = ieee->crypt_info.tx_keyidx;erq->flags = key + 1;if (!sec->enabled) {erq->length = 0;erq->flags |= IW_ENCODE_DISABLED;return 0;}len = sec->key_sizes[key];memcpy(keybuf, sec->keys[key], len);erq->length = len;erq->flags |= IW_ENCODE_ENABLED;if (ieee->open_wep)erq->flags |= IW_ENCODE_OPEN;elseerq->flags |= IW_ENCODE_RESTRICTED;return 0;}int libipw_wx_set_encodeext(struct libipw_device  ieee,       struct iw_request_info  info,       union iwreq_data  wrqu, char  extra){struct net_device  dev = ieee->dev;struct iw_point  encoding = &wrqu->encoding;struct iw_encode_ext  ext = (struct iw_encode_ext  )extra;int i, idx, ret = 0;int group_key = 0;const char  alg,  module;struct lib80211_crypto_ops  ops;struct lib80211_crypt_data   crypt;struct libipw_security sec = {.flags = 0,};idx = encoding->flags & IW_ENCODE_INDEX;if (idx) {if (idx < 1 || idx > WEP_KEYS)return -EINVAL;idx--;} elseidx = ieee->crypt_info.tx_keyidx;if (ext->ext_flags & IW_ENCODE_EXT_GROUP_KEY) {crypt = &ieee->crypt_info.crypt[idx];group_key = 1;} else {  some Cisco APs use idx>0 for unicast in dynamic WEP ", ".duration_id = 0,.seq_ctl = 0,.qos_ctl = 0};u8 dest[ETH_ALEN], src[ETH_ALEN];struct lib80211_crypt_data *crypt;int priority = skb->priority;int snapped = 0;if (ieee->is_queue_full && (*ieee->is_queue_full) (dev, priority))return NETDEV_TX_BUSY;spin_lock_irqsave(&ieee->lock, flags);/* If there is no driver handler to take the TXB, dont' bother * creating it... ": "libipw_xmit(struct sk_buff  skb, struct net_device  dev){struct libipw_device  ieee = netdev_priv(dev);struct libipw_txb  txb = NULL;struct libipw_hdr_3addrqos  frag_hdr;int i, bytes_per_frag, nr_frags, bytes_last_frag, frag_size,    rts_required;unsigned long flags;int encrypt, host_encrypt, host_encrypt_msdu;__be16 ether_type;int bytes, fc, hdr_len;struct sk_buff  skb_frag;struct libipw_hdr_3addrqos header = {  Ensure zero initialized ", "netdev_tx_t libipw_xmit(struct sk_buff *skb, struct net_device *dev)": "libipw_txb_free(struct libipw_txb  txb){int i;if (unlikely(!txb))return;for (i = 0; i < txb->nr_frags; i++)if (txb->fragments[i])dev_kfree_skb_any(txb->fragments[i]);kfree(txb);}static struct libipw_txb  libipw_alloc_txb(int nr_frags, int txb_size, int headroom, gfp_t gfp_mask){struct libipw_txb  txb;int i;txb = kmalloc(struct_size(txb, fragments, nr_frags), gfp_mask);if (!txb)return NULL;memset(txb, 0, sizeof(struct libipw_txb));txb->nr_frags = nr_frags;txb->frag_size = txb_size;for (i = 0; i < nr_frags; i++) {txb->fragments[i] = __dev_alloc_skb(txb_size + headroom,    gfp_mask);if (unlikely(!txb->fragments[i])) {i--;break;}skb_reserve(txb->fragments[i], headroom);}if (unlikely(i != nr_frags)) {while (i >= 0)dev_kfree_skb_any(txb->fragments[i--]);kfree(txb);return NULL;}return txb;}static int libipw_classify(struct sk_buff  skb){struct ethhdr  eth;struct iphdr  ip;eth = (struct ethhdr  )skb->data;if (eth->h_proto != htons(ETH_P_IP))return 0;ip = ip_hdr(skb);switch (ip->tos & 0xfc) {case 0x20:return 2;case 0x40:return 1;case 0x60:return 3;case 0x80:return 4;case 0xa0:return 5;case 0xc0:return 6;case 0xe0:return 7;default:return 0;}}  Incoming skb is converted to a txb which consists of   a block of 802.11 fragment packets (stored as skbs) ", "if (ieee->geo.bg_channels == 0 && ieee->geo.a_channels == 0)return 0;freq /= 100000;if (ieee->freq_band & LIBIPW_24GHZ_BAND)for (i = 0; i < ieee->geo.bg_channels; i++)if (ieee->geo.bg[i].freq == freq)return ieee->geo.bg[i].channel;if (ieee->freq_band & LIBIPW_52GHZ_BAND)for (i = 0; i < ieee->geo.a_channels; i++)if (ieee->geo.a[i].freq == freq)return ieee->geo.a[i].channel;return 0;}void libipw_set_geo(struct libipw_device *ieee,      const struct libipw_geo *geo)": "libipw_freq_to_channel(struct libipw_device   ieee, u32 freq){int i;  Driver needs to initialize the geography map before using   these helper functions ", "if (ieee->geo.bg_channels == 0 && ieee->geo.a_channels == 0)return 0;if (ieee->freq_band & LIBIPW_24GHZ_BAND)for (i = 0; i < ieee->geo.bg_channels; i++)/* NOTE: If G mode is currently supported but * this is a B only channel, we don't see it * as valid. ": "libipw_is_valid_channel(struct libipw_device  ieee, u8 channel){int i;  Driver needs to initialize the geography map before using   these helper functions ", "if (ieee->geo.bg_channels == 0 && ieee->geo.a_channels == 0)return 0;ch = libipw_get_channel(ieee, channel);if (!ch->channel)return 0;return ch->freq;}u8 libipw_freq_to_channel(struct libipw_device * ieee, u32 freq)": "libipw_channel_to_freq(struct libipw_device   ieee, u8 channel){const struct libipw_channel   ch;  Driver needs to initialize the geography map before using   these helper functions ", "if (ieee->geo.bg_channels == 0 && ieee->geo.a_channels == 0)return -1;if (ieee->freq_band & LIBIPW_24GHZ_BAND)for (i = 0; i < ieee->geo.bg_channels; i++)if (ieee->geo.bg[i].channel == channel)return i;if (ieee->freq_band & LIBIPW_52GHZ_BAND)for (i = 0; i < ieee->geo.a_channels; i++)if (ieee->geo.a[i].channel == channel)return i;return -1;}u32 libipw_channel_to_freq(struct libipw_device * ieee, u8 channel)": "libipw_channel_to_index(struct libipw_device  ieee, u8 channel){int i;  Driver needs to initialize the geography map before using   these helper functions ", "ieee->wdev.wiphy->privid = libipw_wiphy_privid;ieee->wdev.wiphy->max_scan_ssids = 1;ieee->wdev.wiphy->max_scan_ie_len = 0;ieee->wdev.wiphy->interface_modes = BIT(NL80211_IFTYPE_STATION)| BIT(NL80211_IFTYPE_ADHOC);}err = libipw_networks_allocate(ieee);if (err) ": "alloc_libipw(int sizeof_priv, int monitor){struct libipw_device  ieee;struct net_device  dev;int err;LIBIPW_DEBUG_INFO(\"Initializing...\\n\");dev = alloc_etherdev(sizeof(struct libipw_device) + sizeof_priv);if (!dev)goto failed;ieee = netdev_priv(dev);ieee->dev = dev;if (!monitor) {ieee->wdev.wiphy = wiphy_new(&libipw_config_ops, 0);if (!ieee->wdev.wiphy) {LIBIPW_ERROR(\"Unable to allocate wiphy.\\n\");goto failed_free_netdev;}ieee->dev->ieee80211_ptr = &ieee->wdev;ieee->wdev.iftype = NL80211_IFTYPE_STATION;  Fill-out wiphy structure bits we know...  Not enough info   here to call set_wiphy_dev or set MAC address or channel info   -- have to do that in ->ndo_init... ", "if (!monitor)wiphy_free(ieee->wdev.wiphy);free_netdev(dev);}EXPORT_SYMBOL(free_libipw": "free_libipw(struct net_device  dev, int monitor){struct libipw_device  ieee = netdev_priv(dev);lib80211_crypt_info_free(&ieee->crypt_info);libipw_networks_free(ieee);  free cfg80211 resources ", "if (ether_addr_equal(hdr->addr3, ieee->bssid))if ((fc & (IEEE80211_FCTL_TODS+IEEE80211_FCTL_FROMDS)) == 0) ": "libipw_rx_mgt(ieee, hdr, stats);dev_kfree_skb_irq(skb);return;case IEEE80211_FTYPE_DATA:break;case IEEE80211_FTYPE_CTL:return;default:return;}is_packet_for_us = 0;switch (ieee->iw_mode) {case IW_MODE_ADHOC:  our BSS and not fromto DS ", "static struct libipw_frag_entry *libipw_frag_cache_find(struct      libipw_device      *ieee,      unsigned int seq,      unsigned int frag,      u8 * src,      u8 * dst)": "libipw_rx_stats  rx_stats){struct ieee80211_hdr  hdr = (struct ieee80211_hdr  )skb->data;u16 fc = le16_to_cpu(hdr->frame_control);skb->dev = ieee->dev;skb_reset_mac_header(skb);skb_pull(skb, libipw_get_hdrlen(fc));skb->pkt_type = PACKET_OTHERHOST;skb->protocol = htons(ETH_P_80211_RAW);memset(skb->cb, 0, sizeof(skb->cb));netif_rx(skb);}  Called only as a tasklet (software IRQ) ", "ret = wlcore_read(wl, le32_to_cpu(memmap->tx_result),  wl->tx_res_if, sizeof(*wl->tx_res_if), false);if (ret < 0)goto out;fw_counter = le32_to_cpu(wl->tx_res_if->tx_result_fw_counter);/* write host counter to chipset (to ack) ": "wlcore_tx_complete(struct wl1271  wl){struct wl1271_acx_mem_map  memmap = wl->target_mem_map;u32 count, fw_counter;u32 i;int ret;  read the tx results from the chipset ", "orinoco_lock_irq(priv);priv->open = 0;err = __orinoco_down(priv);orinoco_unlock_irq(priv);return err;}EXPORT_SYMBOL(orinoco_stop": "orinoco_stop(struct net_device  dev){struct orinoco_private  priv = ndev_priv(dev);int err = 0;  We mustn't use orinoco_lock() here, because we need to be   able to close the interface even if hw_unavailable is set   (e.g. as we're released after a PC Card removal) ", "/* Internal helper functions                                        ": "orinoco_down(struct orinoco_private  priv);static int __orinoco_commit(struct orinoco_private  priv);                                                                   ", "if ((new_mtu + ENCAPS_OVERHEAD + sizeof(struct ieee80211_hdr)) >     (priv->nicbuf_size - ETH_HLEN))return -EINVAL;dev->mtu = new_mtu;return 0;}EXPORT_SYMBOL(orinoco_change_mtu": "orinoco_change_mtu(struct net_device  dev, int new_mtu){struct orinoco_private  priv = ndev_priv(dev);  MTU + encapsulation + header length ", "if (ntohs(eh->h_proto) > ETH_DATA_LEN) ": "orinoco_process_xmit_skb(struct sk_buff  skb,     struct net_device  dev,     struct orinoco_private  priv,     int  tx_control,     u8  mic_buf){struct orinoco_tkip_key  key;struct ethhdr  eh;int do_mic;key = (struct orinoco_tkip_key  ) priv->keys[priv->tx_key].key;do_mic = ((priv->encode_alg == ORINOCO_ALG_TKIP) &&  (key != NULL));if (do_mic) tx_control |= (priv->tx_key << HERMES_MIC_KEY_ID_SHIFT) |HERMES_TXCTRL_MIC;eh = (struct ethhdr  )skb->data;  Encapsulate Ethernet-II frames ", "if (priv->iw_mode == NL80211_IFTYPE_MONITOR) ": "__orinoco_ev_rx(struct net_device  dev, struct hermes  hw){struct orinoco_private  priv = ndev_priv(dev);struct net_device_stats  stats = &dev->stats;struct iw_statistics  wstats = &priv->wstats;struct sk_buff  skb = NULL;u16 rxfid, status;int length;struct hermes_rx_descriptor  desc;struct orinoco_rx_data  rx_data;int err;desc = kmalloc(sizeof( desc), GFP_ATOMIC);if (!desc)goto update_stats;rxfid = hermes_read_regn(hw, RXFID);err = hw->ops->bap_pread(hw, IRQ_BAP, desc, sizeof( desc), rxfid, 0);if (err) {printk(KERN_ERR \"%s: error %d reading Rx descriptor. \"       \"Frame dropped.\\n\", dev->name, err);goto update_stats;}status = le16_to_cpu(desc->status);if (status & HERMES_RXSTAT_BADCRC) {DEBUG(1, \"%s: Bad CRC on Rx. Frame dropped.\\n\",      dev->name);stats->rx_crc_errors++;goto update_stats;}  Handle frames in monitor mode ", "infofid = hermes_read_regn(hw, INFOFID);/* Read the info frame header - don't try too hard ": "__orinoco_ev_info(struct net_device  dev, struct hermes  hw){struct orinoco_private  priv = ndev_priv(dev);u16 infofid;struct {__le16 len;__le16 type;} __packed info;int len, type;int err;  This is an answer to an INQUIRE command that we did earlier,   or an information \"event\" generated by the card   The controller return to us a pseudo frame containing   the information in question - Jean II ", "/* jiffies value the last time we were called ": "orinoco_interrupt(int irq, void  dev_id){struct orinoco_private  priv = dev_id;struct net_device  dev = priv->ndev;struct hermes  hw = &priv->hw;int count = MAX_IRQLOOPS_PER_IRQ;u16 evstat, events;  These are used to detect a runaway interrupt situation.     If we get more than MAX_IRQLOOPS_PER_JIFFY iterations in a jiffy,   we panic and shut down the hardware ", "priv->nicbuf_size = IEEE80211_MAX_FRAME_LEN + ETH_HLEN;/* Initialize the firmware ": "alloc_orinocodev() ", "wdev = netdev_priv(dev);wdev->wiphy = wiphy;wdev->iftype = NL80211_IFTYPE_STATION;/* Setup / override net_device fields ": "orinoco_if_add(struct orinoco_private  priv,   unsigned long base_addr,   unsigned int irq,   const struct net_device_ops  ops){struct wiphy  wiphy = priv_to_wiphy(priv);struct wireless_dev  wdev;struct net_device  dev;int ret;dev = alloc_etherdev(sizeof(struct wireless_dev));if (!dev)return -ENOMEM;  Initialise wireless_dev ", "tasklet_kill(&priv->rx_tasklet);/* Explicitly drain priv->rx_list ": "free_orinocodev(struct orinoco_private  priv){struct wiphy  wiphy = priv_to_wiphy(priv);struct orinoco_rx_data  rx_data,  temp;struct orinoco_scan_data  sd,  sdtemp;  If the tasklet is scheduled when we call tasklet_kill it   will run one final time. However the tasklet will only   drain priv->rx_list if the hw is still available. ", "INIT_WORK(&local->ap->add_sta_proc_queue, handle_add_proc_queue);ap->tx_callback_idx =hostap_tx_callback_register(local, hostap_ap_tx_cb, ap);if (ap->tx_callback_idx == 0)printk(KERN_WARNING \"%s: failed to register TX callback for \"       \"AP\\n\", local->dev->name);#ifndef PRISM2_NO_KERNEL_IEEE80211_MGMTINIT_WORK(&local->ap->wds_oper_queue, handle_wds_oper_queue);ap->tx_callback_auth =hostap_tx_callback_register(local, hostap_ap_tx_cb_auth, ap);ap->tx_callback_assoc =hostap_tx_callback_register(local, hostap_ap_tx_cb_assoc, ap);ap->tx_callback_poll =hostap_tx_callback_register(local, hostap_ap_tx_cb_poll, ap);if (ap->tx_callback_auth == 0 || ap->tx_callback_assoc == 0 ||ap->tx_callback_poll == 0)printk(KERN_WARNING \"%s: failed to register TX callback for \"       \"AP\\n\", local->dev->name);spin_lock_init(&ap->mac_restrictions.lock);INIT_LIST_HEAD(&ap->mac_restrictions.mac_list);#endif /* PRISM2_NO_KERNEL_IEEE80211_MGMT ": "hostap_init_data(local_info_t  local){struct ap_data  ap = local->ap;if (ap == NULL) {printk(KERN_WARNING \"hostap_init_data: ap == NULL\\n\");return;}memset(ap, 0, sizeof(struct ap_data));ap->local = local;ap->ap_policy = GET_INT_PARM(other_ap_policy, local->card_idx);ap->bridge_packets = GET_INT_PARM(ap_bridge_packets, local->card_idx);ap->max_inactivity =GET_INT_PARM(ap_max_inactivity, local->card_idx)   HZ;ap->autom_ap_wds = GET_INT_PARM(autom_ap_wds, local->card_idx);spin_lock_init(&ap->sta_table_lock);INIT_LIST_HEAD(&ap->sta_list);  Initialize task queue structure for AP management ", "#ifndef PRISM2_NO_KERNEL_IEEE80211_MGMTproc_create_seq_data(\"ap_control\", 0, ap->proc, &ap_control_proc_seqops,ap);proc_create_seq_data(\"ap\", 0, ap->proc, &prism2_ap_proc_seqops, ap);#endif /* PRISM2_NO_KERNEL_IEEE80211_MGMT ": "hostap_init_ap_proc(local_info_t  local){struct ap_data  ap = local->ap;ap->proc = local->proc;if (ap->proc == NULL)return;#ifndef PRISM2_NO_PROCFS_DEBUGproc_create_single_data(\"ap_debug\", 0, ap->proc, ap_debug_proc_show, ap);#endif   PRISM2_NO_PROCFS_DEBUG ", "list_for_each_entry_safe(sta, n, &ap->sta_list, list) ": "hostap_free_data(struct ap_data  ap){struct sta_info  n,  sta;if (ap == NULL || !ap->initialized) {printk(KERN_DEBUG \"hostap_free_data: ap has not yet been \"       \"initialized - skip resource freeing\\n\");return;}flush_work(&ap->add_sta_proc_queue);#ifndef PRISM2_NO_KERNEL_IEEE80211_MGMTflush_work(&ap->wds_oper_queue);if (ap->crypt)ap->crypt->deinit(ap->crypt_priv);ap->crypt = ap->crypt_priv = NULL;#endif   PRISM2_NO_KERNEL_IEEE80211_MGMT ", "static void hostap_ap_tx_cb(struct sk_buff *skb, int ok, void *data)": "hostap_check_sta_fw_version(struct ap_data  ap, int sta_fw_ver){if (!ap)return;if (sta_fw_ver == PRISM2_FW_VER(0,8,0)) {PDEBUG(DEBUG_AP, \"Using data::nullfunc ACK workaround - \"       \"firmware upgrade recommended\\n\");ap->nullfunc_ack = 1;} elseap->nullfunc_ack = 0;if (sta_fw_ver == PRISM2_FW_VER(1,4,2)) {printk(KERN_WARNING \"%s: Warning: secondary station firmware \"       \"version 1.4.2 does not seem to work in Host AP mode\\n\",       ap->local->dev->name);}}  Called only as a tasklet (software IRQ) ", "int old, rate;old = rate = sta->tx_rate_idx;while (rate > 0) ": "hostap_handle_sta_tx_exc(local_info_t  local, struct sk_buff  skb){struct sta_info  sta;struct ieee80211_hdr  hdr;struct hostap_skb_tx_data  meta;hdr = (struct ieee80211_hdr  ) skb->data;meta = (struct hostap_skb_tx_data  ) skb->cb;spin_lock(&local->ap->sta_table_lock);sta = ap_get_sta(local->ap, hdr->addr1);if (!sta) {spin_unlock(&local->ap->sta_table_lock);PDEBUG(DEBUG_AP, \"%s: Could not find STA %pM\"       \" for this TX error (@%lu)\\n\",       local->dev->name, hdr->addr1, jiffies);return;}sta->tx_since_last_failure = 0;sta->tx_consecutive_exc++;if (sta->tx_consecutive_exc >= WLAN_RATE_DECREASE_THRESHOLD &&    sta->tx_rate_idx > 0 && meta->rate <= sta->tx_rate) {  use next lower rate ", "proc_create_single_data(\"stats\", 0, local->proc, prism2_stats_proc_show,local);proc_create_seq_data(\"wds\", 0, local->proc,&prism2_wds_proc_seqops, local);proc_create_data(\"pda\", 0, local->proc, &prism2_pda_proc_ops, local);proc_create_data(\"aux_dump\", 0, local->proc, local->func->read_aux_proc_ops ?: &prism2_aux_dump_proc_ops, local);proc_create_seq_data(\"bss_list\", 0, local->proc,&prism2_bss_list_proc_seqops, local);proc_create_single_data(\"crypt\", 0, local->proc, prism2_crypt_proc_show,local);#ifdef PRISM2_IO_DEBUGproc_create_single_data(\"io_debug\", 0, local->proc,prism2_debug_proc_show, local);#endif /* PRISM2_IO_DEBUG ": "hostap_init_proc(local_info_t  local){local->proc = NULL;if (hostap_proc == NULL) {printk(KERN_WARNING \"%s: hostap proc directory not created\\n\",       local->dev->name);return;}local->proc = proc_mkdir(local->ddev->name, hostap_proc);if (local->proc == NULL) {printk(KERN_INFO \"procnethostap%s creation failed\\n\",       local->ddev->name);return;}#ifndef PRISM2_NO_PROCFS_DEBUGproc_create_single_data(\"debug\", 0, local->proc,prism2_debug_proc_show, local);#endif   PRISM2_NO_PROCFS_DEBUG ", "tx.crypt = local->crypt_info.crypt[local->crypt_info.tx_keyidx];tx.host_encrypt = 1;} else ": "hostap_master_start_xmit(struct sk_buff  skb,     struct net_device  dev){struct hostap_interface  iface;local_info_t  local;netdev_tx_t ret = NETDEV_TX_BUSY;u16 fc;struct hostap_tx_data tx;ap_tx_ret tx_ret;struct hostap_skb_tx_data  meta;int no_encrypt = 0;struct ieee80211_hdr  hdr;iface = netdev_priv(dev);local = iface->local;tx.skb = skb;tx.sta_ptr = NULL;meta = (struct hostap_skb_tx_data  ) skb->cb;if (meta->magic != HOSTAP_SKB_TX_DATA_MAGIC) {printk(KERN_DEBUG \"%s: invalid skb->cb magic (0x%08x, \"       \"expected 0x%08x)\\n\",       dev->name, meta->magic, HOSTAP_SKB_TX_DATA_MAGIC);ret = NETDEV_TX_OK;iface->stats.tx_dropped++;goto fail;}if (local->host_encrypt) {  Set crypt to default algorithm and key; will be replaced in   AP code if STA has own algkey ", "memcpy(buf + 2, val, len);return iface->local->func->set_rid(dev, rid, &buf, MAX_SSID_LEN + 2);}u16 hostap_get_porttype(local_info_t *local)": "hostap_set_string(struct net_device  dev, int rid, const char  val){struct hostap_interface  iface;char buf[MAX_SSID_LEN + 2];int len;iface = netdev_priv(dev);len = strlen(val);if (len > MAX_SSID_LEN)return -1;memset(buf, 0, sizeof(buf));buf[0] = len;   little endian 16 bit word ", "keylen = 6; /* first 5 octets ": "hostap_set_encryption(local_info_t  local){u16 val, old_val;int i, keylen, len, idx;char keybuf[WEP_KEY_LEN + 1];enum { NONE, WEP, OTHER } encrypt_type;idx = local->crypt_info.tx_keyidx;if (local->crypt_info.crypt[idx] == NULL ||    local->crypt_info.crypt[idx]->ops == NULL)encrypt_type = NONE;else if (strcmp(local->crypt_info.crypt[idx]->ops->name, \"WEP\") == 0)encrypt_type = WEP;elseencrypt_type = OTHER;if (local->func->get_rid(local->dev, HFA384X_RID_CNFWEPFLAGS, &val, 2, 1) < 0) {printk(KERN_DEBUG \"Could not read current WEP flags.\\n\");goto fail;}le16_to_cpus(&val);old_val = val;if (encrypt_type != NONE || local->privacy_invoked)val |= HFA384X_WEPFLAGS_PRIVACYINVOKED;elseval &= ~HFA384X_WEPFLAGS_PRIVACYINVOKED;if (local->open_wep || encrypt_type == NONE ||    ((local->ieee_802_1x || local->wpa) && local->host_decrypt))val &= ~HFA384X_WEPFLAGS_EXCLUDEUNENCRYPTED;elseval |= HFA384X_WEPFLAGS_EXCLUDEUNENCRYPTED;if ((encrypt_type != NONE || local->privacy_invoked) &&    (encrypt_type == OTHER || local->host_encrypt))val |= HFA384X_WEPFLAGS_HOSTENCRYPT;elseval &= ~HFA384X_WEPFLAGS_HOSTENCRYPT;if ((encrypt_type != NONE || local->privacy_invoked) &&    (encrypt_type == OTHER || local->host_decrypt))val |= HFA384X_WEPFLAGS_HOSTDECRYPT;elseval &= ~HFA384X_WEPFLAGS_HOSTDECRYPT;if (val != old_val &&    hostap_set_word(local->dev, HFA384X_RID_CNFWEPFLAGS, val)) {printk(KERN_DEBUG \"Could not write new WEP flags (0x%x)\\n\",       val);goto fail;}if (encrypt_type != WEP)return 0;  104-bit support seems to require that all the keys are set to the   same keylen ", "if (local->sta_fw_ver < PRISM2_FW_VER(0,7,0) &&    val != PRISM2_AUTH_OPEN && val != PRISM2_AUTH_SHARED_KEY)val = PRISM2_AUTH_OPEN;if (hostap_set_word(local->dev, HFA384X_RID_CNFAUTHENTICATION, val)) ": "hostap_set_auth_algs(local_info_t  local){int val = local->auth_algs;  At least STA fw v0.6.2 seems to have issues with cnfAuthentication   set to include both Open and Shared Key flags. It tries to use   Shared Key authentication in that case even if WEP keys are not   configured.. STA fw v0.7.6 is able to handle such configuration,   but it is unknown when this was fixed between 0.6.2 .. 0.7.6. ", "return ETH_ALEN;}int hostap_80211_get_hdrlen(__le16 fc)": "hostap_dump_tx_header(const char  name, const struct hfa384x_tx_frame  tx){u16 fc;printk(KERN_DEBUG \"%s: TX status=0x%04x retry_count=%d tx_rate=%d \"       \"tx_control=0x%04x; jiffies=%ld\\n\",       name, __le16_to_cpu(tx->status), tx->retry_count, tx->tx_rate,       __le16_to_cpu(tx->tx_control), jiffies);fc = __le16_to_cpu(tx->frame_control);printk(KERN_DEBUG \"   FC=0x%04x (type=%d:%d) dur=0x%04x seq=0x%04x \"       \"data_len=%d%s%s\\n\",       fc, (fc & IEEE80211_FCTL_FTYPE) >> 2,       (fc & IEEE80211_FCTL_STYPE) >> 4,       __le16_to_cpu(tx->duration_id), __le16_to_cpu(tx->seq_ctrl),       __le16_to_cpu(tx->data_len),       fc & IEEE80211_FCTL_TODS ? \" [ToDS]\" : \"\",       fc & IEEE80211_FCTL_FROMDS ? \" [FromDS]\" : \"\");printk(KERN_DEBUG \"   A1=%pM A2=%pM A3=%pM A4=%pM\\n\",       tx->addr1, tx->addr2, tx->addr3, tx->addr4);printk(KERN_DEBUG \"   dst=%pM src=%pM len=%d\\n\",       tx->dst_addr, tx->src_addr,       __be16_to_cpu(tx->len));}static int hostap_80211_header_parse(const struct sk_buff  skb,     unsigned char  haddr){memcpy(haddr, skb_mac_header(skb) + 10, ETH_ALEN);   addr2 ", "else if (ieee80211_is_cts(fc) || ieee80211_is_ack(fc))return 10;else if (ieee80211_is_ctl(fc))return 16;return 24;}static int prism2_close(struct net_device *dev)": "hostap_80211_get_hdrlen(__le16 fc){if (ieee80211_is_data(fc) && ieee80211_has_a4 (fc))return 30;   Addr4 ", "}static inline int prism2_wds_special_addr(u8 *addr)": "hostap_remove_interface(struct net_device  dev, int rtnl_locked,     int remove_from_list){struct hostap_interface  iface;if (!dev)return;iface = netdev_priv(dev);if (remove_from_list) {list_del(&iface->list);}if (dev == iface->local->ddev)iface->local->ddev = NULL;else if (dev == iface->local->apdev)iface->local->apdev = NULL;else if (dev == iface->local->stadev)iface->local->stadev = NULL;if (rtnl_locked)unregister_netdevice(dev);elseunregister_netdev(dev);  'dev->needs_free_netdev = true' implies device data, including   private data, will be freed when the device is removed ", "struct hostap_interface *iface;local_info_t *local;iface = netdev_priv(dev);local = iface->local;if ((dev->flags & IFF_ALLMULTI) || (dev->flags & IFF_PROMISC)) ": "hostap_set_multicast_list_queue(struct work_struct  work){local_info_t  local =container_of(work, local_info_t, set_multicast_list_queue);struct net_device  dev = local->dev;if (hostap_set_word(dev, HFA384X_RID_PROMISCUOUSMODE,    local->is_promisc)) {printk(KERN_INFO \"%s: %sabling promiscuous mode failed\\n\",       dev->name, local->is_promisc ? \"en\" : \"dis\");}}static void hostap_set_multicast_list(struct net_device  dev){#if 0  FIX: promiscuous mode seems to be causing a lot of problems with   some station firmware versions (FCSErr frames, invalid MACPort, etc.   corrupted incoming frames). This code is now commented out while the   problems are investigated. ", "int prism2_rx_80211(struct net_device *dev, struct sk_buff *skb,    struct hostap_80211_rx_status *rx_stats, int type)": "hostap_80211_rx_status  rx_stats){struct ieee80211_hdr  hdr;u16 fc;hdr = (struct ieee80211_hdr  ) skb->data;printk(KERN_DEBUG \"%s: RX signal=%d noise=%d rate=%d len=%d \"       \"jiffies=%ld\\n\",       name, rx_stats->signal, rx_stats->noise, rx_stats->rate,       skb->len, jiffies);if (skb->len < 2)return;fc = le16_to_cpu(hdr->frame_control);printk(KERN_DEBUG \"   FC=0x%04x (type=%d:%d)%s%s\",       fc, (fc & IEEE80211_FCTL_FTYPE) >> 2,       (fc & IEEE80211_FCTL_STYPE) >> 4,       fc & IEEE80211_FCTL_TODS ? \" [ToDS]\" : \"\",       fc & IEEE80211_FCTL_FROMDS ? \" [FromDS]\" : \"\");if (skb->len < IEEE80211_DATA_HDR3_LEN) {printk(\"\\n\");return;}printk(\" dur=0x%04x seq=0x%04x\\n\", le16_to_cpu(hdr->duration_id),       le16_to_cpu(hdr->seq_ctrl));printk(KERN_DEBUG \"   A1=%pM\", hdr->addr1);printk(\" A2=%pM\", hdr->addr2);printk(\" A3=%pM\", hdr->addr3);if (skb->len >= 30)printk(\" A4=%pM\", hdr->addr4);printk(\"\\n\");}  Send RX frame to netif with 802.11 (and possible prism) header.   Called from hardware or software IRQ context. ", "}EXPORT_SYMBOL(hostap_info_init": "hostap_info_init(local_info_t  local){skb_queue_head_init(&local->info_list);#ifndef PRISM2_NO_STATION_MODESINIT_WORK(&local->info_queue, handle_info_queue);#endif   PRISM2_NO_STATION_MODES ", "info = (struct hfa384x_info_frame *) skb->data;buf = skb->data + sizeof(*info);left = skb->len - sizeof(*info);switch (le16_to_cpu(info->type)) ": "hostap_info_process(local_info_t  local, struct sk_buff  skb){struct hfa384x_info_frame  info;unsigned char  buf;int left;#ifndef PRISM2_NO_DEBUGint i;#endif   PRISM2_NO_DEBUG ", "spin_lock_bh(&priv->timerlock);/* then stop the hardware ISR ": "atmel_open (struct net_device  dev);static inline u16 atmel_hi(struct atmel_private  priv, u16 offset){return priv->host_info_base + offset;}static inline u16 atmel_co(struct atmel_private  priv, u16 offset){return priv->host_info.command_pos + offset;}static inline u16 atmel_rx(struct atmel_private  priv, u16 offset, u16 desc){return priv->host_info.rx_desc_pos + (sizeof(struct rx_desc)   desc) + offset;}static inline u16 atmel_tx(struct atmel_private  priv, u16 offset, u16 desc){return priv->host_info.tx_desc_pos + (sizeof(struct tx_desc)   desc) + offset;}static inline u8 atmel_read8(struct net_device  dev, u16 offset){return inb(dev->base_addr + offset);}static inline void atmel_write8(struct net_device  dev, u16 offset, u8 data){outb(data, dev->base_addr + offset);}static inline u16 atmel_read16(struct net_device  dev, u16 offset){return inw(dev->base_addr + offset);}static inline void atmel_write16(struct net_device  dev, u16 offset, u16 data){outw(data, dev->base_addr + offset);}static inline u8 atmel_rmem8(struct atmel_private  priv, u16 pos){atmel_writeAR(priv->dev, pos);return atmel_read8(priv->dev, DR);}static inline void atmel_wmem8(struct atmel_private  priv, u16 pos, u16 data){atmel_writeAR(priv->dev, pos);atmel_write8(priv->dev, DR, data);}static inline u16 atmel_rmem16(struct atmel_private  priv, u16 pos){atmel_writeAR(priv->dev, pos);return atmel_read16(priv->dev, DR);}static inline void atmel_wmem16(struct atmel_private  priv, u16 pos, u16 data){atmel_writeAR(priv->dev, pos);atmel_write16(priv->dev, DR, data);}static const struct iw_handler_def atmel_handler_def;static void tx_done_irq(struct atmel_private  priv){int i;for (i = 0;     atmel_rmem8(priv, atmel_tx(priv, TX_DESC_FLAGS_OFFSET, priv->tx_desc_head)) == TX_DONE &&     i < priv->host_info.tx_desc_count;     i++) {u8 status = atmel_rmem8(priv, atmel_tx(priv, TX_DESC_STATUS_OFFSET, priv->tx_desc_head));u16 msdu_size = atmel_rmem16(priv, atmel_tx(priv, TX_DESC_SIZE_OFFSET, priv->tx_desc_head));u8 type = atmel_rmem8(priv, atmel_tx(priv, TX_DESC_PACKET_TYPE_OFFSET, priv->tx_desc_head));atmel_wmem8(priv, atmel_tx(priv, TX_DESC_FLAGS_OFFSET, priv->tx_desc_head), 0);priv->tx_free_mem += msdu_size;priv->tx_desc_free++;if (priv->tx_buff_head + msdu_size > (priv->host_info.tx_buff_pos + priv->host_info.tx_buff_size))priv->tx_buff_head = 0;elsepriv->tx_buff_head += msdu_size;if (priv->tx_desc_head < (priv->host_info.tx_desc_count - 1))priv->tx_desc_head++ ;elsepriv->tx_desc_head = 0;if (type == TX_PACKET_TYPE_DATA) {if (status == TX_STATUS_SUCCESS)priv->dev->stats.tx_packets++;elsepriv->dev->stats.tx_errors++;netif_wake_queue(priv->dev);}}}static u16 find_tx_buff(struct atmel_private  priv, u16 len){u16 bottom_free = priv->host_info.tx_buff_size - priv->tx_buff_tail;if (priv->tx_desc_free == 3 || priv->tx_free_mem < len)return 0;if (bottom_free >= len)return priv->host_info.tx_buff_pos + priv->tx_buff_tail;if (priv->tx_free_mem - bottom_free >= len) {priv->tx_buff_tail = 0;return priv->host_info.tx_buff_pos;}return 0;}static void tx_update_descriptor(struct atmel_private  priv, int is_bcast, u16 len, u16 buff, u8 type){atmel_wmem16(priv, atmel_tx(priv, TX_DESC_POS_OFFSET, priv->tx_desc_tail), buff);atmel_wmem16(priv, atmel_tx(priv, TX_DESC_SIZE_OFFSET, priv->tx_desc_tail), len);if (!priv->use_wpa)atmel_wmem16(priv, atmel_tx(priv, TX_DESC_HOST_LENGTH_OFFSET, priv->tx_desc_tail), len);atmel_wmem8(priv, atmel_tx(priv, TX_DESC_PACKET_TYPE_OFFSET, priv->tx_desc_tail), type);atmel_wmem8(priv, atmel_tx(priv, TX_DESC_RATE_OFFSET, priv->tx_desc_tail), priv->tx_rate);atmel_wmem8(priv, atmel_tx(priv, TX_DESC_RETRY_OFFSET, priv->tx_desc_tail), 0);if (priv->use_wpa) {int cipher_type, cipher_length;if (is_bcast) {cipher_type = priv->group_cipher_suite;if (cipher_type == CIPHER_SUITE_WEP_64 ||    cipher_type == CIPHER_SUITE_WEP_128)cipher_length = 8;else if (cipher_type == CIPHER_SUITE_TKIP)cipher_length = 12;else if (priv->pairwise_cipher_suite == CIPHER_SUITE_WEP_64 || priv->pairwise_cipher_suite == CIPHER_SUITE_WEP_128) {cipher_type = priv->pairwise_cipher_suite;cipher_length = 8;} else {cipher_type = CIPHER_SUITE_NONE;cipher_length = 0;}} else {cipher_type = priv->pairwise_cipher_suite;if (cipher_type == CIPHER_SUITE_WEP_64 ||    cipher_type == CIPHER_SUITE_WEP_128)cipher_length = 8;else if (cipher_type == CIPHER_SUITE_TKIP)cipher_length = 12;else if (priv->group_cipher_suite == CIPHER_SUITE_WEP_64 || priv->group_cipher_suite == CIPHER_SUITE_WEP_128) {cipher_type = priv->group_cipher_suite;cipher_length = 8;} else {cipher_type = CIPHER_SUITE_NONE;cipher_length = 0;}}atmel_wmem8(priv, atmel_tx(priv, TX_DESC_CIPHER_TYPE_OFFSET, priv->tx_desc_tail),    cipher_type);atmel_wmem8(priv, atmel_tx(priv, TX_DESC_CIPHER_LENGTH_OFFSET, priv->tx_desc_tail),    cipher_length);}atmel_wmem32(priv, atmel_tx(priv, TX_DESC_NEXT_OFFSET, priv->tx_desc_tail), 0x80000000L);atmel_wmem8(priv, atmel_tx(priv, TX_DESC_FLAGS_OFFSET, priv->tx_desc_tail), TX_FIRM_OWN);if (priv->tx_desc_previous != priv->tx_desc_tail)atmel_wmem32(priv, atmel_tx(priv, TX_DESC_NEXT_OFFSET, priv->tx_desc_previous), 0);priv->tx_desc_previous = priv->tx_desc_tail;if (priv->tx_desc_tail < (priv->host_info.tx_desc_count - 1))priv->tx_desc_tail++;elsepriv->tx_desc_tail = 0;priv->tx_desc_free--;priv->tx_free_mem -= len;}static netdev_tx_t start_tx(struct sk_buff  skb, struct net_device  dev){struct atmel_private  priv = netdev_priv(dev);struct ieee80211_hdr header;unsigned long flags;u16 buff, frame_ctl, len = (ETH_ZLEN < skb->len) ? skb->len : ETH_ZLEN;if (priv->card && priv->present_callback &&    !( priv->present_callback)(priv->card)) {dev->stats.tx_errors++;dev_kfree_skb(skb);return NETDEV_TX_OK;}if (priv->station_state != STATION_STATE_READY) {dev->stats.tx_errors++;dev_kfree_skb(skb);return NETDEV_TX_OK;}  first ensure the timer func cannot run ", "dev = alloc_etherdev(sizeof(*priv));if (!dev)return NULL;if (dev_alloc_name(dev, dev->name) < 0) ": "init_atmel_card(unsigned short irq, unsigned long port,   const AtmelFWType fw_type,   struct device  sys_dev,   int ( card_present)(void  ), void  card){struct net_device  dev;struct atmel_private  priv;int rc;  Create the network device object. ", "if (priv->bus_type == BUS_TYPE_PCCARD)atmel_write16(dev, GCR, 0x0060);atmel_write16(dev, GCR, 0x0040);del_timer_sync(&priv->management_timer);unregister_netdev(dev);remove_proc_entry(\"driver/atmel\", NULL);free_irq(dev->irq, dev);kfree(priv->firmware);release_region(dev->base_addr, 32);free_netdev(dev);}EXPORT_SYMBOL(stop_atmel_card": "stop_atmel_card(struct net_device  dev){struct atmel_private  priv = netdev_priv(dev);  put a brick on it... ", "val32 = rtw_read32(rtwdev, rf_sipi_addr_a->hssi_2);rtw_write32(rtwdev, rf_sipi_addr_a->hssi_2, val32 & ~LSSI_READ_EDGE_MASK);rtw_write32(rtwdev, rf_sipi_addr_a->hssi_2, val32 | LSSI_READ_EDGE_MASK);udelay(120);en_pi = rtw_read32_mask(rtwdev, rf_sipi_addr->hssi_1, BIT(8));r_addr = en_pi ? rf_sipi_addr->lssi_read_pi : rf_sipi_addr->lssi_read;val32 = rtw_read32_mask(rtwdev, r_addr, LSSI_READ_DATA_MASK);shift = __ffs(mask);return (val32 & mask) >> shift;}EXPORT_SYMBOL(rtw_phy_read_rf_sipi": "rtw_phy_read_rf_sipi(struct rtw_dev  rtwdev, enum rtw_rf_path rf_path, u32 addr, u32 mask){struct rtw_hal  hal = &rtwdev->hal;const struct rtw_chip_info  chip = rtwdev->chip;const struct rtw_rf_sipi_addr  rf_sipi_addr;const struct rtw_rf_sipi_addr  rf_sipi_addr_a;u32 val32;u32 en_pi;u32 r_addr;u32 shift;if (rf_path >= hal->rf_phy_num) {rtw_err(rtwdev, \"unsupported rf path (%d)\\n\", rf_path);return INV_RF_DATA;}if (!chip->rf_sipi_read_addr) {rtw_err(rtwdev, \"rf_sipi_read_addr isn't defined\\n\");return INV_RF_DATA;}rf_sipi_addr = &chip->rf_sipi_read_addr[rf_path];rf_sipi_addr_a = &chip->rf_sipi_read_addr[RF_PATH_A];addr &= 0xff;val32 = rtw_read32(rtwdev, rf_sipi_addr->hssi_2);val32 = (val32 & ~LSSI_READ_ADDR_MASK) | (addr << 23);rtw_write32(rtwdev, rf_sipi_addr->hssi_2, val32);  toggle read edge of path A ", "if (hal->current_band_type == RTW_BAND_2G)rs = RTW_RATE_SECTION_CCK;elsers = RTW_RATE_SECTION_OFDM;for (; rs < RTW_RATE_SECTION_MAX; rs++)rtw_phy_set_tx_power_index_by_rs(rtwdev, ch, path, rs);}void rtw_phy_set_tx_power_level(struct rtw_dev *rtwdev, u8 channel)": "rtw_phy_set_tx_power_level_by_path(struct rtw_dev  rtwdev,       u8 ch, u8 path){struct rtw_hal  hal = &rtwdev->hal;u8 rs;  do not need cck rates if we are not in 2.4G ", "rtw_write8_mask(rtwdev, REG_SND_PTCL_CTRL, BIT_MASK_BEAMFORM,RTW_SND_CTRL_SOUNDING);/* MAC address/Partial AID of Beamformer ": "rtw_bf_enable_bfee_su(struct rtw_dev  rtwdev, struct rtw_vif  vif,   struct rtw_bfee  bfee){u8 nc_index = hweight8(rtwdev->hal.antenna_rx) - 1;u8 nr_index = bfee->sound_dim;u8 grouping = 0, codebookinfo = 1, coefficientsize = 3;u32 addr_bfer_info, addr_csi_rpt, csi_param;u8 i;rtw_dbg(rtwdev, RTW_DBG_BF, \"config as an su bfee\\n\");switch (bfee->su_reg_index) {case 1:addr_bfer_info = REG_ASSOCIATED_BFMER1_INFO;addr_csi_rpt = REG_TX_CSI_RPT_PARAM_BW20 + 2;break;case 0:default:addr_bfer_info = REG_ASSOCIATED_BFMER0_INFO;addr_csi_rpt = REG_TX_CSI_RPT_PARAM_BW20;break;}  Sounding protocol control ", "rtw_write16_set(rtwdev, REG_RXFLTMAP0, BIT_RXFLTMAP0_ACTIONNOACK);/* accept NDPA and BF report poll ": "rtw_bf_enable_bfee_mu(struct rtw_dev  rtwdev, struct rtw_vif  vif,   struct rtw_bfee  bfee){struct rtw_bf_info  bf_info = &rtwdev->bf_info;struct mu_bfer_init_para param;u8 nc_index = hweight8(rtwdev->hal.antenna_rx) - 1;u8 nr_index = 1;u8 grouping = 0, codebookinfo = 1, coefficientsize = 0;u32 csi_param;rtw_dbg(rtwdev, RTW_DBG_BF, \"config as an mu bfee\\n\");csi_param = (u16)((coefficientsize << 10) |  (codebookinfo << 8) |  (grouping << 6) |  (nr_index << 3) |  nc_index);rtw_dbg(rtwdev, RTW_DBG_BF, \"nc=%d nr=%d group=%d codebookinfo=%d coefficientsize=%d\\n\",nc_index, nr_index, grouping, codebookinfo,coefficientsize);param.paid = bfee->p_aid;param.csi_para = csi_param;param.my_aid = bfee->aid & 0xfff;param.csi_length_sel = HAL_CSI_SEG_4K;ether_addr_copy(param.bfer_address, bfee->mac_addr);rtw_bf_init_bfer_entry_mu(rtwdev, &param);bf_info->cur_csi_rpt_rate = DESC_RATE6M;rtw_bf_cfg_sounding(rtwdev, vif, DESC_RATE6M);  accept action_no_ack ", "tmp32 |= BIT_MU_P1_WAIT_STATE_EN;/* MU Retry Limit ": "rtw_bf_phy_init(struct rtw_dev  rtwdev){u8 tmp8;u32 tmp32;u8 retry_limit = 0xA;u8 ndpa_rate = 0x10;u8 ack_policy = 3;tmp32 = rtw_read32(rtwdev, REG_MU_TX_CTL);  Enable P1 aggr new packet according to P0 transfer time ", "request ^= request | BIT_RPWM_TOGGLE;if (enter) ": "rtw_power_mode_change(struct rtw_dev  rtwdev, bool enter){u8 request, confirm, polling;int ret;request = rtw_read8(rtwdev, rtwdev->hci.rpwm_addr);confirm = rtw_read8(rtwdev, rtwdev->hci.cpwm_addr);  toggle to request power mode, others remain 0 ", "*((u32 *)skb->cb) = pkt_offset;skb_queue_tail(&rtwdev->c2h_queue, skb);ieee80211_queue_work(rtwdev->hw, &rtwdev->c2h_work);break;}}EXPORT_SYMBOL(rtw_fw_c2h_cmd_rx_irqsafe": "rtw_fw_c2h_cmd_rx_irqsafe(struct rtw_dev  rtwdev, u32 pkt_offset,       struct sk_buff  skb){struct rtw_c2h_cmd  c2h;u8 len;c2h = (struct rtw_c2h_cmd  )(skb->data + pkt_offset);len = skb->len - pkt_offset - 2; ((u32  )skb->cb) = pkt_offset;rtw_dbg(rtwdev, RTW_DBG_FW, \"recv C2H, id=0x%02x, seq=0x%02x, len=%d\\n\",c2h->id, c2h->seq, len);switch (c2h->id) {case C2H_BT_MP_INFO:rtw_coex_info_response(rtwdev, skb);break;case C2H_WLAN_RFON:complete(&rtwdev->lps_leave_check);dev_kfree_skb_any(skb);break;case C2H_SCAN_RESULT:complete(&rtwdev->fw_scan_density);rtw_fw_scan_result(rtwdev, c2h->payload, len);dev_kfree_skb_any(skb);break;case C2H_ADAPTIVITY:rtw_fw_adaptivity_result(rtwdev, c2h->payload, len);dev_kfree_skb_any(skb);break;default:  pass offset for further operation ", "drv_data = (u8 *)IEEE80211_SKB_CB(skb)->status.status_driver_data;*drv_data = sn;spin_lock_irqsave(&tx_report->q_lock, flags);__skb_queue_tail(&tx_report->queue, skb);spin_unlock_irqrestore(&tx_report->q_lock, flags);mod_timer(&tx_report->purge_timer, jiffies + RTW_TX_PROBE_TIMEOUT);}EXPORT_SYMBOL(rtw_tx_report_enqueue": "rtw_tx_report_enqueue(struct rtw_dev  rtwdev, struct sk_buff  skb, u8 sn){struct rtw_tx_report  tx_report = &rtwdev->tx_report;unsigned long flags;u8  drv_data;  pass sn to tx report handler through driver data ", "rtwdev->hal.rcr = BIT_APP_FCS | BIT_APP_MIC | BIT_APP_ICV |  BIT_PKTCTL_DLEN | BIT_HTC_LOC_CTRL | BIT_APP_PHYSTS |  BIT_AB | BIT_AM | BIT_APM;ret = rtw_load_firmware(rtwdev, RTW_NORMAL_FW);if (ret) ": "rtw_core_init(struct rtw_dev  rtwdev){const struct rtw_chip_info  chip = rtwdev->chip;struct rtw_coex  coex = &rtwdev->coex;int ret;INIT_LIST_HEAD(&rtwdev->rsvd_page_list);INIT_LIST_HEAD(&rtwdev->txqs);timer_setup(&rtwdev->tx_report.purge_timer,    rtw_tx_report_purge_timer, 0);rtwdev->tx_wq = alloc_workqueue(\"rtw_tx_wq\", WQ_UNBOUND | WQ_HIGHPRI, 0);if (!rtwdev->tx_wq) {rtw_warn(rtwdev, \"alloc_workqueue rtw_tx_wq failed\\n\");return -ENOMEM;}INIT_DELAYED_WORK(&rtwdev->watch_dog_work, rtw_watch_dog_work);INIT_DELAYED_WORK(&coex->bt_relink_work, rtw_coex_bt_relink_work);INIT_DELAYED_WORK(&coex->bt_reenable_work, rtw_coex_bt_reenable_work);INIT_DELAYED_WORK(&coex->defreeze_work, rtw_coex_defreeze_work);INIT_DELAYED_WORK(&coex->wl_remain_work, rtw_coex_wl_remain_work);INIT_DELAYED_WORK(&coex->bt_remain_work, rtw_coex_bt_remain_work);INIT_DELAYED_WORK(&coex->wl_connecting_work, rtw_coex_wl_connecting_work);INIT_DELAYED_WORK(&coex->bt_multi_link_remain_work,  rtw_coex_bt_multi_link_remain_work);INIT_DELAYED_WORK(&coex->wl_ccklock_work, rtw_coex_wl_ccklock_work);INIT_WORK(&rtwdev->tx_work, rtw_tx_work);INIT_WORK(&rtwdev->c2h_work, rtw_c2h_work);INIT_WORK(&rtwdev->ips_work, rtw_ips_work);INIT_WORK(&rtwdev->fw_recovery_work, rtw_fw_recovery_work);INIT_WORK(&rtwdev->update_beacon_work, rtw_fw_update_beacon_work);INIT_WORK(&rtwdev->ba_work, rtw_txq_ba_work);skb_queue_head_init(&rtwdev->c2h_queue);skb_queue_head_init(&rtwdev->coex.queue);skb_queue_head_init(&rtwdev->tx_report.queue);spin_lock_init(&rtwdev->txq_lock);spin_lock_init(&rtwdev->tx_report.q_lock);mutex_init(&rtwdev->mutex);mutex_init(&rtwdev->hal.tx_power_mutex);init_waitqueue_head(&rtwdev->coex.wait);init_completion(&rtwdev->lps_leave_check);init_completion(&rtwdev->fw_scan_density);rtwdev->sec.total_cam_num = 32;rtwdev->hal.current_channel = 1;rtwdev->dm_info.fix_rate = U8_MAX;set_bit(RTW_BC_MC_MACID, rtwdev->mac_id_map);rtw_stats_init(rtwdev);  default rx filter setting ", "if (rtwdev->chip->id == RTW_CHIP_TYPE_8821C && bridge->vendor == PCI_VENDOR_ID_INTEL)rtwpci->rx_no_aspm = true;rtw_pci_phy_cfg(rtwdev);ret = rtw_register_hw(rtwdev, hw);if (ret) ": "rtw_pci_probe(struct pci_dev  pdev,  const struct pci_device_id  id){struct pci_dev  bridge = pci_upstream_bridge(pdev);struct ieee80211_hw  hw;struct rtw_dev  rtwdev;struct rtw_pci  rtwpci;int drv_data_size;int ret;drv_data_size = sizeof(struct rtw_dev) + sizeof(struct rtw_pci);hw = ieee80211_alloc_hw(drv_data_size, &rtw_ops);if (!hw) {dev_err(&pdev->dev, \"failed to allocate hw\\n\");return -ENOMEM;}rtwdev = hw->priv;rtwdev->hw = hw;rtwdev->dev = &pdev->dev;rtwdev->chip = (struct rtw_chip_info  )id->driver_data;rtwdev->hci.ops = &rtw_pci_ops;rtwdev->hci.type = RTW_HCI_TYPE_PCIE;rtwpci = (struct rtw_pci  )rtwdev->priv;atomic_set(&rtwpci->link_usage, 1);ret = rtw_core_init(rtwdev);if (ret)goto err_release_hw;rtw_dbg(rtwdev, RTW_DBG_PCI,\"rtw88 pci probe: vendor=0x%4.04X device=0x%4.04X rev=%d\\n\",pdev->vendor, pdev->device, pdev->revision);ret = rtw_pci_claim(rtwdev, pdev);if (ret) {rtw_err(rtwdev, \"failed to claim pci device\\n\");goto err_deinit_core;}ret = rtw_pci_setup_resource(rtwdev, pdev);if (ret) {rtw_err(rtwdev, \"failed to setup pci resources\\n\");goto err_pci_declaim;}rtw_pci_napi_init(rtwdev);ret = rtw_chip_info_setup(rtwdev);if (ret) {rtw_err(rtwdev, \"failed to setup chip information\\n\");goto err_destroy_pci;}  Disable PCIe ASPM L1 while doing NAPI poll for 8821CE ", "if (txq->ac == IEEE80211_AC_VO)__rtw_tx_work(rtwdev);elsequeue_work(rtwdev->tx_wq, &rtwdev->tx_work);}static int rtw_ops_start(struct ieee80211_hw *hw)": "rtw_ops_tx(struct ieee80211_hw  hw,       struct ieee80211_tx_control  control,       struct sk_buff  skb){struct rtw_dev  rtwdev = hw->priv;if (!test_bit(RTW_FLAG_RUNNING, rtwdev->flags)) {ieee80211_free_txskb(hw, skb);return;}rtw_tx(rtwdev, control, skb);}static void rtw_ops_wake_tx_queue(struct ieee80211_hw  hw,  struct ieee80211_txq  txq){struct rtw_dev  rtwdev = hw->priv;struct rtw_txq  rtwtxq = (struct rtw_txq  )txq->drv_priv;if (!test_bit(RTW_FLAG_RUNNING, rtwdev->flags))return;spin_lock_bh(&rtwdev->txq_lock);if (list_empty(&rtwtxq->list))list_add_tail(&rtwtxq->list, &rtwdev->txqs);spin_unlock_bh(&rtwdev->txq_lock);  ensure to dequeue EAPOL (44) at the right time ", "if (!chip->new_scbd10_def && (bitpos & COEX_SCBD_FIX2M)) ": "rtw_coex_write_scbd(struct rtw_dev  rtwdev, u16 bitpos, bool set){const struct rtw_chip_info  chip = rtwdev->chip;struct rtw_coex  coex = &rtwdev->coex;struct rtw_coex_stat  coex_stat = &coex->stat;u16 val = 0x2;if (!chip->scbd_support)return;val |= coex_stat->score_board;  for 8822b, scbd[10] is CQDDR on   for 8822c, scbd[10] is no fix 2M ", "event = SER_EV_L1_RESET_PREPARE;break;case MAC_AX_ERR_L1_ERR_DMAC:case MAC_AX_ERR_L0_PROMOTE_TO_L1:event = SER_EV_L1_RESET; /* M1 ": "rtw89_ser_notify(struct rtw89_dev  rtwdev, u32 err){u8 event = SER_EV_NONE;rtw89_info(rtwdev, \"SER catches error: 0x%x\\n\", err);switch (err) {case MAC_AX_ERR_L1_PREERR_DMAC:   pre-M0 ", "udelay(1);return true;}EXPORT_SYMBOL(rtw89_phy_write_rf": "rtw89_phy_write_rf(struct rtw89_dev  rtwdev, enum rtw89_rf_path rf_path,u32 addr, u32 mask, u32 data){const struct rtw89_chip_info  chip = rtwdev->chip;const u32  base_addr = chip->rf_base_addr;u32 direct_addr;if (rf_path >= rtwdev->chip->rf_path_num) {rtw89_err(rtwdev, \"unsupported rf path (%d)\\n\", rf_path);return false;}addr &= 0xff;direct_addr = base_addr[rf_path] + (addr << 2);mask &= RFREG_MASK;rtw89_phy_write32_mask(rtwdev, direct_addr, mask, data);  delay to ensure writing properly ", "ofst = 0;break;}if (phy_page >= 0x40 && phy_page <= 0x4f)ofst = 0x2000;return ofst;}void rtw89_phy_write32_idx(struct rtw89_dev *rtwdev, u32 addr, u32 mask,   u32 data, enum rtw89_phy_idx phy_idx)": "rtw89_rfk_parser(rtwdev, chip->nctl_post_table);}static u32 rtw89_phy0_phy1_offset(struct rtw89_dev  rtwdev, u32 addr){u32 phy_page = addr >> 8;u32 ofst = 0;switch (phy_page) {case 0x6:case 0x7:case 0x8:case 0x9:case 0xa:case 0xb:case 0xc:case 0xd:case 0x19:case 0x1a:case 0x1b:ofst = 0x2000;break;default:  warning case ", "drv_info_len = desc_info->drv_info_size << 3; /* 8-byte unit ": "rtw89_core_query_rxdesc(struct rtw89_dev  rtwdev,     struct rtw89_rx_desc_info  desc_info,     u8  data, u32 data_offset){const struct rtw89_chip_info  chip = rtwdev->chip;struct rtw89_rxdesc_short  rxd_s;struct rtw89_rxdesc_long  rxd_l;u8 shift_len, drv_info_len;rxd_s = (struct rtw89_rxdesc_short  )(data + data_offset);desc_info->pkt_size = le32_get_bits(rxd_s->dword0, AX_RXD_RPKT_LEN_MASK);desc_info->drv_info_size = le32_get_bits(rxd_s->dword0, AX_RXD_DRV_INFO_SIZE_MASK);desc_info->long_rxdesc = le32_get_bits(rxd_s->dword0,  AX_RXD_LONG_RXD);desc_info->pkt_type = le32_get_bits(rxd_s->dword0,  AX_RXD_RPKT_TYPE_MASK);desc_info->mac_info_valid = le32_get_bits(rxd_s->dword0, AX_RXD_MAC_INFO_VLD);if (chip->chip_id == RTL8852C)desc_info->bw = le32_get_bits(rxd_s->dword1, AX_RXD_BW_v1_MASK);elsedesc_info->bw = le32_get_bits(rxd_s->dword1, AX_RXD_BW_MASK);desc_info->data_rate = le32_get_bits(rxd_s->dword1, AX_RXD_RX_DATARATE_MASK);desc_info->gi_ltf = le32_get_bits(rxd_s->dword1, AX_RXD_RX_GI_LTF_MASK);desc_info->user_id = le32_get_bits(rxd_s->dword1, AX_RXD_USER_ID_MASK);desc_info->sr_en = le32_get_bits(rxd_s->dword1, AX_RXD_SR_EN);desc_info->ppdu_cnt = le32_get_bits(rxd_s->dword1, AX_RXD_PPDU_CNT_MASK);desc_info->ppdu_type = le32_get_bits(rxd_s->dword1, AX_RXD_PPDU_TYPE_MASK);desc_info->free_run_cnt = le32_get_bits(rxd_s->dword2, AX_RXD_FREERUN_CNT_MASK);desc_info->icv_err = le32_get_bits(rxd_s->dword3, AX_RXD_ICV_ERR);desc_info->crc32_err = le32_get_bits(rxd_s->dword3, AX_RXD_CRC32_ERR);desc_info->hw_dec = le32_get_bits(rxd_s->dword3, AX_RXD_HW_DEC);desc_info->sw_dec = le32_get_bits(rxd_s->dword3, AX_RXD_SW_DEC);desc_info->addr1_match = le32_get_bits(rxd_s->dword3, AX_RXD_A1_MATCH);shift_len = desc_info->shift << 1;   2-byte unit ", "if (usr_num & BIT(0))phy_sts += RTW89_PPDU_MAC_INFO_USR_SIZE;if (rx_cnt_valid)phy_sts += RTW89_PPDU_MAC_RX_CNT_SIZE;phy_sts += plcp_size;phy_ppdu->buf = phy_sts;phy_ppdu->len = skb->data + skb->len - phy_sts;return 0;}static void rtw89_core_rx_process_phy_ppdu_iter(void *data,struct ieee80211_sta *sta)": "rtw89_core_rx_process_mac_ppdu(struct rtw89_dev  rtwdev,  struct sk_buff  skb,  struct rtw89_rx_phy_ppdu  phy_ppdu){const struct rtw89_rxinfo  rxinfo = (const struct rtw89_rxinfo  )skb->data;bool rx_cnt_valid = false;u8 plcp_size = 0;u8 usr_num = 0;u8  phy_sts;rx_cnt_valid = le32_get_bits(rxinfo->w0, RTW89_RXINFO_W0_RX_CNT_VLD);plcp_size = le32_get_bits(rxinfo->w1, RTW89_RXINFO_W1_PLCP_LEN) << 3;usr_num = le32_get_bits(rxinfo->w0, RTW89_RXINFO_W0_USR_NUM);if (usr_num > RTW89_PPDU_MAX_USR) {rtw89_warn(rtwdev, \"Invalid user number in mac info\\n\");return -EINVAL;}phy_sts = skb->data + RTW89_PPDU_MAC_INFO_SIZE;phy_sts += usr_num   RTW89_PPDU_MAC_INFO_USR_SIZE;  8-byte alignment ", ".wde_size0 = ": "rtw89_mac_size_set rtw89_mac_size = {.hfc_preccfg_pcie = {2, 40, 0, 0, 1, 0, 0, 0},  PCIE 64 ", "cancel_work_sync(&rtwdev->ips_work);mutex_lock(&rtwdev->mutex);rtw89_leave_ps_mode(rtwdev);if ((changed & IEEE80211_CONF_CHANGE_IDLE) &&    !(hw->conf.flags & IEEE80211_CONF_IDLE))rtw89_leave_ips(rtwdev);if (changed & IEEE80211_CONF_CHANGE_CHANNEL) ": "rtw89_ops_tx(struct ieee80211_hw  hw, struct ieee80211_tx_control  control, struct sk_buff  skb){struct rtw89_dev  rtwdev = hw->priv;struct ieee80211_tx_info  info = IEEE80211_SKB_CB(skb);struct ieee80211_vif  vif = info->control.vif;struct rtw89_vif  rtwvif = (struct rtw89_vif  )vif->drv_priv;struct ieee80211_sta  sta = control->sta;u32 flags = IEEE80211_SKB_CB(skb)->flags;int ret, qsel;if (rtwvif->offchan && !(flags & IEEE80211_TX_CTL_TX_OFFCHAN) && sta) {struct rtw89_sta  rtwsta = (struct rtw89_sta  )sta->drv_priv;rtw89_debug(rtwdev, RTW89_DBG_TXRX, \"ops_tx during offchan\\n\");skb_queue_tail(&rtwsta->roc_queue, skb);return;}ret = rtw89_core_tx_write(rtwdev, vif, sta, skb, &qsel);if (ret) {rtw89_err(rtwdev, \"failed to transmit skb: %d\\n\", ret);ieee80211_free_txskb(hw, skb);return;}rtw89_core_tx_kick_off(rtwdev, qsel);}static void rtw89_ops_wake_tx_queue(struct ieee80211_hw  hw,    struct ieee80211_txq  txq){struct rtw89_dev  rtwdev = hw->priv;ieee80211_schedule_txq(hw, txq);queue_work(rtwdev->txq_wq, &rtwdev->txq_work);}static int rtw89_ops_start(struct ieee80211_hw  hw){struct rtw89_dev  rtwdev = hw->priv;int ret;mutex_lock(&rtwdev->mutex);ret = rtw89_core_start(rtwdev);mutex_unlock(&rtwdev->mutex);return ret;}static void rtw89_ops_stop(struct ieee80211_hw  hw){struct rtw89_dev  rtwdev = hw->priv;mutex_lock(&rtwdev->mutex);rtw89_core_stop(rtwdev);mutex_unlock(&rtwdev->mutex);}static int rtw89_ops_config(struct ieee80211_hw  hw, u32 changed){struct rtw89_dev  rtwdev = hw->priv;  let previous ips work finish to ensure we don't leave ips twice ", "_write_scbd(rtwdev, BTC_WSCB_TDMA, false);*t = t_def[CXTD_OFF];s[CXST_OFF] = s_def[CXST_OFF];switch (policy_type) ": "rtw89_btc_set_policy(struct rtw89_dev  rtwdev, u16 policy_type){struct rtw89_btc  btc = &rtwdev->btc;struct rtw89_btc_dm  dm = &btc->dm;struct rtw89_btc_fbtc_tdma  t = &dm->tdma;struct rtw89_btc_fbtc_slot  s = dm->slot;u8 type;u32 tbl_w1, tbl_b1, tbl_b4;if (btc->mdinfo.ant.type == BTC_ANT_SHARED) {if (btc->cx.wl.status.map._4way)tbl_w1 = cxtbl[1];elsetbl_w1 = cxtbl[8];tbl_b1 = cxtbl[3];tbl_b4 = cxtbl[3];} else {tbl_w1 = cxtbl[16];tbl_b1 = cxtbl[17];tbl_b4 = cxtbl[17];}type = (u8)((policy_type & BTC_CXP_MASK) >> 8);btc->bt_req_en = false;switch (type) {case BTC_CXP_USERDEF0: t = t_def[CXTD_OFF];s[CXST_OFF] = s_def[CXST_OFF];_slot_set_tbl(btc, CXST_OFF, cxtbl[2]);btc->update_policy_force = true;break;case BTC_CXP_OFF:   TDMA off ", "elsetbl_w1 = cxtbl[8];if (dm->leak_ap &&    (type == BTC_CXP_PFIX || type == BTC_CXP_PAUTO2)) ": "rtw89_btc_set_policy_v1(struct rtw89_dev  rtwdev, u16 policy_type){struct rtw89_btc  btc = &rtwdev->btc;struct rtw89_btc_dm  dm = &btc->dm;struct rtw89_btc_fbtc_tdma  t = &dm->tdma;struct rtw89_btc_fbtc_slot  s = dm->slot;struct rtw89_btc_wl_role_info_v1  wl_rinfo = &btc->cx.wl.role_info_v1;struct rtw89_btc_bt_hid_desc  hid = &btc->cx.bt.link_info.hid_desc;struct rtw89_btc_bt_hfp_desc  hfp = &btc->cx.bt.link_info.hfp_desc;u8 type, null_role;u32 tbl_w1, tbl_b1, tbl_b4;type = FIELD_GET(BTC_CXP_MASK, policy_type);if (btc->mdinfo.ant.type == BTC_ANT_SHARED) {if (btc->cx.wl.status.map._4way)tbl_w1 = cxtbl[1];else if (hid->exist && hid->type == BTC_HID_218)tbl_w1 = cxtbl[7];   AckBA no break bt Hi-Pri-rx ", "rtlpriv->rfkill.rfkill_state = true;wiphy_rfkill_set_hw_state(hw->wiphy, 0);radio_state = rtlpriv->cfg->ops->radio_onoff_checking(hw, &valid);if (valid) ": "rtl_init_rfkill(struct ieee80211_hw  hw){struct rtl_priv  rtlpriv = rtl_priv(hw);bool radio_state;bool blocked;u8 valid = 0; set init state to on ", "/* *So tcb_desc->hw_rate is just used for *special data and mgt frames ": "rtl_get_tcb_desc(struct ieee80211_hw  hw,      struct ieee80211_tx_info  info,      struct ieee80211_sta  sta,      struct sk_buff  skb, struct rtl_tcb_desc  tcb_desc){#define SET_RATE_ID(rate_id)\\({typeof(rate_id) _id = rate_id;\\  ((rtlpriv->cfg->spec_ver & RTL_SPEC_NEW_RATEID) ?\\rtl_mrate_idx_to_arfr_id(hw, _id,\\(sta_entry ? sta_entry->wireless_mode :\\ WIRELESS_MODE_G)) :\\_id); })struct rtl_priv  rtlpriv = rtl_priv(hw);struct rtl_mac  rtlmac = rtl_mac(rtl_priv(hw));struct ieee80211_hdr  hdr = rtl_get_hdr(skb);struct rtl_sta_info  sta_entry =(sta ? (struct rtl_sta_info  )sta->drv_priv : NULL);__le16 fc = rtl_get_fc(skb);tcb_desc->hw_rate = _rtl_get_tx_hw_rate(hw, info);if (rtl_is_tx_report_skb(hw, skb))tcb_desc->use_spe_rpt = 1;if (ieee80211_is_data(fc)) {   we set data rate INX 0  in rtl_rc.c   if skb is special data or  mgt which need low data rate. ", "if (!mac->act_scanning)return;/* check if this really is a beacon ": "rtl_collect_scan_list(struct ieee80211_hw  hw, struct sk_buff  skb){struct rtl_priv  rtlpriv = rtl_priv(hw);struct ieee80211_hdr  hdr = rtl_get_hdr(skb);struct rtl_mac  mac = rtl_mac(rtl_priv(hw));unsigned long flags;struct rtl_bssid_entry  entry = NULL,  iter;  check if it is scanning ", "skb_queue_tail(&rtlpriv->c2hcmd_queue, skb);/* wake up wq ": "rtl_c2hcmd_enqueue(struct ieee80211_hw  hw, struct sk_buff  skb){struct rtl_priv  rtlpriv = rtl_priv(hw);if (rtl_c2h_fast_cmd(hw, skb)) {rtl_c2h_content_parsing(hw, skb);kfree_skb(skb);return;}  enqueue ", "if (skb) ": "rtl_send_smps_action(struct ieee80211_hw  hw, struct ieee80211_sta  sta, enum ieee80211_smps_mode smps){struct rtl_priv  rtlpriv = rtl_priv(hw);struct rtl_hal  rtlhal = rtl_hal(rtl_priv(hw));struct rtl_ps_ctl  ppsc = rtl_psc(rtl_priv(hw));struct sk_buff  skb = NULL;struct rtl_tcb_desc tcb_desc;u8 bssid[ETH_ALEN] = {0};memset(&tcb_desc, 0, sizeof(struct rtl_tcb_desc));if (rtlpriv->mac80211.act_scanning)goto err_free;if (!sta)goto err_free;if (unlikely(is_hal_stop(rtlhal) || ppsc->rfpwr_state != ERFON))goto err_free;if (!test_bit(RTL_STATUS_INTERFACE_START, &rtlpriv->status))goto err_free;if (rtlpriv->mac80211.opmode == NL80211_IFTYPE_AP)memcpy(bssid, rtlpriv->efuse.dev_addr, ETH_ALEN);elsememcpy(bssid, rtlpriv->mac80211.bssid, ETH_ALEN);skb = rtl_make_smps_action(hw, smps, sta->addr, bssid);  this is a type = mgmt   stype = action frame ", "spin_lock_init(&rtlpriv->locks.usb_lock);INIT_WORK(&rtlpriv->works.fill_h2c_cmd,  rtl_fill_h2c_cmd_work_callback);INIT_WORK(&rtlpriv->works.lps_change_work,  rtl_lps_change_work_callback);INIT_WORK(&rtlpriv->works.update_beacon_work,  rtl_update_beacon_work_callback);rtlpriv->usb_data_index = 0;init_completion(&rtlpriv->firmware_loading_complete);SET_IEEE80211_DEV(hw, &intf->dev);udev = interface_to_usbdev(intf);usb_get_dev(udev);usb_priv = rtl_usbpriv(hw);memset(usb_priv, 0, sizeof(*usb_priv));usb_priv->dev.intf = intf;usb_priv->dev.udev = udev;usb_set_intfdata(intf, hw);/* init cfg & intf_ops ": "rtl_usb_probe(struct usb_interface  intf,  const struct usb_device_id  id,  struct rtl_hal_cfg  rtl_hal_cfg){int err;struct ieee80211_hw  hw = NULL;struct rtl_priv  rtlpriv = NULL;struct usb_device udev;struct rtl_usb_priv  usb_priv;hw = ieee80211_alloc_hw(sizeof(struct rtl_priv) +sizeof(struct rtl_usb_priv), &rtl_ops);if (!hw) {pr_warn(\"rtl_usb: ieee80211 alloc failed\\n\");return -ENOMEM;}rtlpriv = hw->priv;rtlpriv->hw = hw;rtlpriv->usb_data = kcalloc(RTL_USB_MAX_RX_COUNT, sizeof(u32),    GFP_KERNEL);if (!rtlpriv->usb_data) {ieee80211_free_hw(hw);return -ENOMEM;}  this spin lock must be initialized early ", "wait_for_completion(&rtlpriv->firmware_loading_complete);clear_bit(RTL_STATUS_INTERFACE_START, &rtlpriv->status);/*ieee80211_unregister_hw will call ops_stop ": "rtl_usb_disconnect(struct usb_interface  intf){struct ieee80211_hw  hw = usb_get_intfdata(intf);struct rtl_priv  rtlpriv = rtl_priv(hw);struct rtl_mac  rtlmac = rtl_mac(rtl_priv(hw));struct rtl_usb  rtlusb = rtl_usbdev(rtl_usbpriv(hw));if (unlikely(!rtlpriv))return;  just in case driver is removed before firmware callback ", "52, 54, 56, 58, 60, 62, 64,/* Band 2 ": "channel5g[CHANNEL_MAX_NUMBER_5G] = {36, 38, 40, 42, 44, 46, 48,  Band 1 ", "value = rtl_read_byte(rtlpriv, offset);value &= (~(GET_PWR_CFG_MASK(cfg_cmd)));value |= (GET_PWR_CFG_VALUE(cfg_cmd) &  GET_PWR_CFG_MASK(cfg_cmd));/*Write the value back to system register": "rtl_hal_pwrseqcmdparsing(struct rtl_priv  rtlpriv, u8 cut_version,      u8 faversion, u8 interface_type,      struct wlan_pwr_cfg pwrcfgcmd[]){struct wlan_pwr_cfg cfg_cmd;bool polling_bit = false;u32 ary_idx = 0;u8 value = 0;u32 offset = 0;u32 polling_count = 0;u32 max_polling_cnt = 5000;do {cfg_cmd = pwrcfgcmd[ary_idx];rtl_dbg(rtlpriv, COMP_INIT, DBG_TRACE,\"%s: offset(%#x),cut_msk(%#x), famsk(%#x), interface_msk(%#x), base(%#x), cmd(%#x), msk(%#x), value(%#x)\\n\",__func__,GET_PWR_CFG_OFFSET(cfg_cmd),   GET_PWR_CFG_CUT_MASK(cfg_cmd),GET_PWR_CFG_FAB_MASK(cfg_cmd),     GET_PWR_CFG_INTF_MASK(cfg_cmd),GET_PWR_CFG_BASE(cfg_cmd), GET_PWR_CFG_CMD(cfg_cmd),GET_PWR_CFG_MASK(cfg_cmd), GET_PWR_CFG_VALUE(cfg_cmd));if ((GET_PWR_CFG_FAB_MASK(cfg_cmd)&faversion) &&    (GET_PWR_CFG_CUT_MASK(cfg_cmd)&cut_version) &&    (GET_PWR_CFG_INTF_MASK(cfg_cmd)&interface_type)) {switch (GET_PWR_CFG_CMD(cfg_cmd)) {case PWR_CMD_READ:rtl_dbg(rtlpriv, COMP_INIT, DBG_TRACE,\"rtl_hal_pwrseqcmdparsing(): PWR_CMD_READ\\n\");break;case PWR_CMD_WRITE:rtl_dbg(rtlpriv, COMP_INIT, DBG_TRACE,\"%s(): PWR_CMD_WRITE\\n\", __func__);offset = GET_PWR_CFG_OFFSET(cfg_cmd); Read the value from system register", "pdesc = &ring->desc[0];rtlpriv->cfg->ops->fill_tx_cmddesc(hw, (u8 *)pdesc, 1, 1, skb);__skb_queue_tail(&ring->queue, skb);spin_unlock_irqrestore(&rtlpriv->locks.irq_th_lock, flags);rtlpriv->cfg->ops->tx_polling(hw, BEACON_QUEUE);return true;}EXPORT_SYMBOL(rtl_cmd_send_packet": "rtl_cmd_send_packet(struct ieee80211_hw  hw, struct sk_buff  skb){struct rtl_priv  rtlpriv = rtl_priv(hw);struct rtl_pci  rtlpci = rtl_pcidev(rtl_pcipriv(hw));struct rtl8192_tx_ring  ring;struct rtl_tx_desc  pdesc;unsigned long flags;struct sk_buff  pskb = NULL;ring = &rtlpci->tx_ring[BEACON_QUEUE];spin_lock_irqsave(&rtlpriv->locks.irq_th_lock, flags);pskb = __skb_dequeue(&ring->queue);if (pskb)dev_kfree_skb_irq(pskb); this is wrong, fill_tx_cmddesc needs update", "if (rtlhal->interface == INTF_PCI)rtlpriv->intf_ops->reset_trx_ring(hw);if (is_hal_stop(rtlhal))rtl_dbg(rtlpriv, COMP_ERR, DBG_WARNING,\"Driver is already down!\\n\");/*<2> Enable Adapter ": "rtl_ps_enable_nic(struct ieee80211_hw  hw){struct rtl_priv  rtlpriv = rtl_priv(hw);struct rtl_ps_ctl  ppsc = rtl_psc(rtl_priv(hw));struct rtl_hal  rtlhal = rtl_hal(rtl_priv(hw));struct rtl_mac  rtlmac = rtl_mac(rtl_priv(hw)); <1> reset trx ring ", "rtl_deinit_deferred_work(hw, true);/*<2> Disable Interrupt ": "rtl_ps_disable_nic(struct ieee80211_hw  hw){struct rtl_priv  rtlpriv = rtl_priv(hw); <1> Stop all timer ", "rtlpriv->proximity.proxim_on = false;pcipriv = (void *)rtlpriv->priv;pcipriv->dev.pdev = pdev;/* init cfg & intf_ops ": "rtl_pci_probe(struct pci_dev  pdev,  const struct pci_device_id  id){struct ieee80211_hw  hw = NULL;struct rtl_priv  rtlpriv = NULL;struct rtl_pci_priv  pcipriv = NULL;struct rtl_pci  rtlpci;unsigned long pmem_start, pmem_len, pmem_flags;int err;err = pci_enable_device(pdev);if (err) {WARN_ONCE(true, \"%s : Cannot enable new PCI device\\n\",  pci_name(pdev));return err;}if (((struct rtl_hal_cfg  )id->driver_data)->mod_params->dma64 &&    !dma_set_mask(&pdev->dev, DMA_BIT_MASK(64))) {if (dma_set_coherent_mask(&pdev->dev, DMA_BIT_MASK(64))) {WARN_ONCE(true,  \"Unable to obtain 64bit DMA for consistent allocations\\n\");err = -ENOMEM;goto fail1;}platform_enable_dma64(pdev, true);} else if (!dma_set_mask(&pdev->dev, DMA_BIT_MASK(32))) {if (dma_set_coherent_mask(&pdev->dev, DMA_BIT_MASK(32))) {WARN_ONCE(true,  \"rtlwifi: Unable to obtain 32bit DMA for consistent allocations\\n\");err = -ENOMEM;goto fail1;}platform_enable_dma64(pdev, false);}pci_set_master(pdev);hw = ieee80211_alloc_hw(sizeof(struct rtl_pci_priv) +sizeof(struct rtl_priv), &rtl_ops);if (!hw) {WARN_ONCE(true,  \"%s : ieee80211 alloc failed\\n\", pci_name(pdev));err = -ENOMEM;goto fail1;}SET_IEEE80211_DEV(hw, &pdev->dev);pci_set_drvdata(pdev, hw);rtlpriv = hw->priv;rtlpriv->hw = hw;pcipriv = (void  )rtlpriv->priv;pcipriv->dev.pdev = pdev;init_completion(&rtlpriv->firmware_loading_complete); proximity init here", "wait_for_completion(&rtlpriv->firmware_loading_complete);clear_bit(RTL_STATUS_INTERFACE_START, &rtlpriv->status);/* remove form debug ": "rtl_pci_disconnect(struct pci_dev  pdev){struct ieee80211_hw  hw = pci_get_drvdata(pdev);struct rtl_pci_priv  pcipriv = rtl_pcipriv(hw);struct rtl_priv  rtlpriv = rtl_priv(hw);struct rtl_pci  rtlpci = rtl_pcidev(pcipriv);struct rtl_mac  rtlmac = rtl_mac(rtlpriv);  just in case driver is removed before firmware callback ", "efuse_tbl = kzalloc(rtlpriv->cfg->maps[EFUSE_HWSET_MAX_SIZE],    GFP_ATOMIC);if (!efuse_tbl)return;efuse_word = kcalloc(EFUSE_MAX_WORD_UNIT, sizeof(u16 *), GFP_ATOMIC);if (!efuse_word)goto out;for (i = 0; i < EFUSE_MAX_WORD_UNIT; i++) ": "efuse_one_byte_read,.efuse_logical_map_read = efuse_shadow_read,};static void efuse_shadow_read_1byte(struct ieee80211_hw  hw, u16 offset,    u8  value);static void efuse_shadow_read_2byte(struct ieee80211_hw  hw, u16 offset,    u16  value);static void efuse_shadow_read_4byte(struct ieee80211_hw  hw, u16 offset,    u32  value);static void efuse_shadow_write_1byte(struct ieee80211_hw  hw, u16 offset,     u8 value);static void efuse_shadow_write_2byte(struct ieee80211_hw  hw, u16 offset,     u16 value);static void efuse_shadow_write_4byte(struct ieee80211_hw  hw, u16 offset,     u32 value);static int efuse_one_byte_write(struct ieee80211_hw  hw, u16 addr,u8 data);static void efuse_read_all_map(struct ieee80211_hw  hw, u8  efuse);static int efuse_pg_packet_read(struct ieee80211_hw  hw, u8 offset,u8  data);static int efuse_pg_packet_write(struct ieee80211_hw  hw, u8 offset, u8 word_en, u8  data);static void efuse_word_enable_data_read(u8 word_en, u8  sourdata,u8  targetdata);static u8 enable_efuse_data_write(struct ieee80211_hw  hw,  u16 efuse_addr, u8 word_en, u8  data);static u16 efuse_get_current_size(struct ieee80211_hw  hw);static u8 efuse_calculate_word_cnts(u8 word_en);void efuse_initialize(struct ieee80211_hw  hw){struct rtl_priv  rtlpriv = rtl_priv(hw);u8 bytetemp;u8 temp;bytetemp = rtl_read_byte(rtlpriv, rtlpriv->cfg->maps[SYS_FUNC_EN] + 1);temp = bytetemp | 0x20;rtl_write_byte(rtlpriv, rtlpriv->cfg->maps[SYS_FUNC_EN] + 1, temp);bytetemp = rtl_read_byte(rtlpriv, rtlpriv->cfg->maps[SYS_ISO_CTRL] + 1);temp = bytetemp & 0xFE;rtl_write_byte(rtlpriv, rtlpriv->cfg->maps[SYS_ISO_CTRL] + 1, temp);bytetemp = rtl_read_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_TEST] + 3);temp = bytetemp | 0x80;rtl_write_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_TEST] + 3, temp);rtl_write_byte(rtlpriv, 0x2F8, 0x3);rtl_write_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 3, 0x72);}u8 efuse_read_1byte(struct ieee80211_hw  hw, u16 address){struct rtl_priv  rtlpriv = rtl_priv(hw);u8 data;u8 bytetemp;u8 temp;u32 k = 0;const u32 efuse_len =rtlpriv->cfg->maps[EFUSE_REAL_CONTENT_SIZE];if (address < efuse_len) {temp = address & 0xFF;rtl_write_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 1,       temp);bytetemp = rtl_read_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 2);temp = ((address >> 8) & 0x03) | (bytetemp & 0xFC);rtl_write_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 2,       temp);bytetemp = rtl_read_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 3);temp = bytetemp & 0x7F;rtl_write_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 3,       temp);bytetemp = rtl_read_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 3);while (!(bytetemp & 0x80)) {bytetemp = rtl_read_byte(rtlpriv, rtlpriv->cfg-> maps[EFUSE_CTRL] + 3);k++;if (k == 1000)break;}data = rtl_read_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL]);return data;} elsereturn 0xFF;}EXPORT_SYMBOL(efuse_read_1byte);void efuse_write_1byte(struct ieee80211_hw  hw, u16 address, u8 value){struct rtl_priv  rtlpriv = rtl_priv(hw);u8 bytetemp;u8 temp;u32 k = 0;const u32 efuse_len =rtlpriv->cfg->maps[EFUSE_REAL_CONTENT_SIZE];rtl_dbg(rtlpriv, COMP_EFUSE, DBG_LOUD, \"Addr=%x Data =%x\\n\",address, value);if (address < efuse_len) {rtl_write_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL], value);temp = address & 0xFF;rtl_write_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 1,       temp);bytetemp = rtl_read_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 2);temp = ((address >> 8) & 0x03) | (bytetemp & 0xFC);rtl_write_byte(rtlpriv,       rtlpriv->cfg->maps[EFUSE_CTRL] + 2, temp);bytetemp = rtl_read_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 3);temp = bytetemp | 0x80;rtl_write_byte(rtlpriv,       rtlpriv->cfg->maps[EFUSE_CTRL] + 3, temp);bytetemp = rtl_read_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 3);while (bytetemp & 0x80) {bytetemp = rtl_read_byte(rtlpriv, rtlpriv->cfg-> maps[EFUSE_CTRL] + 3);k++;if (k == 100) {k = 0;break;}}}}void read_efuse_byte(struct ieee80211_hw  hw, u16 _offset, u8  pbuf){struct rtl_priv  rtlpriv = rtl_priv(hw);u32 value32;u8 readbyte;u16 retry;rtl_write_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 1,       (_offset & 0xff));readbyte = rtl_read_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 2);rtl_write_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 2,       ((_offset >> 8) & 0x03) | (readbyte & 0xfc));readbyte = rtl_read_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 3);rtl_write_byte(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL] + 3,       (readbyte & 0x7f));retry = 0;value32 = rtl_read_dword(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL]);while (!(((value32 >> 24) & 0xff) & 0x80) && (retry < 10000)) {value32 = rtl_read_dword(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL]);retry++;}udelay(50);value32 = rtl_read_dword(rtlpriv, rtlpriv->cfg->maps[EFUSE_CTRL]); pbuf = (u8) (value32 & 0xff);}EXPORT_SYMBOL_GPL(read_efuse_byte);void read_efuse(struct ieee80211_hw  hw, u16 _offset, u16 _size_byte, u8  pbuf){struct rtl_priv  rtlpriv = rtl_priv(hw);struct rtl_efuse  rtlefuse = rtl_efuse(rtl_priv(hw));u8  efuse_tbl;u8 rtemp8[1];u16 efuse_addr = 0;u8 offset, wren;u8 u1temp = 0;u16 i;u16 j;const u16 efuse_max_section =rtlpriv->cfg->maps[EFUSE_MAX_SECTION_MAP];const u32 efuse_len =rtlpriv->cfg->maps[EFUSE_REAL_CONTENT_SIZE];u16   efuse_word;u16 efuse_utilized = 0;u8 efuse_usage;if ((_offset + _size_byte) > rtlpriv->cfg->maps[EFUSE_HWSET_MAX_SIZE]) {rtl_dbg(rtlpriv, COMP_EFUSE, DBG_LOUD,\"%s: Invalid offset(%#x) with read bytes(%#x)!!\\n\",__func__, _offset, _size_byte);return;}  allocate memory for efuse_tbl and efuse_word ", "for (i = 4; i < TOTAL_CAM_ENTRY; i++) ": "rtl_cam_del_entry(struct ieee80211_hw  hw, u8  sta_addr){struct rtl_priv  rtlpriv = rtl_priv(hw);u32 bitmap;u8 i,  addr;if (NULL == sta_addr) {pr_err(\"sta_addr is NULL.\\n\");return;}if (is_zero_ether_addr(sta_addr)) {pr_err(\"sta_addr is %pM\\n\", sta_addr);return;}  Does STA already exist? ", "_rtl92c_phy_path_a_fill_iqk_matrix(hw, b_patha_ok, result,   final_candidate,   (reg_ea4 == 0));if (IS_92C_SERIAL(rtlhal->version)) ": "rtl92c_phy_set_rfpath_switch(struct ieee80211_hw  hw,  bool bmain, bool is2t){struct rtl_hal  rtlhal = rtl_hal(rtl_priv(hw));if (is_hal_stop(rtlhal)) {rtl_set_bbreg(hw, REG_LEDCFG0, BIT(23), 0x01);rtl_set_bbreg(hw, RFPGA0_XAB_RFPARAMETER, BIT(13), 0x01);}if (is2t) {if (bmain)rtl_set_bbreg(hw, RFPGA0_XB_RFINTERFACEOE,      BIT(5) | BIT(6), 0x1);elsertl_set_bbreg(hw, RFPGA0_XB_RFINTERFACEOE,      BIT(5) | BIT(6), 0x2);} else {if (bmain)rtl_set_bbreg(hw, RFPGA0_XA_RFINTERFACEOE, 0x300, 0x2);elsertl_set_bbreg(hw, RFPGA0_XA_RFINTERFACEOE, 0x300, 0x1);}}#undef IQK_ADDA_REG_NUM#undef IQK_DELAY_TIMEvoid rtl92c_phy_iq_calibrate(struct ieee80211_hw  hw, bool b_recovery){struct rtl_priv  rtlpriv = rtl_priv(hw);struct rtl_phy  rtlphy = &(rtlpriv->phy);struct rtl_hal  rtlhal = rtl_hal(rtl_priv(hw));long result[4][8];u8 i, final_candidate;bool b_patha_ok, b_pathb_ok;long reg_e94, reg_e9c, reg_ea4, reg_eb4, reg_ebc, reg_ec4,    reg_tmp = 0;bool is12simular, is13simular, is23simular;u32 iqk_bb_reg[10] = {ROFDM0_XARXIQIMBALANCE,ROFDM0_XBRXIQIMBALANCE,ROFDM0_ECCATHRESHOLD,ROFDM0_AGCRSSITABLE,ROFDM0_XATXIQIMBALANCE,ROFDM0_XBTXIQIMBALANCE,ROFDM0_XCTXIQIMBALANCE,ROFDM0_XCTXAFE,ROFDM0_XDTXAFE,ROFDM0_RXIQEXTANTA};if (b_recovery) {_rtl92c_phy_reload_adda_registers(hw,  iqk_bb_reg,  rtlphy->iqk_bb_backup, 10);return;}for (i = 0; i < 8; i++) {result[0][i] = 0;result[1][i] = 0;result[2][i] = 0;result[3][i] = 0;}final_candidate = 0xff;b_patha_ok = false;b_pathb_ok = false;is12simular = false;is23simular = false;is13simular = false;for (i = 0; i < 3; i++) {if (IS_92C_SERIAL(rtlhal->version))_rtl92c_phy_iq_calibrate(hw, result, i, true);else_rtl92c_phy_iq_calibrate(hw, result, i, false);if (i == 1) {is12simular = _rtl92c_phy_simularity_compare(hw,     result, 0,     1);if (is12simular) {final_candidate = 0;break;}}if (i == 2) {is13simular = _rtl92c_phy_simularity_compare(hw,     result, 0,     2);if (is13simular) {final_candidate = 0;break;}is23simular = _rtl92c_phy_simularity_compare(hw,     result, 1,     2);if (is23simular)final_candidate = 1;else {for (i = 0; i < 8; i++)reg_tmp += result[3][i];if (reg_tmp != 0)final_candidate = 3;elsefinal_candidate = 0xFF;}}}for (i = 0; i < 4; i++) {reg_e94 = result[i][0];reg_e9c = result[i][1];reg_ea4 = result[i][2];reg_eb4 = result[i][4];reg_ebc = result[i][5];reg_ec4 = result[i][6];}if (final_candidate != 0xff) {rtlphy->reg_e94 = reg_e94 = result[final_candidate][0];rtlphy->reg_e9c = reg_e9c = result[final_candidate][1];reg_ea4 = result[final_candidate][2];rtlphy->reg_eb4 = reg_eb4 = result[final_candidate][4];rtlphy->reg_ebc = reg_ebc = result[final_candidate][5];reg_ec4 = result[final_candidate][6];b_patha_ok = true;b_pathb_ok = true;} else {rtlphy->reg_e94 = rtlphy->reg_eb4 = 0x100;rtlphy->reg_e9c = rtlphy->reg_ebc = 0x0;}if (reg_e94 != 0)  &&(reg_ea4 != 0) ", "if (rtlpriv->falsealm_cnt.cnt_all > 10000) ": "rtl92c_dm_initial_gain_min_pwdb(struct ieee80211_hw  hw){struct rtl_priv  rtlpriv = rtl_priv(hw);struct dig_t  dm_digtable = &rtlpriv->dm_digtable;long rssi_val_min = 0;if ((dm_digtable->curmultista_cstate == DIG_MULTISTA_CONNECT) &&    (dm_digtable->cursta_cstate == DIG_STA_CONNECT)) {if (rtlpriv->dm.entry_min_undec_sm_pwdb != 0)rssi_val_min =    (rtlpriv->dm.entry_min_undec_sm_pwdb >     rtlpriv->dm.undec_sm_pwdb) ?    rtlpriv->dm.undec_sm_pwdb :    rtlpriv->dm.entry_min_undec_sm_pwdb;elserssi_val_min = rtlpriv->dm.undec_sm_pwdb;} else if (dm_digtable->cursta_cstate == DIG_STA_CONNECT ||   dm_digtable->cursta_cstate == DIG_STA_BEFORE_CONNECT) {rssi_val_min = rtlpriv->dm.undec_sm_pwdb;} else if (dm_digtable->curmultista_cstate == DIG_MULTISTA_CONNECT) {rssi_val_min = rtlpriv->dm.entry_min_undec_sm_pwdb;}if (rssi_val_min > 100)rssi_val_min = 100;return (u8)rssi_val_min;}static void rtl92c_dm_false_alarm_counter_statistics(struct ieee80211_hw  hw){u32 ret_value;struct rtl_priv  rtlpriv = rtl_priv(hw);struct false_alarm_statistics  falsealm_cnt = &(rtlpriv->falsealm_cnt);ret_value = rtl_get_bbreg(hw, ROFDM_PHYCOUNTER1, MASKDWORD);falsealm_cnt->cnt_parity_fail = ((ret_value & 0xffff0000) >> 16);ret_value = rtl_get_bbreg(hw, ROFDM_PHYCOUNTER2, MASKDWORD);falsealm_cnt->cnt_rate_illegal = (ret_value & 0xffff);falsealm_cnt->cnt_crc8_fail = ((ret_value & 0xffff0000) >> 16);ret_value = rtl_get_bbreg(hw, ROFDM_PHYCOUNTER3, MASKDWORD);falsealm_cnt->cnt_mcs_fail = (ret_value & 0xffff);ret_value = rtl_get_bbreg(hw, ROFDM0_FRAMESYNC, MASKDWORD);falsealm_cnt->cnt_fast_fsync_fail = (ret_value & 0xffff);falsealm_cnt->cnt_sb_search_fail = ((ret_value & 0xffff0000) >> 16);falsealm_cnt->cnt_ofdm_fail = falsealm_cnt->cnt_parity_fail +      falsealm_cnt->cnt_rate_illegal +      falsealm_cnt->cnt_crc8_fail +      falsealm_cnt->cnt_mcs_fail +      falsealm_cnt->cnt_fast_fsync_fail +      falsealm_cnt->cnt_sb_search_fail;rtl_set_bbreg(hw, RCCK0_FALSEALARMREPORT, BIT(14), 1);ret_value = rtl_get_bbreg(hw, RCCK0_FACOUNTERLOWER, MASKBYTE0);falsealm_cnt->cnt_cck_fail = ret_value;ret_value = rtl_get_bbreg(hw, RCCK0_FACOUNTERUPPER, MASKBYTE3);falsealm_cnt->cnt_cck_fail += (ret_value & 0xff) << 8;falsealm_cnt->cnt_all = (falsealm_cnt->cnt_parity_fail + falsealm_cnt->cnt_rate_illegal + falsealm_cnt->cnt_crc8_fail + falsealm_cnt->cnt_mcs_fail + falsealm_cnt->cnt_cck_fail);rtl_set_bbreg(hw, ROFDM1_LSTF, 0x08000000, 1);rtl_set_bbreg(hw, ROFDM1_LSTF, 0x08000000, 0);rtl_set_bbreg(hw, RCCK0_FALSEALARMREPORT, 0x0000c000, 0);rtl_set_bbreg(hw, RCCK0_FALSEALARMREPORT, 0x0000c000, 2);rtl_dbg(rtlpriv, COMP_DIG, DBG_TRACE,\"cnt_parity_fail = %d, cnt_rate_illegal = %d, cnt_crc8_fail = %d, cnt_mcs_fail = %d\\n\",falsealm_cnt->cnt_parity_fail,falsealm_cnt->cnt_rate_illegal,falsealm_cnt->cnt_crc8_fail, falsealm_cnt->cnt_mcs_fail);rtl_dbg(rtlpriv, COMP_DIG, DBG_TRACE,\"cnt_ofdm_fail = %x, cnt_cck_fail = %x, cnt_all = %x\\n\",falsealm_cnt->cnt_ofdm_fail,falsealm_cnt->cnt_cck_fail, falsealm_cnt->cnt_all);}static void rtl92c_dm_ctrl_initgain_by_fa(struct ieee80211_hw  hw){struct rtl_priv  rtlpriv = rtl_priv(hw);struct dig_t  dm_digtable = &rtlpriv->dm_digtable;u8 value_igi = dm_digtable->cur_igvalue;if (rtlpriv->falsealm_cnt.cnt_all < DM_DIG_FA_TH0)value_igi--;else if (rtlpriv->falsealm_cnt.cnt_all < DM_DIG_FA_TH1)value_igi += 0;else if (rtlpriv->falsealm_cnt.cnt_all < DM_DIG_FA_TH2)value_igi++;else if (rtlpriv->falsealm_cnt.cnt_all >= DM_DIG_FA_TH2)value_igi += 2;if (value_igi > DM_DIG_FA_UPPER)value_igi = DM_DIG_FA_UPPER;else if (value_igi < DM_DIG_FA_LOWER)value_igi = DM_DIG_FA_LOWER;if (rtlpriv->falsealm_cnt.cnt_all > 10000)value_igi = DM_DIG_FA_UPPER;dm_digtable->cur_igvalue = value_igi;rtl92c_dm_write_dig(hw);}static void rtl92c_dm_ctrl_initgain_by_rssi(struct ieee80211_hw  hw){struct rtl_priv  rtlpriv = rtl_priv(hw);struct dig_t  digtable = &rtlpriv->dm_digtable;u32 isbt;  modify DIG lower bound, deal with abnormally large false alarm ", "rtl92c_dm_bt_coexist(hw);rtl92c_dm_check_edca_turbo(hw);}}EXPORT_SYMBOL(rtl92c_dm_watchdog": "rtl92c_dm_watchdog(struct ieee80211_hw  hw){struct rtl_priv  rtlpriv = rtl_priv(hw);struct rtl_ps_ctl  ppsc = rtl_psc(rtl_priv(hw));bool fw_current_inpsmode = false;bool fw_ps_awake = true;rtlpriv->cfg->ops->get_hw_reg(hw, HW_VAR_FW_PSMODE_STATUS,      (u8  ) (&fw_current_inpsmode));rtlpriv->cfg->ops->get_hw_reg(hw, HW_VAR_FWLPS_RF_ON,      (u8  ) (&fw_ps_awake));if (ppsc->p2p_ps_info.p2p_ps_mode)fw_ps_awake = false;if ((ppsc->rfpwr_state == ERFON) && ((!fw_current_inpsmode) &&     fw_ps_awake)    && (!ppsc->rfchange_inprogress)) {rtl92c_dm_pwdb_monitor(hw);rtl92c_dm_dig(hw);rtl92c_dm_false_alarm_counter_statistics(hw);rtl92c_dm_dynamic_bb_powersaving(hw);rtl92c_dm_dynamic_txpower(hw);rtl92c_dm_check_txpower_tracking(hw);  rtl92c_dm_refresh_rate_adaptive_mask(hw); ", "if (undec_sm_pwdb >= 67)curr_bt_rssi_state &= (~BT_RSSI_STATE_NORMAL_POWER);else if (undec_sm_pwdb < 62)curr_bt_rssi_state |= BT_RSSI_STATE_NORMAL_POWER;/* Check RSSI to determine AMPDU setting for BT coexistence. ": "rtl92c_dm_bt_coexist(hw);rtl92c_dm_check_edca_turbo(hw);}}EXPORT_SYMBOL(rtl92c_dm_watchdog);u8 rtl92c_bt_rssi_state_change(struct ieee80211_hw  hw){struct rtl_priv  rtlpriv = rtl_priv(hw);long undec_sm_pwdb;u8 curr_bt_rssi_state = 0x00;if (rtlpriv->mac80211.link_state == MAC80211_LINKED) {undec_sm_pwdb = GET_UNDECORATED_AVERAGE_RSSI(rtlpriv);} else {if (rtlpriv->dm.entry_min_undec_sm_pwdb == 0)undec_sm_pwdb = 100;elseundec_sm_pwdb = rtlpriv->dm.entry_min_undec_sm_pwdb;}  Check RSSI to determine HighPowerNormalPower state for   BT coexistence. ", "#define PSPOLL_PG2#define NULL_PG3#define PROBERSP_PG4 /*->5": "rtl92c_set_fw_rsvdpagepkt(): u1_h2c_set_pwrmode\\n\",      u1_h2c_set_pwrmode, 3);rtl92c_fill_h2c_cmd(hw, H2C_SETPWRMODE, 3, u1_h2c_set_pwrmode);}EXPORT_SYMBOL(rtl92c_set_fw_pwrmode_cmd);#define BEACON_PG0  ->1", "if (test_bit(FLAG_MPI, &ai->flags) && !skb_queue_empty(&ai->txq)) ": "stop_airo_card(struct net_device  dev, int freeres){struct airo_info  ai = dev->ml_priv;set_bit(FLAG_RADIO_DOWN, &ai->flags);disable_MAC(ai, 1);disable_interrupts(ai);takedown_proc_entry(dev, ai);if (test_bit(FLAG_REGISTERED, &ai->flags)) {unregister_netdev(dev);if (ai->wifidev) {unregister_netdev(ai->wifidev);free_netdev(ai->wifidev);ai->wifidev = NULL;}clear_bit(FLAG_REGISTERED, &ai->flags);}    Clean out tx queue ", "dev = alloc_netdev(sizeof(*ai), \"\", NET_NAME_UNKNOWN, ether_setup);if (!dev) ": "init_airo_card(unsigned short irq, int port,   int is_pcmcia, struct pci_dev  pci,   struct device  dmdev){struct net_device  dev;struct airo_info  ai;int i, rc;CapabilityRid cap_rid;  Create the network device object. ", "if (!test_bit(FLAG_MPI,&ai->flags))for (i = 0; i < MAX_FIDS; i++)ai->fids[i] = transmit_allocate (ai, AIRO_DEF_MTU, i>=MAX_FIDS/2);enable_interrupts(ai);netif_wake_queue(dev);return 0;}EXPORT_SYMBOL(reset_airo_card": "reset_airo_card(struct net_device  dev){int i;struct airo_info  ai = dev->ml_priv;if (reset_card (dev, 1))return -1;if (setup_card(ai, dev, 1) != SUCCESS) {airo_print_err(dev->name, \"MAC could not be enabled\");return -1;}airo_print_info(dev->name, \"MAC enabled %pM\", dev->dev_addr);  Allocate the transmit buffers if needed ", "cancel_work_sync(&dev->kevent);del_timer_sync(&dev->delay);free_percpu(net->tstats);out0:free_netdev(net);out:return status;}EXPORT_SYMBOL_GPL(usbnet_probe);/*-------------------------------------------------------------------------": "usbnet_link_change(dev, 0, 0);return 0;out5:kfree(dev->padding_pkt);out4:usb_free_urb(dev->interrupt);out3:if (info->unbind)info->unbind (dev, udev);out1:  subdrivers must undo all they did in bind() if they   fail it, but we may fail later and a deferred kevent   may trigger an error resubmitting itself and, worse,   schedule a timer. So we kill it all just in case. ", "void phy_print_status(struct phy_device *phydev)": "phy_print_status - Convenience function to print out the current phy status   @phydev: the phy_device struct ", "int phy_aneg_done(struct phy_device *phydev)": "phy_aneg_done - return auto-negotiation status   @phydev: target phy_device struct     Description: Return the auto-negotiation status from this @phydev   Returns > 0 on success or < 0 on error. 0 means that auto-negotiation   is still pending. ", "bool phy_check_valid(int speed, int duplex, unsigned long *features)": "phy_check_valid - check if there is a valid PHY setting which matches       speed, duplex, and feature mask   @speed: speed to match   @duplex: duplex to match   @features: A mask of the valid settings     Description: Returns true if there is a valid setting, false otherwise. ", "int phy_mii_ioctl(struct phy_device *phydev, struct ifreq *ifr, int cmd)": "phy_mii_ioctl - generic PHY MII ioctl interface   @phydev: the phy_device struct   @ifr: &struct ifreq for socket ioctl's   @cmd: ioctl cmd to execute     Note that this function is currently incompatible with the   PHYCONTROL layer.  It changes registers without regard to   current state.  Use at own risk. ", "int phy_do_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)": "phy_start_aneg(phydev);return 0;case SIOCSHWTSTAMP:if (phydev->mii_ts && phydev->mii_ts->hwtstamp)return phydev->mii_ts->hwtstamp(phydev->mii_ts, ifr);fallthrough;default:return -EOPNOTSUPP;}}EXPORT_SYMBOL(phy_mii_ioctl);     phy_do_ioctl - generic ndo_eth_ioctl implementation   @dev: the net_device struct   @ifr: &struct ifreq for socket ioctl's   @cmd: ioctl cmd to execute ", "int phy_do_ioctl_running(struct net_device *dev, struct ifreq *ifr, int cmd)": "phy_do_ioctl_running - generic ndo_eth_ioctl implementation but test first     @dev: the net_device struct   @ifr: &struct ifreq for socket ioctl's   @cmd: ioctl cmd to execute     Same as phy_do_ioctl, but ensures that net_device is running before   handling the ioctl. ", "void phy_queue_state_machine(struct phy_device *phydev, unsigned long jiffies)": "phy_queue_state_machine - Trigger the state machine to run soon     @phydev: the phy_device struct   @jiffies: Run the state machine after these jiffies ", "void phy_trigger_machine(struct phy_device *phydev)": "phy_trigger_machine - Trigger the state machine to run now     @phydev: the phy_device struct ", "int phy_ethtool_get_strings(struct phy_device *phydev, u8 *data)": "phy_ethtool_get_strings - Get the statistic counter names     @phydev: the phy_device struct   @data: Where to put the strings ", "int phy_ethtool_get_sset_count(struct phy_device *phydev)": "phy_ethtool_get_sset_count - Get the number of statistic counters     @phydev: the phy_device struct ", "int phy_ethtool_get_stats(struct phy_device *phydev,  struct ethtool_stats *stats, u64 *data)": "phy_ethtool_get_stats - Get the statistic counters     @phydev: the phy_device struct   @stats: What counters to get   @data: Where to store the counters ", "int phy_start_cable_test(struct phy_device *phydev, struct netlink_ext_ack *extack)": "phy_start_cable_test - Start a cable test     @phydev: the phy_device struct   @extack: extack for reporting useful error messages ", "int phy_start_cable_test_tdr(struct phy_device *phydev,     struct netlink_ext_ack *extack,     const struct phy_tdr_config *config)": "phy_start_cable_test_tdr - Start a raw TDR cable test     @phydev: the phy_device struct   @extack: extack for reporting useful error messages   @config: Configuration of the test to run ", "if (phydev->is_c45 && !(phydev->c45_ids.devices_in_package & BIT(0)))return genphy_c45_config_aneg(phydev);return genphy_config_aneg(phydev);}EXPORT_SYMBOL(phy_config_aneg": "phy_config_aneg(struct phy_device  phydev){if (phydev->drv->config_aneg)return phydev->drv->config_aneg(phydev);  Clause 45 PHYs that don't implement Clause 22 registers are not   allowed to call genphy_config_aneg() ", "linkmode_and(advertising, advertising, phydev->supported);/* Verify the settings we care about. ": "phy_ethtool_ksettings_set(struct phy_device  phydev,      const struct ethtool_link_ksettings  cmd){__ETHTOOL_DECLARE_LINK_MODE_MASK(advertising);u8 autoneg = cmd->base.autoneg;u8 duplex = cmd->base.duplex;u32 speed = cmd->base.speed;if (cmd->base.phy_address != phydev->mdio.addr)return -EINVAL;linkmode_copy(advertising, cmd->link_modes.advertising);  We make sure that we don't pass unsupported values in to the PHY ", "void phy_error(struct phy_device *phydev)": "phy_error_precise(struct phy_device  phydev,      const void  func, int err){WARN(1, \"%pS: returned: %d\\n\", func, err);phy_process_error(phydev);}     phy_error - enter ERROR state for this PHY device   @phydev: target phy_device struct     Moves the PHY to the ERROR state in response to a read   or write error, and tells the controller the link is down.   Must not be called from interrupt context, or while the   phydev->lock is held. ", "void phy_request_interrupt(struct phy_device *phydev)": "phy_request_interrupt - request and enable interrupt for a PHY device   @phydev: target phy_device struct     Description: Request and enable the interrupt for the given PHY.     If this fails, then we set irq to PHY_POLL.     This should only be called with a valid IRQ number. ", "void phy_free_interrupt(struct phy_device *phydev)": "phy_free_interrupt(phydev);phydev->irq = PHY_POLL;}}}EXPORT_SYMBOL(phy_request_interrupt);     phy_free_interrupt - disable and free interrupt for a PHY device   @phydev: target phy_device struct     Description: Disable and free the interrupt for the given PHY.     This should only be called with a valid IRQ number. ", "void phy_stop_machine(struct phy_device *phydev)": "phy_stop_machine - stop the PHY state machine tracking   @phydev: target phy_device struct     Description: Stops the state machine delayed workqueue, sets the     state to UP (unless it wasn't up yet). This function must be     called BEFORE phy_detach. ", "mutex_lock(&phydev->lock);if (phy_polling_mode(phydev) && phy_is_started(phydev))phy_queue_state_machine(phydev, PHY_STATE_TIME);mutex_unlock(&phydev->lock);}/** * phy_mac_interrupt - MAC says the link has changed * @phydev: phy_device struct with changed link * * The MAC layer is able to indicate there has been a change in the PHY link * status. Trigger the state machine and work a work queue. ": "phy_mac_interrupt().     In state PHY_HALTED the PHY gets suspended, so rescheduling the   state machine would be pointless and possibly error prone when   called from phy_disconnect() synchronously. ", "int phy_init_eee(struct phy_device *phydev, bool clk_stop_enable)": "phy_init_eee - init and check the EEE feature   @phydev: target phy_device struct   @clk_stop_enable: PHY may stop the clock during LPI     Description: it checks if the Energy-Efficient Ethernet (EEE)   is supported by looking at the MMD registers 3.20 and 7.6061   and it programs the MMD register 3.0 setting the \"Clock stop enable\"   bit if required. ", "int phy_get_eee_err(struct phy_device *phydev)": "phy_get_eee_err - report the EEE wake error count   @phydev: target phy_device struct     Description: it is to report the number of time where the PHY   failed to complete its normal wake sequence. ", "int phy_ethtool_get_eee(struct phy_device *phydev, struct ethtool_eee *data)": "phy_ethtool_get_eee - get EEE supported and status   @phydev: target phy_device struct   @data: ethtool_eee data     Description: it reportes the SupportedAdvertisementLP Advertisement   capabilities. ", "int phy_ethtool_set_eee(struct phy_device *phydev, struct ethtool_eee *data)": "phy_ethtool_set_eee - set EEE supported and status   @phydev: target phy_device struct   @data: ethtool_eee data     Description: it is to program the Advertisement EEE register. ", "int phy_ethtool_set_wol(struct phy_device *phydev, struct ethtool_wolinfo *wol)": "phy_ethtool_set_wol - Configure Wake On LAN     @phydev: target phy_device struct   @wol: Configuration requested ", "void phy_ethtool_get_wol(struct phy_device *phydev, struct ethtool_wolinfo *wol)": "phy_ethtool_get_wol - Get the current Wake On LAN configuration     @phydev: target phy_device struct   @wol: Store the current configuration here ", "int phy_ethtool_nway_reset(struct net_device *ndev)": "phy_ethtool_nway_reset - Restart auto negotiation   @ndev: Network device to restart autoneg for ", "mdiodev = kzalloc(sizeof(*mdiodev), GFP_KERNEL);if (!mdiodev)return ERR_PTR(-ENOMEM);mdiodev->dev.release = mdio_device_release;mdiodev->dev.parent = &bus->dev;mdiodev->dev.bus = &mdio_bus_type;mdiodev->device_free = mdio_device_free;mdiodev->device_remove = mdio_device_remove;mdiodev->bus = bus;mdiodev->addr = addr;dev_set_name(&mdiodev->dev, PHY_ID_FMT, bus->id, addr);device_initialize(&mdiodev->dev);return mdiodev;}EXPORT_SYMBOL(mdio_device_create": "mdio_device_create(struct mii_bus  bus, int addr){struct mdio_device  mdiodev;  We allocate the device, and initialize the default values ", "int mdio_device_register(struct mdio_device *mdiodev)": "mdio_device_remove;mdiodev->bus = bus;mdiodev->addr = addr;dev_set_name(&mdiodev->dev, PHY_ID_FMT, bus->id, addr);device_initialize(&mdiodev->dev);return mdiodev;}EXPORT_SYMBOL(mdio_device_create);     mdio_device_register - Register the mdio device on the MDIO bus   @mdiodev: mdio_device structure to be added to the MDIO bus ", "int mdio_driver_register(struct mdio_driver *drv)": "mdio_driver_register - register an mdio_driver with the MDIO layer   @drv: new mdio_driver to register ", "int genphy_c45_eee_is_active(struct phy_device *phydev, unsigned long *adv,     unsigned long *lp, bool *is_enabled)": "genphy_c45_eee_is_active - get EEE status   @phydev: target phy_device struct   @adv: variable to store advertised linkmodes   @lp: variable to store LP advertised linkmodes   @is_enabled: variable to store EEE enableddisabled configuration value     Description: this function will read local and link partner PHY   advertisements. Compare them return current EEE state. ", "int genphy_c45_ethtool_get_eee(struct phy_device *phydev,       struct ethtool_eee *data)": "genphy_c45_ethtool_get_eee - get EEE supported and status   @phydev: target phy_device struct   @data: ethtool_eee data     Description: it reports the SupportedAdvertisementLP Advertisement   capabilities. ", "int genphy_c45_ethtool_set_eee(struct phy_device *phydev,       struct ethtool_eee *data)": "genphy_c45_ethtool_set_eee - set EEE supported and status   @phydev: target phy_device struct   @data: ethtool_eee data     Description: sets the SupportedAdvertisementLP Advertisement   capabilities. If eee_enabled is false, no links modes are   advertised, but the previously advertised link modes are   retained. This allows EEE to be enableddisabled in a   non-destructive way. ", "mdio_device_reset(mdiodev, 1);}mdiodev->bus->mdio_map[mdiodev->addr] = mdiodev;return 0;}EXPORT_SYMBOL(mdiobus_register_device": "mdiobus_register_device(struct mdio_device  mdiodev){int err;if (mdiodev->bus->mdio_map[mdiodev->addr])return -EBUSY;if (mdiodev->flags & MDIO_DEVICE_FLAG_PHY) {err = mdiobus_register_gpiod(mdiodev);if (err)return err;err = mdiobus_register_reset(mdiodev);if (err)return err;  Assert the reset signal ", "struct mii_bus *mdiobus_alloc_size(size_t size)": "mdiobus_alloc_size - allocate a mii_bus structure   @size: extra amount of memory to allocate for private storage.   If non-zero, then bus->priv is points to that memory.     Description: called by a bus driver to allocate an mii_bus   structure to fill in. ", "struct mii_bus *mdio_find_bus(const char *mdio_name)": "mdio_find_bus - Given the name of a mdiobus, find the mii_bus.   @mdio_name: The name of a mdiobus.     Returns a reference to the mii_bus, or NULL if none found.  The   embedded struct device will have its reference count incremented,   and this must be put_deviced'ed once the bus is finished with. ", "struct mii_bus *of_mdio_find_bus(struct device_node *mdio_bus_np)": "of_mdio_find_bus - Given an mii_bus node, find the mii_bus.   @mdio_bus_np: Pointer to the mii_bus.     Returns a reference to the mii_bus, or NULL if none found.  The   embedded struct device will have its reference count incremented,   and this must be put once the bus is finished with.     Because the association of a device_node and mii_bus is made via   of_mdiobus_register(), the mii_bus cannot be found before it is   registered with of_mdiobus_register().   ", "struct phy_device *mdiobus_scan_c22(struct mii_bus *bus, int addr)": "mdiobus_scan_c22 - scan one address on a bus for C22 MDIO devices.   @bus: mii_bus to scan   @addr: address on bus to scan     This function scans one address on the MDIO bus, looking for   devices which can be identified using a vendorproduct ID in   registers 2 and 3. Not all MDIO devices have such registers, but   PHY devices typically do. Hence this function assumes anything   found is a PHY, or can be treated as a PHY. Other MDIO devices,   such as switches, will probably not be found during the scan. ", "int __mdiobus_register(struct mii_bus *bus, struct module *owner)": "__mdiobus_register - bring up all the PHYs on a given bus and attach them to bus   @bus: target mii_bus   @owner: module containing bus accessor functions     Description: Called by a bus driver to bring up all the PHYs     on a given bus, and attach them to the bus. Drivers should use     mdiobus_register() rather than __mdiobus_register() unless they     need to pass a specific owner module. MDIO devices which are not     PHYs will not be brought up by this function. They are expected     to be explicitly listed in DT and instantiated by of_mdiobus_register().     Returns 0 on success or < 0 on error. ", "bus->state = MDIOBUS_UNREGISTERED;err = device_register(&bus->dev);if (err) ": "mdiobus_free()     State will be updated later in this function in case of success ", "int __mdiobus_read(struct mii_bus *bus, int addr, u32 regnum)": "mdiobus_read - Unlocked version of the mdiobus_read function   @bus: the mii_bus struct   @addr: the phy address   @regnum: register number to read     Read a MDIO bus register. Caller must hold the mdio bus lock.     NOTE: MUST NOT be called from interrupt context. ", "int __mdiobus_write(struct mii_bus *bus, int addr, u32 regnum, u16 val)": "mdiobus_write - Unlocked version of the mdiobus_write function   @bus: the mii_bus struct   @addr: the phy address   @regnum: register number to write   @val: value to write to @regnum     Write a MDIO bus register. Caller must hold the mdio bus lock.     NOTE: MUST NOT be called from interrupt context. ", "int __mdiobus_c45_read(struct mii_bus *bus, int addr, int devad, u32 regnum)": "mdiobus_c45_read - Unlocked version of the mdiobus_c45_read function   @bus: the mii_bus struct   @addr: the phy address   @devad: device address to read   @regnum: register number to read     Read a MDIO bus register. Caller must hold the mdio bus lock.     NOTE: MUST NOT be called from interrupt context. ", "int __mdiobus_c45_write(struct mii_bus *bus, int addr, int devad, u32 regnum,u16 val)": "mdiobus_c45_write - Unlocked version of the mdiobus_write function   @bus: the mii_bus struct   @addr: the phy address   @devad: device address to read   @regnum: register number to write   @val: value to write to @regnum     Write a MDIO bus register. Caller must hold the mdio bus lock.     NOTE: MUST NOT be called from interrupt context. ", "int mdiobus_read_nested(struct mii_bus *bus, int addr, u32 regnum)": "mdiobus_read_nested - Nested version of the mdiobus_read function   @bus: the mii_bus struct   @addr: the phy address   @regnum: register number to read     In case of nested MDIO bus access avoid lockdep false positives by   using mutex_lock_nested().     NOTE: MUST NOT be called from interrupt context,   because the bus readwrite functions may wait for an interrupt   to conclude the operation. ", "int mdiobus_c45_read_nested(struct mii_bus *bus, int addr, int devad,    u32 regnum)": "mdiobus_c45_read_nested - Nested version of the mdiobus_c45_read function   @bus: the mii_bus struct   @addr: the phy address   @devad: device address to read   @regnum: register number to read     In case of nested MDIO bus access avoid lockdep false positives by   using mutex_lock_nested().     NOTE: MUST NOT be called from interrupt context,   because the bus readwrite functions may wait for an interrupt   to conclude the operation. ", "int mdiobus_write_nested(struct mii_bus *bus, int addr, u32 regnum, u16 val)": "mdiobus_write_nested - Nested version of the mdiobus_write function   @bus: the mii_bus struct   @addr: the phy address   @regnum: register number to write   @val: value to write to @regnum     In case of nested MDIO bus access avoid lockdep false positives by   using mutex_lock_nested().     NOTE: MUST NOT be called from interrupt context,   because the bus readwrite functions may wait for an interrupt   to conclude the operation. ", "int mdiobus_c45_write_nested(struct mii_bus *bus, int addr, int devad,     u32 regnum, u16 val)": "mdiobus_c45_write_nested - Nested version of the mdiobus_c45_write function   @bus: the mii_bus struct   @addr: the phy address   @devad: device address to read   @regnum: register number to write   @val: value to write to @regnum     In case of nested MDIO bus access avoid lockdep false positives by   using mutex_lock_nested().     NOTE: MUST NOT be called from interrupt context,   because the bus readwrite functions may wait for an interrupt   to conclude the operation. ", "int register_mii_tstamp_controller(struct device *device,   struct mii_timestamping_ctrl *ctrl)": "register_mii_tstamp_controller() - registers an MII time stamping device.     @device:The device to be registered.   @ctrl:Pointer to device's control interface.     Returns zero on success or non-zero on failure. ", "void unregister_mii_tstamp_controller(struct device *device)": "unregister_mii_tstamp_controller() - unregisters an MII time stamping device.     @device:A device previously passed to register_mii_tstamp_controller(). ", "struct mii_timestamper *register_mii_timestamper(struct device_node *node, unsigned int port)": "register_mii_timestamper - Enables a given port of an MII time stamper.     @node:The device tree node of the MII time stamp controller.   @port:The index of the port to be enabled.     Returns a valid interface on success or ERR_PTR otherwise. ", "void unregister_mii_timestamper(struct mii_timestamper *mii_ts)": "unregister_mii_timestamper - Disables a given MII time stamper.     @mii_ts:An interface obtained via register_mii_timestamper().   ", "void phy_set_max_speed(struct phy_device *phydev, u32 max_speed)": "phy_set_max_speed - Set the maximum speed the PHY should support     @phydev: The phy_device struct   @max_speed: Maximum speed     The PHY might be more capable than the MAC. For example a Fast Ethernet   is connected to a 1G PHY. This function allows the MAC to indicate its   maximum speed, and so limit what the PHY will advertise. ", "int __phy_read_mmd(struct phy_device *phydev, int devad, u32 regnum)": "phy_read_mmd - Convenience function for reading a register   from an MMD on a given PHY.   @phydev: The phy_device struct   @devad: The MMD to read from (0..31)   @regnum: The register on the MMD to read (0..65535)     Same rules as for __phy_read(); ", "int __phy_write_mmd(struct phy_device *phydev, int devad, u32 regnum, u16 val)": "phy_write_mmd - Convenience function for writing a register   on an MMD on a given PHY.   @phydev: The phy_device struct   @devad: The MMD to read from   @regnum: The register on the MMD to read   @val: value to write to @regnum     Same rules as for __phy_write(); ", "int phy_read_paged(struct phy_device *phydev, int page, u32 regnum)": "phy_read_paged() - Convenience function for reading a paged register   @phydev: a pointer to a &struct phy_device   @page: the page for the phy   @regnum: register number     Same rules as for phy_read(). ", "int phy_write_paged(struct phy_device *phydev, int page, u32 regnum, u16 val)": "phy_write_paged() - Convenience function for writing a paged register   @phydev: a pointer to a &struct phy_device   @page: the page for the phy   @regnum: register number   @val: value to write     Same rules as for phy_write(). ", "int phy_modify_paged_changed(struct phy_device *phydev, int page, u32 regnum,     u16 mask, u16 set)": "phy_modify_paged_changed() - Function for modifying a paged register   @phydev: a pointer to a &struct phy_device   @page: the page for the phy   @regnum: register number   @mask: bit mask of bits to clear   @set: bit mask of bits to set     Returns negative errno, 0 if there was no change, and 1 in case of change ", "int phy_register_fixup(const char *bus_id, u32 phy_uid, u32 phy_uid_mask,       int (*run)(struct phy_device *))": "phy_register_fixup - creates a new phy_fixup and adds it to the list   @bus_id: A string which matches phydev->mdio.dev.bus_id (or PHY_ANY_ID)   @phy_uid: Used to match against phydev->phy_id (the UID of the PHY)  It can also be PHY_ANY_UID   @phy_uid_mask: Applied to phydev->phy_id and fixup->phy_uid before  comparison   @run: The actual code to be run when a matching PHY is found ", "int phy_unregister_fixup(const char *bus_id, u32 phy_uid, u32 phy_uid_mask)": "phy_unregister_fixup - remove a phy_fixup from the list   @bus_id: A string matches fixup->bus_id (or PHY_ANY_ID) in phy_fixup_list   @phy_uid: A phy id matches fixup->phy_id (or PHY_ANY_UID) in phy_fixup_list   @phy_uid_mask: Applied to phy_uid and fixup->phy_uid before comparison ", "dev = kzalloc(sizeof(*dev), GFP_KERNEL);if (!dev)return ERR_PTR(-ENOMEM);mdiodev = &dev->mdio;mdiodev->dev.parent = &bus->dev;mdiodev->dev.bus = &mdio_bus_type;mdiodev->dev.type = &mdio_bus_phy_type;mdiodev->bus = bus;mdiodev->bus_match = phy_bus_match;mdiodev->addr = addr;mdiodev->flags = MDIO_DEVICE_FLAG_PHY;mdiodev->device_free = phy_mdio_device_free;mdiodev->device_remove = phy_mdio_device_remove;dev->speed = SPEED_UNKNOWN;dev->duplex = DUPLEX_UNKNOWN;dev->pause = 0;dev->asym_pause = 0;dev->link = 0;dev->port = PORT_TP;dev->interface = PHY_INTERFACE_MODE_GMII;dev->autoneg = AUTONEG_ENABLE;dev->pma_extable = -ENODATA;dev->is_c45 = is_c45;dev->phy_id = phy_id;if (c45_ids)dev->c45_ids = *c45_ids;dev->irq = bus->irq[addr];dev_set_name(&mdiodev->dev, PHY_ID_FMT, bus->id, addr);device_initialize(&mdiodev->dev);dev->state = PHY_DOWN;INIT_LIST_HEAD(&dev->leds);mutex_init(&dev->lock);INIT_DELAYED_WORK(&dev->state_queue, phy_state_machine);/* Request the appropriate module unconditionally; don't * bother trying to do so only if it isn't already loaded, * because that gets complicated. A hotplug event would have * done an unconditional modprobe anyway. * We don't do normal hotplug because it won't work for MDIO * -- because it relies on the device staying around for long * enough for the driver to get loaded. With MDIO, the NIC * driver will get bored and give up as soon as it finds that * there's no driver _already_ loaded. ": "phy_device_create(struct mii_bus  bus, int addr, u32 phy_id,     bool is_c45,     struct phy_c45_device_ids  c45_ids){struct phy_device  dev;struct mdio_device  mdiodev;int ret = 0;  We allocate the device, and initialize the default values ", "struct phy_device *get_phy_device(struct mii_bus *bus, int addr, bool is_c45)": "get_phy_device - reads the specified PHY device and returns its @phy_device      struct   @bus: the target MII bus   @addr: PHY address on the MII bus   @is_c45: If true the PHY uses the 802.3 clause 45 protocol     Probe for a PHY at @addr on @bus.     When probing for a clause 22 PHY, then read the ID registers. If we find   a valid ID, allocate and return a &struct phy_device.     When probing for a clause 45 PHY, read the \"devices in package\" registers.   If the \"devices in package\" appears valid, read the ID registers for each   MMD, allocate and return a &struct phy_device.     Returns an allocated &struct phy_device on success, %-ENODEV if there is   no PHY present, or %-EIO on bus access error. ", "int phy_device_register(struct phy_device *phydev)": "phy_device_register - Register the phy device on the MDIO bus   @phydev: phy_device structure to be added to the MDIO bus ", "if (!netdev)goto out;if (netdev->wol_enabled)return false;/* As long as not all affected network drivers support the * wol_enabled flag, let's check for hints that WoL is enabled. * Don't suspend PHY if the attached netdev parent may wake up. * The parent may point to a PCI device, as in tg3 driver. ": "phy_suspend() because the parent netdev might be the   MDIO bus driver and clock gated at this point. ", "int phy_get_c45_ids(struct phy_device *phydev)": "phy_get_c45_ids - Read 802.3-c45 IDs for phy device.   @phydev: phy_device structure to read 802.3-c45 IDs     Returns zero on success, %-EIO on bus access error, or %-ENODEV if   the \"devices in package\" is invalid. ", "struct phy_device *phy_find_first(struct mii_bus *bus)": "phy_find_first - finds the first PHY device on the bus   @bus: the target MII bus ", "int phy_connect_direct(struct net_device *dev, struct phy_device *phydev,       void (*handler)(struct net_device *),       phy_interface_t interface)": "phy_connect_direct - connect an ethernet device to a specific phy_device   @dev: the network device to connect   @phydev: the pointer to the phy device   @handler: callback function for state change notifications   @interface: PHY device's interface ", "if (phydev->irq_rerun) ": "phy_init_hw(phydev);if (ret < 0)return ret;ret = phy_resume(phydev);if (ret < 0)return ret;no_resume:if (phy_interrupt_is_valid(phydev)) {phydev->irq_suspended = 0;synchronize_irq(phydev->irq);  Rerun interrupts which were postponed by phy_interrupt()   because they occurred during the system sleep transition. ", "void phy_sfp_attach(void *upstream, struct sfp_bus *bus)": "phy_sfp_attach - attach the SFP bus to the PHY upstream network device   @upstream: pointer to the phy device   @bus: sfp bus representing cage being attached     This is used to fill in the sfp_upstream_ops .attach member. ", "void phy_sfp_detach(void *upstream, struct sfp_bus *bus)": "phy_sfp_detach - detach the SFP bus from the PHY upstream network device   @upstream: pointer to the phy device   @bus: sfp bus representing cage being attached     This is used to fill in the sfp_upstream_ops .detach member. ", "int phy_sfp_probe(struct phy_device *phydev,  const struct sfp_upstream_ops *ops)": "phy_sfp_probe - probe for a SFP cage attached to this PHY device   @phydev: Pointer to phy_device   @ops: SFP's upstream operations ", "struct phy_device *phy_connect(struct net_device *dev, const char *bus_id,       void (*handler)(struct net_device *),       phy_interface_t interface)": "phy_attach_direct(dev, phydev, phydev->dev_flags, interface);if (rc)return rc;phy_prepare_link(phydev, handler);if (phy_interrupt_is_valid(phydev))phy_request_interrupt(phydev);return 0;}EXPORT_SYMBOL(phy_connect_direct);     phy_connect - connect an ethernet device to a PHY device   @dev: the network device to connect   @bus_id: the id string of the PHY device to connect   @handler: callback function for state change notifications   @interface: PHY device's interface     Description: Convenience function for connecting ethernet     devices to PHY devices.  The default behavior is for     the PHY infrastructure to handle everything, and only notify     the connected driver when the link status changes.  If you     don't want, or can't use the provided functionality, you may     choose to call only the subset of functions which provide     the desired functionality. ", "WARN_ON(phydev->state != PHY_HALTED && phydev->state != PHY_READY &&phydev->state != PHY_UP);ret = phy_init_hw(phydev);if (ret < 0)return ret;ret = phy_resume(phydev);if (ret < 0)return ret;no_resume:if (phy_interrupt_is_valid(phydev)) ": "phy_resume(struct device  dev){struct phy_device  phydev = to_phy_device(dev);int ret;if (phydev->mac_managed_pm)return 0;if (!phydev->suspended_by_mdio_bus)goto no_resume;phydev->suspended_by_mdio_bus = 0;  If we managed to get here with the PHY state machine in a state   neither PHY_HALTED, PHY_READY nor PHY_UP, this is an indication   that something went wrong and we should most likely be using   MAC managed PM, but we are not. ", "int phy_reset_after_clk_enable(struct phy_device *phydev)": "genphy_loopback(phydev, enable);if (ret)goto out;phydev->loopback_enabled = enable;out:mutex_unlock(&phydev->lock);return ret;}EXPORT_SYMBOL(phy_loopback);     phy_reset_after_clk_enable - perform a PHY reset if needed   @phydev: target phy_device struct     Description: Some PHYs are known to need a reset after their refclk was     enabled. This function evaluates the flags and perform the reset if it's     needed. Returns < 0 on error, 0 if the phy wasn't reset and 1 if the phy     was reset. ", "int genphy_config_eee_advert(struct phy_device *phydev)": "genphy_config_eee_advert - disable unwanted eee mode advertisement   @phydev: target phy_device struct     Description: Writes MDIO_AN_EEE_ADV after disabling unsupported energy     efficent ethernet modes. Returns 0 if the PHY's advertisement hasn't     changed, and 1 if it has changed. ", "int genphy_setup_forced(struct phy_device *phydev)": "genphy_setup_forced - configuresforces speedduplex from @phydev   @phydev: target phy_device struct     Description: Configures MII_BMCR to force speedduplex     to the values in phydev. Assumes that the values are valid.     Please see phy_sanitize_settings(). ", "if (val & LPA_1000MSRES)state = MASTER_SLAVE_STATE_MASTER;elsestate = MASTER_SLAVE_STATE_SLAVE;} else ": "genphy_read_master_slave(struct phy_device  phydev){int cfg, state;int val;phydev->master_slave_get = MASTER_SLAVE_CFG_UNKNOWN;phydev->master_slave_state = MASTER_SLAVE_STATE_UNKNOWN;val = phy_read(phydev, MII_CTRL1000);if (val < 0)return val;if (val & CTL1000_ENABLE_MASTER) {if (val & CTL1000_AS_MASTER)cfg = MASTER_SLAVE_CFG_MASTER_FORCE;elsecfg = MASTER_SLAVE_CFG_SLAVE_FORCE;} else {if (val & CTL1000_PREFER_MASTER)cfg = MASTER_SLAVE_CFG_MASTER_PREFERRED;elsecfg = MASTER_SLAVE_CFG_SLAVE_PREFERRED;}val = phy_read(phydev, MII_STAT1000);if (val < 0)return val;if (val & LPA_1000MSFAIL) {state = MASTER_SLAVE_STATE_ERR;} else if (phydev->link) {  this bits are valid only for active link ", "int genphy_restart_aneg(struct phy_device *phydev)": "genphy_restart_aneg - Enable and Restart Autonegotiation   @phydev: target phy_device struct ", "int genphy_check_and_restart_aneg(struct phy_device *phydev, bool restart)": "genphy_check_and_restart_aneg - Enable and restart auto-negotiation   @phydev: target phy_device struct   @restart: whether aneg restart is requested     Check, and restart auto-negotiation if needed. ", "int __genphy_config_aneg(struct phy_device *phydev, bool changed)": "__genphy_config_aneg - restart auto-negotiation or write BMCR   @phydev: target phy_device struct   @changed: whether autoneg is requested     Description: If auto-negotiation is enabled, we configure the     advertising, and then restart auto-negotiation.  If it is not     enabled, then we write the BMCR. ", "int genphy_c37_config_aneg(struct phy_device *phydev)": "genphy_c37_config_aneg - restart auto-negotiation or write BMCR   @phydev: target phy_device struct     Description: If auto-negotiation is enabled, we configure the     advertising, and then restart auto-negotiation.  If it is not     enabled, then we write the BMCR. This function is intended     for use with Clause 37 1000Base-X mode. ", "int genphy_aneg_done(struct phy_device *phydev)": "genphy_aneg_done - return auto-negotiation status   @phydev: target phy_device struct     Description: Reads the status register and returns 0 either if     auto-negotiation is incomplete, or if there was an error.     Returns BMSR_ANEGCOMPLETE if auto-negotiation is done. ", "int genphy_update_link(struct phy_device *phydev)": "genphy_update_link - update link status in @phydev   @phydev: target phy_device struct     Description: Update the value in phydev->link to reflect the     current link value.  In order to do this, we need to read     the status register twice, keeping the second value. ", "int genphy_read_status_fixed(struct phy_device *phydev)": "genphy_read_status_fixed - read the link parameters for !aneg mode   @phydev: target phy_device struct     Read the current duplex and speed state for a PHY operating with   autonegotiation disabled. ", "int genphy_c37_read_status(struct phy_device *phydev)": "genphy_c37_read_status - check the link status and update current link state   @phydev: target phy_device struct     Description: Check the link, then figure out the current state     by comparing what we advertise with what the link partner     advertises. This function is for Clause 37 1000Base-X mode. ", "if (!ret)phydev->suspended = 0;}if (ret < 0)return ret;ret = phy_scan_fixups(phydev);if (ret < 0)return ret;if (phydev->drv->config_init) ": "genphy_soft_reset for an explanation ", "phy_trigger_machine(phydev);return 0;}EXPORT_SYMBOL(genphy_handle_interrupt_no_ack": "genphy_handle_interrupt_no_ack(struct phy_device  phydev){  It seems there are cases where the interrupts are handled by another   entity (ie an IRQ controller embedded inside the PHY) and do not   need any other interraction from phylib. In this case, just trigger   the state machine directly. ", "int genphy_read_abilities(struct phy_device *phydev)": "genphy_read_abilities - read PHY abilities from Clause 22 registers   @phydev: target phy_device struct     Description: Reads the PHY's abilities and populates   phydev->supported accordingly.     Returns: 0 on success, < 0 on failure ", "void phy_remove_link_mode(struct phy_device *phydev, u32 link_mode)": "phy_remove_link_mode - Remove a supported link mode   @phydev: phy_device structure to remove link mode from   @link_mode: Link mode to be removed     Description: Some MACs don't support all link modes which the PHY   does.  e.g. a 1G MAC often does not support 1000Half. Add a helper   to remove a link mode. ", "void phy_advertise_supported(struct phy_device *phydev)": "phy_advertise_supported(phydev);}EXPORT_SYMBOL(phy_remove_link_mode);static void phy_copy_pause_bits(unsigned long  dst, unsigned long  src){linkmode_mod_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT, dst,linkmode_test_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT, src));linkmode_mod_bit(ETHTOOL_LINK_MODE_Pause_BIT, dst,linkmode_test_bit(ETHTOOL_LINK_MODE_Pause_BIT, src));}     phy_advertise_supported - Advertise all supported modes   @phydev: target phy_device struct     Description: Called to advertise all supported modes, doesn't touch   pause mode advertising. ", "void phy_support_sym_pause(struct phy_device *phydev)": "phy_support_sym_pause - Enable support of symmetrical pause   @phydev: target phy_device struct     Description: Called by the MAC to indicate is supports symmetrical   Pause, but not asym pause. ", "void phy_support_asym_pause(struct phy_device *phydev)": "phy_support_asym_pause - Enable support of asym pause   @phydev: target phy_device struct     Description: Called by the MAC to indicate is supports Asym Pause. ", "void phy_set_sym_pause(struct phy_device *phydev, bool rx, bool tx,       bool autoneg)": "phy_set_sym_pause - Configure symmetric Pause   @phydev: target phy_device struct   @rx: Receiver Pause is supported   @tx: Transmit Pause is supported   @autoneg: Auto neg should be used     Description: Configure advertised Pause support depending on if   receiver pause and pause auto neg is supported. Generally called   from the set_pauseparam .ndo. ", "void phy_set_asym_pause(struct phy_device *phydev, bool rx, bool tx)": "phy_set_asym_pause - Configure Pause and Asym Pause   @phydev: target phy_device struct   @rx: Receiver Pause is supported   @tx: Transmit Pause is supported     Description: Configure advertised Pause support depending on if   transmit and receiver pause is supported. If there has been a   change in adverting, trigger a new autoneg. Generally called from   the set_pauseparam .ndo. ", "bool phy_validate_pause(struct phy_device *phydev,struct ethtool_pauseparam *pp)": "phy_validate_pause - Test if the PHYMAC support the pause configuration   @phydev: phy_device struct   @pp: requested pause configuration     Description: Test if the PHYMAC combination supports the Pause   configuration the user is requesting. Returns True if it is   supported, false otherwise. ", "void phy_get_pause(struct phy_device *phydev, bool *tx_pause, bool *rx_pause)": "phy_get_pause - resolve negotiated pause modes   @phydev: phy_device struct   @tx_pause: pointer to bool to indicate whether transmit pause should be   enabled.   @rx_pause: pointer to bool to indicate whether receive pause should be   enabled.     Resolve and return the flow control modes according to the negotiation   result. This includes checking that we are operating in full duplex mode.   See linkmode_resolve_pause() for further details. ", "s32 phy_get_internal_delay(struct phy_device *phydev, struct device *dev,   const int *delay_values, int size, bool is_rx)": "phy_get_internal_delay - returns the index of the internal delay   @phydev: phy_device struct   @dev: pointer to the devices device struct   @delay_values: array of delays the PHY supports   @size: the size of the delay array   @is_rx: boolean to indicate to get the rx internal delay     Returns the index within the array of internal delay passed in.   If the device property is not present then the interface type is checked   if the interface defines use of internal delay then a 1 is returned otherwise   a 0 is returned.   The array must be in ascending order. If PHY does not have an ascending order   array then size = 0 and the value of the delay property is returned.   Return -EINVAL if the delay is invalid or cannot be found. ", "struct mdio_device *fwnode_mdio_find_device(struct fwnode_handle *fwnode)": "fwnode_mdio_find_device - Given a fwnode, find the mdio_device   @fwnode: pointer to the mdio_device's fwnode     If successful, returns a pointer to the mdio_device with the embedded   struct device refcount incremented by one, or NULL on failure.   The caller should call put_device() on the mdio_device after its use. ", "struct phy_device *fwnode_phy_find_device(struct fwnode_handle *phy_fwnode)": "fwnode_phy_find_device - For provided phy_fwnode, find phy_device.     @phy_fwnode: Pointer to the phy's fwnode.     If successful, returns a pointer to the phy_device with the embedded   struct device refcount incremented by one, or NULL on failure. ", "int phy_driver_register(struct phy_driver *new_driver, struct module *owner)": "phy_driver_register - register a phy_driver with the PHY layer   @new_driver: new phy_driver to register   @owner: module owning this PHY ", "struct mii_bus *devm_mdiobus_alloc_size(struct device *dev, int sizeof_priv)": "devm_mdiobus_alloc_size - Resource-managed mdiobus_alloc_size()   @dev:Device to allocate mii_bus for   @sizeof_priv:Space to allocate for private structure     Managed mdiobus_alloc_size. mii_bus allocated with this function is   automatically freed on driver detach.     RETURNS:   Pointer to allocated mii_bus on success, NULL on out-of-memory error. ", "int __devm_mdiobus_register(struct device *dev, struct mii_bus *bus,    struct module *owner)": "__devm_mdiobus_register - Resource-managed variant of mdiobus_register()   @dev:Device to register mii_bus for   @bus:MII bus structure to register   @owner:Owning module     Returns 0 on success, negative error number on failure. ", "int __devm_of_mdiobus_register(struct device *dev, struct mii_bus *mdio,       struct device_node *np, struct module *owner)": "__devm_of_mdiobus_register - Resource managed variant of of_mdiobus_register()   @dev:Device to register mii_bus for   @mdio:MII bus structure to register   @np:Device node to parse   @owner:Owning module ", "void mdiobus_setup_mdiodev_from_board_info(struct mii_bus *bus,   int (*cb)   (struct mii_bus *bus,    struct mdio_board_info *bi))": "mdiobus_setup_mdiodev_from_board_info - create and setup MDIO devices   from pre-collected board specific MDIO information   @bus: Bus the board_info belongs to   @cb: Callback to create device on bus   Context: can sleep ", "int mdiobus_register_board_info(const struct mdio_board_info *info,unsigned int n)": "mdiobus_register_board_info - register MDIO devices for a given board   @info: array of devices descriptors   @n: number of descriptors provided   Context: can sleep     The board info passed can be marked with __initdata but be pointers   such as platform_data etc. are copied as-is ", "pcs_np = of_get_parent(np);if (!pcs_np)return ERR_PTR(-ENODEV);if (!of_device_is_available(pcs_np)) ": "miic_create(struct device  dev, struct device_node  np){struct platform_device  pdev;struct miic_port  miic_port;struct device_node  pcs_np;struct miic  miic;u32 port;if (!of_device_is_available(np))return ERR_PTR(-ENODEV);if (of_property_read_u32(np, \"reg\", &port))return ERR_PTR(-EINVAL);if (port > MIIC_MAX_NR_PORTS || port < 1)return ERR_PTR(-EINVAL);  The PCS pdev is attached to the parent node ", "mdio_device_put(mdio);return pcs;}EXPORT_SYMBOL(lynx_pcs_create_mdiodev": "lynx_pcs_create_mdiodev(struct mii_bus  bus, int addr){struct mdio_device  mdio;struct phylink_pcs  pcs;mdio = mdio_device_create(bus, addr);if (IS_ERR(mdio))return ERR_CAST(mdio);pcs = lynx_pcs_create(mdio);  lynx_create() has taken a refcount on the mdiodev if it was   successful. If lynx_create() fails, this will free the mdio   device here. In any case, we don't need to hold our reference   anymore, and putting it here will allow mdio_device_put() in   lynx_destroy() to automatically free the mdio device. ", "}EXPORT_SYMBOL(qed_put_eth_ops": "qed_put_eth_ops(void){  TODO - reference count for module? ", "if (edev->rdma_info.qedr_dev && !edev->rdma_info.exp_recovery)_qede_rdma_dev_remove(edev);}qedr_drv = NULL;mutex_unlock(&qedr_dev_list_lock);}EXPORT_SYMBOL(qede_rdma_unregister_driver": "qede_rdma_unregister_driver(struct qedr_driver  drv){struct qede_dev  edev;mutex_lock(&qedr_dev_list_lock);list_for_each_entry(edev, &qedr_dev_list, rdma_info.entry) {  If device has experienced recovery it was already removed ", "cq->tasklet_ctx.priv = &eq->tasklet_ctx;INIT_LIST_HEAD(&cq->tasklet_ctx.list);/* Add to comp EQ CQ tree to recv comp events ": "mlx5_create_cq(struct mlx5_core_dev  dev, struct mlx5_core_cq  cq,   u32  in, int inlen, u32  out, int outlen){int eqn = MLX5_GET(cqc, MLX5_ADDR_OF(create_cq_in, in, cq_context),   c_eqn_or_apu_element);u32 din[MLX5_ST_SZ_DW(destroy_cq_in)] = {};struct mlx5_eq_comp  eq;int err;eq = mlx5_eqn2comp_eq(dev, eqn);if (IS_ERR(eq))return PTR_ERR(eq);memset(out, 0, outlen);MLX5_SET(create_cq_in, in, opcode, MLX5_CMD_OP_CREATE_CQ);err = mlx5_cmd_do(dev, in, inlen, out, outlen);if (err)return err;cq->cqn = MLX5_GET(create_cq_out, out, cqn);cq->cons_index = 0;cq->arm_sn     = 0;cq->eq         = eq;cq->uid = MLX5_GET(create_cq_in, in, uid);refcount_set(&cq->refcount, 1);init_completion(&cq->free);if (!cq->comp)cq->comp = mlx5_add_cq_to_tasklet;  assuming CQ will be deleted before the EQ ", "if (ft_attr->level >= fs_prio->num_levels) ": "mlx5_create_flow_table(struct mlx5_flow_namespace  ns,struct mlx5_flow_table_attr  ft_attr,enum fs_flow_table_op_mod op_mod,u16 vport){struct mlx5_flow_root_namespace  root = find_root(&ns->node);bool unmanaged = ft_attr->flags & MLX5_FLOW_TABLE_UNMANAGED;struct mlx5_flow_table  next_ft;struct fs_prio  fs_prio = NULL;struct mlx5_flow_table  ft;int err;if (!root) {pr_err(\"mlx5: flow steering failed to find root of namespace\\n\");return ERR_PTR(-ENODEV);}mutex_lock(&root->chain_lock);fs_prio = find_prio(ns, ft_attr->prio);if (!fs_prio) {err = -EINVAL;goto unlock_root;}if (!unmanaged) {  The level is related to the   priority level range. ", "if (autogroups_max_fte / (max_num_groups + 1) > MAX_FLOW_GROUP_SIZE)max_num_groups = (autogroups_max_fte / MAX_FLOW_GROUP_SIZE) - 1;ft->autogroup.active = true;ft->autogroup.required_groups = max_num_groups;ft->autogroup.max_fte = autogroups_max_fte;/* We save place for flow groups in addition to max types ": "mlx5_create_auto_grouped_flow_table(struct mlx5_flow_namespace  ns,    struct mlx5_flow_table_attr  ft_attr){int num_reserved_entries = ft_attr->autogroup.num_reserved_entries;int max_num_groups = ft_attr->autogroup.max_num_groups;struct mlx5_flow_table  ft;int autogroups_max_fte;ft = mlx5_create_flow_table(ns, ft_attr);if (IS_ERR(ft))return ft;autogroups_max_fte = ft->max_fte - num_reserved_entries;if (max_num_groups > autogroups_max_fte)goto err_validate;if (num_reserved_entries > ft->max_fte)goto err_validate;  Align the number of groups according to the largest group size ", "err = build_match_list(&match_head, ft, spec, flow_act->fg, take_write);if (err) ": "mlx5_add_flow_rules(struct mlx5_flow_table  ft,     const struct mlx5_flow_spec  spec,     struct mlx5_flow_act  flow_act,     struct mlx5_flow_destination  dest,     int dest_num){struct mlx5_flow_steering  steering = get_steering(&ft->node);struct mlx5_flow_handle  rule;struct match_list match_head;struct mlx5_flow_group  g;bool take_write = false;struct fs_fte  fte;int version;int err;int i;if (!check_valid_spec(spec))return ERR_PTR(-EINVAL);if (flow_act->fg && ft->autogroup.active)return ERR_PTR(-EINVAL);if (dest && dest_num <= 0)return ERR_PTR(-EINVAL);for (i = 0; i < dest_num; i++) {if (!dest_is_valid(&dest[i], flow_act, ft))return ERR_PTR(-EINVAL);}nested_down_read_ref_node(&ft->node, FS_LOCK_GRANDPARENT);search_again_locked:version = atomic_read(&ft->node.version);  Collect all fgs which has a matching match_criteria ", "fs_get_obj(fte, handle->rule[0]->node.parent);down_write_ref_node(&fte->node, false);for (i = handle->num_rules - 1; i >= 0; i--)tree_remove_node(&handle->rule[i]->node, true);if (list_empty(&fte->node.children)) ": "mlx5_del_flow_rules(struct mlx5_flow_handle  handle){struct fs_fte  fte;int i;  In order to consolidate the HW changes we lock the FTE for other   changes, and increase its refcount, in order not to perform the   \"del\" functions of the FTE. Will handle them here.   The removal of the rules is done under locked FTE.   After removing all the handle's rules, if there are remaining   rules, it means we just need to modify the FTE in FW, and   unlockdecrease the refcount we increased before.   Otherwise, it means the FTE should be deleted. First delete the   FTE in FW. Then, unlock the FTE, and proceed the tree_put_node of   the FTE, which will handle the last decrease of the refcount, as   well as required handling of its parent. ", "tree_init_node(&rule->node, NULL, del_sw_hw_rule);if (dest &&    dest[i].type != MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE)list_add(&rule->node.list, &fte->node.children);elselist_add_tail(&rule->node.list, &fte->node.children);if (dest) ": "mlx5_destroy_flow_table(ft);return ERR_PTR(-ENOSPC);}EXPORT_SYMBOL(mlx5_create_auto_grouped_flow_table);struct mlx5_flow_group  mlx5_create_flow_group(struct mlx5_flow_table  ft,       u32  fg_in){struct mlx5_flow_root_namespace  root = find_root(&ft->node);void  match_criteria = MLX5_ADDR_OF(create_flow_group_in,    fg_in, match_criteria);u8 match_criteria_enable = MLX5_GET(create_flow_group_in,    fg_in,    match_criteria_enable);int start_index = MLX5_GET(create_flow_group_in, fg_in,   start_flow_index);int end_index = MLX5_GET(create_flow_group_in, fg_in, end_flow_index);struct mlx5_flow_group  fg;int err;if (ft->autogroup.active && start_index < ft->autogroup.max_fte)return ERR_PTR(-EPERM);down_write_ref_node(&ft->node, false);fg = alloc_insert_flow_group(ft, match_criteria_enable, match_criteria,     start_index, end_index,     ft->node.children.prev);up_write_ref_node(&ft->node, false);if (IS_ERR(fg))return fg;err = root->cmds->create_flow_group(root, ft, fg_in, fg);if (err) {tree_put_node(&fg->node, false);return ERR_PTR(err);}trace_mlx5_fs_add_fg(fg);fg->node.active = true;return fg;}EXPORT_SYMBOL(mlx5_create_flow_group);static struct mlx5_flow_rule  alloc_rule(struct mlx5_flow_destination  dest){struct mlx5_flow_rule  rule;rule = kzalloc(sizeof( rule), GFP_KERNEL);if (!rule)return NULL;INIT_LIST_HEAD(&rule->next_ft);rule->node.type = FS_TYPE_FLOW_DEST;if (dest)memcpy(&rule->dest_attr, dest, sizeof( dest));elserule->dest_attr.type = MLX5_FLOW_DESTINATION_TYPE_NONE;return rule;}static struct mlx5_flow_handle  alloc_handle(int num_rules){struct mlx5_flow_handle  handle;handle = kzalloc(struct_size(handle, rule, num_rules), GFP_KERNEL);if (!handle)return NULL;handle->num_rules = num_rules;return handle;}static void destroy_flow_handle(struct fs_fte  fte,struct mlx5_flow_handle  handle,struct mlx5_flow_destination  dest,int i){for (; --i >= 0;) {if (refcount_dec_and_test(&handle->rule[i]->node.refcount)) {fte->dests_size--;list_del(&handle->rule[i]->node.list);kfree(handle->rule[i]);}}kfree(handle);}static struct mlx5_flow_handle  create_flow_handle(struct fs_fte  fte,   struct mlx5_flow_destination  dest,   int dest_num,   int  modify_mask,   bool  new_rule){struct mlx5_flow_handle  handle;struct mlx5_flow_rule  rule = NULL;static int count = BIT(MLX5_SET_FTE_MODIFY_ENABLE_MASK_FLOW_COUNTERS);static int dst = BIT(MLX5_SET_FTE_MODIFY_ENABLE_MASK_DESTINATION_LIST);int type;int i = 0;handle = alloc_handle((dest_num) ? dest_num : 1);if (!handle)return ERR_PTR(-ENOMEM);do {if (dest) {rule = find_flow_rule(fte, dest + i);if (rule) {refcount_inc(&rule->node.refcount);goto rule_found;}} new_rule = true;rule = alloc_rule(dest + i);if (!rule)goto free_rules;  Add dest to dests list- we need flow tables to be in the   end of the list for forward to next prio rules. ", "WARN_ON(!is_nic_rx_ns(type));root_ns = steering->root_ns;prio = type;break;}if (!root_ns)return NULL;fs_prio = find_prio(&root_ns->ns, prio);if (!fs_prio)return NULL;ns = list_first_entry(&fs_prio->node.children,      typeof(*ns),      node.list);return ns;}EXPORT_SYMBOL(mlx5_get_flow_namespace": "mlx5_get_flow_namespace(struct mlx5_core_dev  dev,    enum mlx5_flow_namespace_type type){struct mlx5_flow_steering  steering = dev->priv.steering;struct mlx5_flow_root_namespace  root_ns;int prio = 0;struct fs_prio  fs_prio;struct mlx5_flow_namespace  ns;if (!steering)return NULL;switch (type) {case MLX5_FLOW_NAMESPACE_FDB:if (steering->fdb_root_ns)return &steering->fdb_root_ns->ns;return NULL;case MLX5_FLOW_NAMESPACE_PORT_SEL:if (steering->port_sel_root_ns)return &steering->port_sel_root_ns->ns;return NULL;case MLX5_FLOW_NAMESPACE_SNIFFER_RX:if (steering->sniffer_rx_root_ns)return &steering->sniffer_rx_root_ns->ns;return NULL;case MLX5_FLOW_NAMESPACE_SNIFFER_TX:if (steering->sniffer_tx_root_ns)return &steering->sniffer_tx_root_ns->ns;return NULL;case MLX5_FLOW_NAMESPACE_FDB_BYPASS:root_ns = steering->fdb_root_ns;prio =  FDB_BYPASS_PATH;break;case MLX5_FLOW_NAMESPACE_EGRESS:case MLX5_FLOW_NAMESPACE_EGRESS_IPSEC:case MLX5_FLOW_NAMESPACE_EGRESS_MACSEC:root_ns = steering->egress_root_ns;prio = type - MLX5_FLOW_NAMESPACE_EGRESS;break;case MLX5_FLOW_NAMESPACE_RDMA_RX:root_ns = steering->rdma_rx_root_ns;prio = RDMA_RX_BYPASS_PRIO;break;case MLX5_FLOW_NAMESPACE_RDMA_RX_KERNEL:root_ns = steering->rdma_rx_root_ns;prio = RDMA_RX_KERNEL_PRIO;break;case MLX5_FLOW_NAMESPACE_RDMA_TX:root_ns = steering->rdma_tx_root_ns;break;case MLX5_FLOW_NAMESPACE_RDMA_RX_COUNTERS:root_ns = steering->rdma_rx_root_ns;prio = RDMA_RX_COUNTERS_PRIO;break;case MLX5_FLOW_NAMESPACE_RDMA_TX_COUNTERS:root_ns = steering->rdma_tx_root_ns;prio = RDMA_TX_COUNTERS_PRIO;break;case MLX5_FLOW_NAMESPACE_RDMA_RX_IPSEC:root_ns = steering->rdma_rx_root_ns;prio = RDMA_RX_IPSEC_PRIO;break;case MLX5_FLOW_NAMESPACE_RDMA_TX_IPSEC:root_ns = steering->rdma_tx_root_ns;prio = RDMA_TX_IPSEC_PRIO;break;default:   Must be NIC RX ", "int mlx5_eq_enable(struct mlx5_core_dev *dev, struct mlx5_eq *eq,   struct notifier_block *nb)": "mlx5_eq_enable - Enable EQ for receiving EQEs   @dev : Device which owns the eq   @eq  : EQ to enable   @nb  : Notifier call block     Must be called after EQ is created in device.     @return: 0 if no error ", "void mlx5_eq_disable(struct mlx5_core_dev *dev, struct mlx5_eq *eq,     struct notifier_block *nb)": "mlx5_eq_disable - Disable EQ for receiving EQEs   @dev : Device which owns the eq   @eq  : EQ to disable   @nb  : Notifier call block     Must be called before EQ is destroyed. ", "if (eqe)dma_rmb();return eqe;}EXPORT_SYMBOL(mlx5_eq_get_eqe": "mlx5_eq_get_eqe(struct mlx5_eq  eq, u32 cc){u32 ci = eq->cons_index + cc;u32 nent = eq_get_size(eq);struct mlx5_eqe  eqe;eqe = get_eqe(eq, ci & (nent - 1));eqe = ((eqe->owner & 1) ^ !!(ci & nent)) ? NULL : eqe;  Make sure we read EQ entry contents after we've   checked the ownership bit. ", "wmb();}EXPORT_SYMBOL(mlx5_eq_update_ci": "mlx5_eq_update_ci(struct mlx5_eq  eq, u32 cc, bool arm){__be32 __iomem  addr = eq->doorbell + (arm ? 0 : 2);u32 val;eq->cons_index += cc;val = (eq->cons_index & 0xffffff) | (eq->eqn << 24);__raw_writel((__force u32)cpu_to_be32(val), addr);  We still want ordering, just not swabbing, so add a barrier ", "list_add_tail(&eq->list, &table->comp_eqs_list);}table->num_comp_eqs = ncomp_eqs;return 0;clean_eq:kfree(eq);clean:destroy_comp_eqs(dev);err_irqs_req:free_rmap(dev);return err;}static int vector2eqnirqn(struct mlx5_core_dev *dev, int vector, int *eqn,  unsigned int *irqn)": "mlx5_vector2eqn to work ", " 1,.mask[0] = 1ull << MLX5_EVENT_TYPE_PAGE_REQUEST,};err = setup_async_eq(dev, &table->pages_eq, &param, \"pages\");if (err)goto err3;return 0;err3:cleanup_async_eq(dev, &table->async_eq, \"async\");err2:mlx5_cmd_use_polling(dev);cleanup_async_eq(dev, &table->cmd_eq, \"cmd\");err1:mlx5_cmd_allowed_opcode(dev, CMD_ALLOWED_OPCODE_ALL);mlx5_eq_notifier_unregister(dev, &table->cq_err_nb);mlx5_ctrl_irq_release(table->ctrl_irq);return err;}static void destroy_async_eqs(struct mlx5_core_dev *dev)": "mlx5_eq_notifier_register(dev, &table->cq_err_nb);param = (struct mlx5_eq_param) {.irq = table->ctrl_irq,.nent = MLX5_NUM_CMD_EQE,.mask[0] = 1ull << MLX5_EVENT_TYPE_CMD,};mlx5_cmd_allowed_opcode(dev, MLX5_CMD_OP_CREATE_EQ);err = setup_async_eq(dev, &table->cmd_eq, &param, \"cmd\");if (err)goto err1;mlx5_cmd_use_events(dev);mlx5_cmd_allowed_opcode(dev, CMD_ALLOWED_OPCODE_ALL);param = (struct mlx5_eq_param) {.irq = table->ctrl_irq,.nent = async_eq_depth_devlink_param_get(dev),};gather_async_events_mask(dev, param.mask);err = setup_async_eq(dev, &table->async_eq, &param, \"async\");if (err)goto err2;param = (struct mlx5_eq_param) {.irq = table->ctrl_irq,.nent =   TODO: sriov max_vf + ", "struct mlx5_eq *mlx5_eq_create_generic(struct mlx5_core_dev *dev,       struct mlx5_eq_param *param)": "mlx5_eq_notifier_unregister(dev, &table->cq_err_nb);mlx5_ctrl_irq_release(table->ctrl_irq);return err;}static void destroy_async_eqs(struct mlx5_core_dev  dev){struct mlx5_eq_table  table = dev->priv.eq_table;cleanup_async_eq(dev, &table->pages_eq, \"pages\");cleanup_async_eq(dev, &table->async_eq, \"async\");mlx5_cmd_allowed_opcode(dev, MLX5_CMD_OP_DESTROY_EQ);mlx5_cmd_use_polling(dev);cleanup_async_eq(dev, &table->cmd_eq, \"cmd\");mlx5_cmd_allowed_opcode(dev, CMD_ALLOWED_OPCODE_ALL);mlx5_eq_notifier_unregister(dev, &table->cq_err_nb);mlx5_ctrl_irq_release(table->ctrl_irq);}struct mlx5_eq  mlx5_get_async_eq(struct mlx5_core_dev  dev){return &dev->priv.eq_table->async_eq.core;}void mlx5_eq_synchronize_async_irq(struct mlx5_core_dev  dev){synchronize_irq(dev->priv.eq_table->async_eq.core.irqn);}void mlx5_eq_synchronize_cmd_irq(struct mlx5_core_dev  dev){synchronize_irq(dev->priv.eq_table->cmd_eq.core.irqn);}  Generic EQ API for mlx5_core consumers   Needed For RDMA ODP EQ for now ", "static struct list_head *mlx5_fc_counters_lookup_next(struct mlx5_core_dev *dev,      u32 id)": "mlx5_fc_destroy() and accessing a reference   to struct mlx5_fc.   e.g en_tc.c is protected by RTNL lock of its caller, and will never call a   dump (access to struct mlx5_fc) after a counter is destroyed.     access to counter list:   - create (user context)     - mlx5_fc_create() only adds to an addlist to be used by       mlx5_fc_stats_work(). addlist is a lockless single linked list       that doesn't require any additional synchronization when adding single       node.     - spawn thread to do the actual destroy     - destroy (user context)     - add a counter to lockless dellist     - spawn thread to do the actual del     - dump (user context)     user should not call dump after destroy     - query (single thread workqueue context)     destroydump - no conflict (see destroy)     querydump - packets and bytes might be inconsistent (since update is not                  atomic)     querycreate - no conflict (see create)     since every createdestroy spawn the work, only after necessary time has     elapsed, the thread will actually query the hardware. ", "struct msi_map mlx5_msix_alloc(struct mlx5_core_dev *dev,       irqreturn_t (*handler)(int, void *),       const struct irq_affinity_desc *affdesc,       const char *name)": "mlx5_msix_free() if shutdown was initiated. ", "static struct mlx5_rl_entry *find_rl_entry(struct mlx5_rl_table *table,   void *rl_in, u16 uid, bool dedicated)": "mlx5_rl_are_equal_raw(struct mlx5_rl_entry  entry, void  rl_in,  u16 uid){return (!memcmp(entry->rl_raw, rl_in, sizeof(entry->rl_raw)) &&entry->uid == uid);}  Finds an entry where we can register the given rate   If the rate already exists, return the entry where it is registered,   otherwise return the first available entry.   If the table is full, return NULL ", "memcpy(entry->rl_raw, rl_in, sizeof(entry->rl_raw));entry->uid = uid;err = mlx5_set_pp_rate_limit_cmd(dev, entry, true);if (err) ": "mlx5_rl_add_rate_raw(struct mlx5_core_dev  dev, void  rl_in, u16 uid, bool dedicated_entry, u16  index){struct mlx5_rl_table  table = &dev->priv.rl_table;struct mlx5_rl_entry  entry;u32 rate;int err;if (!table->max_size)return -EOPNOTSUPP;rate = MLX5_GET(set_pp_rate_limit_context, rl_in, rate_limit);if (!rate || !mlx5_rl_is_in_range(dev, rate)) {mlx5_core_err(dev, \"Invalid rate: %u, should be %u to %u\\n\",      rate, table->min_rate, table->max_rate);return -EINVAL;}mutex_lock(&table->rl_lock);err = mlx5_rl_table_get(table);if (err)goto out;entry = find_rl_entry(table, rl_in, uid, dedicated_entry);if (!entry) {mlx5_core_err(dev, \"Max number of %u rates reached\\n\",      table->max_size);err = -ENOSPC;goto rl_err;}if (!entry->refcount) {  new rate limit ", "unsigned int dbi;bool fp;unsigned int *avail;unsigned long *bitmap;struct list_head *head;bfregs = &mdev->priv.bfregs;if (bfreg->wc) ": "mlx5_free_bfreg(struct mlx5_core_dev  mdev, struct mlx5_sq_bfreg  bfreg){struct mlx5_bfreg_data  bfregs;struct mlx5_uars_page  up;struct mutex  lock;   pointer to right mutex ", "vport = from_esw->manager_vport;if (mlx5_eswitch_vport_match_metadata_enabled(on_esw)) ": "mlx5_eswitch_add_send_to_vport_rule(struct mlx5_eswitch  on_esw,    struct mlx5_eswitch  from_esw,    struct mlx5_eswitch_rep  rep,    u32 sqn){struct mlx5_flow_act flow_act = {0};struct mlx5_flow_destination dest = {};struct mlx5_flow_handle  flow_rule;struct mlx5_flow_spec  spec;void  misc;u16 vport;spec = kvzalloc(sizeof( spec), GFP_KERNEL);if (!spec) {flow_rule = ERR_PTR(-ENOMEM);goto out;}misc = MLX5_ADDR_OF(fte_match_param, spec->match_value, misc_parameters);MLX5_SET(fte_match_set_misc, misc, source_sqn, sqn);misc = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, misc_parameters);MLX5_SET_TO_ONES(fte_match_set_misc, misc, source_sqn);spec->match_criteria_enable = MLX5_MATCH_MISC_PARAMETERS;  source vport is the esw manager ", "esw_warn(dev, \"Tc chains and priorities offload aren't supported, check firmware version, or mlxconfig settings\\n\");*flags &= ~MLX5_CHAINS_AND_PRIOS_SUPPORTED;} else ": "mlx5_eswitch_reg_c1_loopback_enabled(esw)) { flags &= ~MLX5_CHAINS_AND_PRIOS_SUPPORTED;esw_warn(dev, \"Tc chains and priorities offload aren't supported\\n\");} else if (!fdb_modify_header_fwd_to_table_supported(esw)) {  Disabled when ttl workaround is needed, e.g   when ESWITCH_IPV4_TTL_MODIFY_ENABLE = true in mlxconfig ", "if (mlx5_eswitch_vport_match_metadata_enabled(esw)) ": "mlx5_eswitch_vport_match_metadata_enabled(esw)) {void  misc2;misc2 = MLX5_ADDR_OF(fte_match_param, spec->match_value, misc_parameters_2);MLX5_SET(fte_match_set_misc2, misc2, metadata_reg_c_0, 0);misc2 = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, misc_parameters_2);MLX5_SET(fte_match_set_misc2, misc2, metadata_reg_c_0, 0);if (!memchr_inv(misc2, 0, MLX5_ST_SZ_BYTES(fte_match_set_misc2)))spec->match_criteria_enable &= ~MLX5_MATCH_MISC_PARAMETERS_2;}}static voidmlx5_eswitch_set_rule_source_port(struct mlx5_eswitch  esw,  struct mlx5_flow_spec  spec,  struct mlx5_flow_attr  attr,  struct mlx5_eswitch  src_esw,  u16 vport){struct mlx5_esw_flow_attr  esw_attr = attr->esw_attr;u32 metadata;void  misc2;void  misc;  Use metadata matching because vport is not represented by single   VHCA in dual-port RoCE mode, and matching on source vport may fail. ", "if (esw_attr->out_count - esw_attr->split_count > 1)return -EOPNOTSUPP;err = esw_setup_chain_dest(dest, flow_act, chains, attr->dest_chain, 1, 0, *i);if (err)return err;if (esw_attr->dests[esw_attr->split_count].pkt_reformat) ": "mlx5_eswitch_get_vport_metadata_for_match(src_esw, vport);misc2 = MLX5_ADDR_OF(fte_match_param, spec->match_value, misc_parameters_2);MLX5_SET(fte_match_set_misc2, misc2, metadata_reg_c_0, metadata);misc2 = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, misc_parameters_2);MLX5_SET(fte_match_set_misc2, misc2, metadata_reg_c_0, mlx5_eswitch_get_vport_metadata_mask());spec->match_criteria_enable |= MLX5_MATCH_MISC_PARAMETERS_2;} else {misc = MLX5_ADDR_OF(fte_match_param, spec->match_value, misc_parameters);MLX5_SET(fte_match_set_misc, misc, source_port, vport);if (MLX5_CAP_ESW(esw->dev, merged_eswitch))MLX5_SET(fte_match_set_misc, misc, source_eswitch_owner_vhca_id, MLX5_CAP_GEN(src_esw->dev, vhca_id));misc = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, misc_parameters);MLX5_SET_TO_ONES(fte_match_set_misc, misc, source_port);if (MLX5_CAP_ESW(esw->dev, merged_eswitch))MLX5_SET_TO_ONES(fte_match_set_misc, misc, source_eswitch_owner_vhca_id);spec->match_criteria_enable |= MLX5_MATCH_MISC_PARAMETERS;}}static intesw_setup_decap_indir(struct mlx5_eswitch  esw,      struct mlx5_flow_attr  attr){struct mlx5_flow_table  ft;if (!(attr->flags & MLX5_ATTR_FLAG_SRC_REWRITE))return -EOPNOTSUPP;ft = mlx5_esw_indir_table_get(esw, attr,      mlx5_esw_indir_table_decap_vport(attr), true);return PTR_ERR_OR_ZERO(ft);}static voidesw_cleanup_decap_indir(struct mlx5_eswitch  esw,struct mlx5_flow_attr  attr){if (mlx5_esw_indir_table_decap_vport(attr))mlx5_esw_indir_table_put(esw, mlx5_esw_indir_table_decap_vport(attr), true);}static intesw_setup_mtu_dest(struct mlx5_flow_destination  dest,   struct mlx5e_meter_attr  meter,   int i){dest[i].type = MLX5_FLOW_DESTINATION_TYPE_RANGE;dest[i].range.field = MLX5_FLOW_DEST_RANGE_FIELD_PKT_LEN;dest[i].range.min = 0;dest[i].range.max = meter->params.mtu;dest[i].range.hit_ft = mlx5e_post_meter_get_mtu_true_ft(meter->post_meter);dest[i].range.miss_ft = mlx5e_post_meter_get_mtu_false_ft(meter->post_meter);return 0;}static intesw_setup_sampler_dest(struct mlx5_flow_destination  dest,       struct mlx5_flow_act  flow_act,       u32 sampler_id,       int i){flow_act->flags |= FLOW_ACT_IGNORE_FLOW_LEVEL;dest[i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_SAMPLER;dest[i].sampler_id = sampler_id;return 0;}static intesw_setup_ft_dest(struct mlx5_flow_destination  dest,  struct mlx5_flow_act  flow_act,  struct mlx5_eswitch  esw,  struct mlx5_flow_attr  attr,  int i){flow_act->flags |= FLOW_ACT_IGNORE_FLOW_LEVEL;dest[i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;dest[i].ft = attr->dest_ft;if (mlx5_esw_indir_table_decap_vport(attr))return esw_setup_decap_indir(esw, attr);return 0;}static voidesw_setup_accept_dest(struct mlx5_flow_destination  dest, struct mlx5_flow_act  flow_act,      struct mlx5_fs_chains  chains, int i){if (mlx5_chains_ignore_flow_level_supported(chains))flow_act->flags |= FLOW_ACT_IGNORE_FLOW_LEVEL;dest[i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;dest[i].ft = mlx5_chains_get_tc_end_ft(chains);}static voidesw_setup_slow_path_dest(struct mlx5_flow_destination  dest, struct mlx5_flow_act  flow_act, struct mlx5_eswitch  esw, int i){if (MLX5_CAP_ESW_FLOWTABLE_FDB(esw->dev, ignore_flow_level))flow_act->flags |= FLOW_ACT_IGNORE_FLOW_LEVEL;dest[i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;dest[i].ft = mlx5_eswitch_get_slow_fdb(esw);}static intesw_setup_chain_dest(struct mlx5_flow_destination  dest,     struct mlx5_flow_act  flow_act,     struct mlx5_fs_chains  chains,     u32 chain, u32 prio, u32 level,     int i){struct mlx5_flow_table  ft;flow_act->flags |= FLOW_ACT_IGNORE_FLOW_LEVEL;ft = mlx5_chains_get_table(chains, chain, prio, level);if (IS_ERR(ft))return PTR_ERR(ft);dest[i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;dest[i].ft = ft;return  0;}static void esw_put_dest_tables_loop(struct mlx5_eswitch  esw, struct mlx5_flow_attr  attr,     int from, int to){struct mlx5_esw_flow_attr  esw_attr = attr->esw_attr;struct mlx5_fs_chains  chains = esw_chains(esw);int i;for (i = from; i < to; i++)if (esw_attr->dests[i].flags & MLX5_ESW_DEST_CHAIN_WITH_SRC_PORT_CHANGE)mlx5_chains_put_table(chains, 0, 1, 0);else if (mlx5_esw_indir_table_needed(esw, attr, esw_attr->dests[i].rep->vport,     esw_attr->dests[i].mdev))mlx5_esw_indir_table_put(esw, esw_attr->dests[i].rep->vport, false);}static boolesw_is_chain_src_port_rewrite(struct mlx5_eswitch  esw, struct mlx5_esw_flow_attr  esw_attr){int i;for (i = esw_attr->split_count; i < esw_attr->out_count; i++)if (esw_attr->dests[i].flags & MLX5_ESW_DEST_CHAIN_WITH_SRC_PORT_CHANGE)return true;return false;}static intesw_setup_chain_src_port_rewrite(struct mlx5_flow_destination  dest, struct mlx5_flow_act  flow_act, struct mlx5_eswitch  esw, struct mlx5_fs_chains  chains, struct mlx5_flow_attr  attr, int  i){struct mlx5_esw_flow_attr  esw_attr = attr->esw_attr;int err;if (!(attr->flags & MLX5_ATTR_FLAG_SRC_REWRITE))return -EOPNOTSUPP;  flow steering cannot handle more than one dest with the same ft   in a single flow ", "void mlx5_sriov_blocking_notifier_unregister(struct mlx5_core_dev *mdev,     int vf_id,     struct notifier_block *nb)": "mlx5_sriov_blocking_notifier_unregister - Unregister a VF from   a notification block chain.     @mdev: The mlx5 core device.   @vf_id: The VF id.   @nb: The notifier block to be unregistered. ", "int mlx5_sriov_blocking_notifier_register(struct mlx5_core_dev *mdev,  int vf_id,  struct notifier_block *nb)": "mlx5_sriov_blocking_notifier_register - Register a VF notification   block chain.     @mdev: The mlx5 core device.   @vf_id: The VF id.   @nb: The notifier block to be called upon the VF events.     Returns 0 on success or an error code. ", "struct mlx5_core_dev *mlx5_eswitch_get_core_dev(struct mlx5_eswitch *esw)": "mlx5_eswitch_get_core_dev - Get the mdev device   @esw : eswitch device.     Return the mellanox core device which manages the eswitch. ", "if (err == -ENXIO) ": "mlx5_cmd_check(struct mlx5_core_dev  dev, int err, void  in, void  out){  aborted due to PCI error or via reset flow mlx5_cmd_trigger_completions() ", "int mlx5_cmd_do(struct mlx5_core_dev *dev, void *in, int in_size, void *out, int out_size)": "mlx5_cmd_do - Executes a fw command, wait for completion.   Unlike mlx5_cmd_exec, this function will not translate or intercept   outbox.status and will return -EREMOTEIO when   outbox.status != MLX5_CMD_STAT_OK     @dev: mlx5 core device   @in: inbox mlx5_ifc command buffer   @in_size: inbox buffer size   @out: outbox mlx5_ifc buffer   @out_size: outbox size     @return:   -EREMOTEIO : Command executed by FW, outbox.status != MLX5_CMD_STAT_OK.                Caller must check FW outbox status.     0 : Command execution successful, outbox.status == MLX5_CMD_STAT_OK.   < 0 : Command execution couldn't be performed by firmware or driver ", "return -ENOMEM;}copy = min_t(int, size, MLX5_CMD_DATA_BLOCK_SIZE);block = next->buf;memcpy(block->data, from, copy);from += copy;size -= copy;block->token = token;next = next->next;}return 0;}static int mlx5_copy_from_msg(void *to, struct mlx5_cmd_msg *from, int size)": "mlx5_cmd_exec(dev, dbg->in_msg, dbg->inlen, dbg->out_msg, dbg->outlen);return err ? err : count;}static const struct file_operations fops = {.owner= THIS_MODULE,.open= simple_open,.write= dbg_write,};static int mlx5_copy_to_msg(struct mlx5_cmd_msg  to, void  from, int size,    u8 token){struct mlx5_cmd_prot_block  block;struct mlx5_cmd_mailbox  next;int copy;if (!to || !from)return -ENOMEM;copy = min_t(int, size, sizeof(to->first.data));memcpy(to->first.data, from, copy);size -= copy;from += copy;next = to->next;while (size) {if (!next) {  this is a BUG ", "int mlx5_cmd_exec_polling(struct mlx5_core_dev *dev, void *in, int in_size,  void *out, int out_size)": "mlx5_cmd_exec_polling - Executes a fw command, poll for completion  Needed for driver force teardown, when command completion EQ  will not be available to complete the command     @dev: mlx5 core device   @in: inbox mlx5_ifc command buffer   @in_size: inbox buffer size   @out: outbox mlx5_ifc buffer   @out_size: outbox size     @return: 0 if no error, FW command execution was successful            and outbox status is ok. ", "atomic_set(&ctx->num_inflight, 1);init_completion(&ctx->inflight_done);}EXPORT_SYMBOL(mlx5_cmd_init_async_ctx": "mlx5_cmd_init_async_ctx(struct mlx5_core_dev  dev,     struct mlx5_async_ctx  ctx){ctx->dev = dev;  Starts at 1 to avoid doing wake_up if we are not cleaning up ", "void mlx5_cmd_cleanup_async_ctx(struct mlx5_async_ctx *ctx)": "mlx5_cmd_exec_cb() have been called. The   caller must ensure that mlx5_cmd_exec_cb() is not called during or after   the call mlx5_cleanup_async_ctx(). ", "struct mlx5_core_dev *mlx5_vf_get_core_dev(struct pci_dev *pdev)": "mlx5_vf_put_core_dev() immediately once usage was over.     Return: Pointer to the associated mlx5_core_dev or NULL. ", "static void mlx5_infer_tx_affinity_mapping(struct lag_tracker *tracker,   u8 num_ports,   u8 buckets,   u8 *ports)": "mlx5_lag_is_sriov(struct mlx5_lag  ldev){return ldev->mode == MLX5_LAG_MODE_SRIOV;}  Create a mapping between steering slots and active ports.   As we have ldev->buckets slots per port first assume the native   mapping should be used.   If there are ports that are disabled fill the relevant slots   with mapping that points to active ports. ", "if (mlx5_lag_is_multipath(dev0))return;tracker = ldev->tracker;do_bond = tracker.is_bonded && mlx5_lag_check_prereq(ldev);}if (do_bond && !__mlx5_lag_is_active(ldev)) ": "mlx5_lag_is_active(ldev) &&       ldev->mode != MLX5_LAG_MODE_MPESW;}static bool mlx5_lag_should_disable_lag(struct mlx5_lag  ldev, bool do_bond){return !do_bond && __mlx5_lag_is_active(ldev) &&       ldev->mode != MLX5_LAG_MODE_MPESW;}static void mlx5_do_bond(struct mlx5_lag  ldev){struct mlx5_core_dev  dev0 = ldev->pf[MLX5_LAG_P1].dev;struct lag_tracker tracker = { };bool do_bond, roce_lag;int err;int i;if (!mlx5_lag_is_ready(ldev)) {do_bond = false;} else {  VF LAG is in multipath mode, ignore bond change requests ", "if (dev->persist->num_vfs < slave)return 0;memset(&eqe, 0, sizeof(eqe));eqe.type = MLX4_EVENT_TYPE_PORT_MNG_CHG_EVENT;eqe.subtype = MLX4_DEV_PMC_SUBTYPE_GUID_INFO;eqe.event.port_mgmt_change.port = mlx4_phys_to_slave_port(dev, slave, port);return mlx4_GEN_EQE(dev, slave, &eqe);}EXPORT_SYMBOL(mlx4_gen_guid_change_eqe": "mlx4_gen_guid_change_eqe(struct mlx4_dev  dev, int slave, u8 port){struct mlx4_eqe eqe; don't send if we don't have the that slave ", "if (dev->persist->num_vfs < slave)return 0;memset(&eqe, 0, sizeof(eqe));eqe.type = MLX4_EVENT_TYPE_PORT_CHANGE;eqe.subtype = port_subtype_change;eqe.event.port_change.port = cpu_to_be32(slave_port << 28);mlx4_dbg(dev, \"%s: sending: %d to slave: %d on port: %d\\n\", __func__, port_subtype_change, slave, port);return mlx4_GEN_EQE(dev, slave, &eqe);}EXPORT_SYMBOL(mlx4_gen_port_state_change_eqe": "mlx4_gen_port_state_change_eqe(struct mlx4_dev  dev, int slave, u8 port,   u8 port_subtype_change){struct mlx4_eqe eqe;u8 slave_port = mlx4_phys_to_slave_port(dev, slave, port); don't send if we don't have the that slave ", "int set_and_calc_slave_port_state(struct mlx4_dev *dev, int slave,  u8 port, int event,  enum slave_port_gen_event *gen_event)": "set_and_calc_slave_port_state(dev, i, port,      event, &gen_event);}                                                                          The function get as input the new event to that port,and according to the prev state change the slave's port state.The events are:MLX4_PORT_STATE_DEV_EVENT_PORT_DOWN,MLX4_PORT_STATE_DEV_EVENT_PORT_UPMLX4_PORT_STATE_IB_EVENT_GID_VALIDMLX4_PORT_STATE_IB_EVENT_GID_INVALID                                                                          ", "mlx4_cmd_use_polling(dev);/* Map the new eq to handle all asynchronous events ": "mlx4_test_interrupt(struct mlx4_dev  dev, int vector){struct mlx4_priv  priv = mlx4_priv(dev);int err;  Temporary use polling for command completions ", "eq_set_ci(&priv->eq_table.eq[MLX4_EQ_ASYNC], 1);return 0;err_out_unmap:while (i > 0)mlx4_free_eq(dev, &priv->eq_table.eq[--i]);#ifdef CONFIG_RFS_ACCELfor (i = 1; i <= dev->caps.num_ports; i++) ": "mlx4_get_eqs_per_port(dev, port));if (!info->rmap) {mlx4_warn(dev, \"Failed to allocate cpu rmap\\n\");err = -ENOMEM;goto err_out_unmap;}}err = irq_cpu_rmap_add(info->rmap, eq->irq);if (err)mlx4_warn(dev, \"Failed adding irq rmap\\n\");}#endiferr = mlx4_create_eq(dev, dev->quotas.cq +     MLX4_NUM_SPARE_EQE,     (dev->flags & MLX4_FLAG_MSI_X) ?     i + 1 - !!(i > MLX4_EQ_ASYNC) : 0,     eq);}if (err)goto err_out_unmap;}if (dev->flags & MLX4_FLAG_MSI_X) {const char  eq_name;snprintf(priv->eq_table.irq_names + MLX4_EQ_ASYNC   MLX4_IRQNAME_SIZE, MLX4_IRQNAME_SIZE, \"mlx4-async@pci:%s\", pci_name(dev->persist->pdev));eq_name = priv->eq_table.irq_names +MLX4_EQ_ASYNC   MLX4_IRQNAME_SIZE;err = request_irq(priv->eq_table.eq[MLX4_EQ_ASYNC].irq,  mlx4_msi_x_interrupt, 0, eq_name,  priv->eq_table.eq + MLX4_EQ_ASYNC);if (err)goto err_out_unmap;priv->eq_table.eq[MLX4_EQ_ASYNC].have_irq = 1;} else {snprintf(priv->eq_table.irq_names, MLX4_IRQNAME_SIZE, DRV_NAME \"@pci:%s\", pci_name(dev->persist->pdev));err = request_irq(dev->persist->pdev->irq, mlx4_interrupt,  IRQF_SHARED, priv->eq_table.irq_names, dev);if (err)goto err_out_unmap;priv->eq_table.have_irq = 1;}err = mlx4_MAP_EQ(dev, get_async_ev_mask(dev), 0,  priv->eq_table.eq[MLX4_EQ_ASYNC].eqn);if (err)mlx4_warn(dev, \"MAP_EQ for async EQ %d failed (%d)\\n\",   priv->eq_table.eq[MLX4_EQ_ASYNC].eqn, err);  arm ASYNC eq ", "mutex_unlock(&priv->msix_ctl.pool_lock);}EXPORT_SYMBOL(mlx4_release_eq": "mlx4_release_eq(struct mlx4_dev  dev, int vec){struct mlx4_priv  priv = mlx4_priv(dev);int eq_vec = MLX4_CQ_TO_EQ_VECTOR(vec);mutex_lock(&priv->msix_ctl.pool_lock);priv->eq_table.eq[eq_vec].ref_count--;  once we allocated EQ, we don't release it because it might be binded   to cpu_rmap. ", "if (found_ix < MLX4_ROCE_PF_GIDS)slave_gid = 0;else if (found_ix < MLX4_ROCE_PF_GIDS + (vf_gids % num_vfs) * (vf_gids / num_vfs + 1))slave_gid = ((found_ix - MLX4_ROCE_PF_GIDS) /     (vf_gids / num_vfs + 1)) + 1;elseslave_gid =((found_ix - MLX4_ROCE_PF_GIDS -  ((vf_gids % num_vfs) * ((vf_gids / num_vfs + 1)))) / (vf_gids / num_vfs)) + vf_gids % num_vfs + 1;/* Calculate the globally unique slave id ": "mlx4_get_slave_from_roce_gid(struct mlx4_dev  dev, int port, u8  gid, int  slave_id){struct mlx4_priv  priv = mlx4_priv(dev);int i, found_ix = -1;int vf_gids = MLX4_ROCE_MAX_GIDS - MLX4_ROCE_PF_GIDS;struct mlx4_slaves_pport slaves_pport;unsigned num_vfs;int slave_gid;if (!mlx4_is_mfunc(dev))return -EINVAL;slaves_pport = mlx4_phys_to_slaves_pport(dev, port);num_vfs = bitmap_weight(slaves_pport.slaves,dev->persist->num_vfs + 1) - 1;for (i = 0; i < MLX4_ROCE_MAX_GIDS; i++) {if (!memcmp(priv->port[port].gid_table.roce_gids[i].raw, gid,    MLX4_ROCE_GID_ENTRY_SIZE)) {found_ix = i;break;}}if (found_ix >= 0) {  Calculate a slave_gid which is the slave number in the gid   table and not a globally unique slave number. ", "int mlx4_get_module_info(struct mlx4_dev *dev, u8 port, u16 offset, u16 size, u8 *data)": "mlx4_get_module_info - Read cable module eeprom data   @dev: mlx4_dev.   @port: port number.   @offset: byte offset in eeprom to start reading data from.   @size: num of bytes to read.   @data: output buffer to put the requested data into.     Reads cable module eeprom data, puts the outcome data into   data pointer paramer.   Returns num of read bytes on success or a negative error   code. ", "*available_vpp = (u16)be32_to_cpu(out_param->available_vpp);for (i = 0; i < MLX4_NUM_UP; i++)vpp_p_up[i] = (u8)be32_to_cpu(out_param->vpp_p_up[i]);out:mlx4_free_cmd_mailbox(dev, mailbox);return err;}EXPORT_SYMBOL(mlx4_ALLOCATE_VPP_get": "mlx4_ALLOCATE_VPP_get(struct mlx4_dev  dev, u8 port,  u16  available_vpp, u8  vpp_p_up){int i;int err;struct mlx4_cmd_mailbox  mailbox;struct mlx4_alloc_vpp_param  out_param;mailbox = mlx4_alloc_cmd_mailbox(dev);if (IS_ERR(mailbox))return PTR_ERR(mailbox);out_param = mailbox->buf;err = mlx4_cmd_box(dev, 0, mailbox->dma, port,   MLX4_ALLOCATE_VPP_QUERY,   MLX4_CMD_ALLOCATE_VPP,   MLX4_CMD_TIME_CLASS_A,   MLX4_CMD_NATIVE);if (err)goto out;  Total number of supported VPPs ", "qk += qpn - dev->phys_caps.base_tunnel_sqpn;elseqk += qpn - dev->phys_caps.base_proxy_sqpn;*qkey = qk;return 0;}EXPORT_SYMBOL(mlx4_get_parav_qkey": "mlx4_get_parav_qkey(struct mlx4_dev  dev, u32 qpn, u32  qkey){u32 qk = MLX4_RESERVED_QKEY_BASE;if (qpn >= dev->phys_caps.base_tunnel_sqpn + 8   MLX4_MFUNC_MAX ||    qpn < dev->phys_caps.base_proxy_sqpn)return -EINVAL;if (qpn >= dev->phys_caps.base_tunnel_sqpn)  tunnel qp ", "spec_eth_inner.id = MLX4_NET_TRANS_RULE_ID_ETH; /* any inner eth header ": "mlx4_tunnel_steer_add(struct mlx4_dev  dev, const unsigned char  addr,  int port, int qpn, u16 prio, u64  reg_id){int err;struct mlx4_spec_list spec_eth_outer = { {NULL} };struct mlx4_spec_list spec_vxlan     = { {NULL} };struct mlx4_spec_list spec_eth_inner = { {NULL} };struct mlx4_net_trans_rule rule = {.queue_mode = MLX4_NET_TRANS_Q_FIFO,.exclusive = 0,.allow_loopback = 1,.promisc_mode = MLX4_FS_REGULAR,};__be64 mac_mask = cpu_to_be64(MLX4_MAC_MASK << 16);rule.port = port;rule.qpn = qpn;rule.priority = prio;INIT_LIST_HEAD(&rule.list);spec_eth_outer.id = MLX4_NET_TRANS_RULE_ID_ETH;memcpy(spec_eth_outer.eth.dst_mac, addr, ETH_ALEN);memcpy(spec_eth_outer.eth.dst_mac_msk, &mac_mask, ETH_ALEN);spec_vxlan.id = MLX4_NET_TRANS_RULE_ID_VXLAN;      any vxlan header ", "return -EOPNOTSUPP;val = mlxsw_reg_mfde_irisc_id_get(mfde_pl);err = devlink_fmsg_u8_pair_put(fmsg, \"irisc_id\", val);if (err)return err;err = devlink_fmsg_arr_pair_nest_start(fmsg, \"event\");if (err)return err;event_id = mlxsw_reg_mfde_event_id_get(mfde_pl);err = devlink_fmsg_u32_pair_put(fmsg, \"id\", event_id);if (err)return err;switch (event_id) ": "mlxsw_core_schedule_work(&event->work);}static const struct mlxsw_listener mlxsw_core_health_listener =MLXSW_CORE_EVENTL(mlxsw_core_health_listener_func, MFDE);static intmlxsw_core_health_fw_fatal_dump_fatal_cause(const char  mfde_pl,    struct devlink_fmsg  fmsg){u32 val, tile_v;int err;val = mlxsw_reg_mfde_fatal_cause_id_get(mfde_pl);err = devlink_fmsg_u32_pair_put(fmsg, \"cause_id\", val);if (err)return err;tile_v = mlxsw_reg_mfde_fatal_cause_tile_v_get(mfde_pl);if (tile_v) {val = mlxsw_reg_mfde_fatal_cause_tile_index_get(mfde_pl);err = devlink_fmsg_u8_pair_put(fmsg, \"tile_index\", val);if (err)return err;}return 0;}static intmlxsw_core_health_fw_fatal_dump_fw_assert(const char  mfde_pl,  struct devlink_fmsg  fmsg){u32 val, tile_v;int err;val = mlxsw_reg_mfde_fw_assert_var0_get(mfde_pl);err = devlink_fmsg_u32_pair_put(fmsg, \"var0\", val);if (err)return err;val = mlxsw_reg_mfde_fw_assert_var1_get(mfde_pl);err = devlink_fmsg_u32_pair_put(fmsg, \"var1\", val);if (err)return err;val = mlxsw_reg_mfde_fw_assert_var2_get(mfde_pl);err = devlink_fmsg_u32_pair_put(fmsg, \"var2\", val);if (err)return err;val = mlxsw_reg_mfde_fw_assert_var3_get(mfde_pl);err = devlink_fmsg_u32_pair_put(fmsg, \"var3\", val);if (err)return err;val = mlxsw_reg_mfde_fw_assert_var4_get(mfde_pl);err = devlink_fmsg_u32_pair_put(fmsg, \"var4\", val);if (err)return err;val = mlxsw_reg_mfde_fw_assert_existptr_get(mfde_pl);err = devlink_fmsg_u32_pair_put(fmsg, \"existptr\", val);if (err)return err;val = mlxsw_reg_mfde_fw_assert_callra_get(mfde_pl);err = devlink_fmsg_u32_pair_put(fmsg, \"callra\", val);if (err)return err;val = mlxsw_reg_mfde_fw_assert_oe_get(mfde_pl);err = devlink_fmsg_bool_pair_put(fmsg, \"old_event\", val);if (err)return err;tile_v = mlxsw_reg_mfde_fw_assert_tile_v_get(mfde_pl);if (tile_v) {val = mlxsw_reg_mfde_fw_assert_tile_index_get(mfde_pl);err = devlink_fmsg_u8_pair_put(fmsg, \"tile_index\", val);if (err)return err;}val = mlxsw_reg_mfde_fw_assert_ext_synd_get(mfde_pl);err = devlink_fmsg_u32_pair_put(fmsg, \"ext_synd\", val);if (err)return err;return 0;}static intmlxsw_core_health_fw_fatal_dump_kvd_im_stop(const char  mfde_pl,    struct devlink_fmsg  fmsg){u32 val;int err;val = mlxsw_reg_mfde_kvd_im_stop_oe_get(mfde_pl);err = devlink_fmsg_bool_pair_put(fmsg, \"old_event\", val);if (err)return err;val = mlxsw_reg_mfde_kvd_im_stop_pipes_mask_get(mfde_pl);return devlink_fmsg_u32_pair_put(fmsg, \"pipes_mask\", val);}static intmlxsw_core_health_fw_fatal_dump_crspace_to(const char  mfde_pl,   struct devlink_fmsg  fmsg){u32 val;int err;val = mlxsw_reg_mfde_crspace_to_log_address_get(mfde_pl);err = devlink_fmsg_u32_pair_put(fmsg, \"log_address\", val);if (err)return err;val = mlxsw_reg_mfde_crspace_to_oe_get(mfde_pl);err = devlink_fmsg_bool_pair_put(fmsg, \"old_event\", val);if (err)return err;val = mlxsw_reg_mfde_crspace_to_log_id_get(mfde_pl);err = devlink_fmsg_u8_pair_put(fmsg, \"log_irisc_id\", val);if (err)return err;val = mlxsw_reg_mfde_crspace_to_log_ip_get(mfde_pl);err = devlink_fmsg_u64_pair_put(fmsg, \"log_ip\", val);if (err)return err;return 0;}static int mlxsw_core_health_fw_fatal_dump(struct devlink_health_reporter  reporter,   struct devlink_fmsg  fmsg, void  priv_ctx,   struct netlink_ext_ack  extack){char  mfde_pl = priv_ctx;char  val_str;u8 event_id;u32 val;int err;if (!priv_ctx)  User-triggered dumps are not possible ", "static void mlxsw_emad_rx_listener_func(struct sk_buff *skb, u16 local_port,void *priv)": "mlxsw_core_skb_transmit(mlxsw_core, skb, &trans->tx_info);if (err) {dev_kfree_skb(skb);return err;}mlxsw_emad_trans_timeout_schedule(trans);return 0;}static void mlxsw_emad_trans_finish(struct mlxsw_reg_trans  trans, int err){struct mlxsw_core  mlxsw_core = trans->core;dev_kfree_skb(trans->tx_skb);spin_lock_bh(&mlxsw_core->emad.trans_list_lock);list_del_rcu(&trans->list);spin_unlock_bh(&mlxsw_core->emad.trans_list_lock);trans->err = err;complete(&trans->completion);}static void mlxsw_emad_transmit_retry(struct mlxsw_core  mlxsw_core,      struct mlxsw_reg_trans  trans){int err;if (trans->retries < MLXSW_EMAD_MAX_RETRY) {trans->retries++;err = mlxsw_emad_transmit(trans->core, trans);if (err == 0)return;if (!atomic_dec_and_test(&trans->active))return;} else {err = -EIO;}mlxsw_emad_trans_finish(trans, err);}static void mlxsw_emad_trans_timeout_work(struct work_struct  work){struct mlxsw_reg_trans  trans = container_of(work,     struct mlxsw_reg_trans,     timeout_dw.work);if (!atomic_dec_and_test(&trans->active))return;mlxsw_emad_transmit_retry(trans->core, trans);}static void mlxsw_emad_process_response(struct mlxsw_core  mlxsw_core,struct mlxsw_reg_trans  trans,struct sk_buff  skb){int err;if (!atomic_dec_and_test(&trans->active))return;err = mlxsw_emad_process_status_skb(skb, &trans->emad_status);if (err == -EAGAIN) {mlxsw_emad_transmit_retry(mlxsw_core, trans);} else {if (err == 0) {char  reg_tlv = mlxsw_emad_reg_tlv(skb);if (trans->cb)trans->cb(mlxsw_core,  mlxsw_emad_reg_payload(reg_tlv),  trans->reg->len, trans->cb_priv);} else {mlxsw_emad_process_string_tlv(skb, trans);}mlxsw_emad_trans_finish(trans, err);}}  called with rcu read lock held ", "list_add_rcu(&el_item->list, &mlxsw_core->event_listener_list);return 0;err_rx_listener_register:kfree(el_item);return err;}EXPORT_SYMBOL(mlxsw_core_event_listener_register": "mlxsw_core_event_listener_register(struct mlxsw_core  mlxsw_core,       const struct mlxsw_event_listener  el,       void  priv){int err;struct mlxsw_event_listener_item  el_item;const struct mlxsw_rx_listener rxl = {.func = mlxsw_core_event_listener_func,.local_port = MLXSW_PORT_DONT_CARE,.trap_id = el->trap_id,};el_item = __find_event_listener_item(mlxsw_core, el);if (el_item)return -EEXIST;el_item = kmalloc(sizeof( el_item), GFP_KERNEL);if (!el_item)return -ENOMEM;el_item->mlxsw_core = mlxsw_core;el_item->el =  el;el_item->priv = priv;err = mlxsw_core_rx_listener_register(mlxsw_core, &rxl, el_item, true);if (err)goto err_rx_listener_register;  No reason to save item if we did not manage to register an RX   listener for it. ", "int mlxsw_core_driver_register(struct mlxsw_driver *mlxsw_driver)": "mlxsw_core_trap_unregister(mlxsw_core, &mlxsw_emad_rx_listener,   mlxsw_core);err_trap_register:destroy_workqueue(mlxsw_core->emad_wq);return err;}static void mlxsw_emad_fini(struct mlxsw_core  mlxsw_core){if (!(mlxsw_core->bus->features & MLXSW_BUS_F_TXRX))return;mlxsw_core->emad.use_emad = false;mlxsw_emad_tlv_disable(mlxsw_core);mlxsw_core_trap_unregister(mlxsw_core, &mlxsw_emad_rx_listener,   mlxsw_core);destroy_workqueue(mlxsw_core->emad_wq);}static struct sk_buff  mlxsw_emad_alloc(const struct mlxsw_core  mlxsw_core,u16 reg_len){struct sk_buff  skb;u16 emad_len;emad_len = (reg_len + sizeof(u32) + MLXSW_EMAD_ETH_HDR_LEN +    (MLXSW_EMAD_OP_TLV_LEN + MLXSW_EMAD_END_TLV_LEN)      sizeof(u32) + mlxsw_core->driver->txhdr_len);if (mlxsw_core->emad.enable_string_tlv)emad_len += MLXSW_EMAD_STRING_TLV_LEN   sizeof(u32);if (mlxsw_core->emad.enable_latency_tlv)emad_len +=  MLXSW_EMAD_LATENCY_TLV_LEN   sizeof(u32);if (emad_len > MLXSW_EMAD_MAX_FRAME_LEN)return NULL;skb = netdev_alloc_skb(NULL, emad_len);if (!skb)return NULL;memset(skb->data, 0, emad_len);skb_reserve(skb, emad_len);return skb;}static int mlxsw_emad_reg_access(struct mlxsw_core  mlxsw_core, const struct mlxsw_reg_info  reg, char  payload, enum mlxsw_core_reg_access_type type, struct mlxsw_reg_trans  trans, struct list_head  bulk_list, mlxsw_reg_trans_cb_t  cb, unsigned long cb_priv, u64 tid){struct sk_buff  skb;int err;dev_dbg(mlxsw_core->bus_info->dev, \"EMAD reg access (tid=%llx,reg_id=%x(%s),type=%s)\\n\",tid, reg->id, mlxsw_reg_id_str(reg->id),mlxsw_core_reg_access_type_str(type));skb = mlxsw_emad_alloc(mlxsw_core, reg->len);if (!skb)return -ENOMEM;list_add_tail(&trans->bulk_list, bulk_list);trans->core = mlxsw_core;trans->tx_skb = skb;trans->tx_info.local_port = MLXSW_PORT_CPU_PORT;trans->tx_info.is_emad = true;INIT_DELAYED_WORK(&trans->timeout_dw, mlxsw_emad_trans_timeout_work);trans->tid = tid;init_completion(&trans->completion);trans->cb = cb;trans->cb_priv = cb_priv;trans->reg = reg;trans->type = type;mlxsw_emad_construct(mlxsw_core, skb, reg, payload, type, trans->tid);mlxsw_core->driver->txhdr_construct(skb, &trans->tx_info);spin_lock_bh(&mlxsw_core->emad.trans_list_lock);list_add_tail_rcu(&trans->list, &mlxsw_core->emad.trans_list);spin_unlock_bh(&mlxsw_core->emad.trans_list_lock);err = mlxsw_emad_transmit(mlxsw_core, trans);if (err)goto err_out;return 0;err_out:spin_lock_bh(&mlxsw_core->emad.trans_list_lock);list_del_rcu(&trans->list);spin_unlock_bh(&mlxsw_core->emad.trans_list_lock);list_del(&trans->bulk_list);dev_kfree_skb(trans->tx_skb);return err;}                    Core functions                 ", "if (WARN_ON(listener->is_event))return -EINVAL;action = enabled ? listener->en_action : listener->dis_action;trap_group = enabled ? listener->en_trap_group :       listener->dis_trap_group;mlxsw_reg_hpkt_pack(hpkt_pl, action, listener->trap_id,    trap_group, listener->is_ctrl);err = mlxsw_reg_write(mlxsw_core, MLXSW_REG(hpkt), hpkt_pl);if (err)return err;mlxsw_core_rx_listener_state_set(mlxsw_core, &listener->rx_listener, enabled);return 0;}EXPORT_SYMBOL(mlxsw_core_trap_state_set": "mlxsw_core_trap_state_set(struct mlxsw_core  mlxsw_core,      const struct mlxsw_listener  listener,      bool enabled){enum mlxsw_reg_htgt_trap_group trap_group;enum mlxsw_reg_hpkt_action action;char hpkt_pl[MLXSW_REG_HPKT_LEN];int err;  Not supported for event listener ", "}return sum_err;}EXPORT_SYMBOL(mlxsw_reg_trans_bulk_wait": "mlxsw_reg_trans_bulk_wait(struct list_head  bulk_list){struct mlxsw_reg_trans  trans;struct mlxsw_reg_trans  tmp;int sum_err = 0;int err;list_for_each_entry_safe(trans, tmp, bulk_list, bulk_list) {err = mlxsw_reg_trans_wait(trans);if (err && sum_err == 0)sum_err = err;   first error to be returned ", "get_random_bytes(&tid, 4);tid <<= 32;atomic64_set(&mlxsw_core->emad.tid, tid);INIT_LIST_HEAD(&mlxsw_core->emad.trans_list);spin_lock_init(&mlxsw_core->emad.trans_list_lock);err = mlxsw_core_trap_register(mlxsw_core, &mlxsw_emad_rx_listener,       mlxsw_core);if (err)goto err_trap_register;err = mlxsw_emad_tlv_enable(mlxsw_core);if (err)goto err_emad_tlv_enable;mlxsw_core->emad.use_emad = true;return 0;err_emad_tlv_enable:mlxsw_core_trap_unregister(mlxsw_core, &mlxsw_emad_rx_listener,   mlxsw_core);err_trap_register:destroy_workqueue(mlxsw_core->emad_wq);return err;}static void mlxsw_emad_fini(struct mlxsw_core *mlxsw_core)": "mlxsw_reg_query(mlxsw_core, MLXSW_REG(mgir), mgir_pl);if (err)return err;string_tlv = mlxsw_reg_mgir_fw_info_string_tlv_get(mgir_pl);mlxsw_core->emad.enable_string_tlv = string_tlv;latency_tlv = mlxsw_reg_mgir_fw_info_latency_tlv_get(mgir_pl);mlxsw_core->emad.enable_latency_tlv = latency_tlv;return 0;}static void mlxsw_emad_tlv_disable(struct mlxsw_core  mlxsw_core){mlxsw_core->emad.enable_latency_tlv = false;mlxsw_core->emad.enable_string_tlv = false;}static int mlxsw_emad_init(struct mlxsw_core  mlxsw_core){struct workqueue_struct  emad_wq;u64 tid;int err;if (!(mlxsw_core->bus->features & MLXSW_BUS_F_TXRX))return 0;emad_wq = alloc_workqueue(\"mlxsw_core_emad\", 0, 0);if (!emad_wq)return -ENOMEM;mlxsw_core->emad_wq = emad_wq;  Set the upper 32 bits of the transaction ID field to a random   number. This allows us to discard EMADs addressed to other   devices. ", "/* emad_eth_hdr_dmac * Destination MAC in EMAD's Ethernet header. * Must be set to 01:02:c9:00:00:01 ": "mlxsw_reg_write(mlxsw_core, MLXSW_REG(htgt), htgt_pl);if (err)return err;}return 0;}                     EMAD processing                  ", "local_port = mlxsw_core_lag_mapping_get(mlxsw_core,rx_info->u.lag_id,rx_info->lag_port_index);} else ": "mlxsw_core_skb_receive(struct mlxsw_core  mlxsw_core, struct sk_buff  skb,    struct mlxsw_rx_info  rx_info){struct mlxsw_rx_listener_item  rxl_item;const struct mlxsw_rx_listener  rxl;u16 local_port;bool found = false;if (rx_info->is_lag) {dev_dbg_ratelimited(mlxsw_core->bus_info->dev, \"%s: lag_id = %d, lag_port_index = 0x%x\\n\",    __func__, rx_info->u.lag_id,    rx_info->trap_id);  Upper layer does not care if the skb came from LAG or not,   so just get the local_port for the lag port and push it up. ", "if (err == -EAGAIN && !called_again) ": "mlxsw_core_flush_owq();devl_health_reporter_destroy(mlxsw_core->health.fw_fatal);}static void mlxsw_core_irq_event_handler_init(struct mlxsw_core  mlxsw_core){INIT_LIST_HEAD(&mlxsw_core->irq_event_handler_list);mutex_init(&mlxsw_core->irq_event_handler_lock);}static void mlxsw_core_irq_event_handler_fini(struct mlxsw_core  mlxsw_core){mutex_destroy(&mlxsw_core->irq_event_handler_lock);WARN_ON(!list_empty(&mlxsw_core->irq_event_handler_list));}static int__mlxsw_core_bus_device_register(const struct mlxsw_bus_info  mlxsw_bus_info, const struct mlxsw_bus  mlxsw_bus, void  bus_priv, bool reload, struct devlink  devlink, struct netlink_ext_ack  extack){const char  device_kind = mlxsw_bus_info->device_kind;struct mlxsw_core  mlxsw_core;struct mlxsw_driver  mlxsw_driver;size_t alloc_size;u16 max_lag;int err;mlxsw_driver = mlxsw_core_driver_get(device_kind);if (!mlxsw_driver)return -EINVAL;if (!reload) {alloc_size = sizeof( mlxsw_core) + mlxsw_driver->priv_size;devlink = devlink_alloc(&mlxsw_devlink_ops, alloc_size,mlxsw_bus_info->dev);if (!devlink) {err = -ENOMEM;goto err_devlink_alloc;}devl_lock(devlink);devl_register(devlink);}mlxsw_core = devlink_priv(devlink);INIT_LIST_HEAD(&mlxsw_core->rx_listener_list);INIT_LIST_HEAD(&mlxsw_core->event_listener_list);mlxsw_core->driver = mlxsw_driver;mlxsw_core->bus = mlxsw_bus;mlxsw_core->bus_priv = bus_priv;mlxsw_core->bus_info = mlxsw_bus_info;mlxsw_core_irq_event_handler_init(mlxsw_core);err = mlxsw_bus->init(bus_priv, mlxsw_core, mlxsw_driver->profile,      &mlxsw_core->res);if (err)goto err_bus_init;if (mlxsw_driver->resources_register && !reload) {err = mlxsw_driver->resources_register(mlxsw_core);if (err)goto err_register_resources;}err = mlxsw_ports_init(mlxsw_core, reload);if (err)goto err_ports_init;err = mlxsw_core_max_lag(mlxsw_core, &max_lag);if (!err && MLXSW_CORE_RES_VALID(mlxsw_core, MAX_LAG_MEMBERS)) {alloc_size = sizeof( mlxsw_core->lag.mapping)   max_lag  MLXSW_CORE_RES_GET(mlxsw_core, MAX_LAG_MEMBERS);mlxsw_core->lag.mapping = kzalloc(alloc_size, GFP_KERNEL);if (!mlxsw_core->lag.mapping) {err = -ENOMEM;goto err_alloc_lag_mapping;}}err = mlxsw_core_trap_groups_set(mlxsw_core);if (err)goto err_trap_groups_set;err = mlxsw_emad_init(mlxsw_core);if (err)goto err_emad_init;if (!reload) {err = mlxsw_core_params_register(mlxsw_core);if (err)goto err_register_params;}err = mlxsw_core_fw_rev_validate(mlxsw_core, mlxsw_bus_info, mlxsw_driver->fw_req_rev, mlxsw_driver->fw_filename);if (err)goto err_fw_rev_validate;err = mlxsw_linecards_init(mlxsw_core, mlxsw_bus_info);if (err)goto err_linecards_init;err = mlxsw_core_health_init(mlxsw_core);if (err)goto err_health_init;err = mlxsw_hwmon_init(mlxsw_core, mlxsw_bus_info, &mlxsw_core->hwmon);if (err)goto err_hwmon_init;err = mlxsw_thermal_init(mlxsw_core, mlxsw_bus_info, &mlxsw_core->thermal);if (err)goto err_thermal_init;err = mlxsw_env_init(mlxsw_core, mlxsw_bus_info, &mlxsw_core->env);if (err)goto err_env_init;if (mlxsw_driver->init) {err = mlxsw_driver->init(mlxsw_core, mlxsw_bus_info, extack);if (err)goto err_driver_init;}if (!reload)devl_unlock(devlink);return 0;err_driver_init:mlxsw_env_fini(mlxsw_core->env);err_env_init:mlxsw_thermal_fini(mlxsw_core->thermal);err_thermal_init:mlxsw_hwmon_fini(mlxsw_core->hwmon);err_hwmon_init:mlxsw_core_health_fini(mlxsw_core);err_health_init:mlxsw_linecards_fini(mlxsw_core);err_linecards_init:err_fw_rev_validate:if (!reload)mlxsw_core_params_unregister(mlxsw_core);err_register_params:mlxsw_emad_fini(mlxsw_core);err_emad_init:err_trap_groups_set:kfree(mlxsw_core->lag.mapping);err_alloc_lag_mapping:mlxsw_ports_fini(mlxsw_core, reload);err_ports_init:if (!reload)devl_resources_unregister(devlink);err_register_resources:mlxsw_bus->fini(bus_priv);err_bus_init:mlxsw_core_irq_event_handler_fini(mlxsw_core);if (!reload) {devl_unregister(devlink);devl_unlock(devlink);devlink_free(devlink);}err_devlink_alloc:return err;}int mlxsw_core_bus_device_register(const struct mlxsw_bus_info  mlxsw_bus_info,   const struct mlxsw_bus  mlxsw_bus,   void  bus_priv, bool reload,   struct devlink  devlink,   struct netlink_ext_ack  extack){bool called_again = false;int err;again:err = __mlxsw_core_bus_device_register(mlxsw_bus_info, mlxsw_bus,       bus_priv, reload,       devlink, extack);  -EAGAIN is returned in case the FW was updated. FW needs   a reset, so lets try to call __mlxsw_core_bus_device_register()   again. ", "return -EIO;}EXPORT_SYMBOL(mlxsw_core_resources_query": "mlxsw_core_resources_query(struct mlxsw_core  mlxsw_core, char  mbox,       struct mlxsw_res  res){int index, i;u64 data;u16 id;int err;mlxsw_cmd_mbox_zero(mbox);for (index = 0; index < MLXSW_CMD_QUERY_RESOURCES_MAX_QUERIES;     index++) {err = mlxsw_cmd_query_resources(mlxsw_core, mbox, index);if (err)return err;for (i = 0; i < MLXSW_CMD_QUERY_RESOURCES_PER_QUERY; i++) {id = mlxsw_cmd_mbox_query_resource_id_get(mbox, i);data = mlxsw_cmd_mbox_query_resource_data_get(mbox, i);if (id == MLXSW_CMD_QUERY_RESOURCES_TABLE_END_ID)return 0;mlxsw_res_parse(res, id, data);}}  If after MLXSW_RESOURCES_QUERY_MAX_QUERIES we still didn't get   MLXSW_RESOURCES_TABLE_END_ID, something went bad in the FW. ", "block->first_set = mlxsw_afa_set_create(true);if (!block->first_set)goto err_first_set_create;/* In case user instructs to have dummy first set, we leave it * empty here and create another, real, set right away. ": "mlxsw_afa_block_create(struct mlxsw_afa  mlxsw_afa){struct mlxsw_afa_block  block;block = kzalloc(sizeof( block), GFP_KERNEL);if (!block)return ERR_PTR(-ENOMEM);INIT_LIST_HEAD(&block->resource_list);block->afa = mlxsw_afa;  At least one action set is always present, so just create it here ", "if (WARN_ON(!block->first_set->next))return 0;return block->first_set->next->kvdl_index;}EXPORT_SYMBOL(mlxsw_afa_block_first_kvdl_index": "mlxsw_afa_block_first_kvdl_index(struct mlxsw_afa_block  block){  First set is never in KVD linear. So the first set   with valid KVD linear index is always the second one. ", "if (!cookie_index)return NULL;cookie = idr_find(&mlxsw_afa->cookie_idr, cookie_index);if (!cookie)return NULL;return &cookie->fa_cookie;}EXPORT_SYMBOL(mlxsw_afa_cookie_lookup": "mlxsw_afa_cookie_lookup(struct mlxsw_afa  mlxsw_afa, u32 cookie_index){struct mlxsw_afa_cookie  cookie;  0 index means no cookie ", "return;if (WARN_ON(elinfo->type != MLXSW_AFK_ELEMENT_TYPE_BUF) ||    WARN_ON(elinfo->item.size.bytes != len))return;__mlxsw_item_memcpy_to(values->storage.key, key_value,       storage_item, 0);__mlxsw_item_memcpy_to(values->storage.mask, mask_value,       storage_item, 0);mlxsw_afk_element_usage_add(&values->elusage, element);}EXPORT_SYMBOL(mlxsw_afk_values_add_buf": "mlxsw_afk_values_add_buf(struct mlxsw_afk_element_values  values,      enum mlxsw_afk_element element,      const char  key_value, const char  mask_value,      unsigned int len){const struct mlxsw_afk_element_info  elinfo =&mlxsw_afk_element_infos[element];const struct mlxsw_item  storage_item = &elinfo->item;if (!memchr_inv(mask_value, 0, len))   If mask is zero ", "err = mlxsw_env_query_module_eeprom(mlxsw_core, slot_index,    module, SFP_DIAGMON, 1,    &diag_mon, false,    &read_size);if (err)return err;if (read_size < 1)return -EIO;modinfo->type       = ETH_MODULE_SFF_8472;if (diag_mon)modinfo->eeprom_len = ETH_MODULE_SFF_8472_LEN;elsemodinfo->eeprom_len = ETH_MODULE_SFF_8472_LEN / 2;break;case MLXSW_REG_MCIA_EEPROM_MODULE_INFO_ID_QSFP_DD:case MLXSW_REG_MCIA_EEPROM_MODULE_INFO_ID_OSFP:/* Use SFF_8636 as base type. ethtool should recognize specific * type through the identifier value. ": "mlxsw_env_get_module_info(struct net_device  netdev,      struct mlxsw_core  mlxsw_core, u8 slot_index,      int module, struct ethtool_modinfo  modinfo){struct mlxsw_env  mlxsw_env = mlxsw_core_env(mlxsw_core);u8 module_info[MLXSW_REG_MCIA_EEPROM_MODULE_INFO_SIZE];u16 offset = MLXSW_REG_MCIA_EEPROM_MODULE_INFO_SIZE;u8 module_rev_id, module_id, diag_mon;unsigned int read_size;int err;if (!mlxsw_env_linecard_is_active(mlxsw_env, slot_index)) {netdev_err(netdev, \"Cannot read EEPROM of module on an inactive line card\\n\");return -EIO;}err = mlxsw_env_validate_module_type(mlxsw_core, slot_index, module);if (err) {netdev_err(netdev,   \"EEPROM is not equipped on port module type\");return err;}err = mlxsw_env_query_module_eeprom(mlxsw_core, slot_index, module, 0,    offset, module_info, false,    &read_size);if (err)return err;if (read_size < offset)return -EIO;module_rev_id = module_info[MLXSW_REG_MCIA_EEPROM_MODULE_INFO_REV_ID];module_id = module_info[MLXSW_REG_MCIA_EEPROM_MODULE_INFO_ID];switch (module_id) {case MLXSW_REG_MCIA_EEPROM_MODULE_INFO_ID_QSFP:modinfo->type       = ETH_MODULE_SFF_8436;modinfo->eeprom_len = ETH_MODULE_SFF_8436_MAX_LEN;break;case MLXSW_REG_MCIA_EEPROM_MODULE_INFO_ID_QSFP_PLUS:case MLXSW_REG_MCIA_EEPROM_MODULE_INFO_ID_QSFP28:if (module_id == MLXSW_REG_MCIA_EEPROM_MODULE_INFO_ID_QSFP28 ||    module_rev_id >=    MLXSW_REG_MCIA_EEPROM_MODULE_INFO_REV_ID_8636) {modinfo->type       = ETH_MODULE_SFF_8636;modinfo->eeprom_len = ETH_MODULE_SFF_8636_MAX_LEN;} else {modinfo->type       = ETH_MODULE_SFF_8436;modinfo->eeprom_len = ETH_MODULE_SFF_8436_MAX_LEN;}break;case MLXSW_REG_MCIA_EEPROM_MODULE_INFO_ID_SFP:  Verify if transceiver provides diagnostic monitoring page ", "err = mlxsw_env_validate_cable_ident(mlxsw_core, slot_index, module,     &qsfp, &cmis);if (err)return err;while (i < ee->len) ": "mlxsw_env_get_module_eeprom(struct net_device  netdev,struct mlxsw_core  mlxsw_core, u8 slot_index,int module, struct ethtool_eeprom  ee,u8  data){struct mlxsw_env  mlxsw_env = mlxsw_core_env(mlxsw_core);int offset = ee->offset;unsigned int read_size;bool qsfp, cmis;int i = 0;int err;if (!ee->len)return -EINVAL;if (!mlxsw_env_linecard_is_active(mlxsw_env, slot_index)) {netdev_err(netdev, \"Cannot read EEPROM of module on an inactive line card\\n\");return -EIO;}memset(data, 0, ee->len);  Validate module identifier value. ", "device_addr = page->offset;while (bytes_read < page->length) ": "mlxsw_env_get_module_eeprom_by_page(struct mlxsw_core  mlxsw_core,    u8 slot_index, u8 module,    const struct ethtool_module_eeprom  page,    struct netlink_ext_ack  extack){struct mlxsw_env  mlxsw_env = mlxsw_core_env(mlxsw_core);u32 bytes_read = 0;u16 device_addr;int err;if (!mlxsw_env_linecard_is_active(mlxsw_env, slot_index)) {NL_SET_ERR_MSG_MOD(extack,   \"Cannot read EEPROM of module on an inactive line card\");return -EIO;}err = mlxsw_env_validate_module_type(mlxsw_core, slot_index, module);if (err) {NL_SET_ERR_MSG_MOD(extack, \"EEPROM is not equipped on port module type\");return err;}  Offset cannot be larger than 2   ETH_MODULE_EEPROM_PAGE_LEN ", "if (!__mlxsw_env_linecard_is_active(mlxsw_env, slot_index))goto out;mlxsw_reg_mcion_pack(mcion_pl, slot_index, module);err = mlxsw_reg_query(mlxsw_core, MLXSW_REG(mcion), mcion_pl);if (err) ": "mlxsw_env_get_module_power_mode(struct mlxsw_core  mlxsw_core, u8 slot_index,u8 module,struct ethtool_module_power_mode_params  params,struct netlink_ext_ack  extack){struct mlxsw_env  mlxsw_env = mlxsw_core_env(mlxsw_core);struct mlxsw_env_module_info  module_info;char mcion_pl[MLXSW_REG_MCION_LEN];u32 status_bits;int err = 0;mutex_lock(&mlxsw_env->line_cards_lock);err = __mlxsw_env_validate_module_type(mlxsw_core, slot_index, module);if (err) {NL_SET_ERR_MSG_MOD(extack, \"Power mode is not supported on port module type\");goto out;}module_info = mlxsw_env_module_info_get(mlxsw_core, slot_index, module);params->policy = module_info->power_mode_policy;  Avoid accessing an inactive line card, as it will result in an error. ", "if (!__mlxsw_env_linecard_is_active(mlxsw_env, slot_index))return 0;err = mlxsw_env_module_enable_set(mlxsw_core, slot_index, module, false);if (err) ": "mlxsw_env_set_module_power_mode(struct mlxsw_core  mlxsw_core,     u8 slot_index, u8 module,     bool low_power,     struct netlink_ext_ack  extack){struct mlxsw_env  mlxsw_env = mlxsw_core_env(mlxsw_core);int err;  Avoid accessing an inactive line card, as it will result in an error.   Cached configuration will be applied by mlxsw_env_got_active() when   line card becomes active. ", "err = __mlxsw_env_set_module_power_mode(mlxsw_core, slot_index, module,false, NULL);if (err)goto out_unlock;out_inc:module_info->num_ports_up++;out_unlock:mutex_unlock(&mlxsw_env->line_cards_lock);return err;}EXPORT_SYMBOL(mlxsw_env_module_port_up": "mlxsw_env_module_port_up(struct mlxsw_core  mlxsw_core, u8 slot_index,     u8 module){struct mlxsw_env  mlxsw_env = mlxsw_core_env(mlxsw_core);struct mlxsw_env_module_info  module_info;int err = 0;mutex_lock(&mlxsw_env->line_cards_lock);module_info = mlxsw_env_module_info_get(mlxsw_core, slot_index, module);if (module_info->power_mode_policy !=    ETHTOOL_MODULE_POWER_MODE_POLICY_AUTO)goto out_inc;if (module_info->num_ports_up != 0)goto out_inc;  Transition to high power mode following first port using the module   being put administratively up. ", "__mlxsw_env_set_module_power_mode(mlxsw_core, slot_index, module, true,  NULL);out_unlock:mutex_unlock(&mlxsw_env->line_cards_lock);}EXPORT_SYMBOL(mlxsw_env_module_port_down": "mlxsw_env_module_port_down(struct mlxsw_core  mlxsw_core, u8 slot_index,u8 module){struct mlxsw_env  mlxsw_env = mlxsw_core_env(mlxsw_core);struct mlxsw_env_module_info  module_info;mutex_lock(&mlxsw_env->line_cards_lock);module_info = mlxsw_env_module_info_get(mlxsw_core, slot_index, module);module_info->num_ports_up--;if (module_info->power_mode_policy !=    ETHTOOL_MODULE_POWER_MODE_POLICY_AUTO)goto out_unlock;if (module_info->num_ports_up != 0)goto out_unlock;  Transition to low power mode following last port using the module   being put administratively down. ", "rtnl_lock();list_for_each_entry(dev, &cnic_dev_list, list) ": "cnic_register_driver(int ulp_type, struct cnic_ulp_ops  ulp_ops){struct cnic_dev  dev;if (ulp_type < 0 || ulp_type >= MAX_CNIC_ULP_TYPE) {pr_err(\"%s: Bad type %d\\n\", __func__, ulp_type);return -EINVAL;}mutex_lock(&cnic_lock);if (cnic_ulp_tbl_prot(ulp_type)) {pr_err(\"%s: Type %d has already been registered\\n\",       __func__, ulp_type);mutex_unlock(&cnic_lock);return -EBUSY;}read_lock(&cnic_dev_lock);list_for_each_entry(dev, &cnic_dev_list, list) {struct cnic_local  cp = dev->cnic_priv;clear_bit(ULP_F_INIT, &cp->ulp_flags[ulp_type]);}read_unlock(&cnic_dev_lock);atomic_set(&ulp_ops->ref_count, 0);rcu_assign_pointer(cnic_ulp_tbl[ulp_type], ulp_ops);mutex_unlock(&cnic_lock);  Prevent race conditions with netdev_event ", ");if (rc)return rc;rc = hwrm_req_replace(bp, req, fw_msg->msg, fw_msg->msg_len);if (rc)return rc;hwrm_req_timeout(bp, req, fw_msg->timeout);resp = hwrm_req_hold(bp, req);rc = hwrm_req_send(bp, req);resp_len = le16_to_cpu(resp->resp_len);if (resp_len) ": "bnxt_send_msg(struct bnxt_en_dev  edev, struct bnxt_fw_msg  fw_msg){struct net_device  dev = edev->net;struct bnxt  bp = netdev_priv(dev);struct output  resp;struct input  req;u32 resp_len;int rc;if (bp->fw_reset_state)return -EBUSY;rc = hwrm_req_init(bp, req, 0   don't care ", "smp_wmb();ulp->max_async_event_id = max_id;bnxt_hwrm_func_drv_rgtr(bp, events_bmap, max_id + 1, true);return 0;}EXPORT_SYMBOL(bnxt_register_async_events": "bnxt_register_async_events(struct bnxt_en_dev  edev,       unsigned long  events_bmap,       u16 max_id){struct net_device  dev = edev->net;struct bnxt  bp = netdev_priv(dev);struct bnxt_ulp  ulp;ulp = edev->ulp_tbl;ulp->async_events_bmap = events_bmap;  Make sure bnxt_ulp_async_events() sees this order ", "list_for_each_entry(client_tmp, &hnae3_client_list, node) ": "hnae3_unregister_client(struct hnae3_client  client){struct hnae3_client  client_tmp;struct hnae3_ae_dev  ae_dev;bool existed = false;if (!client)return;mutex_lock(&hnae3_common_lock);  one system should only have one client for every type ", "void hnae3_register_ae_algo(struct hnae3_ae_algo *ae_algo)": "hnae3_register_ae_algo - register a AE algorithm to hnae3 framework   @ae_algo: AE algorithm   NOTE: the duplicated name will not be checked ", "int hnae3_register_ae_dev(struct hnae3_ae_dev *ae_dev)": "hnae3_register_ae_dev - registers a AE device to hnae3 framework   @ae_dev: the AE device   NOTE: the duplicated name will not be checked ", "void hnae3_unregister_ae_dev(struct hnae3_ae_dev *ae_dev)": "hnae3_unregister_ae_dev - unregisters a AE device   @ae_dev: the AE device to unregister ", "int hns_dsaf_roce_reset(struct fwnode_handle *dsaf_fwnode, bool dereset)": "hns_dsaf_roce_reset - reset dsaf and roce   @dsaf_fwnode: Pointer to framework node for the dasf   @dereset: false - request reset , true - drop reset   return 0 - success , negative -fail ", "hnae_fini_queue(handle->qs[i]);if (handle->dev->ops->reset)handle->dev->ops->reset(handle);for (i = 0; i < handle->q_num; i++) ": "hnae_reinit_handle(struct hnae_handle  handle){int i, j;int ret;for (i = 0; i < handle->q_num; i++)   free ring", "struct hnae_handle *hnae_get_handle(struct device *owner_dev,    const struct fwnode_handle*fwnode,    u32 port_id,    struct hnae_buf_ops *bops)": "hnae_get_handle - get a handle from the AE   @owner_dev: the dev use this handle   @ae_id: the id of the ae to be used   @ae_opts: the options set for the handle   @bops: the callbacks for buffer management     return handle ptr or ERR_PTR ", "int hnae_ae_register(struct hnae_ae_dev *hdev, struct module *owner)": "hnae_ae_register - register a AE engine to hnae framework   @hdev: the hnae ae engine device   @owner:  the module who provides this dev   NOTE: the duplicated name will not be checked ", "void hnae_ae_unregister(struct hnae_ae_dev *hdev)": "hnae_ae_unregister - unregisters a HNAE AE engine   @hdev: the device to unregister ", "void wx_irq_disable(struct wx *wx)": "wx_irq_disable - Mask off interrupt generation on the NIC   @wx: board private structure  ", "if (!(rd32(hw, WX_SPI_STATUS) &      WX_SPI_STATUS_FLASH_BYPASS)) ": "wx_check_flash_load(struct wx  hw, u32 check_bit){u32 reg = 0;int err = 0;  if there's flash existing ", "wr32m(wx, WX_CFG_PORT_CTL, WX_CFG_PORT_CTL_DRV_LOAD,      drv ? WX_CFG_PORT_CTL_DRV_LOAD : 0);}EXPORT_SYMBOL(wx_control_hw": "wx_control_hw(struct wx  wx, bool drv){  True : Let firmware know the driver has taken over   False : Let firmware take over control of hw ", "int wx_mng_present(struct wx *wx)": "wx_mng_present - returns 0 when management capability is present   @wx: pointer to hardware structure ", "int wx_host_interface_command(struct wx *wx, u32 *buffer,      u32 length, u32 timeout, bool return_data)": "wx_host_interface_command - Issue command to manageability block    @wx: pointer to the HW structure    @buffer: contains the command to write and where the return status will     be placed    @length: length of buffer, must be multiple of 4 bytes    @timeout: time in ms to wait for command completion    @return_data: read and return data from the buffer (true) or not (false)     Needed because FW structures are big endian and decoding of     these fields can be 8 bit or 16 bit based on command. Decoding     is not easily understood without making a table of commands.     So we will leave this up to the caller to read back the data     in these cases.  ", "static int wx_read_ee_hostif_data(struct wx *wx, u16 offset, u16 *data)": "wx_read_ee_hostif_data - Read EEPROM word using a host interface cmd    assuming that the semaphore is already obtained.    @wx: pointer to hardware structure    @offset: offset of  word in the EEPROM to read    @data: word read from the EEPROM      Reads a 16 bit word from the EEPROM using the hostif.  ", "int wx_read_ee_hostif_buffer(struct wx *wx,     u16 offset, u16 words, u16 *data)": "wx_read_ee_hostif_buffer- Read EEPROM word(s) using hostif    @wx: pointer to hardware structure    @offset: offset of  word in the EEPROM to read    @words: number of words    @data: word(s) read from the EEPROM      Reads a 16 bit word(s) from the EEPROM using the hostif.  ", "int wx_reset_hostif(struct wx *wx)": "wx_reset_hostif - send reset cmd to fw    @wx: pointer to hardware structure      Sends reset cmd to firmware through the manageability    block.  ", "void wx_init_eeprom_params(struct wx *wx)": "wx_init_eeprom_params - Initialize EEPROM params    @wx: pointer to hardware structure      Initializes the EEPROM parameters wx_eeprom_info within the    wx_hw struct in order to set up EEPROM access.  ", "void wx_get_mac_addr(struct wx *wx, u8 *mac_addr)": "wx_get_mac_addr - Generic get MAC address    @wx: pointer to hardware structure    @mac_addr: Adapter MAC address      Reads the adapter's MAC address from first Receive Address Register (RAR0)    A reset of the adapter must be performed prior to calling this function    in order for the MAC address to have been loaded from the EEPROM into RAR0  ", "void wx_init_rx_addrs(struct wx *wx)": "wx_init_rx_addrs - Initializes receive address filters.    @wx: pointer to hardware structure      Places the MAC address in receive address register 0 and clears the rest    of the receive address registers. Clears the multicast table. Assumes    the receiver is in reset when the routine is called.  ", "int wx_set_mac(struct net_device *netdev, void *p)": "wx_set_mac - Change the Ethernet Address of the NIC   @netdev: network interface device structure   @p: pointer to an address structure     Returns 0 on success, negative on failure  ", "wr32m(wx, WX_MAC_RX_CFG,      WX_MAC_RX_CFG_RE, 0);}}}EXPORT_SYMBOL(wx_disable_rx": "wx_disable_rx(struct wx  wx){u32 pfdtxgswc;u32 rxctrl;rxctrl = rd32(wx, WX_RDB_PB_CTL);if (rxctrl & WX_RDB_PB_CTL_RXEN) {pfdtxgswc = rd32(wx, WX_PSR_CTL);if (pfdtxgswc & WX_PSR_CTL_SW_EN) {pfdtxgswc &= ~WX_PSR_CTL_SW_EN;wr32(wx, WX_PSR_CTL, pfdtxgswc);wx->mac.set_lben = true;} else {wx->mac.set_lben = false;}rxctrl &= ~WX_RDB_PB_CTL_RXEN;wr32(wx, WX_RDB_PB_CTL, rxctrl);if (!(((wx->subsystem_device_id & WX_NCSI_MASK) == WX_NCSI_SUP) ||      ((wx->subsystem_device_id & WX_WOL_MASK) == WX_WOL_SUP))) {  disable mac receiver ", "fctrl = rd32(wx, WX_PSR_CTL);fctrl &= ~(WX_PSR_CTL_UPE | WX_PSR_CTL_MPE);vmolr = rd32(wx, WX_PSR_VM_L2CTL(0));vmolr &= ~(WX_PSR_VM_L2CTL_UPE |   WX_PSR_VM_L2CTL_MPE |   WX_PSR_VM_L2CTL_ROPE |   WX_PSR_VM_L2CTL_ROMPE);vlnctrl = rd32(wx, WX_PSR_VLAN_CTL);vlnctrl &= ~(WX_PSR_VLAN_CTL_VFE | WX_PSR_VLAN_CTL_CFIEN);/* set all bits that we expect to always be set ": "wx_set_rx_mode(struct net_device  netdev){struct wx  wx = netdev_priv(netdev);netdev_features_t features;u32 fctrl, vmolr, vlnctrl;int count;features = netdev->features;  Check for Promiscuous and All Multicast modes ", "int wx_change_mtu(struct net_device *netdev, int new_mtu)": "wx_change_mtu - Change the Maximum Transfer Unit   @netdev: network interface device structure   @new_mtu: new value for maximum frame size     Returns 0 on success, negative on failure  ", "wr32m(wx, WX_PX_RR_CFG(reg_idx),      WX_PX_RR_CFG_RR_EN, 0);/* the hardware may take up to 100us to really disable the rx queue ": "wx_disable_rx_queue(struct wx  wx, struct wx_ring  ring){u8 reg_idx = ring->reg_idx;u32 rxdctl;int ret;  write value back with RRCFG.EN bit cleared ", "static int wx_disable_sec_rx_path(struct wx *wx)": "wx_configure_port(struct wx  wx){u32 value, i;value = WX_CFG_PORT_CTL_D_VLAN | WX_CFG_PORT_CTL_QINQ;wr32m(wx, WX_CFG_PORT_CTL,      WX_CFG_PORT_CTL_D_VLAN |      WX_CFG_PORT_CTL_QINQ,      value);wr32(wx, WX_CFG_TAG_TPID(0),     ETH_P_8021Q | ETH_P_8021AD << 16);wx->tpid[0] = ETH_P_8021Q;wx->tpid[1] = ETH_P_8021AD;for (i = 1; i < 4; i++)wr32(wx, WX_CFG_TAG_TPID(i),     ETH_P_8021Q | ETH_P_8021Q << 16);for (i = 2; i < 8; i++)wx->tpid[i] = ETH_P_8021Q;}      wx_disable_sec_rx_path - Stops the receive data path    @wx: pointer to private structure      Stops the receive data path and waits for the HW to internally empty    the Rx security block  ", "int wx_disable_pcie_master(struct wx *wx)": "wx_disable_pcie_master - Disable PCI-express master access    @wx: pointer to hardware structure      Disables PCI-Express master access and verifies there are no pending    requests.  ", "int wx_stop_adapter(struct wx *wx)": "wx_stop_adapter - Generic stop TxRx units    @wx: pointer to hardware structure      Sets the adapter_stopped flag within wx_hw struct. Clears interrupts,    disables transmit and receive units. The adapter_stopped flag is used by    the shared code and drivers to determine if the adapter is in a stopped    state and should not touch the hardware.  ", "wr32m(wx, WX_MAC_RX_CFG, WX_MAC_RX_CFG_JE, WX_MAC_RX_CFG_JE);/* clear counters on read ": "wx_reset_misc(struct wx  wx){int i;  receive packets that size > 2048 ", "int wx_get_pcie_msix_counts(struct wx *wx, u16 *msix_count, u16 max_msix_count)": "wx_get_pcie_msix_counts - Gets MSI-X vector count    @wx: pointer to hardware structure    @msix_count: number of MSI interrupts that can be obtained    @max_msix_count: number of MSI interrupts that mac need      Read PCIe configuration space, and get the MSI-X vector count from    the capabilities table.  ", "static void wx_configure_rx(struct wx *wx)": "wx_vlan_rx_add_vid(wx->netdev, htons(ETH_P_8021Q), 0);for_each_set_bit_from(vid, wx->active_vlans, VLAN_N_VID)wx_vlan_rx_add_vid(wx->netdev, htons(ETH_P_8021Q), vid);}     wx_configure_rx - Configure Receive Unit after Reset   @wx: pointer to private structure     Configure the Rx unit of the MAC after a reset.  ", "if (vid)wx_set_vfta(wx, vid, VMDQ_P(0), false);clear_bit(vid, wx->active_vlans);return 0;}EXPORT_SYMBOL(wx_vlan_rx_kill_vid": "wx_vlan_rx_kill_vid(struct net_device  netdev, __be16 proto, u16 vid){struct wx  wx = netdev_priv(netdev);  remove VID from filter table ", "void wx_start_hw(struct wx *wx)": "wx_start_hw - Prepare hardware for TxRx    @wx: pointer to hardware structure      Starts the hardware using the generic start_hw function    and the generation start_hw function.    Then performs revision-specific operations, if any.  ", "for (f = 0; f < skb_shinfo(skb)->nr_frags; f++)count += TXD_USE_COUNT(skb_frag_size(&skb_shinfo(skb)->     frags[f]));if (wx_maybe_stop_tx(tx_ring, count + 3))return NETDEV_TX_BUSY;/* record the location of the first descriptor for this packet ": "wx_xmit_frame_ring(struct sk_buff  skb,      struct wx_ring  tx_ring){u16 count = TXD_USE_COUNT(skb_headlen(skb));struct wx_tx_buffer  first;u8 hdr_len = 0, ptype;unsigned short f;u32 tx_flags = 0;int tso;  need: 1 descriptor per page   PAGE_SIZEWX_MAX_DATA_PER_TXD,         + 1 desc for skb_headlenWX_MAX_DATA_PER_TXD,         + 2 desc gap to keep tail from touching head,         + 1 desc for context descriptor,   otherwise try next time ", "void wx_clear_interrupt_scheme(struct wx *wx)": "wx_clear_interrupt_scheme - Clear the current interrupt scheme settings   @wx: board private structure to clear interrupt scheme on     We go through and clear interrupt specific resources and reset the structure   to pre-load conditions  ", "wx_set_num_queues(wx);/* Set interrupt mode ": "wx_init_interrupt_scheme(struct wx  wx){int ret;  Number of supported queues ", "if (q_vector->rx.ring || q_vector->tx.ring)napi_schedule_irqoff(&q_vector->napi);return IRQ_HANDLED;}EXPORT_SYMBOL(wx_msix_clean_rings": "wx_msix_clean_rings(int __always_unused irq, void  data){struct wx_q_vector  q_vector = data;  EIAM disabled interrupts (on this vector) for us ", "if (!q_vector->rx.ring && !q_vector->tx.ring)continue;free_irq(entry->vector, q_vector);}if (wx->mac.type == wx_mac_em)free_irq(wx->msix_entries[vector].vector, wx);}EXPORT_SYMBOL(wx_free_irq": "wx_free_irq(struct wx  wx){struct pci_dev  pdev = wx->pdev;int vector;if (!(pdev->msix_enabled)) {free_irq(pdev->irq, wx);return;}for (vector = 0; vector < wx->num_q_vectors; vector++) {struct wx_q_vector  q_vector = wx->q_vector[vector];struct msix_entry  entry = &wx->msix_entries[vector];  free only the irqs that were actually requested ", "int wx_setup_isb_resources(struct wx *wx)": "wx_setup_isb_resources - allocate interrupt status resources   @wx: board private structure     Return 0 on success, negative on failure  ", "void wx_free_isb_resources(struct wx *wx)": "wx_free_isb_resources - allocate all queues Rx resources   @wx: board private structure     Return 0 on success, negative on failure  ", "void wx_configure_vectors(struct wx *wx)": "wx_configure_vectors - Configure vectors for hardware   @wx: board private structure     wx_configure_vectors sets up the hardware to properly generate MSI-XMSILEGACY   interrupts.  ", "void wx_clean_all_rx_rings(struct wx *wx)": "wx_clean_all_rx_rings - Free Rx Buffers for all queues   @wx: board private structure  ", "void wx_clean_all_tx_rings(struct wx *wx)": "wx_clean_all_tx_rings - Free Tx Buffers for all queues   @wx: board private structure  ", "err = wx_setup_all_tx_resources(wx);if (err)return err;/* allocate receive descriptors ": "wx_setup_resources(struct wx  wx){int err;  allocate transmit descriptors ", "void wx_get_stats64(struct net_device *netdev,    struct rtnl_link_stats64 *stats)": "wx_get_stats64 - Get System Network Statistics   @netdev: network interface device structure   @stats: storage space for 64bit statistics ", "if (netif_running(netdev) &&    pfvf->flags & OTX2_FLAG_RX_VLAN_SUPPORT)otx2_install_rxvlan_offload_flow(pfvf);/* update dmac address in ntuple and DMAC filter list ": "otx2_set_mac_address(struct net_device  netdev, void  p){struct otx2_nic  pfvf = netdev_priv(netdev);struct sockaddr  addr = p;if (!is_valid_ether_addr(addr->sa_data))return -EADDRNOTAVAIL;if (!otx2_hw_set_mac_addr(pfvf, addr->sa_data)) {eth_hw_addr_set(netdev, addr->sa_data);  update dmac field in vlan offload rule ", "if (!is_valid_ether_addr(netdev->dev_addr))eth_hw_addr_random(netdev);}EXPORT_SYMBOL(otx2_get_mac_from_af": "otx2_get_mac_from_af(struct net_device  netdev){struct otx2_nic  pfvf = netdev_priv(netdev);int err;err = otx2_hw_get_mac_addr(pfvf, netdev);if (err)dev_warn(pfvf->dev, \"Failed to read mac from hardware\\n\");  If AF doesn't provide a valid MAC, generate a random one ", "if (lvl == NIX_TXSCH_LVL_SMQ) ": "otx2_txschq_config(struct otx2_nic  pfvf, int lvl, int prio, bool txschq_for_pfc){u16 ( schq_list)[MAX_TXSCHQ_PER_FUNC];struct otx2_hw  hw = &pfvf->hw;struct nix_txschq_config  req;u64 schq, parent;u64 dwrr_val;dwrr_val = mtu_to_dwrr_weight(pfvf, pfvf->tx_max_pktlen);req = otx2_mbox_alloc_msg_nix_txschq_cfg(&pfvf->mbox);if (!req)return -ENOMEM;req->lvl = lvl;req->num_regs = 1;schq_list = hw->txschq_list;#ifdef CONFIG_DCBif (txschq_for_pfc)schq_list = pfvf->pfc_schq_list;#endifschq = schq_list[lvl][prio];  Set topology e.t.c configuration ", "detach->partial = false;/* Send detach request to AF ": "otx2_detach_resources(struct mbox  mbox){struct rsrc_detach  detach;mutex_lock(&mbox->lock);detach = otx2_mbox_alloc_msg_detach_resources(mbox);if (!detach) {mutex_unlock(&mbox->lock);return -ENOMEM;}  detach all ", "attach = otx2_mbox_alloc_msg_attach_resources(&pfvf->mbox);if (!attach) ": "otx2_attach_npa_nix(struct otx2_nic  pfvf){struct rsrc_attach  attach;struct msg_req  msix;int err;mutex_lock(&pfvf->mbox.lock);  Get memory to put this msg ", "if (is_otx2_lbkvf(pfvf->pdev))req->maxlen = maxlen;err = otx2_sync_mbox_msg(&pfvf->mbox);mutex_unlock(&pfvf->mbox.lock);return err;}int otx2_config_pause_frm(struct otx2_nic *pfvf)": "otx2_get_max_mtu(pfvf) + OTX2_ETH_HLEN + OTX2_HW_TIMESTAMP_LEN;mutex_lock(&pfvf->mbox.lock);req = otx2_mbox_alloc_msg_nix_set_hw_frs(&pfvf->mbox);if (!req) {mutex_unlock(&pfvf->mbox.lock);return -ENOMEM;}req->maxlen = pfvf->netdev->mtu + OTX2_ETH_HLEN + OTX2_HW_TIMESTAMP_LEN;  Use max receive length supported by hardware for loopback devices ", "pfvf->tot_lmt_lines = (num_online_cpus() * LMT_BURST_SIZE);pfvf->hw.lmt_info = alloc_percpu(struct otx2_lmt_info);mutex_lock(&pfvf->mbox.lock);req = otx2_mbox_alloc_msg_lmtst_tbl_setup(&pfvf->mbox);if (!req) ": "cn10k_lmtst_init(struct otx2_nic  pfvf){struct lmtst_tbl_setup_req  req;struct otx2_lmt_info  lmt_info;int err, cpu;if (!test_bit(CN10K_LMTST, &pfvf->hw.cap_flag)) {pfvf->hw_ops = &otx2_hw_ops;return 0;}pfvf->hw_ops = &cn10k_hw_ops;  Total LMTLINES = num_online_cpus()   32 (For Burst flush).", "otx2_free_ntuple_mcam_entries(pfvf);if (!count)return 0;flow_cfg->flow_ent = devm_kmalloc_array(pfvf->dev, count,sizeof(u16), GFP_KERNEL);if (!flow_cfg->flow_ent) ": "otx2_alloc_mcam_entries(struct otx2_nic  pfvf, u16 count){struct otx2_flow_config  flow_cfg = pfvf->flow_cfg;struct npc_mcam_alloc_entry_req  req;struct npc_mcam_alloc_entry_rsp  rsp;int ent, allocated = 0;  Free current ones and allocate new ones with requested count ", "free_desc = (sq->cons_head - sq->head - 1 + sq->sqe_cnt) & (sq->sqe_cnt - 1);if (free_desc < sq->sqe_thresh)return false;if (free_desc < otx2_get_sqe_count(pfvf, skb))return false;num_segs = skb_shinfo(skb)->nr_frags + 1;/* If SKB doesn't fit in a single SQE, linearize it. * TODO: Consider adding JUMP descriptor instead. ": "otx2_sq_append_skb(struct net_device  netdev, struct otx2_snd_queue  sq,struct sk_buff  skb, u16 qidx){struct netdev_queue  txq = netdev_get_tx_queue(netdev, qidx);struct otx2_nic  pfvf = netdev_priv(netdev);int offset, num_segs, free_desc;struct nix_sqe_hdr_s  sqe_hdr;  Check if there is enough room between producer   and consumer index. ", "kfree(tc->tc_entries_bitmap);tc->tc_entries_bitmap =kcalloc(BITS_TO_LONGS(nic->flow_cfg->max_flows),sizeof(long), GFP_KERNEL);if (!tc->tc_entries_bitmap) ": "otx2_tc_alloc_ent_bitmap(struct otx2_nic  nic){struct otx2_tc_info  tc = &nic->tc_info;if (!nic->flow_cfg->max_flows)return 0;  Max flows changed, free the existing bitmap ", "if (is_dev_otx2(nic->pdev)) ": "otx2_setup_tc_cls_flower(struct otx2_nic  nic,    struct flow_cls_offload  cls_flower){switch (cls_flower->command) {case FLOW_CLS_REPLACE:return otx2_tc_add_flow(nic, cls_flower);case FLOW_CLS_DESTROY:return otx2_tc_del_flow(nic, cls_flower);case FLOW_CLS_STATS:return otx2_tc_get_flow_stats(nic, cls_flower);default:return -EOPNOTSUPP;}}static int otx2_tc_ingress_matchall_install(struct otx2_nic  nic,    struct tc_cls_matchall_offload  cls){struct netlink_ext_ack  extack = cls->common.extack;struct flow_action  actions = &cls->rule->action;struct flow_action_entry  entry;u64 rate;int err;err = otx2_tc_validate_flow(nic, actions, extack);if (err)return err;if (nic->flags & OTX2_FLAG_TC_MATCHALL_INGRESS_ENABLED) {NL_SET_ERR_MSG_MOD(extack,   \"Only one ingress MATCHALL ratelimitter can be offloaded\");return -ENOMEM;}entry = &cls->rule->action.entries[0];switch (entry->id) {case FLOW_ACTION_POLICE:  Ingress ratelimiting is not supported on OcteonTx2 ", "set_bit(0, &nic->rq_bmap);if (!nic->flow_cfg) ": "otx2_init_tc(struct otx2_nic  nic){struct otx2_tc_info  tc = &nic->tc_info;int err;  Exclude receive queue 0 being used for police action ", "otx2_write64(pf, RVU_PF_VFME_INT_ENA_W1CX(0), INTR_MASK(vfs));irq = pci_irq_vector(pf->pdev, RVU_PF_INT_VEC_VFME0);free_irq(irq, pf);/* Disable VFs FLR interrupts ": "otx2_stop(netdev);netdev_info(netdev, \"Changing MTU from %d to %d\\n\",    netdev->mtu, new_mtu);netdev->mtu = new_mtu;if (if_up)err = otx2_open(netdev);return err;}static void otx2_disable_flr_me_intr(struct otx2_nic  pf){int irq, vfs = pf->total_vfs;  Disable VFs ME interrupts ", "u16 htb_maj_id = smp_load_acquire(&pf->qos.maj_id);if (unlikely(htb_maj_id)) ": "otx2_select_queue(struct net_device  netdev, struct sk_buff  skb,      struct net_device  sb_dev){struct otx2_nic  pf = netdev_priv(netdev);bool qos_enabled;#ifdef CONFIG_DCBu8 vlan_prio;#endifint txq;qos_enabled = (netdev->real_num_tx_queues > pf->hw.tx_queues) ? true : false;if (unlikely(qos_enabled)) {  This smp_load_acquire() pairs with smp_store_release() in   otx2_qos_root_add() called from htb offload root creation ", "otx2_mbox_reset(mbox, devid);}return 0;}EXPORT_SYMBOL(otx2_mbox_init": "otx2_mbox_init(struct otx2_mbox  mbox, void  hwbase, struct pci_dev  pdev,   void  reg_base, int direction, int ndevs){struct otx2_mbox_dev  mdev;int devid, err;err = otx2_mbox_setup(mbox, pdev, reg_base, direction, ndevs);if (err)return err;mbox->hwbase = hwbase;for (devid = 0; devid < ndevs; devid++) {mdev = &mbox->dev[devid];mdev->mbase = mbox->hwbase + (devid   MBOX_SIZE);mdev->hwbase = mdev->mbase;spin_lock_init(&mdev->mbox_lock);  Init header to reset value ", "otx2_mbox_reset(mbox, devid);}return 0;}EXPORT_SYMBOL(otx2_mbox_regions_init": "otx2_mbox_regions_init(struct otx2_mbox  mbox, void   hwbase,   struct pci_dev  pdev, void  reg_base,   int direction, int ndevs, unsigned long  pf_bmap){struct otx2_mbox_dev  mdev;int devid, err;err = otx2_mbox_setup(mbox, pdev, reg_base, direction, ndevs);if (err)return err;mbox->hwbase = hwbase[0];for (devid = 0; devid < ndevs; devid++) {if (!test_bit(devid, pf_bmap))continue;mdev = &mbox->dev[devid];mdev->mbase = hwbase[devid];mdev->hwbase = hwbase[devid];spin_lock_init(&mdev->mbox_lock);  Init header to reset value ", "if (mdev->mbase != hw_mbase)memcpy(hw_mbase + mbox->tx_start + msgs_offset,       mdev->mbase + mbox->tx_start + msgs_offset,       mdev->msg_size);spin_lock(&mdev->mbox_lock);tx_hdr->msg_size = mdev->msg_size;/* Reset header for next messages ": "otx2_mbox_msg_send(struct otx2_mbox  mbox, int devid){struct otx2_mbox_dev  mdev = &mbox->dev[devid];struct mbox_hdr  tx_hdr,  rx_hdr;void  hw_mbase = mdev->hwbase;tx_hdr = hw_mbase + mbox->tx_start;rx_hdr = hw_mbase + mbox->rx_start;  If bounce buffer is implemented copy mbox messages from   bounce buffer to hw mbox memory. ", "if ((mdev->msg_size + size) > mbox->tx_size - msgs_offset)goto exit;if ((mdev->rsp_size + size_rsp) > mbox->rx_size - msgs_offset)goto exit;if (mdev->msg_size == 0)mdev->num_msgs = 0;mdev->num_msgs++;msghdr = mdev->mbase + mbox->tx_start + msgs_offset + mdev->msg_size;/* Clear the whole msg region ": "otx2_mbox_alloc_msg_rsp(struct otx2_mbox  mbox, int devid,    int size, int size_rsp){struct otx2_mbox_dev  mdev = &mbox->dev[devid];struct mbox_msghdr  msghdr = NULL;spin_lock(&mdev->mbox_lock);size = ALIGN(size, MBOX_MSG_ALIGN);size_rsp = ALIGN(size_rsp, MBOX_MSG_ALIGN);  Check if there is space in mailbox ", "static void ocelot_match_all_as_mac_etype(struct ocelot *ocelot, int port,  int lookup, bool on)": "ocelot_vcap_filter_del_aux_resources(struct ocelot  ocelot,     struct ocelot_vcap_filter  filter){if (filter->block_id == VCAP_IS2 && filter->action.police_ena)ocelot_vcap_policer_del(ocelot, filter->action.pol_ix);if (filter->block_id == VCAP_IS2 && filter->action.mirror_ena)ocelot_mirror_put(ocelot);}static int ocelot_vcap_filter_add_to_block(struct ocelot  ocelot,   struct ocelot_vcap_block  block,   struct ocelot_vcap_filter  filter,   struct netlink_ext_ack  extack){struct list_head  pos = &block->rules;struct ocelot_vcap_filter  tmp;int ret;ret = ocelot_vcap_filter_add_aux_resources(ocelot, filter, extack);if (ret)return ret;block->count++;list_for_each_entry(tmp, &block->rules, list) {if (filter->prio < tmp->prio) {pos = &tmp->list;break;}}list_add_tail(&filter->list, pos);return 0;}static bool ocelot_vcap_filter_equal(const struct ocelot_vcap_filter  a,     const struct ocelot_vcap_filter  b){return !memcmp(&a->id, &b->id, sizeof(struct ocelot_vcap_id));}static int ocelot_vcap_block_get_filter_index(struct ocelot_vcap_block  block,      struct ocelot_vcap_filter  filter){struct ocelot_vcap_filter  tmp;int index = 0;list_for_each_entry(tmp, &block->rules, list) {if (ocelot_vcap_filter_equal(filter, tmp))return index;index++;}return -ENOENT;}static struct ocelot_vcap_filter ocelot_vcap_block_find_filter_by_index(struct ocelot_vcap_block  block,       int index){struct ocelot_vcap_filter  tmp;int i = 0;list_for_each_entry(tmp, &block->rules, list) {if (i == index)return tmp;++i;}return NULL;}struct ocelot_vcap_filter  ocelot_vcap_block_find_filter_by_id(struct ocelot_vcap_block  block,    unsigned long cookie, bool tc_offload){struct ocelot_vcap_filter  filter;list_for_each_entry(filter, &block->rules, list)if (filter->id.tc_offload == tc_offload &&    filter->id.cookie == cookie)return filter;return NULL;}EXPORT_SYMBOL(ocelot_vcap_block_find_filter_by_id);  If @on=false, then SNAP, ARP, IP and OAM frames will not match on keys based   on destination and source MAC addresses, but only on higher-level protocol   information. The only frame types to match on keys containing MAC addresses   in this case are non-SNAP, non-ARP, non-IP and non-OAM frames.     If @on=true, then the above frame types (SNAP, ARP, IP and OAM) will match   on MAC_ETYPE keys such as destination and source MAC on this ingress port.   However the setting has the side effect of making these frames not matching   on any _other_ keys than MAC_ETYPE ones. ", "static void ocelot_disable_reservation_watermarks(struct ocelot *ocelot,  int port)": "ocelot_wm_status(struct ocelot  ocelot, int index, u32  inuse,     u32  maxuse){int res_stat = ocelot_read_gix(ocelot, QSYS_RES_STAT, index);return ocelot->ops->wm_stat(res_stat, inuse, maxuse);}  The hardware comes out of reset with strange defaults: the sum of all   reservations for frame memory is larger than the total buffer size.   One has to wonder how can the reservation watermarks still guarantee   anything under congestion.   Bring some sense into the hardware by changing the defaults to disable all   reservations and rely only on the sharing watermark for frames with drop   precedence 0. The user can still explicitly request reservations per port   and per port-tc through devlink-sb. ", "int ocelot_sb_pool_get(struct ocelot *ocelot, unsigned int sb_index,       u16 pool_index,       struct devlink_sb_pool_info *pool_info)": "ocelot_sb_pool_set ", "int ocelot_sb_port_pool_get(struct ocelot *ocelot, int port,    unsigned int sb_index, u16 pool_index,    u32 *p_threshold)": "ocelot_sb_port_pool_set ", "int ocelot_sb_tc_pool_bind_get(struct ocelot *ocelot, int port,       unsigned int sb_index, u16 tc_index,       enum devlink_sb_pool_type pool_type,       u16 *p_pool_index, u32 *p_threshold)": "ocelot_sb_tc_pool_bind_set ", "if (ns >= 0x3ffffff0 && ns <= 0x3fffffff) ": "ocelot_ptp_gettime64(struct ptp_clock_info  ptp, struct timespec64  ts){struct ocelot  ocelot = container_of(ptp, struct ocelot, ptp_info);unsigned long flags;time64_t s;u32 val;s64 ns;spin_lock_irqsave(&ocelot->ptp_clock_lock, flags);val = ocelot_read_rix(ocelot, PTP_PIN_CFG, TOD_ACC_PIN);val &= ~(PTP_PIN_CFG_SYNC | PTP_PIN_CFG_ACTION_MASK | PTP_PIN_CFG_DOM);val |= PTP_PIN_CFG_ACTION(PTP_PIN_ACTION_SAVE);ocelot_write_rix(ocelot, val, PTP_PIN_CFG, TOD_ACC_PIN);s = ocelot_read_rix(ocelot, PTP_PIN_TOD_SEC_MSB, TOD_ACC_PIN) & 0xffff;s <<= 32;s += ocelot_read_rix(ocelot, PTP_PIN_TOD_SEC_LSB, TOD_ACC_PIN);ns = ocelot_read_rix(ocelot, PTP_PIN_TOD_NSEC, TOD_ACC_PIN);spin_unlock_irqrestore(&ocelot->ptp_clock_lock, flags);  Deal with negative values ", "struct timespec64 ts;u64 now;ocelot_ptp_gettime64(ptp, &ts);now = ktime_to_ns(timespec64_to_ktime(ts));ts = ns_to_timespec64(now + delta);ocelot_ptp_settime64(ptp, &ts);}return 0;}EXPORT_SYMBOL(ocelot_ptp_adjtime": "ocelot_ptp_adjtime(struct ptp_clock_info  ptp, s64 delta){if (delta > -(NSEC_PER_SEC  2) && delta < (NSEC_PER_SEC  2)) {struct ocelot  ocelot = container_of(ptp, struct ocelot,     ptp_info);unsigned long flags;u32 val;spin_lock_irqsave(&ocelot->ptp_clock_lock, flags);val = ocelot_read_rix(ocelot, PTP_PIN_CFG, TOD_ACC_PIN);val &= ~(PTP_PIN_CFG_SYNC | PTP_PIN_CFG_ACTION_MASK | PTP_PIN_CFG_DOM);val |= PTP_PIN_CFG_ACTION(PTP_PIN_ACTION_IDLE);ocelot_write_rix(ocelot, val, PTP_PIN_CFG, TOD_ACC_PIN);ocelot_write_rix(ocelot, 0, PTP_PIN_TOD_SEC_LSB, TOD_ACC_PIN);ocelot_write_rix(ocelot, 0, PTP_PIN_TOD_SEC_MSB, TOD_ACC_PIN);ocelot_write_rix(ocelot, delta, PTP_PIN_TOD_NSEC, TOD_ACC_PIN);val = ocelot_read_rix(ocelot, PTP_PIN_CFG, TOD_ACC_PIN);val &= ~(PTP_PIN_CFG_SYNC | PTP_PIN_CFG_ACTION_MASK | PTP_PIN_CFG_DOM);val |= PTP_PIN_CFG_ACTION(PTP_PIN_ACTION_DELTA);ocelot_write_rix(ocelot, val, PTP_PIN_CFG, TOD_ACC_PIN);spin_unlock_irqrestore(&ocelot->ptp_clock_lock, flags);if (ocelot->ops->tas_clock_adjust)ocelot->ops->tas_clock_adjust(ocelot);} else {  Fall back using ocelot_ptp_settime64 which is not exact. ", "if (adj >= (1L << 30)) ": "ocelot_ptp_adjfine(struct ptp_clock_info  ptp, long scaled_ppm){struct ocelot  ocelot = container_of(ptp, struct ocelot, ptp_info);u32 unit = 0, direction = 0;unsigned long flags;u64 adj = 0;spin_lock_irqsave(&ocelot->ptp_clock_lock, flags);if (!scaled_ppm)goto disable_adj;if (scaled_ppm < 0) {direction = PTP_CFG_CLK_ADJ_CFG_DIR;scaled_ppm = -scaled_ppm;}adj = PSEC_PER_SEC << 16;do_div(adj, scaled_ppm);do_div(adj, 1000);  If the adjustment value is too large, use ns instead ", "if (rq->perout.flags & ~(PTP_PEROUT_DUTY_CYCLE | PTP_PEROUT_PHASE))return -EOPNOTSUPP;pin = ptp_find_pin(ocelot->ptp_clock, PTP_PF_PEROUT,   rq->perout.index);if (pin == 0)ptp_pin = PTP_PIN_0;else if (pin == 1)ptp_pin = PTP_PIN_1;else if (pin == 2)ptp_pin = PTP_PIN_2;else if (pin == 3)ptp_pin = PTP_PIN_3;elsereturn -EBUSY;ts_period.tv_sec = rq->perout.period.sec;ts_period.tv_nsec = rq->perout.period.nsec;if (ts_period.tv_sec == 1 && ts_period.tv_nsec == 0)pps = true;/* Handle turning off ": "ocelot_ptp_enable(struct ptp_clock_info  ptp,      struct ptp_clock_request  rq, int on){struct ocelot  ocelot = container_of(ptp, struct ocelot, ptp_info);struct timespec64 ts_phase, ts_period;enum ocelot_ptp_pins ptp_pin;unsigned long flags;bool pps = false;int pin = -1;s64 wf_high;s64 wf_low;u32 val;switch (rq->type) {case PTP_CLK_REQ_PEROUT:  Reject requests with unsupported flags ", "switch (cfg.tx_type) ": "ocelot_hwstamp_set(struct ocelot  ocelot, int port, struct ifreq  ifr){struct ocelot_port  ocelot_port = ocelot->ports[port];bool l2 = false, l4 = false;struct hwtstamp_config cfg;int err;if (copy_from_user(&cfg, ifr->ifr_data, sizeof(cfg)))return -EFAULT;  Tx type sanity check ", "if (!ptp_cmd)return 0;ptp_class = ptp_classify_raw(skb);if (ptp_class == PTP_CLASS_NONE)return -EINVAL;/* Store ptp_cmd in OCELOT_SKB_CB(skb)->ptp_cmd ": "ocelot_port_txtstamp_request(struct ocelot  ocelot, int port, struct sk_buff  skb, struct sk_buff   clone){struct ocelot_port  ocelot_port = ocelot->ports[port];u8 ptp_cmd = ocelot_port->ptp_cmd;unsigned int ptp_class;int err;  Don't do anything if PTP timestamping not enabled ", "if (!(val & SYS_PTP_STATUS_PTP_MESS_VLD))break;WARN_ON(val & SYS_PTP_STATUS_PTP_OVFL);/* Retrieve the ts ID and Tx port ": "ocelot_get_txtstamp(struct ocelot  ocelot){int budget = OCELOT_PTP_QUEUE_SZ;while (budget--) {struct sk_buff  skb,  skb_tmp,  skb_match = NULL;struct skb_shared_hwtstamps shhwtstamps;u32 val, id, seqid, txport;struct ocelot_port  port;struct timespec64 ts;unsigned long flags;val = ocelot_read(ocelot, SYS_PTP_STATUS);  Check if a timestamp can be retrieved ", "if (!ptp_clock)return 0;ocelot->ptp_clock = ptp_clock;ocelot_write(ocelot, SYS_PTP_CFG_PTP_STAMP_WID(30), SYS_PTP_CFG);ocelot_write(ocelot, 0xffffffff, ANA_TABLES_PTP_ID_LOW);ocelot_write(ocelot, 0xffffffff, ANA_TABLES_PTP_ID_HIGH);ocelot_write(ocelot, PTP_CFG_MISC_PTP_EN, PTP_CFG_MISC);return 0;}EXPORT_SYMBOL(ocelot_init_timestamp": "ocelot_init_timestamp(struct ocelot  ocelot,  const struct ptp_clock_info  info){struct ptp_clock  ptp_clock;int i;ocelot->ptp_info =  info;for (i = 0; i < OCELOT_PTP_PINS_NUM; i++) {struct ptp_pin_desc  p = &ocelot->ptp_pins[i];snprintf(p->name, sizeof(p->name), \"switch_1588_dat%d\", i);p->index = i;p->func = PTP_PF_NONE;}ocelot->ptp_info.pin_config = &ocelot->ptp_pins[0];ptp_clock = ptp_clock_register(&ocelot->ptp_info, ocelot->dev);if (IS_ERR(ptp_clock))return PTR_ERR(ptp_clock);  Check if PHC support is missing at the configuration level ", "if (type == ENTRYTYPE_MACv4)mc_ports = (mac[1] << 8) | mac[2];else if (type == ENTRYTYPE_MACv6)mc_ports = (mac[0] << 8) | mac[1];elsemc_ports = 0;if (mc_ports & BIT(ocelot->num_phys_ports))cmd |= ANA_TABLES_MACACCESS_MAC_CPU_COPY;ocelot_mact_select(ocelot, mac, vid);/* Issue a write command ": "ocelot_mact_learn(struct ocelot  ocelot, int port,       const unsigned char mac[ETH_ALEN],       unsigned int vid, enum macaccess_entry_type type){u32 cmd = ANA_TABLES_MACACCESS_VALID |ANA_TABLES_MACACCESS_DEST_IDX(port) |ANA_TABLES_MACACCESS_ENTRYTYPE(type) |ANA_TABLES_MACACCESS_MAC_TABLE_CMD(MACACCESS_CMD_LEARN);unsigned int mc_ports;int err;  Set MAC_CPU_COPY if the CPU port is used by a multicast entry ", "ocelot_write(ocelot,     ANA_TABLES_MACACCESS_MAC_TABLE_CMD(MACACCESS_CMD_FORGET),     ANA_TABLES_MACACCESS);err = ocelot_mact_wait_for_completion(ocelot);mutex_unlock(&ocelot->mact_lock);return err;}EXPORT_SYMBOL(ocelot_mact_forget": "ocelot_mact_forget(struct ocelot  ocelot,       const unsigned char mac[ETH_ALEN], unsigned int vid){int err;mutex_lock(&ocelot->mact_lock);ocelot_mact_select(ocelot, mac, vid);  Issue a forget command ", "ocelot_write(ocelot, ANA_TABLES_MACACCESS_VALID |     ANA_TABLES_MACACCESS_MAC_TABLE_CMD(MACACCESS_CMD_READ),     ANA_TABLES_MACACCESS);if (ocelot_mact_wait_for_completion(ocelot)) ": "ocelot_mact_lookup(struct ocelot  ocelot, int  dst_idx,       const unsigned char mac[ETH_ALEN],       unsigned int vid, enum macaccess_entry_type  type){int val;mutex_lock(&ocelot->mact_lock);ocelot_mact_select(ocelot, mac, vid);  Issue a read command with MACACCESS_VALID=1. ", "regmap_write(ocelot->targets[HSIO], HSIO_PLL5G_CFG4,     HSIO_PLL5G_CFG4_IB_CTRL(0x7600) |     HSIO_PLL5G_CFG4_IB_BIAS_CTRL(0x8));regmap_write(ocelot->targets[HSIO], HSIO_PLL5G_CFG0,     HSIO_PLL5G_CFG0_CORE_CLK_DIV(0x11) |     HSIO_PLL5G_CFG0_CPU_CLK_DIV(2) |     HSIO_PLL5G_CFG0_ENA_BIAS |     HSIO_PLL5G_CFG0_ENA_VCO_BUF |     HSIO_PLL5G_CFG0_ENA_CP1 |     HSIO_PLL5G_CFG0_SELCPI(2) |     HSIO_PLL5G_CFG0_LOOP_BW_RES(0xe) |     HSIO_PLL5G_CFG0_SELBGV820(4) |     HSIO_PLL5G_CFG0_DIV4 |     HSIO_PLL5G_CFG0_ENA_CLKTREE |     HSIO_PLL5G_CFG0_ENA_LANE);regmap_write(ocelot->targets[HSIO], HSIO_PLL5G_CFG2,     HSIO_PLL5G_CFG2_EN_RESET_FRQ_DET |     HSIO_PLL5G_CFG2_EN_RESET_OVERRUN |     HSIO_PLL5G_CFG2_GAIN_TEST(0x8) |     HSIO_PLL5G_CFG2_ENA_AMPCTRL |     HSIO_PLL5G_CFG2_PWD_AMPCTRL_N |     HSIO_PLL5G_CFG2_AMPC_SEL(0x10));}EXPORT_SYMBOL(ocelot_pll5_init": "ocelot_pll5_init(struct ocelot  ocelot){  Configure PLL5. This will need a proper CCF driver   The values are coming from the VTSS API for Ocelot ", "if (ocelot_port_uses_native_vlan(ocelot, port)) ": "ocelot_vlan_prepare(struct ocelot  ocelot, int port, u16 vid, bool pvid,bool untagged, struct netlink_ext_ack  extack){if (untagged) {  We are adding an egress-tagged VLAN ", "if (!vid)return 0;err = ocelot_vlan_member_add(ocelot, port, vid, untagged);if (err)return err;/* Default ingress vlan classification ": "ocelot_vlan_add(struct ocelot  ocelot, int port, u16 vid, bool pvid,    bool untagged){int err;  Ignore VID 0 added to our RX filter by the 8021q module, since   that collides with OCELOT_STANDALONE_PVID and changes it from   egress-untagged to egress-tagged. ", "if (del_pvid)ocelot_port_set_pvid(ocelot, port, NULL);/* Egress ": "ocelot_vlan_del(struct ocelot  ocelot, int port, u16 vid){struct ocelot_port  ocelot_port = ocelot->ports[port];bool del_pvid = false;int err;if (!vid)return 0;if (ocelot_port->pvid_vlan && ocelot_port->pvid_vlan->vid == vid)del_pvid = true;err = ocelot_vlan_member_del(ocelot, port, vid);if (err)return err;  Ingress ", "sz = ocelot_rx_frame_word(ocelot, grp, false, &val);if (sz < 0) ": "ocelot_xtr_poll_frame(struct ocelot  ocelot, int grp, struct sk_buff   nskb){u64 timestamp, src_port, len;u32 xfh[OCELOT_TAG_LEN  4];struct net_device  dev;struct sk_buff  skb;int sz, buf_len;u32 val,  buf;int err;err = ocelot_xtr_poll_xfh(ocelot, grp, xfh);if (err)return err;ocelot_xfh_get_src_port(xfh, &src_port);ocelot_xfh_get_len(xfh, &len);ocelot_xfh_get_rew_val(xfh, &timestamp);if (WARN_ON(src_port >= ocelot->num_phys_ports))return -EINVAL;dev = ocelot->ops->port_to_netdev(ocelot, src_port);if (!dev)return -EINVAL;skb = netdev_alloc_skb(dev, len);if (unlikely(!skb)) {netdev_err(dev, \"Unable to allocate sk_buff\\n\");return -ENOMEM;}buf_len = len - ETH_FCS_LEN;buf = (u32  )skb_put(skb, buf_len);len = 0;do {sz = ocelot_rx_frame_word(ocelot, grp, false, &val);if (sz < 0) {err = sz;goto out_free_skb;} buf++ = val;len += sz;} while (len < buf_len);  Read the FCS ", "while (i < (OCELOT_BUFFER_CELL_SZ / 4)) ": "ocelot_port_inject_frame(struct ocelot  ocelot, int port, int grp,      u32 rew_op, struct sk_buff  skb){u32 ifh[OCELOT_TAG_LEN  4] = {0};unsigned int i, count, last;ocelot_write_rix(ocelot, QS_INJ_CTRL_GAP_SIZE(1) | QS_INJ_CTRL_SOF, QS_INJ_CTRL, grp);ocelot_ifh_port_set(ifh, port, rew_op, skb_vlan_tag_get(skb));for (i = 0; i < OCELOT_TAG_LEN  4; i++)ocelot_write_rix(ocelot, ifh[i], QS_INJ_WR, grp);count = DIV_ROUND_UP(skb->len, 4);last = skb->len % 4;for (i = 0; i < count; i++)ocelot_write_rix(ocelot, ((u32  )skb->data)[i], QS_INJ_WR, grp);  Add padding ", "mutex_lock(&ocelot->mact_lock);/* Loop through all the mac tables entries. ": "ocelot_fdb_dump(struct ocelot  ocelot, int port,    dsa_fdb_dump_cb_t  cb, void  data){int err = 0;int i, j;  We could take the lock just around ocelot_mact_read, but doing so   thousands of times in a row seems rather pointless and inefficient. ", "if (!age_period)age_period = 1;ocelot_rmw(ocelot, age_period, ANA_AUTOAGE_AGE_PERIOD_M, ANA_AUTOAGE);}EXPORT_SYMBOL(ocelot_set_ageing_time": "ocelot_set_ageing_time(struct ocelot  ocelot, unsigned int msecs){unsigned int age_period = ANA_AUTOAGE_AGE_PERIOD(msecs  2000);  Setting AGE_PERIOD to zero effectively disables automatic aging,   which is clearly not what our intention is. So avoid that. ", "mc = devm_kzalloc(ocelot->dev, sizeof(*mc), GFP_KERNEL);if (!mc)return -ENOMEM;mc->entry_type = ocelot_classify_mdb(mdb->addr);ether_addr_copy(mc->addr, mdb->addr);mc->vid = vid;list_add_tail(&mc->list, &ocelot->multicast);} else ": "ocelot_port_mdb_add(struct ocelot  ocelot, int port,const struct switchdev_obj_port_mdb  mdb,const struct net_device  bridge){unsigned char addr[ETH_ALEN];struct ocelot_multicast  mc;struct ocelot_pgid  pgid;u16 vid = mdb->vid;if (!vid)vid = ocelot_vlan_unaware_pvid(ocelot, bridge);mc = ocelot_multicast_get(ocelot, mdb->addr, vid);if (!mc) {  New entry ", "pgid = ocelot_mdb_get_pgid(ocelot, mc);if (IS_ERR(pgid))return PTR_ERR(pgid);mc->pgid = pgid;ocelot_encode_ports_to_mdb(addr, mc);if (mc->entry_type != ENTRYTYPE_MACv4 &&    mc->entry_type != ENTRYTYPE_MACv6)ocelot_write_rix(ocelot, pgid->ports, ANA_PGID_PGID, pgid->index);return ocelot_mact_learn(ocelot, pgid->index, addr, vid, mc->entry_type);}EXPORT_SYMBOL(ocelot_port_mdb_del": "ocelot_port_mdb_del(struct ocelot  ocelot, int port,const struct switchdev_obj_port_mdb  mdb,const struct net_device  bridge){unsigned char addr[ETH_ALEN];struct ocelot_multicast  mc;struct ocelot_pgid  pgid;u16 vid = mdb->vid;if (!vid)vid = ocelot_vlan_unaware_pvid(ocelot, bridge);mc = ocelot_multicast_get(ocelot, mdb->addr, vid);if (!mc)return -ENOENT;ocelot_encode_ports_to_mdb(addr, mc);ocelot_mact_forget(ocelot, addr, vid);ocelot_pgid_free(ocelot, mc->pgid);mc->ports &= ~BIT(port);if (!mc->ports) {list_del(&mc->list);devm_kfree(ocelot->dev, mc);return 0;}  We have a PGID with fewer ports now ", "ocelot_set_aggr_pgids(ocelot);mutex_unlock(&ocelot->fwd_domain_lock);}EXPORT_SYMBOL(ocelot_port_lag_change": "ocelot_port_lag_change(struct ocelot  ocelot, int port, bool lag_tx_active){struct ocelot_port  ocelot_port = ocelot->ports[port];mutex_lock(&ocelot->fwd_domain_lock);ocelot_port->lag_tx_active = lag_tx_active;  Rebalance the LAGs ", "pause_start = 6 * maxlen / OCELOT_BUFFER_CELL_SZ;pause_stop = 4 * maxlen / OCELOT_BUFFER_CELL_SZ;ocelot_fields_write(ocelot, port, SYS_PAUSE_CFG_PAUSE_START,    pause_start);ocelot_fields_write(ocelot, port, SYS_PAUSE_CFG_PAUSE_STOP,    pause_stop);/* Tail dropping watermarks ": "ocelot_port_set_maxlen(struct ocelot  ocelot, int port, size_t sdu){struct ocelot_port  ocelot_port = ocelot->ports[port];int maxlen = sdu + ETH_HLEN + ETH_FCS_LEN;int pause_start, pause_stop;int atop, atop_tot;if (port == ocelot->npi) {maxlen += OCELOT_TAG_LEN;if (ocelot->npi_inj_prefix == OCELOT_TAG_PREFIX_SHORT)maxlen += OCELOT_SHORT_PREFIX_LEN;else if (ocelot->npi_inj_prefix == OCELOT_TAG_PREFIX_LONG)maxlen += OCELOT_LONG_PREFIX_LEN;}ocelot_port_writel(ocelot_port, maxlen, DEV_MAC_MAXLEN_CFG);  Set Pause watermark hysteresis ", "/* Set MAC IFG Gaps * FDX: TX_IFG = 5, RX_IFG1 = RX_IFG2 = 0 * !FDX: TX_IFG = 5, RX_IFG1 = RX_IFG2 = 5 ": "ocelot_init_port(struct ocelot  ocelot, int port){struct ocelot_port  ocelot_port = ocelot->ports[port];skb_queue_head_init(&ocelot_port->tx_skbs);  Basic L2 initialization ", "err = readx_poll_timeout(ocelot_mem_init_status, ocelot, val, !val, MEM_INIT_SLEEP_US, MEM_INIT_TIMEOUT_US);if (err)return err;err = regmap_field_write(ocelot->regfields[SYS_RESET_CFG_MEM_ENA], 1);if (err)return err;return regmap_field_write(ocelot->regfields[SYS_RESET_CFG_CORE_ENA], 1);}EXPORT_SYMBOL(ocelot_reset": "ocelot_reset(struct ocelot  ocelot){int err;u32 val;err = regmap_field_write(ocelot->regfields[SYS_RESET_CFG_MEM_INIT], 1);if (err)return err;err = regmap_field_write(ocelot->regfields[SYS_RESET_CFG_MEM_ENA], 1);if (err)return err;  MEM_INIT is a self-clearing bit. Wait for it to be cleared (should be   100us) before enabling the switch core. ", "ether_addr_copy(filter->key.etype.dmac.value, mrp_test_dmac);ether_addr_copy(filter->key.etype.dmac.mask, mrp_mask);}static int ocelot_mrp_trap_add(struct ocelot *ocelot, int port)": "ocelot_mrp_del_vcap(struct ocelot  ocelot, int id){struct ocelot_vcap_block  block_vcap_is2;struct ocelot_vcap_filter  filter;block_vcap_is2 = &ocelot->block[VCAP_IS2];filter = ocelot_vcap_block_find_filter_by_id(block_vcap_is2, id,     false);if (!filter)return 0;return ocelot_vcap_filter_del(ocelot, filter);}static int ocelot_mrp_redirect_add_vcap(struct ocelot  ocelot, int src_port,int dst_port){const u8 mrp_test_mask[] = { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff };struct ocelot_vcap_filter  filter;int err;filter = kzalloc(sizeof( filter), GFP_KERNEL);if (!filter)return -ENOMEM;filter->key_type = OCELOT_VCAP_KEY_ETYPE;filter->prio = 1;filter->id.cookie = OCELOT_VCAP_IS2_MRP_REDIRECT(ocelot, src_port);filter->id.tc_offload = false;filter->block_id = VCAP_IS2;filter->type = OCELOT_VCAP_FILTER_OFFLOAD;filter->ingress_port_mask = BIT(src_port);ether_addr_copy(filter->key.etype.dmac.value, mrp_test_dmac);ether_addr_copy(filter->key.etype.dmac.mask, mrp_test_mask);filter->action.mask_mode = OCELOT_MASK_MODE_REDIRECT;filter->action.port_mask = BIT(dst_port);err = ocelot_vcap_filter_add(ocelot, filter, NULL);if (err)kfree(filter);return err;}static void ocelot_populate_mrp_trap_key(struct ocelot_vcap_filter  filter){const u8 mrp_mask[] = { 0xff, 0xff, 0xff, 0xff, 0xff, 0x00 };  Here is possible to use control or test dmac because the mask   doesn't cover the LSB ", "[QSYS_SWITCH_PORT_MODE_PORT_ENA] = REG_FIELD_ID(QSYS_SWITCH_PORT_MODE, 14, 14, 12, 4),[QSYS_SWITCH_PORT_MODE_SCH_NEXT_CFG] = REG_FIELD_ID(QSYS_SWITCH_PORT_MODE, 11, 13, 12, 4),[QSYS_SWITCH_PORT_MODE_YEL_RSRVD] = REG_FIELD_ID(QSYS_SWITCH_PORT_MODE, 10, 10, 12, 4),[QSYS_SWITCH_PORT_MODE_INGRESS_DROP_MODE] = REG_FIELD_ID(QSYS_SWITCH_PORT_MODE, 9, 9, 12, 4),[QSYS_SWITCH_PORT_MODE_TX_PFC_ENA] = REG_FIELD_ID(QSYS_SWITCH_PORT_MODE, 1, 8, 12, 4),[QSYS_SWITCH_PORT_MODE_TX_PFC_MODE] = REG_FIELD_ID(QSYS_SWITCH_PORT_MODE, 0, 0, 12, 4),[SYS_PORT_MODE_DATA_WO_TS] = REG_FIELD_ID(SYS_PORT_MODE, 5, 6, 12, 4),[SYS_PORT_MODE_INCL_INJ_HDR] = REG_FIELD_ID(SYS_PORT_MODE, 3, 4, 12, 4),[SYS_PORT_MODE_INCL_XTR_HDR] = REG_FIELD_ID(SYS_PORT_MODE, 1, 2, 12, 4),[SYS_PORT_MODE_INCL_HDR_ERR] = REG_FIELD_ID(SYS_PORT_MODE, 0, 0, 12, 4),[SYS_PAUSE_CFG_PAUSE_START] = REG_FIELD_ID(SYS_PAUSE_CFG, 10, 18, 12, 4),[SYS_PAUSE_CFG_PAUSE_STOP] = REG_FIELD_ID(SYS_PAUSE_CFG, 1, 9, 12, 4),[SYS_PAUSE_CFG_PAUSE_ENA] = REG_FIELD_ID(SYS_PAUSE_CFG, 0, 1, 12, 4),};EXPORT_SYMBOL(vsc7514_regfields": "vsc7514_regfields[REGFIELD_MAX] = {[ANA_ADVLEARN_VLAN_CHK] = REG_FIELD(ANA_ADVLEARN, 11, 11),[ANA_ADVLEARN_LEARN_MIRROR] = REG_FIELD(ANA_ADVLEARN, 0, 10),[ANA_ANEVENTS_MSTI_DROP] = REG_FIELD(ANA_ANEVENTS, 27, 27),[ANA_ANEVENTS_ACLKILL] = REG_FIELD(ANA_ANEVENTS, 26, 26),[ANA_ANEVENTS_ACLUSED] = REG_FIELD(ANA_ANEVENTS, 25, 25),[ANA_ANEVENTS_AUTOAGE] = REG_FIELD(ANA_ANEVENTS, 24, 24),[ANA_ANEVENTS_VS2TTL1] = REG_FIELD(ANA_ANEVENTS, 23, 23),[ANA_ANEVENTS_STORM_DROP] = REG_FIELD(ANA_ANEVENTS, 22, 22),[ANA_ANEVENTS_LEARN_DROP] = REG_FIELD(ANA_ANEVENTS, 21, 21),[ANA_ANEVENTS_AGED_ENTRY] = REG_FIELD(ANA_ANEVENTS, 20, 20),[ANA_ANEVENTS_CPU_LEARN_FAILED] = REG_FIELD(ANA_ANEVENTS, 19, 19),[ANA_ANEVENTS_AUTO_LEARN_FAILED] = REG_FIELD(ANA_ANEVENTS, 18, 18),[ANA_ANEVENTS_LEARN_REMOVE] = REG_FIELD(ANA_ANEVENTS, 17, 17),[ANA_ANEVENTS_AUTO_LEARNED] = REG_FIELD(ANA_ANEVENTS, 16, 16),[ANA_ANEVENTS_AUTO_MOVED] = REG_FIELD(ANA_ANEVENTS, 15, 15),[ANA_ANEVENTS_DROPPED] = REG_FIELD(ANA_ANEVENTS, 14, 14),[ANA_ANEVENTS_CLASSIFIED_DROP] = REG_FIELD(ANA_ANEVENTS, 13, 13),[ANA_ANEVENTS_CLASSIFIED_COPY] = REG_FIELD(ANA_ANEVENTS, 12, 12),[ANA_ANEVENTS_VLAN_DISCARD] = REG_FIELD(ANA_ANEVENTS, 11, 11),[ANA_ANEVENTS_FWD_DISCARD] = REG_FIELD(ANA_ANEVENTS, 10, 10),[ANA_ANEVENTS_MULTICAST_FLOOD] = REG_FIELD(ANA_ANEVENTS, 9, 9),[ANA_ANEVENTS_UNICAST_FLOOD] = REG_FIELD(ANA_ANEVENTS, 8, 8),[ANA_ANEVENTS_DEST_KNOWN] = REG_FIELD(ANA_ANEVENTS, 7, 7),[ANA_ANEVENTS_BUCKET3_MATCH] = REG_FIELD(ANA_ANEVENTS, 6, 6),[ANA_ANEVENTS_BUCKET2_MATCH] = REG_FIELD(ANA_ANEVENTS, 5, 5),[ANA_ANEVENTS_BUCKET1_MATCH] = REG_FIELD(ANA_ANEVENTS, 4, 4),[ANA_ANEVENTS_BUCKET0_MATCH] = REG_FIELD(ANA_ANEVENTS, 3, 3),[ANA_ANEVENTS_CPU_OPERATION] = REG_FIELD(ANA_ANEVENTS, 2, 2),[ANA_ANEVENTS_DMAC_LOOKUP] = REG_FIELD(ANA_ANEVENTS, 1, 1),[ANA_ANEVENTS_SMAC_LOOKUP] = REG_FIELD(ANA_ANEVENTS, 0, 0),[ANA_TABLES_MACACCESS_B_DOM] = REG_FIELD(ANA_TABLES_MACACCESS, 18, 18),[ANA_TABLES_MACTINDX_BUCKET] = REG_FIELD(ANA_TABLES_MACTINDX, 10, 11),[ANA_TABLES_MACTINDX_M_INDEX] = REG_FIELD(ANA_TABLES_MACTINDX, 0, 9),[QSYS_TIMED_FRAME_ENTRY_TFRM_VLD] = REG_FIELD(QSYS_TIMED_FRAME_ENTRY, 20, 20),[QSYS_TIMED_FRAME_ENTRY_TFRM_FP] = REG_FIELD(QSYS_TIMED_FRAME_ENTRY, 8, 19),[QSYS_TIMED_FRAME_ENTRY_TFRM_PORTNO] = REG_FIELD(QSYS_TIMED_FRAME_ENTRY, 4, 7),[QSYS_TIMED_FRAME_ENTRY_TFRM_TM_SEL] = REG_FIELD(QSYS_TIMED_FRAME_ENTRY, 1, 3),[QSYS_TIMED_FRAME_ENTRY_TFRM_TM_T] = REG_FIELD(QSYS_TIMED_FRAME_ENTRY, 0, 0),[SYS_RESET_CFG_CORE_ENA] = REG_FIELD(SYS_RESET_CFG, 2, 2),[SYS_RESET_CFG_MEM_ENA] = REG_FIELD(SYS_RESET_CFG, 1, 1),[SYS_RESET_CFG_MEM_INIT] = REG_FIELD(SYS_RESET_CFG, 0, 0),  Replicated per number of ports (12), register size 4 per port ", ".count = 1,},},.target = S0,.keys = vsc7514_vcap_es0_keys,.actions = vsc7514_vcap_es0_actions,},[VCAP_IS1] = ": "vsc7514_vcap_props[] = {[VCAP_ES0] = {.action_type_width = 0,.action_table = {[ES0_ACTION_TYPE_NORMAL] = {.width = 73,   HIT_STICKY not included ", "stats->rx_bytes = s[OCELOT_STAT_RX_OCTETS];stats->rx_packets = s[OCELOT_STAT_RX_SHORTS] +    s[OCELOT_STAT_RX_FRAGMENTS] +    s[OCELOT_STAT_RX_JABBERS] +    s[OCELOT_STAT_RX_LONGS] +    s[OCELOT_STAT_RX_64] +    s[OCELOT_STAT_RX_65_127] +    s[OCELOT_STAT_RX_128_255] +    s[OCELOT_STAT_RX_256_511] +    s[OCELOT_STAT_RX_512_1023] +    s[OCELOT_STAT_RX_1024_1526] +    s[OCELOT_STAT_RX_1527_MAX];stats->multicast = s[OCELOT_STAT_RX_MULTICAST];stats->rx_missed_errors = s[OCELOT_STAT_DROP_TAIL];stats->rx_dropped = s[OCELOT_STAT_RX_RED_PRIO_0] +    s[OCELOT_STAT_RX_RED_PRIO_1] +    s[OCELOT_STAT_RX_RED_PRIO_2] +    s[OCELOT_STAT_RX_RED_PRIO_3] +    s[OCELOT_STAT_RX_RED_PRIO_4] +    s[OCELOT_STAT_RX_RED_PRIO_5] +    s[OCELOT_STAT_RX_RED_PRIO_6] +    s[OCELOT_STAT_RX_RED_PRIO_7] +    s[OCELOT_STAT_DROP_LOCAL] +    s[OCELOT_STAT_DROP_YELLOW_PRIO_0] +    s[OCELOT_STAT_DROP_YELLOW_PRIO_1] +    s[OCELOT_STAT_DROP_YELLOW_PRIO_2] +    s[OCELOT_STAT_DROP_YELLOW_PRIO_3] +    s[OCELOT_STAT_DROP_YELLOW_PRIO_4] +    s[OCELOT_STAT_DROP_YELLOW_PRIO_5] +    s[OCELOT_STAT_DROP_YELLOW_PRIO_6] +    s[OCELOT_STAT_DROP_YELLOW_PRIO_7] +    s[OCELOT_STAT_DROP_GREEN_PRIO_0] +    s[OCELOT_STAT_DROP_GREEN_PRIO_1] +    s[OCELOT_STAT_DROP_GREEN_PRIO_2] +    s[OCELOT_STAT_DROP_GREEN_PRIO_3] +    s[OCELOT_STAT_DROP_GREEN_PRIO_4] +    s[OCELOT_STAT_DROP_GREEN_PRIO_5] +    s[OCELOT_STAT_DROP_GREEN_PRIO_6] +    s[OCELOT_STAT_DROP_GREEN_PRIO_7];/* Get Tx stats ": "ocelot_port_get_stats64(struct ocelot  ocelot, int port,     struct rtnl_link_stats64  stats){u64  s = &ocelot->stats[port   OCELOT_NUM_STATS];spin_lock(&ocelot->stats_lock);  Get Rx stats ", "void fun_dev_disable(struct fun_dev *fdev)": "fun_dev_enable(). ", "cfg = readq_relaxed(xcv->reg_base + XCV_RESET);cfg &= ~DLL_RESET;writeq_relaxed(cfg, xcv->reg_base + XCV_RESET);/* Take clock tree out of reset ": "xcv_init_hw(void){u64  cfg;  Take DLL out of reset ", "cfg = readq_relaxed(xcv->reg_base + XCV_CTL);cfg &= ~0x03;cfg |= speed;writeq_relaxed(cfg, xcv->reg_base + XCV_CTL);/* Reset datapaths ": "xcv_setup_link(bool link_up, int link_speed){u64  cfg;int speed = 2;if (!xcv) {pr_err(\"XCV init not done, probe may have failed\\n\");return;}if (link_speed == 100)speed = 1;else if (link_speed == 10)speed = 0;if (link_up) {  set operating speed ", "if (!cam_dmac || !bgx)return -1;lmac = &bgx->lmac[lmacid];/* configure DCAM filtering for designated LMAC ": "bgx_set_dmac_cam_filter_mac(struct bgx  bgx, int lmacid,       u64 cam_dmac, u8 idx){struct lmac  lmac = NULL;u64 cfg = 0;  skip zero addresses as meaningless ", "cfg &= ~(CAM_ACCEPT | BGX_MCAST_MODE(MCAST_MODE_MASK));/* check requested bits and set filtergin mode appropriately ": "bgx_set_xcast_mode(int node, int bgx_idx, int lmacid, u8 mode){struct bgx  bgx = get_bgx(node, bgx_idx);struct lmac  lmac = NULL;u64 cfg = 0;u8 i = 0;if (!bgx)return;lmac = &bgx->lmac[lmacid];cfg = bgx_reg_read(bgx, lmacid, BGX_CMRX_RX_DMAC_CTL);if (mode & BGX_XCAST_BCAST_ACCEPT)cfg |= BCAST_ACCEPT;elsecfg &= ~BCAST_ACCEPT;  disable all MCASTs and DMAC filtering ", "bgx_reg_modify(bgx, lmacid, BGX_GMP_GMI_TXX_INT_ENA_W1S,       GMI_TXX_INT_UNDFLW);} else ": "bgx_lmac_rx_tx_enable(int node, int bgx_idx, int lmacid, bool enable){struct bgx  bgx = get_bgx(node, bgx_idx);struct lmac  lmac;u64 cfg;if (!bgx)return;lmac = &bgx->lmac[lmacid];cfg = bgx_reg_read(bgx, lmacid, BGX_CMRX_CFG);if (enable) {cfg |= CMR_PKT_RX_EN | CMR_PKT_TX_EN;  enable TX FIFO Underflow interrupt ", "int iavf_register_client(struct iavf_client *client)": "iavf_register_client - Register a iavf client driver with the L2 driver   @client: pointer to the iavf_client struct     Returns 0 on success or non-0 on error  ", "int iavf_unregister_client(struct iavf_client *client)": "iavf_unregister_client - Unregister a iavf client driver with the L2 driver   @client: pointer to the iavf_client struct     Returns 0 on success or non-0 on error  ", "neigh_event_send(e->neigh, NULL);spin_lock_bh(&e->lock);if (e->state == L2T_STATE_STALE)e->state = L2T_STATE_VALID;spin_unlock_bh(&e->lock);fallthrough;case L2T_STATE_VALID:/* fast-path, send the packet on ": "t3_l2t_send_slow(struct t3cdev  dev, struct sk_buff  skb,     struct l2t_entry  e){again:switch (e->state) {case L2T_STATE_STALE:  entry is stale, kick off revalidation ", "neigh_event_send(e->neigh, NULL);spin_lock_bh(&e->lock);if (e->state == L2T_STATE_STALE) ": "t3_l2t_send_event(struct t3cdev  dev, struct l2t_entry  e){again:switch (e->state) {case L2T_STATE_STALE:  entry is stale, kick off revalidation ", "if (e->neigh) ": "t3_l2e_free(struct l2t_data  d, struct l2t_entry  e){spin_lock_bh(&e->lock);if (atomic_read(&e->refcnt) == 0) {  hasn't been recycled ", "e = alloc_l2e(d);if (e) ": "cxgb4_l2t_get(struct l2t_data  d, struct neighbour  neigh,const struct net_device  physdev,unsigned int priority){u8 lport;u16 vlan;struct l2t_entry  e;unsigned int addr_len = neigh->tbl->key_len;u32  addr = (u32  )neigh->primary_key;int ifidx = neigh->dev->ifindex;int hash = addr_hash(d, addr, addr_len, ifidx);if (neigh->dev->flags & IFF_LOOPBACK)lport = netdev2pinfo(physdev)->tx_chan + 4;elselport = netdev2pinfo(physdev)->lport;if (is_vlan_dev(neigh->dev)) {vlan = vlan_dev_vlan_id(neigh->dev);vlan |= vlan_dev_get_egress_qos_mask(neigh->dev, priority);} else {vlan = VLAN_NONE;}write_lock_bh(&d->lock);for (e = d->l2tab[hash].first; e; e = e->next)if (!addreq(e, addr) && e->ifindex == ifidx &&    e->vlan == vlan && e->lport == lport) {l2t_hold(d, e);if (atomic_read(&e->refcnt) == 1)reuse_entry(e, neigh);goto done;}  Need to allocate a new entry ", "void cxgb3_register_client(struct cxgb3_client *client)": "cxgb3_register_client - register an offload client  @client: the client    Add the client to the client list,  and call backs the client for each activated offload device ", "void cxgb3_unregister_client(struct cxgb3_client *client)": "cxgb3_unregister_client - unregister an offload client  @client: the client    Remove the client to the client list,  and call backs the client for each activated offload device. ", "void cxgb3_queue_tid_release(struct t3cdev *tdev, unsigned int tid)": "cxgb3_ofld_send(tdev, skb);p->ctx = NULL;if (skb == td->nofail_skb)td->nofail_skb =alloc_skb(sizeof(struct cpl_tid_release),GFP_KERNEL);spin_lock_bh(&td->tid_release_lock);}td->release_list_incomplete = (td->tid_release_list == NULL) ? 0 : 1;spin_unlock_bh(&td->tid_release_lock);if (!td->nofail_skb)td->nofail_skb =alloc_skb(sizeof(struct cpl_tid_release),GFP_KERNEL);}  use ctx as a next pointer in the tid release list ", "idx = ppm_get_cpu_entries(ppm, npods, caller_data);/* try the general pool ": "cxgbi_ppm_ppods_reserve(struct cxgbi_ppm  ppm, unsigned short nr_pages,    u32 per_tag_pg_idx, u32  ppod_idx,    u32  ddp_tag, unsigned long caller_data){struct cxgbi_ppod_data  pdata;unsigned int npods;int idx = -1;unsigned int hwidx;u32 tag;npods = (nr_pages + PPOD_PAGES_MAX - 1) >> PPOD_PAGES_SHIFT;if (!npods) {pr_warn(\"%s: pages %u -> npods %u, full.\\n\",__func__, nr_pages, npods);return -EINVAL;}  grab from cpu pool first ", "tag &= 0x3FFFFFFF;hdr->vld_tid = htonl(PPOD_VALID_FLAG | PPOD_TID(tid));hdr->rsvd = 0;hdr->pgsz_tag_clr = htonl(tag & ppm->tformat.idx_clr_mask);hdr->max_offset = htonl(length);hdr->page_offset = htonl(offset);pr_debug(\"ippm: tag 0x%x, tid 0x%x, xfer %u, off %u.\\n\", tag, tid, length, offset);}EXPORT_SYMBOL(cxgbi_ppm_make_ppod_hdr": "cxgbi_ppm_make_ppod_hdr(struct cxgbi_ppm  ppm, u32 tag,     unsigned int tid, unsigned int offset,     unsigned int length,     struct cxgbi_pagepod_hdr  hdr){  The ddp tag in pagepod should be with bit 31:30 set to 0.   The ddp Tag on the wire should be with non-zero 31:30 to the peer ", "if (*ppm_pp) ": "cxgbi_ppm_init(void   ppm_pp, struct net_device  ndev,   struct pci_dev  pdev, void  lldev,   struct cxgbi_tag_format  tformat, unsigned int iscsi_size,   unsigned int llimit, unsigned int start,   unsigned int reserve_factor, unsigned int iscsi_edram_start,   unsigned int iscsi_edram_size){struct cxgbi_ppm  ppm = (struct cxgbi_ppm  )( ppm_pp);struct cxgbi_ppm_pool  pool = NULL;unsigned int pool_index_max = 0;unsigned int ppmax_pool = 0;unsigned int ppod_bmap_size;unsigned int alloc_sz;unsigned int ppmax;if (!iscsi_edram_start)iscsi_edram_size = 0;if (iscsi_edram_size &&    ((iscsi_edram_start + iscsi_edram_size) != start)) {pr_err(\"iscsi ppod region not contiguous: EDRAM start 0x%x \"\"size 0x%x DDR start 0x%x\\n\",iscsi_edram_start, iscsi_edram_size, start);return -EINVAL;}if (iscsi_edram_size) {reserve_factor = 0;start = iscsi_edram_start;}ppmax = (iscsi_edram_size + iscsi_size) >> PPOD_SIZE_SHIFT;if (ppm) {pr_info(\"ippm: %s, ppm 0x%p,0x%p already initialized, %u%u.\\n\",ndev->name, ppm_pp, ppm, ppm->ppmax, ppmax);kref_get(&ppm->refcnt);return 1;}if (reserve_factor) {ppmax_pool = ppmax  reserve_factor;pool = ppm_alloc_cpu_pool(&ppmax_pool, &pool_index_max);if (!pool) {ppmax_pool = 0;reserve_factor = 0;}pr_debug(\"%s: ppmax %u, cpu total %u, per cpu %u.\\n\", ndev->name, ppmax, ppmax_pool, pool_index_max);}ppod_bmap_size = BITS_TO_LONGS(ppmax - ppmax_pool);alloc_sz = sizeof(struct cxgbi_ppm) +ppmax   (sizeof(struct cxgbi_ppod_data)) +ppod_bmap_size   sizeof(unsigned long);ppm = vzalloc(alloc_sz);if (!ppm)goto release_ppm_pool;ppm->ppod_bmap = (unsigned long  )(&ppm->ppod_data[ppmax]);if ((ppod_bmap_size >> 3) > (ppmax - ppmax_pool)) {unsigned int start = ppmax - ppmax_pool;unsigned int end = ppod_bmap_size >> 3;bitmap_set(ppm->ppod_bmap, ppmax, end - start);pr_info(\"%s: %u - %u < %u   8, mask extra bits %u, %u.\\n\",__func__, ppmax, ppmax_pool, ppod_bmap_size, start,end);}if (iscsi_edram_size) {unsigned int first_ddr_idx =iscsi_edram_size >> PPOD_SIZE_SHIFT;ppm->max_index_in_edram = first_ddr_idx - 1;bitmap_set(ppm->ppod_bmap, first_ddr_idx, 1);pr_debug(\"reserved %u ppod in bitmap\\n\", first_ddr_idx);}spin_lock_init(&ppm->map_lock);kref_init(&ppm->refcnt);memcpy(&ppm->tformat, tformat, sizeof(struct cxgbi_tag_format));ppm->ppm_pp = ppm_pp;ppm->ndev = ndev;ppm->pdev = pdev;ppm->lldev = lldev;ppm->ppmax = ppmax;ppm->next = 0;ppm->llimit = llimit;ppm->base_idx = start > llimit ?(start - llimit + 1) >> PPOD_SIZE_SHIFT : 0;ppm->bmap_index_max = ppmax - ppmax_pool;ppm->pool = pool;ppm->pool_rsvd = ppmax_pool;ppm->pool_index_max = pool_index_max;  check one more time ", "int cxgb4_get_srq_entry(struct net_device *dev,int srq_idx, struct srq_entry *entryp)": "cxgb4_get_srq_entry: read the SRQ table entry   @dev: Pointer to the net_device   @idx: Index to the srq   @entryp: pointer to the srq entry     Sends CPL_SRQ_TABLE_REQ message for the given index.   Contents will be returned in CPL_SRQ_TABLE_RPL message.     Returns zero if the read is successful, else a error   number will be returned. Caller should not use the srq   entry if the return value is non-zero.     ", "neigh_event_send(e->neigh, NULL);spin_lock_bh(&e->lock);if (e->state == L2T_STATE_STALE)e->state = L2T_STATE_VALID;spin_unlock_bh(&e->lock);fallthrough;case L2T_STATE_VALID:     /* fast-path, send the packet on ": "cxgb4_l2t_send(struct net_device  dev, struct sk_buff  skb,   struct l2t_entry  e){struct adapter  adap = netdev2adap(dev);again:switch (e->state) {case L2T_STATE_STALE:       entry is stale, kick off revalidation ", "if (tp->vlan_shift >= 0 && l2t->vlan != VLAN_NONE)ntuple |= (u64)(FT_VLAN_VLD_F | l2t->vlan) << tp->vlan_shift;if (tp->port_shift >= 0)ntuple |= (u64)l2t->lport << tp->port_shift;if (tp->protocol_shift >= 0)ntuple |= (u64)IPPROTO_TCP << tp->protocol_shift;if (tp->vnic_shift >= 0 && (tp->ingress_config & VNIC_F)) ": "cxgb4_select_ntuple(struct net_device  dev,const struct l2t_entry  l2t){struct adapter  adap = netdev2adap(dev);struct tp_params  tp = &adap->params.tp;u64 ntuple = 0;  Initialize each of the fields which we care about which are present   in the Compressed Filter Tuple. ", "struct l2t_entry *cxgb4_l2t_alloc_switching(struct net_device *dev, u16 vlan,    u8 port, u8 *dmac)": "cxgb4_l2t_alloc_switching - Allocates an L2T entry for switch filters   @dev: net_device pointer   @vlan: VLAN Id   @port: Associated port   @dmac: Destination MAC address to add to L2T   Returns pointer to the allocated l2t entry     Allocates an L2T entry for use by switching rule of a filter ", "if (family == PF_INET6) ": "cxgb4_alloc_stid(struct tid_info  t, int family, void  data){int stid;spin_lock_bh(&t->stid_lock);if (family == PF_INET) {stid = find_first_zero_bit(t->stid_bmap, t->nstids);if (stid < t->nstids)__set_bit(stid, t->stid_bmap);elsestid = -1;} else {stid = bitmap_find_free_region(t->stid_bmap, t->nstids, 1);if (stid < 0)stid = -1;}if (stid >= 0) {t->stid_tab[stid].data = data;stid += t->stid_base;  IPv6 requires max of 520 bits or 16 cells in TCAM   This is equivalent to 4 TIDs. With CLIP enabled it   needs 2 TIDs. ", "if (t->nsftids && (stid >= t->sftid_base)) ": "cxgb4_free_stid(struct tid_info  t, unsigned int stid, int family){  Is it a server filter TID? ", "int cxgb4_create_server(const struct net_device *dev, unsigned int stid,__be32 sip, __be16 sport, __be16 vlan,unsigned int queue)": "cxgb4_create_server - create an IP server  @dev: the device  @stid: the server TID  @sip: local IP address to bind server to  @sport: the server's TCP port  @vlan: the VLAN header information  @queue: queue to direct messages from this server to    Create an IP server for the given port and address.  Returns <0 on error and one of the %NET_XMIT_  values on success. ", "int cxgb4_create_server6(const struct net_device *dev, unsigned int stid, const struct in6_addr *sip, __be16 sport, unsigned int queue)": "cxgb4_create_server6 - create an IPv6 server  @dev: the device  @stid: the server TID  @sip: local IPv6 address to bind server to  @sport: the server's TCP port  @queue: queue to direct messages from this server to    Create an IPv6 server for the given port and address.  Returns <0 on error and one of the %NET_XMIT_  values on success. ", "unsigned int cxgb4_best_mtu(const unsigned short *mtus, unsigned short mtu,    unsigned int *idx)": "cxgb4_best_mtu - find the entry in the MTU table closest to an MTU  @mtus: the HW MTU table  @mtu: the target MTU  @idx: index of selected entry in the MTU table    Returns the index and the value in the HW MTU table that is closest to  but does not exceed @mtu, unless @mtu is smaller than any value in the  table, in which case that smallest available value is selected. ", "unsigned int cxgb4_best_aligned_mtu(const unsigned short *mtus,    unsigned short header_size,    unsigned short data_size_max,    unsigned short data_size_align,    unsigned int *mtu_idxp)": "cxgb4_best_aligned_mtu - find best MTU, [hopefully] data size aligned       @mtus: the HW MTU table       @header_size: Header Size       @data_size_max: maximum Data Segment Size       @data_size_align: desired Data Segment Size Alignment (2^N)       @mtu_idxp: HW MTU Table Index return value pointer (possibly NULL)         Similar to cxgb4_best_mtu() but instead of searching the Hardware       MTU Table based solely on a Maximum MTU parameter, we break that       parameter up into a Header Size and Maximum Data Segment Size, and       provide a desired Data Segment Size Alignment.  If we find an MTU in       the Hardware MTU Table which will result in a Data Segment Size with       the requested alignment _and_ that MTU isn't \"too far\" from the       closest MTU, then we'll return that rather than the closest MTU. ", "unsigned int cxgb4_port_chan(const struct net_device *dev)": "cxgb4_port_chan - get the HW channel of a port  @dev: the net device for the port    Return the HW Tx channel of the given port. ", "unsigned int cxgb4_port_e2cchan(const struct net_device *dev)": "cxgb4_port_e2cchan - get the HW c-channel of a port        @dev: the net device for the port          Return the HW RX c-channel of the given port. ", "unsigned int cxgb4_port_viid(const struct net_device *dev)": "cxgb4_port_viid - get the VI id of a port  @dev: the net device for the port    Return the VI id of the given port. ", "unsigned int cxgb4_port_idx(const struct net_device *dev)": "cxgb4_port_idx - get the index of a port  @dev: the net device for the port    Return the index of the given port. ", "size = t4_read_reg(adap, MA_EDRAM0_BAR_A);edc0_size = EDRAM0_SIZE_G(size) << 20;size = t4_read_reg(adap, MA_EDRAM1_BAR_A);edc1_size = EDRAM1_SIZE_G(size) << 20;size = t4_read_reg(adap, MA_EXT_MEMORY0_BAR_A);mc0_size = EXT_MEM0_SIZE_G(size) << 20;if (t4_read_reg(adap, MA_TARGET_MEM_ENABLE_A) & HMA_MUX_F) ": "cxgb4_read_tpte(struct net_device  dev, u32 stag, __be32  tpte){u32 edc0_size, edc1_size, mc0_size, mc1_size, size;u32 edc0_end, edc1_end, mc0_end, mc1_end;u32 offset, memtype, memaddr;struct adapter  adap;u32 hma_size = 0;int ret;adap = netdev2adap(dev);offset = ((stag >> 8)   32) + adap->vres.stag.start;  Figure out where the offset lands in the Memory TypeAddress scheme.   This code assumes that the memory is laid out starting at offset 0   with no breaks as: EDC0, EDC1, MC0, MC1. All cards have both EDC0   and EDC1.  Some cards will have neither MC0 nor MC1, most cards have   MC0, and some have both MC0 and MC1. ", "stid -= adap->tids.sftid_base;stid += adap->tids.nftids;/* Check to make sure the filter requested is writable ... ": "cxgb4_create_server_filter(const struct net_device  dev, unsigned int stid,__be32 sip, __be16 sport, __be16 vlan,unsigned int queue, unsigned char port, unsigned char mask){int ret;struct filter_entry  f;struct adapter  adap;int i;u8  val;adap = netdev2adap(dev);  Adjust stid to correct filter index ", "stid -= adap->tids.sftid_base;stid += adap->tids.nftids;f = &adap->tids.ftid_tab[stid];/* Unlock the filter ": "cxgb4_remove_server_filter(const struct net_device  dev, unsigned int stid,unsigned int queue, bool ipv6){struct filter_entry  f;struct adapter  adap;adap = netdev2adap(dev);  Adjust stid to correct filter index ", "void cxgb4_register_uld(enum cxgb4_uld type,const struct cxgb4_uld_info *p)": "cxgb4_register_uld - register an upper-layer driver   @type: the ULD type   @p: the ULD methods     Registers an upper-layer driver with this driver and notifies the ULD   about any presently available devices that support its type. ", "int cxgb4_unregister_uld(enum cxgb4_uld type)": "cxgb4_unregister_uld - unregister an upper-layer driver  @type: the ULD type    Unregisters an existing upper-layer driver. ", "void cxgb4_reclaim_completed_tx(struct adapter *adap, struct sge_txq *q,bool unmap)": "cxgb4_reclaim_completed_tx - reclaims completed Tx descriptors  @adap: the adapter  @q: the Tx queue to reclaim completed descriptors from  @unmap: whether the buffers should be unmapped for DMA    Reclaims Tx descriptors that the SGE has indicated it has processed,  and frees the associated buffers if possible.  Called with the Tx  queue locked. ", "void cxgb4_write_sgl(const struct sk_buff *skb, struct sge_txq *q,     struct ulptx_sgl *sgl, u64 *end, unsigned int start,     const dma_addr_t *addr)": "cxgb4_write_sgl - populate a scattergather list for a packet  @skb: the packet  @q: the Tx queue we are writing into  @sgl: starting location for writing the SGL  @end: points right after the end of the SGL  @start: start offset into skb main-body data to include in the SGL  @addr: the list of bus addresses for the SGL elements    Generates a gather list for the buffers that make up a packet.  The caller must provide adequate space for the SGL that will be written.  The SGL includes all of the packet's page fragments and the data in its  main body except for the first @start bytes.  @sgl must be 16-byte  aligned and within a Tx descriptor with available space.  @end points  right after the end of the SGL but does not account for any potential  wrap around, i.e., @end > @sgl. ", "void cxgb4_write_partial_sgl(const struct sk_buff *skb, struct sge_txq *q,     struct ulptx_sgl *sgl, u64 *end,     const dma_addr_t *addr, u32 start, u32 len)": "cxgb4_write_partial_sgl - populate SGL for partial packet  @skb: the packet  @q: the Tx queue we are writing into  @sgl: starting location for writing the SGL  @end: points right after the end of the SGL  @addr: the list of bus addresses for the SGL elements  @start: start offset in the SKB where partial data starts  @len: length of data from @start to send out    This API will handle sending out partial data of a skb if required.  Unlike cxgb4_write_sgl, @start can be any offset into the skb data,  and @len will decide how much data after @start offset to send out. ", "inline void cxgb4_ring_tx_db(struct adapter *adap, struct sge_txq *q, int n)": "cxgb4_ring_tx_db - check and potentially ring a Tx queue's doorbell  @adap: the adapter  @q: the Tx queue  @n: number of new descriptors to give to HW    Ring the doorbel for a Tx queue. ", "void cxgb4_inline_tx_skb(const struct sk_buff *skb, const struct sge_txq *q, void *pos)": "cxgb4_inline_tx_skb - inline a packet's data into Tx descriptors  @skb: the packet  @q: the Tx queue where the packet will be inlined  @pos: starting position in the Tx queue where to inline the packet    Inline a packet's contents directly into Tx descriptors, starting at  the given position within the Tx DMA ring.  Most of the complexity of this operation is dealing with wrap arounds  in the middle of the packet we want to inline. ", "int cxgb4_ofld_send(struct net_device *dev, struct sk_buff *skb)": "cxgb4_ofld_send - send an offload packet  @dev: the net device  @skb: the packet    Sends an offload packet.  This is an exported version of @t4_ofld_send,  intended for ULDs. ", "int cxgb4_crypto_send(struct net_device *dev, struct sk_buff *skb)": "cxgb4_crypto_send - send crypto packet  @dev: the net device  @skb: the packet    Sends crypto packet.  This is an exported version of @t4_crypto_send,  intended for ULDs. ", "struct sk_buff *cxgb4_pktgl_to_skb(const struct pkt_gl *gl,   unsigned int skb_len, unsigned int pull_len)": "cxgb4_pktgl_to_skb - build an sk_buff from a packet gather list  @gl: the gather list  @skb_len: size of sk_buff main body if it carries fragments  @pull_len: amount of data to move to the sk_buff's main body    Builds an sk_buff from the given packet gather list.  Returns the  sk_buff or %NULL if sk_buff allocation failed. ", "ret = cxgb4_update_dev_clip(dev, dev);if (ret)return ret;/* Parse all bond and vlan devices layered on top of the physical dev ": "cxgb4_update_root_dev_clip(struct net_device  dev){struct net_device  root_dev = NULL;int i, ret = 0;  First populate the real net device's IPv6 addresses ", "void cxgb4_smt_release(struct smt_entry *e)": "cxgb4_smt_release - Release SMT entry   @e: smt entry to release     Releases ref count and frees up an smt entry from SMT table ", "struct smt_entry *cxgb4_smt_alloc_switching(struct net_device *dev, u8 *smac)": "cxgb4_smt_alloc_switching - Allocates an SMT entry for switch filters.   @dev: net_device pointer   @smac: MAC address to add to SMT   Returns pointer to the SMT entry created     Allocates an SMT entry to be used by switching rule of a filter. ", "int fman_port_config(struct fman_port *port, struct fman_port_params *params)": "fman_port_config   @port:Pointer to the port structure   @params:Pointer to data structure of parameters     Creates a descriptor for the FM PORT module.   The routine returns a pointer to the FM PORT object.   This descriptor must be passed as first parameter to all other FM PORT   function calls.   No actual initialization or configuration of FM hardware is done by this   routine.     Return: 0 on success; Error code otherwise. ", "void fman_port_use_kg_hash(struct fman_port *port, bool enable)": "fman_port_use_kg_hash   @port: A pointer to a FM Port module.   @enable: enable or disable     Sets the HW KeyGen or the BMI as HW Parser next engine, enabling   or bypassing the KeyGen hashing of Rx traffic ", "int fman_port_init(struct fman_port *port)": "fman_port_init   @port:A pointer to a FM Port module.     Initializes the FM PORT module by defining the software structure and   configuring the hardware registers.     Return: 0 on success; Error code otherwise. ", "int fman_port_cfg_buf_prefix_content(struct fman_port *port,     struct fman_buffer_prefix_content *     buffer_prefix_content)": "fman_port_cfg_buf_prefix_content   @port:A pointer to a FM Port module.   @buffer_prefix_content:A structure of parameters describing  the structure of the buffer.  Out parameter:  Start margin - offset of data from  start of external buffer.   Defines the structure, size and content of the application buffer.   The prefix, in Tx ports, if 'pass_prs_result', the application should set   a value to their offsets in the prefix of the FM will save the first   'priv_data_size', than, depending on 'pass_prs_result' and   'pass_time_stamp', copy parse result and timeStamp, and the packet itself   (in this order), to the application buffer, and to offset.   Calling this routine changes the buffer margins definitions in the internal   driver data base from its default configuration:   Data size:  [DEFAULT_PORT_BUFFER_PREFIX_CONTENT_PRIV_DATA_SIZE]   Pass Parser result: [DEFAULT_PORT_BUFFER_PREFIX_CONTENT_PASS_PRS_RESULT].   Pass timestamp: [DEFAULT_PORT_BUFFER_PREFIX_CONTENT_PASS_TIME_STAMP].   May be used for all ports     Allowed only following fman_port_config() and before fman_port_init().     Return: 0 on success; Error code otherwise. ", "int fman_port_disable(struct fman_port *port)": "fman_port_disable   @port:A pointer to a FM Port module.     Gracefully disable an FM port. The port will not start newtasks after all   tasks associated with the port are terminated.     This is a blocking routine, it returns after port is gracefully stopped,   i.e. the port will not except new frames, but it will finish all frames   or tasks which were already began.   Allowed only following fman_port_init().     Return: 0 on success; Error code otherwise. ", "int fman_port_enable(struct fman_port *port)": "fman_port_enable   @port:A pointer to a FM Port module.     A runtime routine provided to allow disableenable of port.     Allowed only following fman_port_init().     Return: 0 on success; Error code otherwise. ", "struct fman_port *fman_port_bind(struct device *dev)": "fman_port_bind   @dev:FMan Port OF device pointer     Bind to a specific FMan Port.     Allowed only after the port was created.     Return: A pointer to the FMan port device. ", "u32 fman_port_get_qman_channel_id(struct fman_port *port)": "fman_port_get_qman_channel_id   @port:Pointer to the FMan port devuce     Get the QMan channel ID for the specific port     Return: QMan channel ID ", "struct device *fman_port_get_device(struct fman_port *port)": "fman_port_get_device   @port:Pointer to the FMan port device     Get the 'struct device' associated to the specified FMan port device     Return: pointer to associated 'struct device' ", "for (i = 0; i < fm_ext_pools->num_of_pools_used; i++) ": "fman_sp_set_buf_pools_in_asc_order_of_buf_sizes(struct fman_ext_pools      fm_ext_pools,     u8  ordered_array,     u16  sizes_array){u16 buf_size = 0;int i = 0, j = 0, k = 0;  First we copy the external buffers pools information   to an ordered local array ", "int_context_data_copy->ext_buf_offset = (u16)((buffer_prefix_content->priv_data_size & (OFFSET_UNITS - 1)) ?((buffer_prefix_content->priv_data_size + OFFSET_UNITS) &~(u16)(OFFSET_UNITS - 1)) :buffer_prefix_content->priv_data_size);/* Translate margin and int_context params to FM parameters ": "fman_sp_build_buffer_struct(struct fman_sp_int_context_data_copy  int_context_data_copy,struct fman_buffer_prefix_content  buffer_prefix_content,struct fman_sp_buf_margins  buf_margins,struct fman_sp_buffer_offsets  buffer_offsets,u8  internal_buf_offset){u32 tmp;  Align start of internal context data to 16 byte ", "struct fman_keygen *keygen_init(struct fman_kg_regs __iomem *keygen_regs)": "keygen_init     KeyGen initialization:   Initializes and enables KeyGen, allocate driver memory, setup registers,   clear port bindings, invalidate all schemes     keygen_regs: KeyGen registers base address     Return: Handle to KeyGen driver ", "int keygen_port_hashing_init(struct fman_keygen *keygen, u8 hw_port_id,     u32 hash_base_fqid, u32 hash_size)": "keygen_port_hashing_init     Initializes a port for Rx Hashing with specified configuration parameters     keygen: KeyGen handle   hw_port_id: HW Port ID   hash_base_fqid: Hashing Base FQID used for spreading   hash_size: Hashing size     Return: Zero for success or error code in case of failure ", "void fman_register_intr(struct fman *fman, enum fman_event_modules module,u8 mod_id, enum fman_intr_type intr_type,void (*isr_cb)(void *src_arg), void *src_arg)": "fman_register_intr   @fman:A Pointer to FMan device   @module:Calling module   @mod_id:Module id (if more than 1 exists, '0' if not)   @intr_type:Interrupt type (errornormal) selection.   @isr_cb:The interrupt service routine.   @src_arg:Argument to be passed to isr_cb.     Used to register an event handler to be processed by FMan     Return: 0 on success; Error code otherwise. ", "void fman_unregister_intr(struct fman *fman, enum fman_event_modules module,  u8 mod_id, enum fman_intr_type intr_type)": "fman_unregister_intr   @fman:A Pointer to FMan device   @module:Calling module   @mod_id:Module id (if more than 1 exists, '0' if not)   @intr_type:Interrupt type (errornormal) selection.     Used to unregister an event handler to be processed by FMan     Return: 0 on success; Error code otherwise. ", "int fman_set_port_params(struct fman *fman, struct fman_port_init_params *port_params)": "fman_set_port_params   @fman:A Pointer to FMan device   @port_params:Port parameters     Used by FMan Port to pass parameters to the FMan     Return: 0 on success; Error code otherwise. ", "int fman_reset_mac(struct fman *fman, u8 mac_id)": "fman_reset_mac   @fman:A Pointer to FMan device   @mac_id:MAC id to be reset     Reset a specific MAC     Return: 0 on success; Error code otherwise. ", "int fman_set_mac_max_frame(struct fman *fman, u8 mac_id, u16 mfl)": "fman_set_mac_max_frame   @fman:A Pointer to FMan device   @mac_id:MAC id   @mfl:Maximum frame length     Set maximum frame length of specific MAC in FMan driver     Return: 0 on success; Error code otherwise. ", "u32 fman_get_bmi_max_fifo_size(struct fman *fman)": "fman_get_bmi_max_fifo_size   @fman:A Pointer to FMan device     Get FMan maximum FIFO size     Return: FMan Maximum FIFO size ", "if (fman->state->rev_info.major >= 6)fman->cfg->dma_aid_mode = FMAN_DMA_AID_OUT_PORT_ID;fman->cfg->qmi_def_tnums_thresh = fman->state->qmi_def_tnums_thresh;fman->state->total_num_of_tasks =(u8)DFLT_TOTAL_NUM_OF_TASKS(fman->state->rev_info.major,    fman->state->rev_info.minor,    fman->state->bmi_max_num_of_tasks);if (fman->state->rev_info.major < 6) ": "fman_get_revision(fman, &fman->state->rev_info);err = fill_soc_specific_params(fman->state);if (err)goto err_fm_soc_specific;  FM_AID_MODE_NO_TNUM_SW005 Errata workaround ", "u32 fman_get_qman_channel_id(struct fman *fman, u32 port_id)": "fman_get_qman_channel_id   @fman:A Pointer to FMan device   @port_id:Port id     Get QMan channel ID associated to the Port id     Return: QMan channel ID ", "struct resource *fman_get_mem_region(struct fman *fman)": "fman_get_mem_region   @fman:A Pointer to FMan device     Get FMan memory region     Return: A structure with FMan memory region information ", "u16 fman_get_max_frm(void)": "fman_get_max_frm     Return: Max frame length configured in the FM driver ", "int fman_get_rx_extra_headroom(void)": "fman_get_rx_extra_headroom     Return: Extra headroom size configured in the FM driver ", "struct fman *fman_bind(struct device *fm_dev)": "fman_bind   @fm_dev:FMan OF device pointer     Bind to a specific FMan device.     Allowed only after the port was created.     Return: A pointer to the FMan device ", "tx_credit = roundup(1000 + ENETC_MAC_MAXFRM_SIZE / 2, 100);/* Internal memory allocated for transmit buffering is guaranteed but * not reserved; i.e. if the total transmit allocation is not used, * then the unused portion is not left idle, it can be used for receive * buffering but it will be reclaimed, if required, from receive by * intelligently dropping already stored receive frames in the internal * memory to ensure that the transmit allocation is respected. * * PaTXMBAR must be set to a value larger than *     PaTXBCR + 2 * max_frame_size + 32 * if frame preemption is not enabled, or to *     2 * PaTXBCR + 2 * p_max_frame_size (pMAC maximum frame size) + *     2 * np_max_frame_size (eMAC maximum frame size) + 64 * if frame preemption is enabled. ": "enetc_ierb_register_pf(struct platform_device  pdev,   struct pci_dev  pf_pdev){struct enetc_ierb  ierb = platform_get_drvdata(pdev);int port = enetc_pf_to_port(pf_pdev);u16 tx_credit, rx_credit, tx_alloc;if (port < 0)return -ENODEV;if (!ierb)return -EPROBE_DEFER;  By default, it is recommended to set the Host Transfer Agent   per port transmit byte credit to \"1000 + max_frame_size2\".   The power-on reset value (1800 bytes) is rounded up to the nearest   100 assuming a maximum frame size of 1536 bytes. ", "comp->flags |= SLF_TOSS;if ( tslots > 0 ) ": "slhc_init(int rslots, int tslots){short i;struct cstate  ts;struct slcompress  comp;if (rslots < 0 || rslots > 255 || tslots < 0 || tslots > 255)return ERR_PTR(-EINVAL);comp = kzalloc(sizeof(struct slcompress), GFP_KERNEL);if (! comp)goto out_fail;if (rslots > 0) {size_t rsize = rslots   sizeof(struct cstate);comp->rstate = kzalloc(rsize, GFP_KERNEL);if (! comp->rstate)goto out_free;comp->rslot_limit = rslots - 1;}if (tslots > 0) {size_t tsize = tslots   sizeof(struct cstate);comp->tstate = kzalloc(tsize, GFP_KERNEL);if (! comp->tstate)goto out_free2;comp->tslot_limit = tslots - 1;}comp->xmit_oldest = 0;comp->xmit_current = 255;comp->recv_current = 255;    don't accept any packets with implicit index until we get   one with an explicit index.  Otherwise the uncompress code   will try to use connection 255, which is almost certainly   out of range ", "static inline unsigned char *put16(unsigned char *cp, unsigned short x)": "slhc_free(struct slcompress  comp){if ( IS_ERR_OR_NULL(comp) )return;if ( comp->tstate != NULLSLSTATE )kfree( comp->tstate );if ( comp->rstate != NULLSLSTATE )kfree( comp->rstate );kfree( comp );}  Put a short in host order into a char array in network order ", "comp->sls_i_runt++;return slhc_toss( comp );}/* Peek at the IP header's IHL field to find its length ": "slhc_toss( comp );}intslhc_remember(struct slcompress  comp, unsigned char  icp, int isize){struct cstate  cs;unsigned ihl;unsigned char index;if(isize < 20) {  The packet is shorter than a legal IP header ", "if(isize<sizeof(struct iphdr))return isize;ip = (struct iphdr *) icp;if (ip->version != 4 || ip->ihl < 5)return isize;/* Bail if this packet isn't TCP, or is an IP fragment ": "slhc_compress(struct slcompress  comp, unsigned char  icp, int isize,unsigned char  ocp, unsigned char   cpp, int compress_cid){struct cstate  ocs = &(comp->tstate[comp->xmit_oldest]);struct cstate  lcs = ocs;struct cstate  cs = lcs->next;unsigned long deltaS, deltaA;short changes = 0;int nlen, hlen;unsigned char new_seq[16];unsigned char  cp = new_seq;struct iphdr  ip;struct tcphdr  th,  oth;__sum16 csum;   Don't play with runt packets. ", "comp->sls_i_compressed++;if(isize < 3)": "slhc_uncompress(struct slcompress  comp, unsigned char  icp, int isize){int changes;long x;struct tcphdr  thp;struct iphdr  ip;struct cstate  cs;int len, hdrlen;unsigned char  cp = icp;  We've got a compressed packet; read the change byte ", "ndev = alloc_candev(sizeof(struct ctucan_priv), ntxbufs);if (!ndev)return -ENOMEM;priv = netdev_priv(ndev);spin_lock_init(&priv->tx_lock);INIT_LIST_HEAD(&priv->peers_on_pdev);priv->ntxbufs = ntxbufs;priv->dev = dev;priv->can.bittiming_const = &ctu_can_fd_bit_timing_max;priv->can.data_bittiming_const = &ctu_can_fd_bit_timing_data_max;priv->can.do_set_mode = ctucan_do_set_mode;/* Needed for timing adjustment to be performed as soon as possible ": "ctucan_probe_common(struct device  dev, void __iomem  addr, int irq, unsigned int ntxbufs,unsigned long can_clk_rate, int pm_enable_call,void ( set_drvdata_fnc)(struct device  dev, struct net_device  ndev)){struct ctucan_priv  priv;struct net_device  ndev;int ret;  Create a CAN device instance ", "if (copy_from_user(&hwts_cfg, ifr->ifr_data, sizeof(hwts_cfg)))return -EFAULT;if (hwts_cfg.tx_type == HWTSTAMP_TX_ON &&    hwts_cfg.rx_filter == HWTSTAMP_FILTER_ALL)return 0;return -ERANGE;case SIOCGHWTSTAMP: /* get ": "can_eth_ioctl_hwts(struct net_device  netdev, struct ifreq  ifr, int cmd){struct hwtstamp_config hwts_cfg = { 0 };switch (cmd) {case SIOCSHWTSTAMP:   set ", "struct ArcProto *arc_proto_map[256];EXPORT_SYMBOL(arc_proto_map": "arc_proto_map are allowed to be NULL; they will get set to   arc_proto_default instead.  It also must not be NULL; if you would like   to set it to NULL, set it to &arc_proto_null instead. ", "struct ArcProto *arc_proto_map[256];EXPORT_SYMBOL(arc_proto_map);struct ArcProto *arc_proto_default;EXPORT_SYMBOL(arc_proto_default": "arc_proto_default instead.  It also must not be NULL; if you would like   to set it to NULL, set it to &arc_proto_null instead. ", "snprintf(hdr, sizeof(hdr), \"%6s:%s skb->data:\", dev->name, desc);print_hex_dump(KERN_DEBUG, hdr, DUMP_PREFIX_OFFSET,       16, 1, skb->data, skb->len, true);}EXPORT_SYMBOL(arcnet_dump_skb": "arcnet_dump_skb(struct net_device  dev,     struct sk_buff  skb, char  desc){char hdr[32];  dump the packet ", "cancel_work_sync(&lp->reset_work);free_netdev(dev);}EXPORT_SYMBOL(free_arcdev": "free_arcdev(struct net_device  dev){struct arcnet_local  lp = netdev_priv(dev);  Do not cancel this at ->ndo_close(), as the workqueue itself   indirectly calls the ifdown path through dev_close(). ", "static void arcdev_setup(struct net_device *dev)": "arcnet_timeout,};  Setup a struct device for ARCnet. ", "if (!netif_running(dev)) ": "arcnet_interrupt(int irq, void  dev_id){struct net_device  dev = dev_id;struct arcnet_local  lp;int recbuf, status, diagstatus, didsomething, boguscount;unsigned long flags;int retval = IRQ_NONE;arc_printk(D_DURING, dev, \"\\n\");arc_printk(D_DURING, dev, \"in arcnet_interrupt\\n\");lp = netdev_priv(dev);BUG_ON(!lp);spin_lock_irqsave(&lp->lock, flags);if (lp->reset_in_progress)goto out;  RESET flag was enabled - if device is not running, we must   clear it right away (but nothing else). ", "/* Enable P1Mode for backplane mode ": "com20020_check(struct net_device  dev){int ioaddr = dev->base_addr, status;struct arcnet_local  lp = netdev_priv(dev);arcnet_outb(XTOcfg(3) | RESETcfg, ioaddr, COM20020_REG_W_CONFIG);udelay(5);arcnet_outb(XTOcfg(3), ioaddr, COM20020_REG_W_CONFIG);mdelay(RESETtime);lp->setup = lp->clockm ? 0 : (lp->clockp << 1);lp->setup2 = (lp->clockm << 4) | 8;  CHECK: should we do this for SOHARD cards ? ", "lp = netdev_priv(dev);lp->hw.owner = THIS_MODULE;lp->hw.command = com20020_command;lp->hw.status = com20020_status;lp->hw.intmask = com20020_setmask;lp->hw.reset = com20020_reset;lp->hw.copy_to_card = com20020_copy_to_card;lp->hw.copy_from_card = com20020_copy_from_card;lp->hw.close = com20020_close;/* FIXME: do this some other way! ": "com20020_found(struct net_device  dev, int shared){struct arcnet_local  lp;int ioaddr = dev->base_addr;  Initialize the rest of the device structure. ", "int com20020_found(struct net_device *dev, int shared)": "com20020_netdev_ops = {.ndo_open= com20020_netdev_open,.ndo_stop= com20020_netdev_close,.ndo_start_xmit = arcnet_send_packet,.ndo_tx_timeout = arcnet_timeout,.ndo_set_mac_address = com20020_set_hwaddr,.ndo_set_rx_mode = com20020_set_mc_list,};  Set up the struct net_device associated with this card.  Called after   probing succeeds. ", "}EXPORT_SYMBOL(hdlc_start_xmit": "hdlc_start_xmit(struct sk_buff  skb, struct net_device  dev){hdlc_device  hdlc = dev_to_hdlc(dev);if (hdlc->proto->xmit)return hdlc->proto->xmit(skb, dev);return hdlc->xmit(skb, dev);   call hardware driver directly ", "if (hdlc->proto->open) ": "hdlc_open(struct net_device  dev){hdlc_device  hdlc = dev_to_hdlc(dev);#ifdef DEBUG_LINKprintk(KERN_DEBUG \"%s: hdlc_open() carrier %i open %i\\n\", dev->name,       hdlc->carrier, hdlc->open);#endifif (!hdlc->proto)return -ENOSYS;  no protocol attached ", "while (proto) ": "hdlc_ioctl(struct net_device  dev, struct if_settings  ifs){struct hdlc_proto  proto = first_proto;int result;if (dev_to_hdlc(dev)->proto) {result = dev_to_hdlc(dev)->proto->ioctl(dev, ifs);if (result != -EINVAL)return result;}  Not handled by currently attached protocol (if any) ", "smi_cmd.s.phy_adr = phy_id;smi_cmd.s.reg_adr = regnum;oct_mdio_writeq(smi_cmd.u64, p->register_base + SMI_CMD);do ": "cavium_mdiobus_write_c22(struct mii_bus  bus, int phy_id, int regnum,     u16 val){struct cavium_mdiobus  p = bus->priv;union cvmx_smix_cmd smi_cmd;union cvmx_smix_wr_dat smi_wr;int timeout = 1000;cavium_mdiobus_set_mode(p, C22);smi_wr.u64 = 0;smi_wr.s.dat = val;oct_mdio_writeq(smi_wr.u64, p->register_base + SMI_WR_DAT);smi_cmd.u64 = 0;smi_cmd.s.phy_op = 0;   MDIO_CLAUSE_22_WRITE ", "smi_cmd.s.phy_adr = phy_id;smi_cmd.s.reg_adr = devad;oct_mdio_writeq(smi_cmd.u64, p->register_base + SMI_CMD);do ": "cavium_mdiobus_write_c45(struct mii_bus  bus, int phy_id, int devad,     int regnum, u16 val){struct cavium_mdiobus  p = bus->priv;union cvmx_smix_cmd smi_cmd;union cvmx_smix_wr_dat smi_wr;int timeout = 1000;int r;r = cavium_mdiobus_c45_addr(p, phy_id, devad, regnum);if (r < 0)return r;smi_wr.u64 = 0;smi_wr.s.dat = val;oct_mdio_writeq(smi_wr.u64, p->register_base + SMI_WR_DAT);smi_cmd.u64 = 0;smi_cmd.s.phy_op = 1;   MDIO_CLAUSE_45_WRITE ", "if (rc == -EPROBE_DEFER)rc = driver_deferred_probe_check_state(&phy->mdio.dev);if (rc == -EPROBE_DEFER)return rc;if (rc > 0) ": "fwnode_mdiobus_phy_device_register(struct mii_bus  mdio,       struct phy_device  phy,       struct fwnode_handle  child, u32 addr){int rc;rc = fwnode_irq_get(child, 0);  Don't wait forever if the IRQ provider doesn't become available,   just fall back to poll mode ", "phy->mdio.dev.fwnode = fwnode_handle_get(child);/* All data is now stored in the phy struct, so register it ": "fwnode_mdiobus_register_phy(struct mii_bus  bus,struct fwnode_handle  child, u32 addr){struct mii_timestamper  mii_ts = NULL;struct pse_control  psec = NULL;struct phy_device  phy;bool is_c45;u32 phy_id;int rc;psec = fwnode_find_pse_control(child);if (IS_ERR(psec))return PTR_ERR(psec);mii_ts = fwnode_find_mii_timestamper(child);if (IS_ERR(mii_ts)) {rc = PTR_ERR(mii_ts);goto clean_pse;}is_c45 = fwnode_device_is_compatible(child, \"ethernet-phy-ieee802.3-c45\");if (is_c45 || fwnode_get_phy_id(child, &phy_id))phy = get_phy_device(bus, addr, is_c45);elsephy = phy_device_create(bus, addr, phy_id, 0, NULL);if (IS_ERR(phy)) {rc = PTR_ERR(phy);goto clean_mii_ts;}if (is_acpi_node(child)) {phy->irq = bus->irq[addr];  Associate the fwnode with the device structure so it   can be looked up later. ", "int __of_mdiobus_register(struct mii_bus *mdio, struct device_node *np,  struct module *owner)": "__of_mdiobus_register - Register mii_bus and create PHYs from the device tree   @mdio: pointer to mii_bus structure   @np: pointer to device_node of MDIO bus.   @owner: module owning the @mdio object.     This function registers the mii_bus structure and registers a phy_device   for each child node of @np. ", "struct mdio_device *of_mdio_find_device(struct device_node *np)": "of_mdio_find_device - Given a device tree node, find the mdio_device   @np: pointer to the mdio_device's device tree node     If successful, returns a pointer to the mdio_device with the embedded   struct device refcount incremented by one, or NULL on failure.   The caller should call put_device() on the mdio_device after its use ", "struct phy_device *of_phy_find_device(struct device_node *phy_np)": "of_phy_find_device - Give a PHY node, find the phy_device   @phy_np: Pointer to the phy's device tree node     If successful, returns a pointer to the phy_device with the embedded   struct device refcount incremented by one, or NULL on failure. ", "struct phy_device *of_phy_connect(struct net_device *dev,  struct device_node *phy_np,  void (*hndlr)(struct net_device *), u32 flags,  phy_interface_t iface)": "of_phy_connect - Connect to the phy described in the device tree   @dev: pointer to net_device claiming the phy   @phy_np: Pointer to device tree node for the PHY   @hndlr: Link state callback for the network device   @flags: flags to pass to the PHY   @iface: PHY data interface type     If successful, returns a pointer to the phy_device with the embedded   struct device refcount incremented by one, or NULL on failure. The   refcount must be dropped by calling phy_disconnect() or phy_detach(). ", "struct phy_device *of_phy_get_and_connect(struct net_device *dev,  struct device_node *np,  void (*hndlr)(struct net_device *))": "of_phy_get_and_connect   - Get phy node and connect to the phy described in the device tree   @dev: pointer to net_device claiming the phy   @np: Pointer to device tree node for the net_device claiming the phy   @hndlr: Link state callback for the network device     If successful, returns a pointer to the phy_device with the embedded   struct device refcount incremented by one, or NULL on failure. The   refcount must be dropped by calling phy_disconnect() or phy_detach(). ", "bool of_phy_is_fixed_link(struct device_node *np)": "of_phy_register_fixed_link(np);if (ret < 0) {netdev_err(dev, \"broken fixed-link specification\\n\");return NULL;}phy_np = of_node_get(np);} else {phy_np = of_parse_phandle(np, \"phy-handle\", 0);if (!phy_np)return NULL;}phy = of_phy_connect(dev, phy_np, hndlr, 0, iface);of_node_put(phy_np);return phy;}EXPORT_SYMBOL(of_phy_get_and_connect);    of_phy_is_fixed_link() and of_phy_register_fixed_link() must   support two DT bindings:   - the old DT binding, where 'fixed-link' was a property with 5     cells encoding various information about the fixed PHY   - the new DT binding, where 'fixed-link' is a sub-node of the     Ethernet device. ", "phy_device_free(phydev);/* fixed_phy_register() ": "of_phy_deregister_fixed_link(struct device_node  np){struct phy_device  phydev;phydev = of_phy_find_device(np);if (!phydev)return;fixed_phy_unregister(phydev);put_device(&phydev->mdio.dev);  of_phy_find_device() ", "int __acpi_mdiobus_register(struct mii_bus *mdio, struct fwnode_handle *fwnode,    struct module *owner)": "__acpi_mdiobus_register - Register mii_bus and create PHYs from the ACPI ASL.   @mdio: pointer to mii_bus structure   @fwnode: pointer to fwnode of MDIO bus. This fwnode is expected to represent   @owner: module owning this @mdio object.   an ACPI device object corresponding to the MDIO bus and its children are   expected to correspond to the PHY devices on that bus.     This function registers the mii_bus structure and registers a phy_device   for each child node of @fwnode. ", "int vmbus_sendpacket_getid(struct vmbus_channel *channel, void *buffer,   u32 bufferlen, u64 requestid, u64 *trans_id,   enum vmbus_packet_type type, u32 flags)": "vmbus_sendpacket_getid() - Send the specified buffer on the given channel   @channel: Pointer to vmbus_channel structure   @buffer: Pointer to the buffer you want to send the data from.   @bufferlen: Maximum size of what the buffer holds.   @requestid: Identifier of the request   @trans_id: Identifier of the transaction associated to this request, if              the send is successful; undefined, otherwise.   @type: Type of packet that is being sent e.g. negotiate, time    packet etc.   @flags: 0 or VMBUS_DATA_PACKET_FLAG_COMPLETION_REQUESTED     Sends data in @buffer directly to Hyper-V via the vmbus.   This will send the data unparsed to Hyper-V.     Mainly used by Hyper-V drivers. ", "static inline int__vmbus_recvpacket(struct vmbus_channel *channel, void *buffer,   u32 bufferlen, u32 *buffer_actual_len, u64 *requestid,   bool raw)": "vmbus_recvpacket() - Retrieve the user packet on the specified channel   @channel: Pointer to vmbus_channel structure   @buffer: Pointer to the buffer you want to receive the data into.   @bufferlen: Maximum size of what the buffer can hold.   @buffer_actual_len: The actual size of the data after it was received.   @requestid: Identifier of the request   @raw: true means keep the vmpacket_descriptor header in the received data.     Receives directly from the hyper-v vmbus and puts the data it received   into Buffer. This will receive the data unparsed from hyper-v.     Mainly used by Hyper-V drivers. ", "np = prev;while (np->parent && !np->sibling)np = np->parent;np = np->sibling; /* Might be null at the end of the tree ": "of_find_all_nodes(struct device_node  prev){struct device_node  np;if (!prev) {np = of_root;} else if (prev->child) {np = prev->child;} else {  Walk back up looking for a sibling, or the end of the structure ", "return OF_ROOT_NODE_ADDR_CELLS_DEFAULT;}int of_n_addr_cells(struct device_node *np)": "of_get_property(np, \"device_type\", NULL);return np && match && type && !strcmp(match, type);}int of_bus_n_addr_cells(struct device_node  np){u32 cells;for (; np; np = np->parent)if (!of_property_read_u32(np, \"#address-cells\", &cells))return cells;  No #address-cells property for the root node ", "static int __of_device_is_compatible(const struct device_node *device,     const char *compat, const char *type, const char *name)": "of_device_is_compatible() - Check if the node matches given constraints   @device: pointer to node   @compat: required compatible string, NULL or \"\" for any match   @type: required device_type value, NULL or \"\" for any match   @name: required node name, NULL or \"\" for any match     Checks if the given @compat, @type and @name strings match the   properties of the given @device. A constraints can be skipped by   passing NULL or an empty string as the constraint.     Returns 0 for no match, and a positive integer on match. The return   value is a relative score with larger values indicating better   matches. The score is weighted for the most specific compatible value   to get the highest score. Matching type is next, followed by matching   name. Practically speaking, this results in the following priority   order for matches:     1. specific compatible && type && name   2. specific compatible && type   3. specific compatible && name   4. specific compatible   5. general compatible && type && name   6. general compatible && type   7. general compatible && name   8. general compatible   9. type && name   10. type   11. name ", "int of_machine_is_compatible(const char *compat)": "of_machine_is_compatible - Test root of device tree for a given compatible value   @compat: compatible string to look for in root node's compatible property.     Return: A positive integer if the root node has the given value in its   compatible property. ", "static bool __of_device_is_available(const struct device_node *device)": "of_device_is_available - check if a device is available for use      @device: Node to check for availability, with locks already held      Return: True if the status property is absent or set to \"okay\" or \"ok\",    false otherwise ", "bool of_device_is_big_endian(const struct device_node *device)": "of_device_is_big_endian - check if a device has BE registers      @device: Node to check for endianness      Return: True if the device has a \"big-endian\" property, or if the kernel    was compiled for BE  and  the device has a \"native-endian\" property.    Returns false otherwise.      Callers would nominally use ioread32beiowrite32be if    of_device_is_big_endian() == true, or readlwritel otherwise. ", "struct device_node *of_get_parent(const struct device_node *node)": "of_get_parent - Get a node's parent if any   @node:Node to get parent     Return: A node pointer with refcount incremented, use   of_node_put() on it when done. ", "struct device_node *of_get_next_parent(struct device_node *node)": "of_get_next_parent - Iterate to a node's parent   @node:Node to get parent of     This is like of_get_parent() except that it drops the   refcount on the passed node, making it suitable for iterating   through a node's parents.     Return: A node pointer with refcount incremented, use   of_node_put() on it when done. ", "struct device_node *of_get_next_child(const struct device_node *node,struct device_node *prev)": "of_get_next_child(const struct device_node  node,struct device_node  prev){struct device_node  next;if (!node)return NULL;next = prev ? prev->sibling : node->child;of_node_get(next);of_node_put(prev);return next;}#define __for_each_child_of_node(parent, child) \\for (child = __of_get_next_child(parent, NULL); child != NULL; \\     child = __of_get_next_child(parent, child))     of_get_next_child - Iterate a node childs   @node:parent node   @prev:previous child of the parent node, or NULL to get first     Return: A node pointer with refcount incremented, use of_node_put() on   it when done. Returns NULL when prev is the last child. Decrements the   refcount of prev. ", "struct device_node *of_get_next_available_child(const struct device_node *node,struct device_node *prev)": "of_get_next_available_child - Find the next available child node   @node:parent node   @prev:previous child of the parent node, or NULL to get first     This function is like of_get_next_child(), except that it   automatically skips any disabled nodes (i.e. status = \"disabled\"). ", "struct device_node *of_get_next_cpu_node(struct device_node *prev)": "of_get_next_cpu_node - Iterate on cpu nodes   @prev:previous child of the cpus node, or NULL to get first     Unusable CPUs (those with the status property set to \"fail\" or \"fail-...\")   will be skipped.     Return: A cpu node pointer with refcount incremented, use of_node_put()   on it when done. Returns NULL when prev is the last child. Decrements   the refcount of prev. ", "struct device_node *of_get_compatible_child(const struct device_node *parent,const char *compatible)": "of_get_compatible_child - Find compatible child node   @parent:parent node   @compatible:compatible string     Lookup child node whose compatible property contains the given compatible   string.     Return: a node pointer with refcount incremented, use of_node_put() on it   when done; or NULL if not found. ", "struct device_node *of_get_child_by_name(const struct device_node *node,const char *name)": "of_get_child_by_name - Find the child node by name for a given parent   @node:parent node   @name:child name to look for.     This function looks for child node for given matching name     Return: A node pointer if found, with refcount incremented, use   of_node_put() on it when done.   Returns NULL if node is not found. ", "struct device_node *of_find_node_opts_by_path(const char *path, const char **opts)": "of_find_node_opts_by_path - Find a node matching a full OF path   @path: Either the full path to match, or if the path does not         start with '', the name of a property of the aliases         node (an alias).  In the case of an alias, the node         matching the alias' value will be returned.   @opts: Address of a pointer into which to store the start of         an options string appended to the end of the path with         a ':' separator.     Valid paths:      foobarFull path      fooValid alias      foobarValid alias + relative path     Return: A node pointer with refcount incremented, use   of_node_put() on it when done. ", "struct device_node *of_find_node_by_name(struct device_node *from,const char *name)": "of_find_node_by_name - Find a node by its \"name\" property   @from:The node to start searching from or NULL; the node  you pass will not be searched, only the next one  will. Typically, you pass what the previous call  returned. of_node_put() will be called on @from.   @name:The name string to match against     Return: A node pointer with refcount incremented, use   of_node_put() on it when done. ", "struct device_node *of_find_node_by_type(struct device_node *from,const char *type)": "of_find_node_by_type - Find a node by its \"device_type\" property   @from:The node to start searching from, or NULL to start searching  the entire device tree. The node you pass will not be  searched, only the next one will; typically, you pass  what the previous call returned. of_node_put() will be  called on from for you.   @type:The type string to match against     Return: A node pointer with refcount incremented, use   of_node_put() on it when done. ", "struct device_node *of_find_compatible_node(struct device_node *from,const char *type, const char *compatible)": "of_find_compatible_node - Find a node based on type and one of the                                  tokens in its \"compatible\" property   @from:The node to start searching from or NULL, the node  you pass will not be searched, only the next one  will; typically, you pass what the previous call  returned. of_node_put() will be called on it   @type:The type string to match \"device_type\" or NULL to ignore   @compatible:The string to match to one of the tokens in the device  \"compatible\" list.     Return: A node pointer with refcount incremented, use   of_node_put() on it when done. ", "struct device_node *of_find_node_with_property(struct device_node *from,const char *prop_name)": "of_find_node_with_property - Find a node which has a property with                                the given name.   @from:The node to start searching from or NULL, the node  you pass will not be searched, only the next one  will; typically, you pass what the previous call  returned. of_node_put() will be called on it   @prop_name:The name of the property to look for.     Return: A node pointer with refcount incremented, use   of_node_put() on it when done. ", "const struct of_device_id *of_match_node(const struct of_device_id *matches, const struct device_node *node)": "of_match_node(const struct of_device_id  matches,   const struct device_node  node){const struct of_device_id  best_match = NULL;int score, best_score = 0;if (!matches)return NULL;for (; matches->name[0] || matches->type[0] || matches->compatible[0]; matches++) {score = __of_device_is_compatible(node, matches->compatible,  matches->type, matches->name);if (score > best_score) {best_match = matches;best_score = score;}}return best_match;}     of_match_node - Tell if a device_node has a matching of_match structure   @matches:array of of device match structures to search in   @node:the of device structure to match against     Low level utility function used by device matching. ", "struct device_node *of_find_matching_node_and_match(struct device_node *from,const struct of_device_id *matches,const struct of_device_id **match)": "of_find_matching_node_and_match - Find a node based on an of_device_id       match table.   @from:The node to start searching from or NULL, the node  you pass will not be searched, only the next one  will; typically, you pass what the previous call  returned. of_node_put() will be called on it   @matches:array of of device match structures to search in   @match:Updated to point at the matches entry which matched     Return: A node pointer with refcount incremented, use   of_node_put() on it when done. ", "struct device_node *of_find_node_by_phandle(phandle handle)": "of_find_node_by_phandle - Find a node given a phandle   @handle:phandle of the node to find     Return: A node pointer with refcount incremented, use   of_node_put() on it when done. ", "of_for_each_phandle(&it, rc, np, list_name, cells_name, cell_count) ": "__of_parse_phandle_with_args(const struct device_node  np, const char  list_name, const char  cells_name, int cell_count, int index, struct of_phandle_args  out_args){struct of_phandle_iterator it;int rc, cur_index = 0;if (index < 0)return -EINVAL;  Loop over the phandles until all the requested entry is found ", "int of_parse_phandle_with_args_map(const struct device_node *np,   const char *list_name,   const char *stem_name,   int index, struct of_phandle_args *out_args)": "of_parse_phandle_with_args_map() - Find a node pointed by phandle in a list and remap it   @np:pointer to a device tree node containing a list   @list_name:property name that contains a list   @stem_name:stem of property names that specify phandles' arguments count   @index:index of a phandle to parse out   @out_args:optional pointer to output arguments structure (will be filled)     This function is useful to parse lists of phandles and their arguments.   Returns 0 on success and fills out_args, on error returns appropriate errno   value. The difference between this function and of_parse_phandle_with_args()   is that this API remaps a phandle if the node the phandle points to has   a <@stem_name>-map property.     Caller is responsible to call of_node_put() on the returned out_args->np   pointer.     Example::      phandle1: node1 {    #list-cells = <2>;    };      phandle2: node2 {    #list-cells = <1>;    };      phandle3: node3 {    #list-cells = <1>;    list-map = <0 &phandle2 3>,       <1 &phandle2 2>,       <2 &phandle1 5 1>;    list-map-mask = <0x3>;    };      node4 {    list = <&phandle1 1 2 &phandle3 0>;    };     To get a device_node of the ``node2`` node you may call this:   of_parse_phandle_with_args(node4, \"list\", \"list\", 1, &args); ", "int of_count_phandle_with_args(const struct device_node *np, const char *list_name,const char *cells_name)": "of_count_phandle_with_args() - Find the number of phandles references in a property   @np:pointer to a device tree node containing a list   @list_name:property name that contains a list   @cells_name:property name that specifies phandles' arguments count     Return: The number of phandle + argument tuples within a property. It   is a typical pattern to encode a list of phandle and variable   arguments into a single property. The number of arguments is encoded   by a property in the phandle-target node. For example, a gpios   property would contain a list of GPIO specifies consisting of a   phandle and 1 or more arguments. The number of arguments are   determined by the #gpio-cells property in the node pointed to by the   phandle. ", "struct platform_device *of_find_device_by_node(struct device_node *np)": "of_find_device_by_node - Find the platform_device associated with a node   @np: Pointer to device tree node     Takes a reference to the embedded struct device which needs to be dropped   after use.     Return: platform_device pointer, or NULL if not found ", "struct platform_device *of_device_alloc(struct device_node *np,  const char *bus_id,  struct device *parent)": "of_device_alloc - Allocate and initialize an of_device   @np: device node to assign to device   @bus_id: Name to assign to the device.  May be null to use default name.   @parent: Parent device. ", "static struct platform_device *of_platform_device_create_pdata(struct device_node *np,const char *bus_id,void *platform_data,struct device *parent)": "of_platform_device_create_pdata - Alloc, initialize and register an of_device   @np: pointer to node to create device for   @bus_id: name to assign device   @platform_data: pointer to populate platform_data pointer with   @parent: Linux device model parent device.     Return: Pointer to created platform device, or NULL if a device was not   registered.  Unavailable devices will not get registered. ", "int of_platform_bus_probe(struct device_node *root,  const struct of_device_id *matches,  struct device *parent)": "of_platform_bus_probe() - Probe the device-tree for platform buses   @root: parent of the first level to probe or NULL for the root of the tree   @matches: match table for bus nodes   @parent: parent to hook devices from, NULL for toplevel     Note that children of the provided root are not instantiated as devices   unless the specified root itself matches the bus list and is not NULL. ", "struct device_node *of_get_cpu_node(int cpu, unsigned int *thread)": "of_get_cpu_node - Get device node associated with the given logical CPU     @cpu: CPU number(logical index) for which device node is required   @thread: if not NULL, local thread number within the physical core is            returned     The main purpose of this function is to retrieve the device node for the   given logical CPU index. It should be used to initialize the of_node in   cpu device. Once of_node in cpu device is populated, all the further   references can use that instead.     CPU logical to physical index mapping is architecture specific and is built   before booting secondary cores. This function uses arch_match_cpu_phys_id   which can be overridden by architecture specific implementation.     Return: A node pointer for the logical cpu with refcount incremented, use   of_node_put() on it when done. Returns NULL if not found. ", "struct device_node *of_cpu_device_node_get(int cpu)": "of_cpu_device_node_get: Get the CPU device_node for a given logical CPU number     @cpu: The logical CPU number     Return: Pointer to the device_node for CPU with its reference count   incremented of the given logical CPU number or NULL if the CPU device_node   is not found. ", "int of_cpu_node_to_id(struct device_node *cpu_node)": "of_cpu_node_to_id: Get the logical CPU number for a given device_node     @cpu_node: Pointer to the device_node for CPU.     Return: The logical CPU number of the given CPU device_node or -ENODEV if the   CPU is not found. ", "struct device_node *of_get_cpu_state_node(struct device_node *cpu_node,  int index)": "of_get_cpu_state_node - Get CPU's idle state node at the given index     @cpu_node: The device node for the CPU   @index: The index in the list of the idle states     Two generic methods can be used to describe a CPU's idle states, either via   a flattened description through the \"cpu-idle-states\" binding or via the   hierarchical layout, using the \"power-domains\" and the \"domain-idle-states\"   bindings. This function check for both and returns the idle state node for   the requested index.     Return: An idle state node if found at @index. The refcount is incremented   for it, so call of_node_put() on it when done. Returns NULL if not found. ", "bool of_graph_is_present(const struct device_node *node)": "of_graph_is_present() - check graph's presence   @node: pointer to device_node containing graph port     Return: True if @node has a port or ports (with a port) sub-node,   false otherwise. ", "int of_graph_parse_endpoint(const struct device_node *node,    struct of_endpoint *endpoint)": "of_graph_parse_endpoint() - parse common endpoint node properties   @node: pointer to endpoint device_node   @endpoint: pointer to the OF endpoint data structure     The caller should hold a reference to @node. ", "struct device_node *of_graph_get_port_by_id(struct device_node *parent, u32 id)": "of_graph_get_port_by_id() - get the port matching a given id   @parent: pointer to the parent device node   @id: id of the port     Return: A 'port' node pointer with refcount incremented. The caller   has to use of_node_put() on it when done. ", "struct device_node *of_graph_get_next_endpoint(const struct device_node *parent,struct device_node *prev)": "of_graph_get_next_endpoint() - get next endpoint node   @parent: pointer to the parent device node   @prev: previous endpoint node, or NULL to get first     Return: An 'endpoint' node pointer with refcount incremented. Refcount   of the passed @prev node is decremented. ", "struct device_node *of_graph_get_endpoint_by_regs(const struct device_node *parent, int port_reg, int reg)": "of_graph_get_endpoint_by_regs() - get endpoint node of specific identifiers   @parent: pointer to the parent device node   @port_reg: identifier (value of reg property) of the parent port node   @reg: identifier (value of reg property) of the endpoint node     Return: An 'endpoint' node pointer which is identified by reg and at the same   is the child of a port node identified by port_reg. reg and port_reg are   ignored when they are -1. Use of_node_put() on the pointer when done. ", "struct device_node *of_graph_get_remote_endpoint(const struct device_node *node)": "of_graph_get_remote_endpoint() - get remote endpoint node   @node: pointer to a local endpoint device_node     Return: Remote endpoint node associated with remote endpoint node linked     to @node. Use of_node_put() on it when done. ", "struct device_node *of_graph_get_port_parent(struct device_node *node)": "of_graph_get_port_parent() - get port's parent node   @node: pointer to a local endpoint device_node     Return: device node associated with endpoint node linked     to @node. Use of_node_put() on it when done. ", "struct device_node *of_graph_get_remote_port_parent(       const struct device_node *node)": "of_graph_get_remote_port_parent() - get remote port's parent node   @node: pointer to a local endpoint device_node     Return: Remote device node associated with remote endpoint node linked     to @node. Use of_node_put() on it when done. ", "struct device_node *of_graph_get_remote_node(const struct device_node *node,     u32 port, u32 endpoint)": "of_graph_get_remote_node() - get remote parent device_node for given portendpoint   @node: pointer to parent device_node containing graph portendpoint   @port: identifier (value of reg property) of the parent port node   @endpoint: identifier (value of reg property) of the endpoint node     Return: Remote device node associated with remote endpoint node linked   to @node. Use of_node_put() on it when done. ", "const struct of_device_id *of_match_device(const struct of_device_id *matches,   const struct device *dev)": "of_match_device - Tell if a struct device matches an of_device_id list   @matches: array of of device match structures to search in   @dev: the of device structure to match against     Used by a driver to check whether an platform_device present in the   system is in its list of supported devices. ", "int of_pci_range_to_resource(struct of_pci_range *range,     struct device_node *np, struct resource *res)": "of_pci_range_to_resource - Create a resource from an of_pci_range   @range:the PCI range that describes the resource   @np:device node where the range belongs to   @res:pointer to a valid resource that will be updated to                reflect the values contained in the range.     Returns -EINVAL if the range cannot be converted to resource.     Note that if the range is an IO range, the resource will be converted   using pci_address_to_pio() which can fail if it is called too early or   if the range cannot be matched to any host bridge IO space (our case here).   To guard against that we try to register the IO range first.   If that fails we know that pci_address_to_pio() will do too. ", "int of_range_to_resource(struct device_node *np, int index, struct resource *res)": "of_range_to_resource - Create a resource from a ranges entry   @np:device node where the range belongs to   @index:the 'ranges' index to convert to a resource   @res:pointer to a valid resource that will be updated to                reflect the values contained in the range.     Returns ENOENT if the entry is not found or EINVAL if the range cannot be   converted to resource. ", "of_node_get(dev);*host = NULL;/* Get parent & match bus type ": "of_translate_address(struct device_node  dev,  struct device_node  ( get_parent)(const struct device_node  ),  const __be32  in_addr, const char  rprop,  struct device_node   host){struct device_node  parent = NULL;struct of_bus  bus,  pbus;__be32 addr[OF_MAX_ADDR_CELLS];int na, ns, pna, pns;u64 result = OF_BAD_ADDR;pr_debug(\"   translation for device %pOF   \\n\", dev);  Increase refcount at current level ", "const __be32 *of_translate_dma_region(struct device_node *dev, const __be32 *prop,      phys_addr_t *start, size_t *length)": "of_translate_dma_region - Translate device tree address and size tuple   @dev: device tree node for which to translate   @prop: pointer into array of cells   @start: return value for the start of the DMA range   @length: return value for the length of the DMA range     Returns a pointer to the cell immediately following the translated DMA region. ", "parent = of_get_parent(dev);if (parent == NULL)return NULL;bus = of_match_bus(parent);if (strcmp(bus->name, \"pci\") && (bar_no >= 0)) ": "__of_get_address(struct device_node  dev, int index, int bar_no,       u64  size, unsigned int  flags){const __be32  prop;unsigned int psize;struct device_node  parent;struct of_bus  bus;int onesize, i, na, ns;  Get parent & match bus type ", "int of_property_read_reg(struct device_node *np, int idx, u64 *addr, u64 *size)": "of_property_read_reg - Retrieve the specified \"reg\" entry index without translating   @np: device tree node for which to retrieve \"reg\" from   @idx: \"reg\" entry index to read   @addr: return value for the untranslated address   @size: return value for the entry size     Returns -EINVAL if \"reg\" is not found. Returns 0 on success with addr and   size values filled in. ", "void __iomem *of_iomap(struct device_node *np, int index)": "of_iomap - Maps the memory mapped IO for a given device_node   @np:the device whose io range will be mapped   @index:index of the io range     Returns a pointer to the mapped memory ", "void __iomem *of_io_request_and_map(struct device_node *np, int index,    const char *name)": "of_io_request_and_map - Requests a resource and maps the memory mapped IO     for a given device_node   @device:the device whose io range will be mapped   @index:index of the io range   @name:name \"override\" for the memory region request or NULL     Returns a pointer to the requested and mapped memory or an ERR_PTR() encoded   error code on failure. Usage example:    base = of_io_request_and_map(node, 0, \"foo\");  if (IS_ERR(base))  return PTR_ERR(base); ", "struct device_node *of_node_get(struct device_node *node)": "of_node_get() - Increment refcount of a node   @node:Node to inc refcount, NULL is supported to simplify writing of  callers     Return: The node with refcount incremented. ", "void of_node_put(struct device_node *node)": "of_node_put() - Decrement refcount of a node   @node:Node to dec refcount, NULL is supported to simplify writing of  callers ", "if (nubus_readdir(dir, &ent) == -1)return -1;if (nubus_get_subdir(&ent, dir) == -1)return -1;return 0;}EXPORT_SYMBOL(nubus_get_board_dir": "nubus_get_board_dir(const struct nubus_board  board,struct nubus_dir  dir){struct nubus_dirent ent;dir->ptr = dir->base = board->directory;dir->done = 0;dir->mask = board->lanes;  Now dereference it (the first directory is always the board   directory) ", "ent->base = nd->ptr;/* This moves nd->ptr forward ": "nubus_readdir(dir, &ent) == -1)return -1;if (nubus_get_subdir(&ent, dir) == -1)return -1;return 0;}EXPORT_SYMBOL(nubus_get_board_dir);int nubus_get_subdir(const struct nubus_dirent  ent,     struct nubus_dir  dir){dir->ptr = dir->base = nubus_dirptr(ent);dir->done = 0;dir->mask = ent->mask;return 0;}EXPORT_SYMBOL(nubus_get_subdir);int nubus_readdir(struct nubus_dir  nd, struct nubus_dirent  ent){u32 resid;if (nd->done)return -1;  Do this first, otherwise nubus_rewind & co are off by 4 ", "int pnp_start_dev(struct pnp_dev *dev)": "pnp_start_dev - low-level start of the PnP device   @dev: pointer to the desired device     assumes that resources have already been allocated ", "int pnp_stop_dev(struct pnp_dev *dev)": "pnp_stop_dev - low-level disable of the PnP device   @dev: pointer to the desired device     does not free resources ", "int pnp_activate_dev(struct pnp_dev *dev)": "pnp_activate_dev - activates a PnP device for use   @dev: pointer to the desired device     does not validate or set resources so be careful. ", "int pnp_disable_dev(struct pnp_dev *dev)": "pnp_disable_dev - disables device   @dev: pointer to the desired device     inform the correct pnp protocol so that resources can be used by other devices ", "struct pnp_dev *pnp_request_card_device(struct pnp_card_link *clink,const char *id, struct pnp_dev *from)": "pnp_request_card_device - Searches for a PnP device under the specified card   @clink: pointer to the card link, cannot be NULL   @id: pointer to a PnP ID structure that explains the rules for finding the device   @from: Starting place to search from. If NULL it will start from the beginning. ", "static struct pnp_id *pnp_add_card_id(struct pnp_card *card, char *id)": "pnp_release_card_device(dev);}kfree(clink);return 0;}     pnp_add_card_id - adds an EISA id to the specified card   @id: pointer to a pnp_id structure   @card: pointer to the desired card ", "int pnp_register_card_driver(struct pnp_card_driver *drv)": "pnp_register_card_driver - registers a PnP card driver with the PnP Layer   @drv: pointer to the driver to register ", "void pnp_unregister_card_driver(struct pnp_card_driver *drv)": "pnp_unregister_card_driver - unregisters a PnP card driver from the PnP Layer   @drv: pointer to the driver to unregister ", "int pnp_is_active(struct pnp_dev *dev)": "pnp_is_active - Determines if a device is active based on its current  resources   @dev: pointer to the desired PnP device ", "pnp_for_each_dev(tdev) ": "pnp_get_resource(dev, IORESOURCE_IO, i)); i++) {if (tres != res && tres->flags & IORESOURCE_IO) {tport = &tres->start;tend = &tres->end;if (ranged_conflict(port, end, tport, tend))return 0;}}  check for conflicts with other pnp devices ", "struct list_head *list;list = pnp_global.next;if (from)list = from->global_list.next;while (list != &pnp_global) ": "pnp_find_dev(struct pnp_card  card, unsigned short vendor,     unsigned short function, struct pnp_dev  from){char id[8];char any[8];pnp_convert_id(id, vendor, function);pnp_convert_id(any, ISAPNP_ANY_ID, ISAPNP_ANY_ID);if (card == NULL) {  look for a logical device from all cards ", "static void __init isapnp_parse_irq_resource(struct pnp_dev *dev,     unsigned int option_flags,     int size)": "isapnp_protocol, number, id);if (!dev)return NULL;dev->card = card;dev->capabilities |= PNP_CONFIGURABLE;dev->capabilities |= PNP_READ;dev->capabilities |= PNP_WRITE;dev->capabilities |= PNP_DISABLE;pnp_init_resources(dev);return dev;}     Add IRQ resource to resources list. ", "/* we must set RDP to our value again ": "isapnp_cfg_begin(int csn, int logdev){if (csn < 1 || csn > isapnp_csn_count || logdev > 10)return -EINVAL;mutex_lock(&isapnp_cfg_mutex);isapnp_wait();isapnp_key();isapnp_wake(csn);#if 0  to avoid malfunction when the isapnptools package is used ", "EXPORT_SYMBOL(isapnp_protocol);EXPORT_SYMBOL(isapnp_present);EXPORT_SYMBOL(isapnp_cfg_begin);EXPORT_SYMBOL(isapnp_cfg_end": "isapnp_cfg_end(void){isapnp_wait();mutex_unlock(&isapnp_cfg_mutex);return 0;}     Initialization. ", "static void isapnp_wait(void)": "isapnp_write_byte(unsigned char idx, unsigned char val){write_address(idx);write_data(val);}static void isapnp_write_word(unsigned char idx, unsigned short val){isapnp_write_byte(idx, val >> 8);isapnp_write_byte(idx + 1, val);}static void isapnp_key(void){unsigned char code = 0x6a, msb;int i;mdelay(1);write_address(0x00);write_address(0x00);write_address(code);for (i = 1; i < 32; i++) {msb = ((code & 0x01) ^ ((code & 0x02) >> 1)) << 7;code = (code >> 1) | msb;write_address(code);}}  place all pnp cards in wait-for-key state ", "list_for_each_entry(dev, &pnpbios_protocol.devices, protocol_list) ": "pnpbios_protocol = {.name = \"Plug and Play BIOS\",.get = pnpbios_get_resources,.set = pnpbios_set_resources,.disable = pnpbios_disable_resources,};static int __init insert_device(struct pnp_bios_node  node){struct pnp_dev  dev;char id[8];int error;  check if the device is already added ", "port_bo = t_data->block_offset;}}}EXPORT_SYMBOL(sdw_compute_slave_ports": "sdw_compute_slave_ports(struct sdw_master_runtime  m_rt,     struct sdw_transport_data  t_data){struct sdw_slave_runtime  s_rt = NULL;struct sdw_port_runtime  p_rt;int port_bo, sample_int;unsigned int rate, bps, ch = 0;unsigned int slave_total_ch;struct sdw_bus_params  b_params = &m_rt->bus->params;port_bo = t_data->block_offset;list_for_each_entry(s_rt, &m_rt->slave_rt_list, m_rt_node) {rate = m_rt->stream->params.rate;bps = m_rt->stream->params.bps;sample_int = (m_rt->bus->params.curr_dr_freq  rate);slave_total_ch = 0;list_for_each_entry(p_rt, &s_rt->port_list, port_node) {ch = hweight32(p_rt->ch_mask);sdw_fill_xport_params(&p_rt->transport_params,      p_rt->num, false,      SDW_BLK_GRP_CNT_1,      sample_int, port_bo, port_bo >> 8,      t_data->hstart,      t_data->hstop,      SDW_BLK_PKG_PER_PORT, 0x0);sdw_fill_port_params(&p_rt->port_params,     p_rt->num, bps,     SDW_PORT_FLOW_MODE_ISOCH,     b_params->s_data_mode);port_bo += bps   ch;slave_total_ch += ch;}if (m_rt->direction == SDW_DATA_DIR_TX &&    m_rt->ch_count == slave_total_ch) {    Slave devices were configured to access all channels   of the stream, which indicates that they operate in   'mirror mode'. Make sure we reset the port offset for   the next device in the list ", "int sdw_compute_params(struct sdw_bus *bus)": "sdw_compute_params: Compute bus, transport and port parameters     @bus: SDW Bus instance ", "int sdw_bus_master_add(struct sdw_bus *bus, struct device *parent,       struct fwnode_handle *fwnode)": "sdw_bus_master_add() - add a bus Master instance   @bus: bus instance   @parent: parent device   @fwnode: firmware node handle     Initializes the bus instance, read properties and create child   devices. ", "void sdw_bus_master_delete(struct sdw_bus *bus)": "sdw_bus_master_delete() - delete the bus master instance   @bus: bus to be deleted     Remove the instance, delete the child devices. ", "void sdw_show_ping_status(struct sdw_bus *bus, bool sync_delay)": "sdw_show_ping_status() - Direct report of PING status, to be used by Peripheral drivers   @bus: SDW bus   @sync_delay: Delay before reading status ", "int sdw_nread_no_pm(struct sdw_slave *slave, u32 addr, size_t count, u8 *val)": "sdw_nread_no_pm() - Read \"n\" contiguous SDW Slave registers with no PM   @slave: SDW Slave   @addr: Register address   @count: length   @val: Buffer for values to be read     Note that if the message crosses a page boundary each page will be   transferred under a separate invocation of the msg_lock. ", "int sdw_nwrite_no_pm(struct sdw_slave *slave, u32 addr, size_t count, const u8 *val)": "sdw_nwrite_no_pm() - Write \"n\" contiguous SDW Slave registers with no PM   @slave: SDW Slave   @addr: Register address   @count: length   @val: Buffer for values to be written     Note that if the message crosses a page boundary each page will be   transferred under a separate invocation of the msg_lock. ", "int sdw_write_no_pm(struct sdw_slave *slave, u32 addr, u8 value)": "sdw_write_no_pm() - Write a SDW Slave register with no PM   @slave: SDW Slave   @addr: Register address   @value: Register value ", "int sdw_read_no_pm(struct sdw_slave *slave, u32 addr)": "sdw_read_no_pm() - Read a SDW Slave register with no PM   @slave: SDW Slave   @addr: Register address ", "int sdw_bus_prep_clk_stop(struct sdw_bus *bus)": "sdw_bus_prep_clk_stop: prepare Slave(s) for clock stop     @bus: SDW bus instance     Query Slave for clock stop mode and prepare for that mode. ", "int sdw_bus_clk_stop(struct sdw_bus *bus)": "sdw_bus_clk_stop: stop bus clock     @bus: SDW bus instance     After preparing the Slaves for clock stop, stop the clock by broadcasting   write to SCP_CTRL register. ", "int sdw_bus_exit_clk_stop(struct sdw_bus *bus)": "sdw_bus_exit_clk_stop: Exit clock stop mode     @bus: SDW bus instance     This De-prepares the Slaves by exiting Clock Stop Mode 0. For the Slaves   exiting Clock Stop Mode 1, they will be de-prepared after they enumerate   back. ", "int sdw_handle_slave_status(struct sdw_bus *bus,    enum sdw_slave_status status[])": "sdw_handle_slave_status() - Handle Slave status   @bus: SDW bus instance   @status: Status for all Slave(s) ", "for (i = 1; i <= SDW_MAX_DEVICES; i++) ": "sdw_clear_slave_status(struct sdw_bus  bus, u32 request){struct sdw_slave  slave;int i;  Check all non-zero devices ", "int sdw_master_read_prop(struct sdw_bus *bus)": "sdw_master_read_prop() - Read Master properties   @bus: SDW bus instance ", "int sdw_slave_read_prop(struct sdw_slave *slave)": "sdw_slave_read_prop() - Read Slave properties   @slave: SDW Slave ", "memcpy(&slave->id, id, sizeof(*id));slave->dev.parent = bus->dev;slave->dev.fwnode = fwnode;if (id->unique_id == SDW_IGNORED_UNIQUE_ID) ": "sdw_slave_add(struct sdw_bus  bus,  struct sdw_slave_id  id, struct fwnode_handle  fwnode){struct sdw_slave  slave;int ret;int i;slave = kzalloc(sizeof( slave), GFP_KERNEL);if (!slave)return -ENOMEM;  Initialize data structure ", "list_for_each_entry(m_rt, &stream->master_list, stream_node) ": "sdw_enable_stream(struct sdw_stream_runtime  stream){struct sdw_master_runtime  m_rt;struct sdw_bus  bus;int ret;  Enable Master(s) and Slave(s) port(s) associated with stream ", "ret = sdw_enable_disable_ports(m_rt, false);if (ret < 0) ": "sdw_disable_stream(struct sdw_stream_runtime  stream){struct sdw_master_runtime  m_rt;int ret;list_for_each_entry(m_rt, &stream->master_list, stream_node) {struct sdw_bus  bus = m_rt->bus;  Disable port(s) ", "ret = sdw_prep_deprep_ports(m_rt, false);if (ret < 0) ": "sdw_deprepare_stream(struct sdw_stream_runtime  stream){struct sdw_master_runtime  m_rt;struct sdw_bus  bus;int ret = 0;list_for_each_entry(m_rt, &stream->master_list, stream_node) {bus = m_rt->bus;  De-prepare port(s) ", "struct sdw_stream_runtime *sdw_alloc_stream(const char *stream_name)": "sdw_alloc_stream() - Allocate and return stream runtime     @stream_name: SoundWire stream name     Allocates a SoundWire stream runtime instance.   sdw_alloc_stream should be called only once per stream. Typically   invoked from ALSAASoC machineplatform driver. ", "int sdw_startup_stream(void *sdw_substream)": "sdw_startup_stream() - Startup SoundWire stream     @sdw_substream: Soundwire stream     Documentationdriver-apisoundwirestream.rst explains this API in detail ", "void sdw_shutdown_stream(void *sdw_substream)": "sdw_release_stream(sdw_stream);set_stream(substream, NULL);error:kfree(name);return ret;}EXPORT_SYMBOL(sdw_startup_stream);     sdw_shutdown_stream() - Shutdown SoundWire stream     @sdw_substream: Soundwire stream     Documentationdriver-apisoundwirestream.rst explains this API in detail ", "int sdw_stream_add_master(struct sdw_bus *bus,  struct sdw_stream_config *stream_config,  struct sdw_port_config *port_config,  unsigned int num_ports,  struct sdw_stream_runtime *stream)": "sdw_stream_add_master() - Allocate and add master runtime to a stream     @bus: SDW Bus instance   @stream_config: Stream configuration for audio stream   @port_config: Port configuration for audio stream   @num_ports: Number of ports   @stream: SoundWire stream ", "int sdw_stream_remove_master(struct sdw_bus *bus,     struct sdw_stream_runtime *stream)": "sdw_stream_remove_master() - Remove master from sdw_stream     @bus: SDW Bus instance   @stream: SoundWire stream     This removes and frees port_rt and master_rt from a stream ", "int sdw_stream_add_slave(struct sdw_slave *slave, struct sdw_stream_config *stream_config, struct sdw_port_config *port_config, unsigned int num_ports, struct sdw_stream_runtime *stream)": "sdw_stream_add_slave() - Allocate and add masterslave runtime to a stream     @slave: SDW Slave instance   @stream_config: Stream configuration for audio stream   @stream: SoundWire stream   @port_config: Port configuration for audio stream   @num_ports: Number of ports     It is expected that Slave is added before adding Master   to the Stream.   ", "int sdw_stream_remove_slave(struct sdw_slave *slave,    struct sdw_stream_runtime *stream)": "sdw_stream_remove_slave() - Remove slave from sdw_stream     @slave: SDW Slave instance   @stream: SoundWire stream     This removes and frees port_rt and slave_rt from a stream ", "void sdw_cdns_config_update(struct sdw_cdns *cdns)": "sdw_cdns_is_clock_stop(cdns)) {dev_err(cdns->dev, \"Cannot program MCP_CONFIG_UPDATE in ClockStopMode\\n\");return -EINVAL;}ret = cdns_clear_bit(cdns, CDNS_MCP_CONFIG_UPDATE,     CDNS_MCP_CONFIG_UPDATE_BIT);if (ret < 0)dev_err(cdns->dev, \"Config update timedout\\n\");return ret;}     sdw_cdns_config_update() - Update configurations   @cdns: Cadence instance ", "int sdw_cdns_config_update_set_wait(struct sdw_cdns *cdns)": "sdw_cdns_config_update_set_wait() - wait until configuration update bit is self-cleared   @cdns: Cadence instance ", "if (cdns->msg_count != count) ": "cdns_xfer_msg(struct sdw_cdns  cdns, struct sdw_msg  msg, int cmd,       int offset, int count, bool defer){unsigned long time;u32 base, i, data;u16 addr;  Program the watermark level for RX FIFO ", "if (msg->len > 1)return -ENOTSUPP;ret = cdns_prep_msg(cdns, msg, &cmd);if (ret)return SDW_CMD_FAIL_OTHER;return _cdns_xfer_msg(cdns, msg, cmd, 0, msg->len, true);}EXPORT_SYMBOL(cdns_xfer_msg_defer": "cdns_xfer_msg_defer(struct sdw_bus  bus){struct sdw_cdns  cdns = bus_to_cdns(bus);struct sdw_defer  defer = &bus->defer_msg;struct sdw_msg  msg = defer->msg;int cmd = 0, ret;  for defer only 1 message is supported ", "irqreturn_t sdw_cdns_irq(int irq, void *dev_id)": "sdw_cdns_irq() - Cadence interrupt handler   @irq: irq number   @dev_id: irq context ", "if (ip_mcp_control & CDNS_IP_MCP_CONTROL_SW_RST)dev_err(cdns->dev, \"%s failed: IP_MCP_CONTROL_SW_RST is not cleared\\n\", string);mcp_control = cdns_readl(cdns, CDNS_MCP_CONTROL);/* the following bits should be cleared immediately ": "sdw_cdns_check_self_clearing_bits(struct sdw_cdns  cdns, const char  string,       bool initial_delay, int reset_iterations){u32 ip_mcp_control;u32 mcp_control;u32 mcp_config_update;int i;if (initial_delay)usleep_range(1000, 1500);ip_mcp_control = cdns_ip_readl(cdns, CDNS_IP_MCP_CONTROL);  the following bits should be cleared immediately ", "ret = pm_runtime_resume_and_get(bus->dev);if (ret < 0 && ret != -EACCES) ": "sdw_cdns_exit_reset(cdns);dev_dbg(cdns->dev, \"link hw_reset done: %d\\n\", ret);return ret;}DEFINE_DEBUGFS_ATTRIBUTE(cdns_hw_reset_fops, NULL, cdns_hw_reset, \"%llu\\n\");static int cdns_parity_error_injection(void  data, u64 value){struct sdw_cdns  cdns = data;struct sdw_bus  bus;int ret;if (value != 1)return -EINVAL;bus = &cdns->bus;    Resume Master device. If this results in a bus reset, the   Slave devices will re-attach and be re-enumerated. ", "int sdw_cdns_enable_interrupt(struct sdw_cdns *cdns, bool state)": "sdw_cdns_enable_interrupt() - Enable SDW interrupts   @cdns: Cadence instance   @state: True if we are trying to enable interrupt. ", "int sdw_cdns_pdi_init(struct sdw_cdns *cdns,      struct sdw_cdns_stream_config config)": "sdw_cdns_pdi_init() - PDI initialization routine     @cdns: Cadence instance   @config: Stream configurations ", "int sdw_cdns_init(struct sdw_cdns *cdns)": "sdw_cdns_init() - Cadence initialization   @cdns: Cadence instance ", "if (params->next_bank)mcp_clkctrl_off = CDNS_MCP_CLK_CTRL1;elsemcp_clkctrl_off = CDNS_MCP_CLK_CTRL0;cdns_updatel(cdns, mcp_clkctrl_off, CDNS_MCP_CLK_MCLKD_MASK, divider);return 0;}EXPORT_SYMBOL(cdns_bus_conf": "cdns_bus_conf(struct sdw_bus  bus, struct sdw_bus_params  params){struct sdw_master_prop  prop = &bus->prop;struct sdw_cdns  cdns = bus_to_cdns(bus);int mcp_clkctrl_off;int divider;if (!params->curr_dr_freq) {dev_err(cdns->dev, \"NULL curr_dr_freq\\n\");return -EINVAL;}divider= prop->mclk_freq   SDW_DOUBLE_RATE_FACTOR params->curr_dr_freq;divider--;   divider is 1(N+1) ", "int sdw_cdns_clock_stop(struct sdw_cdns *cdns, bool block_wake)": "sdw_cdns_clock_stop: Cadence clock stop configuration routine     @cdns: Cadence instance   @block_wake: prevent wakes if required by the platform ", "int sdw_cdns_clock_restart(struct sdw_cdns *cdns, bool bus_reset)": "sdw_cdns_clock_restart: Cadence PM clock restart configuration routine     @cdns: Cadence instance   @bus_reset: context may be lost while in low power modes and the bus   may require a Severe Reset and re-enumeration after a wake. ", "int sdw_cdns_probe(struct sdw_cdns *cdns)": "sdw_cdns_probe() - Cadence probe routine   @cdns: Cadence instance ", "if (dai_runtime) ": "cdns_set_sdw_stream(struct snd_soc_dai  dai,void  stream, int direction){struct sdw_cdns  cdns = snd_soc_dai_get_drvdata(dai);struct sdw_cdns_dai_runtime  dai_runtime;dai_runtime = cdns->dai_runtime_array[dai->id];if (stream) {  first paranoia check ", "void sdw_cdns_config_stream(struct sdw_cdns *cdns,    u32 ch, u32 dir, struct sdw_cdns_pdi *pdi)": "sdw_cdns_config_stream: Configure a stream     @cdns: Cadence instance   @ch: Channel count   @dir: Data direction   @pdi: PDI to be used ", "struct sdw_cdns_pdi *sdw_cdns_alloc_pdi(struct sdw_cdns *cdns,struct sdw_cdns_streams *stream,u32 ch, u32 dir, int dai_id)": "sdw_cdns_alloc_pdi() - Allocate a PDI     @cdns: Cadence instance   @stream: Stream to be allocated   @ch: Channel count   @dir: Data direction   @dai_id: DAI id ", "void ishtp_send_suspend(struct ishtp_device *dev)": "ishtp_send_suspend() - Send suspend message to FW   @dev: ISHTP device instance     Send suspend message to FW. This is useful for system freeze (non S3) case ", "elsedev_err(dev->devc, \"unknown fixed client msg [%02X]\\n\",msg_hdr->cmd);}}/** * fix_cl_hdr() - Initialize fixed client header * @hdr: message header * @length: length of message * @cl_addr: Client address * * Initialize message header for fixed client ": "ishtp_send_resume(dev);  if FW request arrived here, the system is not suspended ", "int ishtp_cl_io_rb_recycle(struct ishtp_cl_rb *rb)": "ishtp_cl_io_rb_recycle() - Recycle IO request blocks   @rb: IO request block     Re-append rb to its client's free list and send flow control if needed     Return: 0 on success else -EFAULT ", "bool ishtp_cl_tx_empty(struct ishtp_cl *cl)": "ishtp_cl_tx_empty() -test whether client device tx buffer is empty   @cl: Pointer to client device instance     Look client device tx buffer list, and check whether this list is empty     Return: true if client tx buffer list is empty else false ", "struct ishtp_cl_rb *ishtp_cl_rx_get_rb(struct ishtp_cl *cl)": "ishtp_cl_rx_get_rb() -Get a rb from client device rx buffer list   @cl: Pointer to client device instance     Check client device in-processing buffer list and get a rb from it.     Return: rb pointer if buffer list isn't empty else NULL ", "void ishtp_device_init(struct ishtp_device *dev)": "ishtp_device_init() - ishtp device init   @dev: ISHTP device instance     After ISHTP device is alloacted, this function is used to initialize   each field which includes spin lock, work struct and lists ", "int ishtp_start(struct ishtp_device *dev)": "ishtp_start() - Start ISH processing   @dev: ISHTP device instance     Start ISHTP processing by sending query subscriber message     Return: 0 on success else -ENODEV ", "int ishtp_cl_flush_queues(struct ishtp_cl *cl)": "ishtp_cl_flush_queues() - Flush all queues for a client   @cl: ishtp client instance     Used to remove all queues for a client. This is called when a client device   needs reset due to error, S3 resume or during module removal     Return: 0 on success else -EINVAL if device is NULL ", "struct ishtp_cl *ishtp_cl_allocate(struct ishtp_cl_device *cl_device)": "ishtp_cl_allocate() - allocates client structure and sets it up.   @cl_device: ishtp client device     Allocate memory for new client device and call to initialize each field.     Return: The allocated client instance or NULL on failure ", "voidishtp_cl_free(struct ishtp_cl *cl)": "ishtp_cl_free() - Frees a client device   @cl: client device instance     Frees a client device ", "int ishtp_cl_link(struct ishtp_cl *cl)": "ishtp_cl_link() - Reserve a host id and link the client instance   @cl: client device instance     This allocates a single bit in the hostmap. This function will make sure   that not many client sessions are opened at the same time. Once allocated   the client device instance is added to the ishtp device in the current   client list     Return: 0 or error code on failure ", "void ishtp_cl_unlink(struct ishtp_cl *cl)": "ishtp_cl_unlink() - remove fw_cl from the client device list   @cl: client device instance     Remove a previously linked device to a ishtp device ", "int ishtp_cl_disconnect(struct ishtp_cl *cl)": "ishtp_cl_disconnect() - Send disconnect request to firmware   @cl: client device instance     Send a disconnect request for a client to firmware.     Return: 0 if successful disconnect response from the firmware or error   code on failure ", "int ishtp_cl_connect(struct ishtp_cl *cl)": "ishtp_cl_connect() - Send connect request to firmware   @cl: client device instance     Send a connect request for a client to firmware. If successful it will   RX and TX ring buffers     Return: 0 if successful connect response from the firmware and able   to bind and allocate ring buffers or error code on failure ", "int ishtp_cl_send(struct ishtp_cl *cl, uint8_t *buf, size_t length)": "ishtp_cl_send() - Send a message to firmware   @cl: client device instance   @buf: message buffer   @length: length of message     If the client is correct state to send message, this function gets a buffer   from tx ring buffers, copy the message data and call to send the message   using ishtp_cl_send_msg()     Return: 0 if successful or error code on failure ", "void ishtp_recv(struct ishtp_device *dev)": "ishtp_device_ready;     ishtp_recv() - process ishtp message   @dev: ishtp device     If a message with valid header and size is received, then   this function calls appropriate handler. The host or firmware   address is zero, then they are host bus management message,   otherwise they are message fo clients. ", "int ishtp_fw_cl_by_uuid(struct ishtp_device *dev, const guid_t *uuid)": "ishtp_fw_cl_by_uuid() - locate index of fw client   @dev: ishtp device   @uuid: uuid of the client to search     Search firmware client using UUID.     Return: fw client index or -ENOENT if not found ", "struct ishtp_fw_client *ishtp_fw_cl_get_client(struct ishtp_device *dev,       const guid_t *uuid)": "ishtp_fw_cl_get_client() - return client information to client   @dev: the ishtp device structure   @uuid: uuid of the client to search     Search firmware client using UUID and reture related client information.     Return: pointer of client information on success, NULL on failure. ", "int ishtp_get_fw_client_id(struct ishtp_fw_client *fw_client)": "ishtp_get_fw_client_id() - Get fw client id   @fw_client:firmware client used to fetch the ID     This interface is used to reset HW get FW client id.     Return: firmware client id. ", "int ishtp_cl_driver_register(struct ishtp_cl_driver *driver,     struct module *owner)": "ishtp_cl_driver_register() - Client driver register   @driver:the client driver instance   @owner:Owner of this driver module     Once a client driver is probed, it created a client   instance and registers with the bus.     Return: Return value of driver_register or -ENODEV if not ready ", "void ishtp_cl_driver_unregister(struct ishtp_cl_driver *driver)": "ishtp_cl_driver_unregister() - Client driver unregister   @driver:the client driver instance     Unregister client during device removal process. ", "int ishtp_register_event_cb(struct ishtp_cl_device *device,void (*event_cb)(struct ishtp_cl_device *))": "ishtp_register_event_cb() - Register callback   @device:client device instance   @event_cb:Event processor for an client     Register a callback for events, called from client driver     Return: Return 0 or -EALREADY if already registered ", "void ishtp_get_device(struct ishtp_cl_device *cl_device)": "ishtp_get_device() - update usage count for the device   @cl_device:client device instance     Increment the usage count. The device can't be deleted ", "void ishtp_put_device(struct ishtp_cl_device *cl_device)": "ishtp_put_device() - decrement usage count for the device   @cl_device:client device instance     Decrement the usage count. The device can be deleted is count = 0 ", "void ishtp_set_drvdata(struct ishtp_cl_device *cl_device, void *data)": "ishtp_set_drvdata() - set client driver data   @cl_device:client device instance   @data:driver data need to be set     Set client driver data to cl_device->driver_data. ", "void *ishtp_get_drvdata(struct ishtp_cl_device *cl_device)": "ishtp_get_drvdata() - get client driver data   @cl_device:client device instance     Get client driver data from cl_device->driver_data.     Return: pointer of driver data ", "struct ishtp_cl_device *ishtp_dev_to_cl_device(struct device *device)": "ishtp_dev_to_cl_device() - get ishtp_cl_device instance from device instance   @device: device instance     Get ish_cl_device instance which embeds device instance in it.     Return: pointer to ishtp_cl_device instance ", "void ishtp_bus_remove_all_clients(struct ishtp_device *ishtp_dev,  bool warm_reset)": "ishtp_bus_remove_all_clients() - Remove all clients   @ishtp_dev:ishtp device   @warm_reset:Reset due to FW reset dure to errors or S3 suspend     This is part of resetremove flow. This function the main processing   only targets error processing, if the FW has forced reset or   error to remove connected clients. When warm reset the client devices are   not removed. ", "void ishtp_reset_handler(struct ishtp_device *dev)": "ishtp_reset_handler() - IPC reset handler   @dev:ishtp device     ISHTP Handler for IPC_RESET notification ", "void ishtp_reset_compl_handler(struct ishtp_device *dev)": "ishtp_reset_compl_handler() - Reset completion handler   @dev:ishtp device     ISHTP handler for IPC_RESET sequence completion to start   host message bus start protocol sequence. ", "struct device *ishtp_get_pci_device(struct ishtp_cl_device *device)": "ishtp_get_pci_device() - Return PCI device dev pointer   This interface is used to return PCI device pointer   from ishtp_cl_device instance.   @device: ISH-TP client device instance     Return: device  . ", "ishtp_print_log ishtp_trace_callback(struct ishtp_cl_device *cl_device)": "ishtp_trace_callback() - Return trace callback   @cl_device: ISH-TP client device instance     This interface is used to return trace callback function pointer.     Return:  ishtp_print_log() ", "int ish_hw_reset(struct ishtp_device *dev)": "ish_hw_reset() - Call HW reset IPC callback   @dev:ISHTP device instance     This interface is used to reset HW in case of error.     Return: value from IPC hw_reset callback ", "cgc.cmd[4] = 1 << 4;/* media event ": "cdrom_get_media_event(struct cdrom_device_info  cdi,  struct media_event_desc  med){struct packet_command cgc;unsigned char buffer[8];struct event_header  eh = (struct event_header  )buffer;init_cdrom_command(&cgc, buffer, sizeof(buffer), CGC_DATA_READ);cgc.cmd[0] = GPCMD_GET_EVENT_STATUS_NOTIFICATION;cgc.cmd[1] = 1;  IMMED ", "cgc.quiet = 1;cgc.timeout = 3000*HZ;cdi->ops->generic_packet(cdi, &cgc);cdi->media_written = 0;}/* badly broken, I know. Is due for a fixup anytime. ": "cdrom_open_write(struct cdrom_device_info  cdi){int mrw, mrw_write, ram_write;int ret = 1;mrw = 0;if (!cdrom_is_mrw(cdi, &mrw_write))mrw = 1;if (CDROM_CAN(CDC_MO_DRIVE))ram_write = 1;else(void) cdrom_is_random_writable(cdi, &ram_write);if (mrw)cdi->mask &= ~CDC_MRW;elsecdi->mask |= CDC_MRW;if (mrw_write)cdi->mask &= ~CDC_MRW_W;elsecdi->mask |= CDC_MRW_W;if (ram_write)cdi->mask &= ~CDC_RAM;elsecdi->mask |= CDC_RAM;if (CDROM_CAN(CDC_MRW_W))ret = cdrom_mrw_open_write(cdi);else if (CDROM_CAN(CDC_DVD_RAM))ret = cdrom_dvdram_open_write(cdi); else if (CDROM_CAN(CDC_RAM) &&  !CDROM_CAN(CDC_CD_R|CDC_CD_RW|CDC_DVD|CDC_DVD_R|CDC_MRW|CDC_MO_DRIVE)) ret = cdrom_ram_open_write(cdi);else if (CDROM_CAN(CDC_MO_DRIVE))ret = mo_open_write(cdi);else if (!cdrom_is_dvd_rw(cdi))ret = 0;return ret;}static void cdrom_dvd_rw_close_write(struct cdrom_device_info  cdi){struct packet_command cgc;if (cdi->mmc3_profile != 0x1a) {cd_dbg(CD_CLOSE, \"%s: No DVD+RW\\n\", cdi->name);return;}if (!cdi->media_written) {cd_dbg(CD_CLOSE, \"%s: DVD+RW media clean\\n\", cdi->name);return;}pr_info(\"%s: dirty DVD+RW media, \\\"finalizing\\\"\\n\", cdi->name);init_cdrom_command(&cgc, NULL, 0, CGC_DATA_NONE);cgc.cmd[0] = GPCMD_FLUSH_CACHE;cgc.timeout = 30 HZ;cdi->ops->generic_packet(cdi, &cgc);init_cdrom_command(&cgc, NULL, 0, CGC_DATA_NONE);cgc.cmd[0] = GPCMD_CLOSE_TRACK;cgc.timeout = 3000 HZ;cgc.quiet = 1;cdi->ops->generic_packet(cdi, &cgc);init_cdrom_command(&cgc, NULL, 0, CGC_DATA_NONE);cgc.cmd[0] = GPCMD_CLOSE_TRACK;cgc.cmd[2] = 2;   Close session ", "cdi->capacity = 0; info = kmalloc(sizeof(*info), GFP_KERNEL);if (!info)return -ENOMEM;if (cdrom_read_mech_status(cdi, info) == 0)nslots = info->hdr.nslots;kfree(info);return nslots;}EXPORT_SYMBOL(cdrom_number_of_slots": "cdrom_number_of_slots(struct cdrom_device_info  cdi) {int nslots = 1;struct cdrom_changer_info  info;cd_dbg(CD_CHANGER, \"entering cdrom_number_of_slots()\\n\");  cdrom_read_mech_status requires a valid value for capacity: ", "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt#define REVISION \"Revision: 3.20\"#define VERSION \"Id: cdrom.c 3.20 2003/12/17\"/* I use an error-log mask to give fine grain control over the type of   messages dumped to the system logs.  The available masks include: ": "cdrom_get_last_written and cdrom_get_next_block as ioctls  and exported functions.  -- Erik Andersen <andersen@xmission.com> modified all SCMD_ commands  to now read GPCMD_ for the new generic packet interface. All low level  drivers are updated as well.  -- Various other cleanups.  3.04 Sep 12, 1999 - Jens Axboe <axboe@image.dk>  -- Fixed a couple of possible memory leaks (if an operation failed and  we didn't free the buffer before returning the error).  -- Integrated Uniform CD Changer handling from Richard Sharman  <rsharman@pobox.com>.  -- Defined CD_DVD and CD_CHANGER log levels.  -- Fixed the CDROMREADxxx ioctls.  -- CDROMPLAYTRKIND uses the GPCMD_PLAY_AUDIO_MSF command - too few  drives supported it. We lose the index part, however.  -- Small modifications to accommodate opens of devhdc1, required  for ide-cd to handle multisession discs.  -- Export cdrom_mode_sense and cdrom_mode_select.  -- init_cdrom_command() for setting up a cgc command.    3.05 Oct 24, 1999 - Jens Axboe <axboe@image.dk>  -- Changed the interface for CDROM_SEND_PACKET. Before it was virtually  impossible to send the drive data in a sensible way.  -- Lowered stack usage in mmc_ioctl(), dvd_read_disckey(), and  dvd_read_manufact.  -- Added setup of write mode for packet writing.  -- Fixed CDDA ripping with cdda2wav - accept much larger requests of  number of frames and split the reads in blocks of 8.  3.06 Dec 13, 1999 - Jens Axboe <axboe@image.dk>  -- Added support for changing the region of DVD drives.  -- Added sense data to generic command.  3.07 Feb 2, 2000 - Jens Axboe <axboe@suse.de>  -- Do same \"read header length\" trick in cdrom_get_disc_info() as  we do in cdrom_get_track_info() -- some drive don't obey specs and  fail if they can't supply the full Mt Fuji size table.  -- Deleted stuff related to setting up write modes. It has a different  home now.  -- Clear header length in mode_select unconditionally.  -- Removed the register_disk() that was added, not needed here.  3.08 May 1, 2000 - Jens Axboe <axboe@suse.de>  -- Fix direction flag in setup_send_key and setup_report_key. This  gave some SCSI adapters problems.  -- Always return -EROFS for write opens  -- Convert to module_initmodule_exit style init and remove some  of the #ifdef MODULE stuff  -- Fix several dvd errors - DVD_LU_SEND_ASF should pass agid,  DVD_HOST_SEND_RPC_STATE did not set buffer size in cdb, and  dvd_do_auth passed uninitialized data to drive because init_cdrom_command  did not clear a 0 sized buffer.    3.09 May 12, 2000 - Jens Axboe <axboe@suse.de>  -- Fix Video-CD on SCSI drives that don't support READ_CD command. In  that case switch block size and issue plain READ_10 again, then switch  back.  3.10 Jun 10, 2000 - Jens Axboe <axboe@suse.de>  -- Fix volume control on CD's - old SCSI-II drives now use their own  code, as doing MODE6 stuff in here is really not my intention.  -- Use READ_DISC_INFO for more reliable end-of-disc.  3.11 Jun 12, 2000 - Jens Axboe <axboe@suse.de>  -- Fix bug in getting rpc phase 2 region info.  -- Reinstate \"correct\" CDROMPLAYTRKIND   3.12 Oct 18, 2000 - Jens Axboe <axboe@suse.de>  -- Use quiet bit on packet commands not known to work   3.20 Dec 17, 2003 - Jens Axboe <axboe@suse.de>  -- Various fixes and lots of cleanups not listed :-)  -- Locking fixes  -- Mt Rainier support  -- DVD-RAM write open fixes  Nov 5 2001, Aug 8 2002. Modified by Andy Polyakov  <appro@fy.chalmers.se> to support MMC-3 compliant DVD+RW units.  Modified by Nigel Kukard <nkukard@lbsd.net> - support DVD+RW  2.4.x patch by Andy Polyakov <appro@fy.chalmers.se>-------------------------------------------------------------------------", "if (!CDROM_CAN(CDC_SELECT_DISC) || arg == CDSL_CURRENT)return media_changed(cdi, 1);if (arg >= cdi->capacity)return -EINVAL;/* Prevent arg from speculatively bypassing the length check ": "cdrom_ioctl_multisession(struct cdrom_device_info  cdi,void __user  argp){struct cdrom_multisession info;int ret;cd_dbg(CD_DO_IOCTL, \"entering CDROMMULTISESSION\\n\");if (copy_from_user(&info, argp, sizeof(info)))return -EFAULT;ret = cdrom_multisession(cdi, &info);if (ret)return ret;if (copy_to_user(argp, &info, sizeof(info)))return -EFAULT;cd_dbg(CD_DO_IOCTL, \"CDROMMULTISESSION successful\\n\");return ret;}static int cdrom_ioctl_eject(struct cdrom_device_info  cdi){cd_dbg(CD_DO_IOCTL, \"entering CDROMEJECT\\n\");if (!CDROM_CAN(CDC_OPEN_TRAY))return -ENOSYS;if (cdi->use_count != 1 || cdi->keeplocked)return -EBUSY;if (CDROM_CAN(CDC_LOCK)) {int ret = cdi->ops->lock_door(cdi, 0);if (ret)return ret;}return cdi->ops->tray_move(cdi, 1);}static int cdrom_ioctl_closetray(struct cdrom_device_info  cdi){cd_dbg(CD_DO_IOCTL, \"entering CDROMCLOSETRAY\\n\");if (!CDROM_CAN(CDC_CLOSE_TRAY))return -ENOSYS;return cdi->ops->tray_move(cdi, 0);}static int cdrom_ioctl_eject_sw(struct cdrom_device_info  cdi,unsigned long arg){cd_dbg(CD_DO_IOCTL, \"entering CDROMEJECT_SW\\n\");if (!CDROM_CAN(CDC_OPEN_TRAY))return -ENOSYS;if (cdi->keeplocked)return -EBUSY;cdi->options &= ~(CDO_AUTO_CLOSE | CDO_AUTO_EJECT);if (arg)cdi->options |= CDO_AUTO_CLOSE | CDO_AUTO_EJECT;return 0;}static int cdrom_ioctl_media_changed(struct cdrom_device_info  cdi,unsigned long arg){struct cdrom_changer_info  info;int ret;cd_dbg(CD_DO_IOCTL, \"entering CDROM_MEDIA_CHANGED\\n\");if (!CDROM_CAN(CDC_MEDIA_CHANGED))return -ENOSYS;  cannot select disc or select current disc ", "inst.op = CMDQ_CODE_EOC;inst.value = CMDQ_EOC_IRQ_EN;err = cmdq_pkt_append_command(pkt, inst);if (err < 0)return err;/* JUMP to end ": "cmdq_pkt_finalize(struct cmdq_pkt  pkt){struct cmdq_instruction inst = { {0} };int err;  insert EOC and generate IRQ for each command iteration ", "mbox_client_txdone(client->chan, 0);return 0;}EXPORT_SYMBOL(cmdq_pkt_flush_async": "cmdq_pkt_flush_async(struct cmdq_pkt  pkt){int err;struct cmdq_client  client = (struct cmdq_client  )pkt->cl;err = mbox_send_message(client->chan, pkt);if (err < 0)return err;  We can send next packet immediately, so just call txdone. ", "qman_cgrs_and(&rr, (struct qman_cgrs *)&mcr->querycongestion.state,      &p->cgrs[0]);/* check previous snapshot for delta, enter/exit congestion ": "qman_p_irqsource_add(p, QM_PIRQ_CSCI);return;}  mask out the ones I'm not interested in ", "static noinline void clear_vdqcr(struct qman_portal *p, struct qman_fq *fq)": "qman_p_irqsource_remove(p, QM_PIRQ_CSCI);queue_work_on(smp_processor_id(), qm_portal_wq,      &p->congestion_work);}if (is & QM_PIRQ_EQRI) {qm_eqcr_cce_update(&p->p);qm_eqcr_set_ithresh(&p->p, 0);wake_up(&affine_queue);}if (is & QM_PIRQ_MRI) {qman_p_irqsource_remove(p, QM_PIRQ_MRI);queue_work_on(smp_processor_id(), qm_portal_wq,      &p->mr_work);}return is;}    remove some slowish-path stuff from the \"fast path\" and make sure it isn't   inlined. ", "if (fqid == 0 || fqid >= num_fqids) ": "qman_create_fq(u32 fqid, u32 flags, struct qman_fq  fq){if (flags & QMAN_FQ_FLAG_DYNAMIC_FQID) {int ret = qman_alloc_fqid(&fqid);if (ret)return ret;}fq->fqid = fqid;fq->flags = flags;fq->state = qman_fq_state_oos;fq->cgr_groupid = 0;  A context_b of 0 is allegedly special, so don't use that fqid ", "switch (fq->state) ": "qman_destroy_fq(struct qman_fq  fq){    We don't need to lock the FQ as it is a pre-condition that the FQ be   quiesced. Instead, run some checks. ", "if (be16_to_cpu(opts->we_mask) & QM_INITFQ_WE_TDTHRESH)return -EINVAL;}/* Issue an INITFQ_[PARKED|SCHED] management command ": "qman_release_fqid(fq->fqid);DPAA_ASSERT(fq_table[fq->idx]);fq_table[fq->idx] = NULL;return;default:break;}DPAA_ASSERT(NULL == \"qman_free_fq() on unquiesced FQ!\");}EXPORT_SYMBOL(qman_destroy_fq);u32 qman_fq_fqid(struct qman_fq  fq){return fq->fqid;}EXPORT_SYMBOL(qman_fq_fqid);int qman_init_fq(struct qman_fq  fq, u32 flags, struct qm_mcc_initfq  opts){union qm_mc_command  mcc;union qm_mc_result  mcr;struct qman_portal  p;u8 res, myverb;int ret = 0;myverb = (flags & QMAN_INITFQ_FLAG_SCHED)? QM_MCC_VERB_INITFQ_SCHED : QM_MCC_VERB_INITFQ_PARKED;if (fq->state != qman_fq_state_oos &&    fq->state != qman_fq_state_parked)return -EINVAL;#ifdef CONFIG_FSL_DPAA_CHECKINGif (fq_isset(fq, QMAN_FQ_FLAG_NO_MODIFY))return -EINVAL;#endifif (opts && (be16_to_cpu(opts->we_mask) & QM_INITFQ_WE_OAC)) {  And can't be set at the same time as TDTHRESH ", "p = get_affine_portal();if (fq_isset(fq, QMAN_FQ_STATE_CHANGING) ||    fq->state != qman_fq_state_parked) ": "qman_schedule_fq(struct qman_fq  fq){union qm_mc_command  mcc;union qm_mc_result  mcr;struct qman_portal  p;int ret = 0;if (fq->state != qman_fq_state_parked)return -EINVAL;#ifdef CONFIG_FSL_DPAA_CHECKINGif (fq_isset(fq, QMAN_FQ_FLAG_NO_MODIFY))return -EINVAL;#endif  Issue a ALTERFQ_SCHED management command ", "static inline unsigned int __poll_portal_fast(struct qman_portal *p,unsigned int poll_limit, bool sched_napi)": "qman_volatile_dequeue() has set the   vdqcr_owned field (which it does before setting VDQCR), and   qman_volatile_dequeue() blocks interrupts and preemption while this is   done so that we can't interfere.    (ii) the NE flag is only cleared after qman_retire_fq() has set it, and as   with (i) that API prevents us from interfering until it's safe.     The good thing is that qman_volatile_dequeue() and qman_retire_fq() run far   less frequently (ie. per-FQ) than __poll_portal_fast() does, so the nett   advantage comes from this function not having to \"lock\" anything at all.     Note also that the callbacks are invoked at points which are safe against the   above potential conflicts, but that this function itself is not re-entrant   (this is because the function tracks one end of each FIFO in the portal and   we do  not  want to lock that). So the consequence is that it is safe for   user callbacks to call into any QMan API. ", "eq = qm_eqcr_start_stash(&p->p);} else ": "qman_enqueue(struct qman_fq  fq, const struct qm_fd  fd){struct qman_portal  p;struct qm_eqcr_entry  eq;unsigned long irqflags;u8 avail;p = get_affine_portal();local_irq_save(irqflags);if (p->use_eqcr_ci_stashing) {    The stashing case is easy, only update if we need to in   order to try and liberate ring entries. ", "if (cgr->cgrid >= CGR_NUM)return -EINVAL;preempt_disable();p = get_affine_portal();qman_cgr_cpus[cgr->cgrid] = smp_processor_id();preempt_enable();cgr->chan = p->config->channel;spin_lock(&p->cgr_lock);if (opts) ": "qman_create_cgr(struct qman_cgr  cgr, u32 flags,    struct qm_mcc_initcgr  opts){struct qm_mcr_querycgr cgr_state;int ret;struct qman_portal  p;    We have to check that the provided CGRID is within the limits of the   data-structures, for obvious reasons. However we'll let hw take   care of determining whether it's within the limits of what exists on   the SoC. ", "list_for_each_entry(i, &p->cgr_cbs, node)if (i->cgrid == cgr->cgrid && i->cb)goto release_lock;ret = qman_query_cgr(cgr, &cgr_state);if (ret)  ": "qman_delete_cgr(struct qman_cgr  cgr){unsigned long irqflags;struct qm_mcr_querycgr cgr_state;struct qm_mcc_initcgr local_opts;int ret = 0;struct qman_cgr  i;struct qman_portal  p = qman_cgr_get_affine_portal(cgr);if (!p)return -EINVAL;memset(&local_opts, 0, sizeof(struct qm_mcc_initcgr));spin_lock_irqsave(&p->cgr_lock, irqflags);list_del(&cgr->node);    If there are no other CGR objects for this CGRID in the list,   update CSCN_TARG accordingly ", "int i = num - 1;DPAA_ASSERT(num > 0 && num <= 8);do ": "bman_release(struct bman_pool  pool, const struct bm_buffer  bufs, u8 num){struct bman_portal  p;struct bm_rcr_entry  r;unsigned long irqflags;int avail, timeout = 1000;   1ms ", "int dpaa2_io_get_cpu(struct dpaa2_io *d)": "dpaa2_io_get_cpu() - get the cpu associated with a given DPIO object     @d: the given DPIO object.     Return the cpu associated with the DPIO object ", "int dpaa2_io_service_pull_fq(struct dpaa2_io *d, u32 fqid,     struct dpaa2_io_store *s)": "dpaa2_io_service_pull_fq() - pull dequeue functions from a fq.   @d: the given DPIO service.   @fqid: the given frame queue id.   @s: the dpaa2_io_store object for the result.     Return 0 for success, or error code for failure. ", "int dpaa2_io_service_enqueue_fq(struct dpaa2_io *d,u32 fqid,const struct dpaa2_fd *fd)": "dpaa2_io_service_enqueue_fq() - Enqueue a frame to a frame queue.   @d: the given DPIO service.   @fqid: the given frame queue id.   @fd: the frame descriptor which is enqueued.     Return 0 for successful enqueue, -EBUSY if the enqueue ring is not ready,   or -ENODEV if there is no dpio service. ", "int dpaa2_io_service_enqueue_multiple_fq(struct dpaa2_io *d,u32 fqid,const struct dpaa2_fd *fd,int nb)": "dpaa2_io_service_enqueue_multiple_fq() - Enqueue multiple frames   to a frame queue using one fqid.   @d: the given DPIO service.   @fqid: the given frame queue id.   @fd: the frame descriptor which is enqueued.   @nb: number of frames to be enqueud     Return 0 for successful enqueue, -EBUSY if the enqueue ring is not ready,   or -ENODEV if there is no dpio service. ", "int dpaa2_io_service_enqueue_multiple_desc_fq(struct dpaa2_io *d,u32 *fqid,const struct dpaa2_fd *fd,int nb)": "dpaa2_io_service_enqueue_multiple_desc_fq() - Enqueue multiple frames   to different frame queue using a list of fqids.   @d: the given DPIO service.   @fqid: the given list of frame queue ids.   @fd: the frame descriptor which is enqueued.   @nb: number of frames to be enqueud     Return 0 for successful enqueue, -EBUSY if the enqueue ring is not ready,   or -ENODEV if there is no dpio service. ", "struct dpaa2_io *dpaa2_io_create(const struct dpaa2_io_desc *desc, struct device *dev)": "dpaa2_io_set_irq_coalescing(d, moder.usec);dim->state = DIM_START_MEASURE;}     dpaa2_io_create() - create a dpaa2_io object.   @desc: the dpaa2_io descriptor   @dev: the actual DPIO device     Activates a \"struct dpaa2_io\" corresponding to the given config of an actual   DPIO object.     Return a valid dpaa2_io object for success, or NULL for failure. ", "void dpaa2_io_get_irq_coalescing(struct dpaa2_io *d, u32 *irq_holdoff)": "dpaa2_io_get_irq_coalescing() - Get the current IRQ coalescing parameters   @d: the given DPIO object   @irq_holdoff: interrupt holdoff (timeout) period in us ", "void dpaa2_io_set_adaptive_coalescing(struct dpaa2_io *d,      int use_adaptive_rx_coalesce)": "dpaa2_io_set_adaptive_coalescing() - Enabledisable adaptive coalescing   @d: the given DPIO object   @use_adaptive_rx_coalesce: adaptive coalescing state ", "int dpaa2_io_get_adaptive_coalescing(struct dpaa2_io *d)": "dpaa2_io_get_adaptive_coalescing() - Query adaptive coalescing state   @d: the given DPIO object     Return 1 when adaptive coalescing is enabled on the DPIO object and 0   otherwise. ", "void dpaa2_io_update_net_dim(struct dpaa2_io *d, __u64 frames, __u64 bytes)": "dpaa2_io_update_net_dim() - Update Net DIM   @d: the given DPIO object   @frames: how many frames have been dequeued by the user since the last call   @bytes: how many bytes have been dequeued by the user since the last call ", "pin_mask1bit = (u32) (1 << (QE_PIO_PINS - (pin + 1)));/* Set open drain, if required ": "par_io_config_pin(struct qe_pio_regs __iomem  par_io, u8 pin, int dir, int open_drain, int assignment, int has_irq){u32 pin_mask1bit;u32 pin_mask2bits;u32 new_mask2bits;u32 tmp_val;  calculate pin location for single and 2 bits information ", "pin_mask = (u32) (1 << (QE_PIO_PINS - 1 - pin));tmp_val = ioread32be(&par_io[port].cpdata);if (val == 0)/* clear ": "par_io_data_set(u8 port, u8 pin, u8 val){u32 pin_mask, tmp_val;if (port >= num_par_io_ports)return -EINVAL;if (pin >= QE_PIO_PINS)return -EINVAL;  calculate pin location ", "if (WARN_ON(tsa_serial->id != out_args.args[0])) ": "tsa_serial_get_byphandle(struct device_node  np,    const char  phandle_name){struct of_phandle_args out_args;struct platform_device  pdev;struct tsa_serial  tsa_serial;struct tsa  tsa;int ret;ret = of_parse_phandle_with_fixed_args(np, phandle_name, 1, 0, &out_args);if (ret < 0)return ERR_PTR(ret);if (!of_match_node(tsa_driver.driver.of_match_table, out_args.np)) {of_node_put(out_args.np);return ERR_PTR(-EINVAL);}pdev = of_find_device_by_node(out_args.np);of_node_put(out_args.np);if (!pdev)return ERR_PTR(-ENODEV);tsa = platform_get_drvdata(pdev);if (!tsa) {platform_device_put(pdev);return ERR_PTR(-EPROBE_DEFER);}if (out_args.args_count != 1) {platform_device_put(pdev);return ERR_PTR(-EINVAL);}if (out_args.args[0] >= ARRAY_SIZE(tsa->serials)) {platform_device_put(pdev);return ERR_PTR(-EINVAL);}tsa_serial = &tsa->serials[out_args.args[0]];    Be sure that the serial id matches the phandle arg.   The tsa_serials table is indexed by serial ids. The serial id is set   during the probe() call and needs to be coherent. ", "gumr = ioread32be(&uf_regs->gumr);if (mode & COMM_DIR_TX) ": "ucc_fast_disable(struct ucc_fast_private   uccf, enum comm_dir mode){struct ucc_fast __iomem  uf_regs;u32 gumr;uf_regs = uccf->uf_regs;  Disable reception andor transmission on this UCC. ", "if ((uf_info->ucc_num < 0) || (uf_info->ucc_num > UCC_MAX_NUM - 1)) ": "ucc_fast_init(struct ucc_fast_info   uf_info, struct ucc_fast_private    uccf_ret){struct ucc_fast_private  uccf;struct ucc_fast __iomem  uf_regs;u32 gumr;int ret;if (!uf_info)return -EINVAL;  check if the UCC port number is in range. ", "/* For more details see the hardware spec. ": "ucc_fast_free(uccf);return ret;}uccf->mrblr = uf_info->max_rx_buf_length;  Set GUMR ", "struct qe_pin *qe_pin_request(struct device *dev, int index)": "qe_pin_request - Request a QE pin   @dev:device to get the pin from   @index:index of the pin in the device tree   Context:non-atomic     This function return qe_pin so that you could use it with the rest of   the QE Pin Multiplexing API. ", "void qe_pin_free(struct qe_pin *qe_pin)": "qe_pin_free - Free a pin   @qe_pin:pointer to the qe_pin structure   Context:any     This function frees the qe_pin structure and makes a pin available   for further qe_pin_request() calls. ", "void qe_pin_set_dedicated(struct qe_pin *qe_pin)": "qe_pin_set_dedicated - Revert a pin to a dedicated peripheral function mode   @qe_pin:pointer to the qe_pin structure   Context:any     This function resets a pin to a dedicated peripheral function that   has been set up by the firmware. ", "void qe_pin_set_gpio(struct qe_pin *qe_pin)": "qe_pin_set_gpio - Set a pin to the GPIO mode   @qe_pin:pointer to the qe_pin structure   Context:any     This function sets a pin to the GPIO mode. ", "ret = tsa_serial_get_info(chan->qmc->tsa_serial, &tsa_info);if (ret)return ret;info->mode = chan->mode;info->rx_fs_rate = tsa_info.rx_fs_rate;info->rx_bit_rate = tsa_info.rx_bit_rate;info->nb_tx_ts = hweight64(chan->tx_ts_mask);info->tx_fs_rate = tsa_info.tx_fs_rate;info->tx_bit_rate = tsa_info.tx_bit_rate;info->nb_rx_ts = hweight64(chan->rx_ts_mask);return 0;}EXPORT_SYMBOL(qmc_chan_get_info": "qmc_chan_get_info(struct qmc_chan  chan, struct qmc_chan_info  info){struct tsa_serial_info tsa_info;int ret;  Retrieve info from the TSA related serial ", "spin_lock_irqsave(&chan->tx_lock, flags);bd = chan->txbd_free;ctrl = qmc_read16(&bd->cbd_sc);if (ctrl & (QMC_BD_TX_R | QMC_BD_TX_UB)) ": "qmc_chan_write_submit(struct qmc_chan  chan, dma_addr_t addr, size_t length,  void ( complete)(void  context), void  context){struct qmc_xfer_desc  xfer_desc;unsigned long flags;cbd_t  __iomem bd;u16 ctrl;int ret;    R bit  UB bit     0       0  : The BD is free     1       1  : The BD is in used, waiting for transfer     0       1  : The BD is in used, waiting for completion     1       0  : Should not append ", "spin_lock_irqsave(&chan->rx_lock, flags);bd = chan->rxbd_free;ctrl = qmc_read16(&bd->cbd_sc);if (ctrl & (QMC_BD_RX_E | QMC_BD_RX_UB)) ": "qmc_chan_read_submit(struct qmc_chan  chan, dma_addr_t addr, size_t length, void ( complete)(void  context, size_t length), void  context){struct qmc_xfer_desc  xfer_desc;unsigned long flags;cbd_t  __iomem bd;u16 ctrl;int ret;    E bit  UB bit     0       0  : The BD is free     1       1  : The BD is in used, waiting for transfer     0       1  : The BD is in used, waiting for completion     1       0  : Should not append ", "ret = qmc_chan_command(chan, 0x0);if (ret) ": "qmc_chan_stop_rx(struct qmc_chan  chan){unsigned long flags;int ret;spin_lock_irqsave(&chan->rx_lock, flags);  Send STOP RECEIVE command ", "if (chan->mode == QMC_TRANSPARENT)qmc_write32(chan->s_param + QMC_SPE_ZDSTATE, 0x18000080);elseqmc_write32(chan->s_param + QMC_SPE_ZDSTATE, 0x00000080);qmc_write32(chan->s_param + QMC_SPE_RSTATE, 0x31000000);chan->is_rx_halted = false;chan->is_rx_stopped = false;spin_unlock_irqrestore(&chan->rx_lock, flags);}static void qmc_chan_start_tx(struct qmc_chan *chan)": "qmc_chan_start_rx(struct qmc_chan  chan){unsigned long flags;spin_lock_irqsave(&chan->rx_lock, flags);  Restart the receiver ", "qmc_setbits16(chan->s_param + QMC_SPE_CHAMR, QMC_SPE_CHAMR_ENT);/* Set the POL bit in the channel mode register ": "qmc_chan_reset() was called. ", "qe_muram_init();if (qe_sdma_init())panic(\"sdma init failed!\");}int qe_issue_cmd(u32 cmd, u32 device, u8 mcn_protocol, u32 cmd_input)": "qe_issue_cmd(QE_RESET, QE_CR_SUBBLOCK_INVALID,     QE_CR_PROTOCOL_UNSPECIFIED, 0);  Reclaim the MURAM memory for our use. ", "mod = brg_clk % CLK_GRAN;if (mod) ": "qe_get_brg_clk(void){struct device_node  qe;u32 brg;unsigned int mod;if (brg_clk)return brg_clk;qe = qe_get_device_node();if (!qe)return brg_clk;if (!of_property_read_u32(qe, \"brg-frequency\", &brg))brg_clk = brg;of_node_put(qe);  round this if near to a multiple of CLK_GRAN ", "if (qe_general4_errata())if (!div16 && (divisor & 1) && (divisor > 3))divisor++;tempval = ((divisor - 1) << QE_BRGC_DIVISOR_SHIFT) |QE_BRGC_ENABLE | div16;iowrite32be(tempval, &qe_immr->brg.brgc[brg - QE_BRG1]);return 0;}EXPORT_SYMBOL(qe_setbrg": "qe_setbrg(enum qe_clock brg, unsigned int rate, unsigned int multiplier){u32 divisor, tempval;u32 div16 = 0;if ((brg < QE_BRG1) || (brg > QE_BRG16))return -EINVAL;divisor = qe_get_brg_clk()  (rate   multiplier);if (divisor > QE_BRGC_DIVISOR_MAX + 1) {div16 = QE_BRGC_DIV16;divisor = 16;}  Errata QE_General4, which affects some MPC832x and MPC836x SOCs, says   that the BRG divisor must be even if you're not using divide-by-16   mode. ", "static void qe_upload_microcode(const void *base,const struct qe_microcode *ucode)": "qe_upload_firmware().  It does   the actual uploading of the microcode. ", "static struct qe_firmware_info qe_firmware_info;/* * Set to 1 if QE firmware has been uploaded, and therefore * qe_firmware_info contains valid data. ": "qe_get_firmware_info() ", "gumr_l = ioread32be(&us_regs->gumr_l);if (mode & COMM_DIR_TX) ": "ucc_slow_disable(struct ucc_slow_private   uccs, enum comm_dir mode){struct ucc_slow __iomem  us_regs;u32 gumr_l;us_regs = uccs->us_regs;  Disable reception andor transmission on this UCC. ", "if ((us_info->ucc_num < 0) || (us_info->ucc_num > UCC_MAX_NUM - 1)) ": "ucc_slow_init(struct ucc_slow_info   us_info, struct ucc_slow_private    uccs_ret){struct ucc_slow_private  uccs;u32 i;struct ucc_slow __iomem  us_regs;u32 gumr;struct qe_bd __iomem  bd;u32 id;u32 command;int ret = 0;if (!us_info)return -EINVAL;  check if the UCC port number is in range. ", "ret = ucc_set_type(us_info->ucc_num, UCC_SPEED_TYPE_SLOW);if (ret) ": "ucc_slow_free(uccs);return -ENOMEM;}id = ucc_slow_get_qe_cr_subblock(us_info->ucc_num);qe_issue_cmd(QE_ASSIGN_PAGE_TO_DEVICE, id, us_info->protocol,     uccs->us_pram_offset);uccs->us_pram = qe_muram_addr(uccs->us_pram_offset);  Set UCC to slow type ", "csel = (ucc_num < 4) ? ucc_num + 9 : ucc_num - 3;siram_entry_valid = SIR_CSEL(csel) | SIR_BYTE | SIR_CNT(0);siram_entry_closed = SIR_IDLE | SIR_BYTE | SIR_CNT(0);for (i = 0; i < utdm->num_of_ts; i++) ": "ucc_tdm_init(struct ucc_tdm  utdm, struct ucc_tdm_info  ut_info){struct si1 __iomem  si_regs;u16 __iomem  siram;u16 siram_entry_valid;u16 siram_entry_closed;u16 ucc_num;u8 csel;u16 sixmr;u16 tdm_port;u32 siram_entry_id;u32 mask;int i;si_regs = utdm->si_regs;siram = utdm->siram;ucc_num = ut_info->uf_info.ucc_num;tdm_port = utdm->tdm_port;siram_entry_id = utdm->siram_entry_id;if (utdm->tdm_framer_type == TDM_FRAMER_T1)utdm->num_of_ts = 24;if (utdm->tdm_framer_type == TDM_FRAMER_E1)utdm->num_of_ts = 32;  set siram table ", "static s32 cpm_muram_alloc_common(unsigned long size,  genpool_algo_t algo, void *data)": "cpm_muram_alloc_common - cpm_muram_alloc common code   @size: number of bytes to allocate   @algo: algorithm for alloc.   @data: data for genalloc's algorithm.     This function returns a non-negative offset into the muram area, or   a negative errno on failure. ", "s32 cpm_muram_alloc(unsigned long size, unsigned long align)": "cpm_muram_addr(start), 0, size);entry->start = start;entry->size = size;list_add(&entry->head, &muram_block_list);return start;}    cpm_muram_alloc - allocate the requested size worth of multi-user ram   @size: number of bytes to allocate   @align: requested alignment, in bytes     This function returns a non-negative offset into the muram area, or   a negative errno on failure.   Use cpm_dpram_addr() to get the virtual address of the area.   Use cpm_muram_free() to free the allocation. ", "s32 cpm_muram_alloc_fixed(unsigned long offset, unsigned long size)": "cpm_muram_alloc_fixed - reserve a specific region of multi-user ram   @offset: offset of allocation start address   @size: number of bytes to allocate   This function returns @offset if the area was available, a negative   errno otherwise.   Use cpm_dpram_addr() to get the virtual address of the area.   Use cpm_muram_free() to free the allocation. ", "dma_addr_t cpm_muram_dma(void __iomem *addr)": "cpm_muram_dma - turn a muram virtual address into a DMA address   @addr: virtual address from cpm_muram_addr() to convert ", "int tegra_powergate_power_on(unsigned int id)": "tegra_powergate_power_on() - power on partition   @id: partition ID ", "int tegra_powergate_power_off(unsigned int id)": "tegra_powergate_power_off() - power off partition   @id: partition ID ", "if (id == TEGRA_POWERGATE_3D) ": "tegra_powergate_remove_clamping(struct tegra_pmc  pmc,     unsigned int id){u32 mask;mutex_lock(&pmc->powergates_lock);    On Tegra124 and later, the clamps for the GPU are controlled by a   separate register (with different semantics). ", "int tegra_powergate_sequence_power_up(unsigned int id, struct clk *clk,      struct reset_control *rst)": "tegra_powergate_sequence_power_up() - power up partition   @id: partition ID   @clk: clock for partition   @rst: reset for partition     Must be called with clk disabled, and returns with clk enabled. ", "int tegra_io_pad_power_enable(enum tegra_io_pad id)": "tegra_io_pad_power_enable() - enable power to IO pad   @id: Tegra IO pad ID for which to enable power     Returns: 0 on success or a negative error code on failure. ", "int tegra_io_pad_power_disable(enum tegra_io_pad id)": "tegra_io_pad_power_disable() - disable power to IO pad   @id: Tegra IO pad ID for which to disable power     Returns: 0 on success or a negative error code on failure. ", "int tegra_io_rail_power_on(unsigned int id)": "tegra_io_rail_power_on() - enable power to IO rail   @id: Tegra IO pad ID for which to enable power     See also: tegra_io_pad_power_enable() ", "int tegra_io_rail_power_off(unsigned int id)": "tegra_io_rail_power_off() - disable power to IO rail   @id: Tegra IO pad ID for which to disable power     See also: tegra_io_pad_power_disable() ", "printk(KERN_DEBUG \"Queue %s(%i) put %X\\n\",       qmgr_queue_descs[queue], queue, val);#endif__raw_writel(val, &qmgr_regs->acc[queue][0]);}u32 qmgr_get_entry(unsigned int queue)": "qmgr_queue_descs[QUEUES][32];#endifvoid qmgr_put_entry(unsigned int queue, u32 val){#if DEBUG_QMGRBUG_ON(!qmgr_queue_descs[queue]);   not yet requested ", "printk(KERN_DEBUG \"Queue %s(%i) get %X\\n\",       qmgr_queue_descs[queue], queue, val);#endifreturn val;}static int __qmgr_get_stat1(unsigned int queue)": "qmgr_get_entry(unsigned int queue){u32 val;val = __raw_readl(&qmgr_regs->acc[queue][0]);#if DEBUG_QMGRBUG_ON(!qmgr_queue_descs[queue]);   not yet requested ", "int qmgr_stat_empty(unsigned int queue)": "qmgr_stat_empty() - checks if a hardware queue is empty   @queue:queue number     Returns non-zero value if the queue is empty. ", "int qmgr_stat_below_low_watermark(unsigned int queue)": "qmgr_stat_below_low_watermark() - checks if a queue is below low watermark   @queue:queue number     Returns non-zero value if the queue is below low watermark. ", "int qmgr_stat_full(unsigned int queue)": "qmgr_stat_full() - checks if a hardware queue is full   @queue:queue number     Returns non-zero value if the queue is full. ", "int qmgr_stat_overflow(unsigned int queue)": "qmgr_stat_overflow() - checks if a hardware queue experienced overflow   @queue:queue number     Returns non-zero value if the queue experienced overflow. ", "bit = (queue % 8) * 4; /* 3 bits + 1 reserved bit per queue ": "qmgr_set_irq(unsigned int queue, int src,  void ( handler)(void  pdev), void  pdev){unsigned long flags;spin_lock_irqsave(&qmgr_lock, flags);if (queue < HALF_QUEUES) {u32 __iomem  reg;int bit;BUG_ON(src > QUEUE_IRQ_SRC_NOT_FULL);reg = &qmgr_regs->irqsrc[queue >> 3];   8 queues per u32 ", "spin_unlock_irqrestore(&qmgr_lock, flags);}static inline void shift_mask(u32 *mask)": "qmgr_disable_irq(unsigned int queue){unsigned long flags;int half = queue  32;u32 mask = 1 << (queue & (HALF_QUEUES - 1));spin_lock_irqsave(&qmgr_lock, flags);__raw_writel(__raw_readl(&qmgr_regs->irqen[half]) & ~mask,     &qmgr_regs->irqen[half]);__raw_writel(mask, &qmgr_regs->irqstat[half]);   clear ", ",       unsigned int nearly_empty_watermark,       unsigned int nearly_full_watermark,       const char *desc_format, const char* name)#elseint __qmgr_request_queue(unsigned int queue, unsigned int len /* dwords ": "qmgr_request_queue(unsigned int queue, unsigned int len   dwords ", ", unsigned int nearly_empty_watermark, unsigned int nearly_full_watermark)#endif": "__qmgr_request_queue(unsigned int queue, unsigned int len   dwords ", "spin_lock_irq(&qmgr_lock);cfg = __raw_readl(&qmgr_regs->sram[queue]);addr = (cfg >> 14) & 0xFF;BUG_ON(!addr);/* not requested ": "qmgr_release_queue(unsigned int queue){u32 cfg, addr, mask[4];BUG_ON(queue >= QUEUES);   not in valid range ", "__raw_readl(&npe->regs->exec_data);__raw_readl(&npe->regs->exec_data);return __raw_readl(&npe->regs->exec_data);}static void npe_clear_active(struct npe *npe, u32 reg)": "npe_running(struct npe  npe){return (__raw_readl(&npe->regs->exec_status_cmd) & STAT_RUN) != 0;}static void npe_cmd_write(struct npe  npe, u32 addr, int cmd, u32 data){__raw_writel(data, &npe->regs->exec_data);__raw_writel(addr, &npe->regs->exec_addr);__raw_writel(cmd, &npe->regs->exec_status_cmd);}static u32 npe_cmd_read(struct npe  npe, u32 addr, int cmd){__raw_writel(addr, &npe->regs->exec_addr);__raw_writel(cmd, &npe->regs->exec_status_cmd);  Iintroduce extra read cycles after issuing read command to NPE   so that we read the register after the NPE has updated it.   This is to overcome race condition between XScale and NPE ", "rmap = syscon_regmap_lookup_by_compatible(\"syscon\");if (IS_ERR(rmap))return dev_err_probe(dev, PTR_ERR(rmap),     \"failed to look up syscon\\n\");for (i = 0; i < NPE_COUNT; i++) ": "npe_release(struct npe  npe){module_put(THIS_MODULE);}static int ixp4xx_npe_probe(struct platform_device  pdev){int i, found = 0;struct device  dev = &pdev->dev;struct device_node  np = dev->of_node;struct resource  res;struct regmap  rmap;u32 val;  This system has only one syscon, so fetch it ", "image->id = swab32(image->id);image->size = swab32(image->size);} else if (image->magic != FW_MAGIC) ": "npe_send_recv_message(struct npe  npe, void  msg, const char  what){int result;u32  send = msg, recv[2];if ((result = npe_send_message(npe, msg, what)) != 0)return result;if ((result = npe_recv_message(npe, recv, what)) != 0)return result;if ((recv[0] != send[0]) || (recv[1] != send[1])) {debug_msg(npe, \"Message %s: unexpected message received\\n\",  what);return -EIO;}return 0;}int npe_load_firmware(struct npe  npe, const char  name, struct device  dev){const struct firmware  fw_entry;struct dl_block {u32 type;u32 offset;}  blk;struct dl_image {u32 magic;u32 id;u32 size;union {DECLARE_FLEX_ARRAY(u32, data);DECLARE_FLEX_ARRAY(struct dl_block, blocks);};}  image;struct dl_codeblock {u32 npe_addr;u32 size;u32 data[];}  cb;int i, j, err, data_size, instr_size, blocks, table_end;u32 cmd;if ((err = request_firmware(&fw_entry, name, dev)) != 0)return err;err = -EINVAL;if (fw_entry->size < sizeof(struct dl_image)) {print_npe(KERN_ERR, npe, \"incomplete firmware file\\n\");goto err;}image = (struct dl_image )fw_entry->data;#if DEBUG_FWprint_npe(KERN_DEBUG, npe, \"firmware: %08X %08X %08X (0x%X bytes)\\n\",  image->magic, image->id, image->size, image->size   4);#endifif (image->magic == swab32(FW_MAGIC)) {   swapped file ", "if (!wait_for_completion_timeout(&sys_controller->c, timeout)) ": "mpfs_blocking_transaction(struct mpfs_sys_controller  sys_controller, struct mpfs_mss_msg  msg){unsigned long timeout = msecs_to_jiffies(MPFS_SYS_CTRL_TIMEOUT_MS);int ret;ret = mutex_lock_interruptible(&transaction_lock);if (ret)return ret;reinit_completion(&sys_controller->c);ret = mbox_send_message(sys_controller->chan, msg);if (ret < 0) {dev_warn(sys_controller->client.dev, \"MPFS sys controller service timeout\\n\");goto out;}    Unfortunately, the system controller will only deliver an interrupt   if a service succeeds. mbox_send_message() will block until the busy   flag is gone. If the busy flag is gone but no interrupt has arrived   to trigger the rx callback then the service can be deemed to have   failed.   The caller can then interrogate msg::response::resp_status to   determine the cause of the failure.   mbox_send_message() returns positive integers in the success path, so   ret needs to be cleared if we do get an interrupt. ", "void *qmi_encode_message(int type, unsigned int msg_id, size_t *len, unsigned int txn_id, const struct qmi_elem_info *ei, const void *c_struct)": "qmi_encode_message() - Encode C structure as QMI encoded message   @type:Type of QMI message   @msg_id:Message ID of the message   @len:Passed as max length of the message, updated to actual size   @txn_id:Transaction ID   @ei:QMI message descriptor   @c_struct:Reference to structure to encode     Return: Buffer with encoded message, or negative ERR_PTR() on error ", "int qmi_decode_message(const void *buf, size_t len,       const struct qmi_elem_info *ei, void *c_struct)": "qmi_decode_message() - Decode QMI encoded message to C structure   @buf:Buffer with encoded message   @len:Amount of data in @buf   @ei:QMI message descriptor   @c_struct:Reference to structure to decode into     Return: The number of bytes of decoded information on success, negative   errno on error. ", "struct pdr_service *pdr_add_lookup(struct pdr_handle *pdr,   const char *service_name,   const char *service_path)": "pdr_add_lookup() - register a tracking request for a PD   @pdr:PDR client handle   @service_name:service name of the tracking request   @service_path:service path of the tracking request     Registering a pdr lookup allows for tracking the life cycle of the PD.     Return: pdr_service object on success, ERR_PTR on failure. -EALREADY is   returned if a lookup is already in progress for the given service path. ", "int pdr_restart_pd(struct pdr_handle *pdr, struct pdr_service *pds)": "pdr_restart_pd() - restart PD   @pdr:PDR client handle   @pds:PD service handle     Restarts the PD tracked by the PDR client handle for a given service path.     Return: 0 on success, negative errno on failure. ", "struct pdr_handle *pdr_handle_alloc(void (*status)(int state,   char *service_path,   void *priv), void *priv)": "pdr_handle_alloc() - initialize the PDR client handle   @status:function to be called on PD state change   @priv:handle for client's use     Initializes the PDR client handle to allow for trackingrestart of PDs.     Return: pdr_handle object on success, ERR_PTR on failure. ", "void pdr_handle_release(struct pdr_handle *pdr)": "pdr_handle_release() - release the PDR client handle   @pdr:PDR client handle     Cleans up pending tracking requests and releases the underlying qmi handles. ", "int qmi_add_lookup(struct qmi_handle *qmi, unsigned int service,   unsigned int version, unsigned int instance)": "qmi_add_lookup() - register a new lookup with the name service   @qmi:qmi handle   @service:service id of the request   @instance:instance id of the request   @version:version number of the request     Registering a lookup query with the name server will cause the name server   to send NEW_SERVER and DEL_SERVER control messages to this socket as   matching services are registered.     Return: 0 on success, negative errno on failure. ", "int qmi_add_server(struct qmi_handle *qmi, unsigned int service,   unsigned int version, unsigned int instance)": "qmi_add_server() - register a service with the name service   @qmi:qmi handle   @service:type of the service   @instance:instance of the service   @version:version of the service     Register a new service with the name service. This allows clients to find   and start sending messages to the client associated with @qmi.     Return: 0 on success, negative errno on failure. ", "int qmi_txn_init(struct qmi_handle *qmi, struct qmi_txn *txn, const struct qmi_elem_info *ei, void *c_struct)": "qmi_txn_cancel() to free up the allocated resources.     Return: Transaction id on success, negative errno on failure. ", "int qmi_handle_init(struct qmi_handle *qmi, size_t recv_buf_size,    const struct qmi_ops *ops,    const struct qmi_msg_handler *handlers)": "qmi_handle_init() - initialize a QMI client handle   @qmi:QMI handle to initialize   @recv_buf_size: maximum size of incoming message   @ops:reference to callbacks for QRTR notifications   @handlers:NULL-terminated list of QMI message handlers     This initializes the QMI client handle to allow sending and receiving QMI   messages. As messages are received the appropriate handler will be invoked.     Return: 0 on success, negative errno on failure. ", "if (!qmi)return;queue_work(qmi->wq, &qmi->work);}static struct socket *qmi_sock_create(struct qmi_handle *qmi,      struct sockaddr_qrtr *sq)": "qmi_handle_release() ", "ssize_t qmi_send_request(struct qmi_handle *qmi, struct sockaddr_qrtr *sq, struct qmi_txn *txn, int msg_id, size_t len, const struct qmi_elem_info *ei, const void *c_struct)": "qmi_send_request() - send a request QMI message   @qmi:QMI client handle   @sq:destination sockaddr   @txn:transaction object to use for the message   @msg_id:message id   @len:max length of the QMI message   @ei:QMI message description   @c_struct:object to be encoded     Return: 0 on success, negative errno on failure. ", "ssize_t qmi_send_response(struct qmi_handle *qmi, struct sockaddr_qrtr *sq,  struct qmi_txn *txn, int msg_id, size_t len,  const struct qmi_elem_info *ei, const void *c_struct)": "qmi_send_response() - send a response QMI message   @qmi:QMI client handle   @sq:destination sockaddr   @txn:transaction object to use for the message   @msg_id:message id   @len:max length of the QMI message   @ei:QMI message description   @c_struct:object to be encoded     Return: 0 on success, negative errno on failure. ", "ssize_t qmi_send_indication(struct qmi_handle *qmi, struct sockaddr_qrtr *sq,    int msg_id, size_t len,    const struct qmi_elem_info *ei,    const void *c_struct)": "qmi_send_indication() - send an indication QMI message   @qmi:QMI client handle   @sq:destination sockaddr   @msg_id:message id   @len:max length of the QMI message   @ei:QMI message description   @c_struct:object to be encoded     Return: 0 on success, negative errno on failure. ", "if (WARN_ON(client != OCMEM_GRAPHICS))return ERR_PTR(-ENODEV);if (size < OCMEM_MIN_ALLOC || !IS_ALIGNED(size, OCMEM_MIN_ALIGN))return ERR_PTR(-EINVAL);if (test_and_set_bit_lock(BIT(client), &ocmem->active_allocations))return ERR_PTR(-EBUSY);buf = kzalloc(sizeof(*buf), GFP_KERNEL);if (!buf) ": "ocmem_allocate(struct ocmem  ocmem, enum ocmem_client client, unsigned long size){struct ocmem_buf  buf;int ret;  TODO: add support for other clients... ", "if (WARN_ON(client != OCMEM_GRAPHICS))return;update_range(ocmem, buf, CLK_OFF, MODE_DEFAULT);if (qcom_scm_ocmem_lock_available()) ": "ocmem_free(struct ocmem  ocmem, enum ocmem_client client,struct ocmem_buf  buf){  TODO: add support for other clients... ", "struct rpmsg_endpoint *qcom_wcnss_open_channel(void *wcnss, const char *name, rpmsg_rx_cb_t cb, void *priv)": "qcom_wcnss_open_channel() - open additional SMD channel to WCNSS   @wcnss:wcnss handle, retrieved from drvdata   @name:SMD channel name   @cb:callback to handle incoming data on the channel   @priv:private data for use in the call-back ", "struct qmp ": "qmp_send()   @qdss_clk: QDSS clock hw struct   @cooling_devs: thermal cooling devices ", "struct qmp *qmp_get(struct device *dev)": "qmp_get() - get a qmp handle from a device   @dev: client device pointer     Return: handle to qmp device on success, ERR_PTR() on failure ", "void qmp_put(struct qmp *qmp)": "qmp_put() - release a qmp handle   @qmp: qmp handle obtained from qmp_get() ", "int qcom_rpm_smd_write(struct qcom_smd_rpm *rpm,       int state,       u32 type, u32 id,       void *buf,       size_t count)": "qcom_rpm_smd_write - write @buf to @type:@id   @rpm:rpm handle   @state:activesleep state flags   @type:resource type   @id:resource identifier   @buf:the data to be written   @count:number of bytes in @buf ", "void kryo_l2_set_indirect_reg(u64 reg, u64 val)": "kryo_l2_set_indirect_reg() - write value to an L2 register   @reg: Address of L2 register.   @val: Value to be written to register.     Use architecturally required barriers for ordering between system register   accesses, and system registers with respect to device memory ", "u64 kryo_l2_get_indirect_reg(u64 reg)": "kryo_l2_get_indirect_reg() - read an L2 register value   @reg: Address of L2 register.     Use architecturally required barriers for ordering between system register   accesses, and system registers with respect to device memory ", "int cmd_db_ready(void)": "cmd_db_ready - Indicates if command DB is available     Return: 0 on success, errno otherwise ", "u32 cmd_db_read_addr(const char *id)": "cmd_db_read_addr() - Query command db for resource id address.     @id: resource id to query for address     Return: resource address on success, 0 on error     This is used to retrieve resource address based on resource   id. ", "const void *cmd_db_read_aux_data(const char *id, size_t *len)": "cmd_db_read_aux_data() - Query command db for aux data.      @id: Resource to retrieve AUX Data on    @len: size of data buffer returned      Return: pointer to data on success, error pointer otherwise ", "enum cmd_db_hw_type cmd_db_read_slave_id(const char *id)": "cmd_db_read_slave_id - Get the slave ID for a given resource address     @id: Resource id to query the DB for version     Return: cmd_db_hw_type enum on success, CMD_DB_HW_INVALID on error ", "int rpmh_write_async(const struct device *dev, enum rpmh_state state,     const struct tcs_cmd *cmd, u32 n)": "rpmh_write_async: Write a set of RPMH commands     @dev: The device making the request   @state: Activesleep set   @cmd: The payload data   @n: The number of elements in payload     Write a set of RPMH commands, the order of commands is maintained   and will be sent as a single shot. ", "static int __rpmh_write(const struct device *dev, enum rpmh_state state,struct rpmh_request *rpm_msg)": "rpmh_write: Cache and send the RPMH request     @dev: The device making the request   @state: ActiveSleep request type   @rpm_msg: The data that needs to be sent (cmds).     Cache the RPMH request and send if the state is ACTIVE_ONLY.   SLEEPWAKE_ONLY requests are not sent to the controller at   this time. Use rpmh_flush() to send them to the controller. ", "int rpmh_write_batch(const struct device *dev, enum rpmh_state state,     const struct tcs_cmd *cmd, u32 *n)": "rpmh_write_batch: Write multiple sets of RPMH commands and wait for the   batch to finish.     @dev: the device making the request   @state: Activesleep set   @cmd: The payload data   @n: The array of count of elements in each batch, 0 terminated.     Write a request to the RSC controller without caching. If the request   state is ACTIVE, then the requests are treated as completion request   and sent to the controller immediately. The function waits until all the   commands are complete. If the request was to SLEEP or WAKE_ONLY, then the   request is sent as fire-n-forget and no ack is expected.     May sleep. Do not call from atomic contexts for ACTIVE_ONLY requests. ", "void rpmh_invalidate(const struct device *dev)": "rpmh_invalidate: Invalidate sleep and wake sets in batch_cache     @dev: The device making the request     Invalidate the sleep and wake values in batch_cache. ", "u32 geni_se_get_qup_hw_version(struct geni_se *se)": "geni_se_get_qup_hw_version() - Read the QUP wrapper Hardware version   @se:Pointer to the corresponding serial engine.     Return: Hardware Version of the wrapper. ", "void geni_se_init(struct geni_se *se, u32 rx_wm, u32 rx_rfr)": "geni_se_init() - Initialize the GENI serial engine   @se:Pointer to the concerned serial engine.   @rx_wm:Receive watermark, in units of FIFO words.   @rx_rfr:Ready-for-receive watermark, in units of FIFO words.     This function is used to initialize the GENI serial engine, configure   receive watermark and ready-for-receive watermarks. ", "void geni_se_select_mode(struct geni_se *se, enum geni_se_xfer_mode mode)": "geni_se_select_mode() - Select the serial engine transfer mode   @se:Pointer to the concerned serial engine.   @mode:Transfer mode to be selected. ", "void geni_se_config_packing(struct geni_se *se, int bpw, int pack_words,    bool msb_to_lsb, bool tx_cfg, bool rx_cfg)": "geni_se_config_packing() - Packing configuration of the serial engine   @se:Pointer to the concerned serial engine   @bpw:Bits of data per transfer word.   @pack_words:Number of words per fifo element.   @msb_to_lsb:Transfer from MSB to LSB or vice-versa.   @tx_cfg:Flag to configure the TX Packing.   @rx_cfg:Flag to configure the RX Packing.     This function is used to configure the packing rules for the current   transfer. ", "int geni_se_resources_off(struct geni_se *se)": "geni_se_resources_off() - Turn off resources associated with the serial                             engine   @se:Pointer to the concerned serial engine.     Return: 0 on success, standard Linux error codes on failureerror. ", "int geni_se_resources_on(struct geni_se *se)": "geni_se_resources_on() - Turn on resources associated with the serial                            engine   @se:Pointer to the concerned serial engine.     Return: 0 on success, standard Linux error codes on failureerror. ", "int geni_se_clk_tbl_get(struct geni_se *se, unsigned long **tbl)": "geni_se_clk_tbl_get() - Get the clock table to program DFS   @se:Pointer to the concerned serial engine.   @tbl:Table in which the output is returned.     This function is called by the protocol drivers to determine the different   clock frequencies supported by serial engine core clock. The protocol   drivers use the output to determine the clock frequency index to be   programmed into DFS.     Return: number of valid performance levels in the table on success,     standard Linux error codes on failure. ", "int geni_se_clk_freq_match(struct geni_se *se, unsigned long req_freq,   unsigned int *index, unsigned long *res_freq,   bool exact)": "geni_se_clk_freq_match() - Get the matching or closest SE clock frequency   @se:Pointer to the concerned serial engine.   @req_freq:Requested clock frequency.   @index:Index of the resultant frequency in the table.   @res_freq:Resultant frequency of the source clock.   @exact:Flag to indicate exact multiple requirement of the requested  frequency.     This function is called by the protocol drivers to determine the best match   of the requested frequency as provided by the serial engine clock in order   to meet the performance requirements.     If we return success:   - if @exact is true  then @res_freq  <an_integer> == @req_freq   - if @exact is false then @res_freq  <an_integer> <= @req_freq     Return: 0 on success, standard Linux error codes on failure. ", "void geni_se_tx_init_dma(struct geni_se *se, dma_addr_t iova, size_t len)": "geni_se_tx_init_dma() - Initiate TX DMA transfer on the serial engine   @se:Pointer to the concerned serial engine.   @iova:Mapped DMA address.   @len:Length of the TX buffer.     This function is used to initiate DMA TX transfer. ", "int geni_se_tx_dma_prep(struct geni_se *se, void *buf, size_t len,dma_addr_t *iova)": "geni_se_tx_dma_prep() - Prepare the serial engine for TX DMA transfer   @se:Pointer to the concerned serial engine.   @buf:Pointer to the TX buffer.   @len:Length of the TX buffer.   @iova:Pointer to store the mapped DMA address.     This function is used to prepare the buffers for DMA TX.     Return: 0 on success, standard Linux error codes on failure. ", "void geni_se_rx_init_dma(struct geni_se *se, dma_addr_t iova, size_t len)": "geni_se_rx_init_dma() - Initiate RX DMA transfer on the serial engine   @se:Pointer to the concerned serial engine.   @iova:Mapped DMA address.   @len:Length of the RX buffer.     This function is used to initiate DMA RX transfer. ", "int geni_se_rx_dma_prep(struct geni_se *se, void *buf, size_t len,dma_addr_t *iova)": "geni_se_rx_dma_prep() - Prepare the serial engine for RX DMA transfer   @se:Pointer to the concerned serial engine.   @buf:Pointer to the RX buffer.   @len:Length of the RX buffer.   @iova:Pointer to store the mapped DMA address.     This function is used to prepare the buffers for DMA RX.     Return: 0 on success, standard Linux error codes on failure. ", "void geni_se_tx_dma_unprep(struct geni_se *se, dma_addr_t iova, size_t len)": "geni_se_tx_dma_unprep() - Unprepare the serial engine after TX DMA transfer   @se:Pointer to the concerned serial engine.   @iova:DMA address of the TX buffer.   @len:Length of the TX buffer.     This function is used to unprepare the DMA buffers after DMA TX. ", "void geni_se_rx_dma_unprep(struct geni_se *se, dma_addr_t iova, size_t len)": "geni_se_rx_dma_unprep() - Unprepare the serial engine after RX DMA transfer   @se:Pointer to the concerned serial engine.   @iova:DMA address of the RX buffer.   @len:Length of the RX buffer.     This function is used to unprepare the DMA buffers after DMA RX. ", "void memstick_detect_change(struct memstick_host *host)": "memstick_detect_change - schedule media detection on memstick host   @host - host to use ", "int memstick_next_req(struct memstick_host *host, struct memstick_request **mrq)": "memstick_next_req - called by host driver to obtain next request to process   @host - host to use   @mrq - pointer to stick the request to     Host calls this function from idle state ( mrq == NULL) or after finishing   previous request ( mrq should point to it). If previous request was   unsuccessful, it is retried for predetermined number of times. Return value   of 0 means that new request was assigned to the host. ", "void memstick_new_req(struct memstick_host *host)": "memstick_new_req - notify the host that some requests are pending   @host - host to use ", "void memstick_init_req_sg(struct memstick_request *mrq, unsigned char tpc,  const struct scatterlist *sg)": "memstick_init_req_sg - set request fields needed for bulk data transfer   @mrq - request to use   @tpc - memstick Transport Protocol Command   @sg - TPC argument ", "int memstick_set_rw_addr(struct memstick_dev *card)": "memstick_set_rw_addr(struct memstick_dev  card,  struct memstick_request   mrq){if (!( mrq)) {memstick_init_req(&card->current_mrq, MS_TPC_SET_RW_REG_ADRS,  (char  )&card->reg_addr,  sizeof(card->reg_addr)); mrq = &card->current_mrq;return 0;} else {complete(&card->mrq_complete);return -EAGAIN;}}     memstick_set_rw_addr - issue SET_RW_REG_ADDR request and wait for it to                          complete   @card - media device to use ", "struct memstick_host *memstick_alloc_host(unsigned int extra,  struct device *dev)": "memstick_alloc_host - allocate a memstick_host structure   @extra: size of the user private data to allocate   @dev: parent device of the host ", "int memstick_add_host(struct memstick_host *host)": "memstick_add_host - start request processing on memstick host   @host - host to use ", "void memstick_remove_host(struct memstick_host *host)": "memstick_remove_host - stop request processing on memstick host   @host - host to use ", "void memstick_free_host(struct memstick_host *host)": "memstick_free_host - free memstick host   @host - host to use ", "void memstick_suspend_host(struct memstick_host *host)": "memstick_suspend_host - notify bus driver of host suspension   @host - host to use ", "void memstick_resume_host(struct memstick_host *host)": "memstick_resume_host - notify bus driver of host resumption   @host - host to use ", "struct host1x_channel *host1x_channel_request(struct host1x_client *client)": "host1x_channel_request() - Allocate a channel   @client: Host1x client this channel will be used to send commands to     Allocates a new host1x channel for @client. May return NULL if CDMA   initialization fails. ", "total = sizeof(struct host1x_job) +(u64)num_relocs * sizeof(struct host1x_reloc) +(u64)num_unpins * sizeof(struct host1x_job_unpin_data) +(u64)num_cmdbufs * sizeof(struct host1x_job_cmd) +(u64)num_unpins * sizeof(dma_addr_t) +(u64)num_unpins * sizeof(u32 *);if (total > ULONG_MAX)return NULL;mem = job = kzalloc(total, GFP_KERNEL);if (!job)return NULL;job->enable_firewall = enable_firewall;kref_init(&job->ref);job->channel = ch;/* Redistribute memory to the structs  ": "host1x_job_alloc(struct host1x_channel  ch,    u32 num_cmdbufs, u32 num_relocs,    bool skip_firewall){struct host1x_job  job = NULL;unsigned int num_unpins = num_relocs;bool enable_firewall;u64 total;void  mem;enable_firewall = IS_ENABLED(CONFIG_TEGRA_HOST1X_FIREWALL) && !skip_firewall;if (!enable_firewall)num_unpins += num_cmdbufs;  Check that we're not going to overflow ", "err = pin_job(host, job);if (err)goto out;if (job->enable_firewall) ": "host1x_job_pin(struct host1x_job  job, struct device  dev){int err;unsigned int i, j;struct host1x  host = dev_get_drvdata(dev->parent);  pin memory ", "mem += sizeof(struct host1x_job);job->relocs = num_relocs ? mem : NULL;mem += num_relocs * sizeof(struct host1x_reloc);job->unpins = num_unpins ? mem : NULL;mem += num_unpins * sizeof(struct host1x_job_unpin_data);job->cmds = num_cmdbufs ? mem : NULL;mem += num_cmdbufs * sizeof(struct host1x_job_cmd);job->addr_phys = num_unpins ? mem : NULL;job->reloc_addr_phys = job->addr_phys;job->gather_addr_phys = &job->addr_phys[num_relocs];return job;}EXPORT_SYMBOL(host1x_job_alloc);struct host1x_job *host1x_job_get(struct host1x_job *job)": "host1x_job_unpin_data) +(u64)num_cmdbufs   sizeof(struct host1x_job_cmd) +(u64)num_unpins   sizeof(dma_addr_t) +(u64)num_unpins   sizeof(u32  );if (total > ULONG_MAX)return NULL;mem = job = kzalloc(total, GFP_KERNEL);if (!job)return NULL;job->enable_firewall = enable_firewall;kref_init(&job->ref);job->channel = ch;  Redistribute memory to the structs  ", "int host1x_device_init(struct host1x_device *device)": "host1x_device_init() - initialize a host1x logical device   @device: host1x logical device     The driver for the host1x logical device can call this during execution of   its &host1x_driver.probe implementation to initialize each of its clients.   The client drivers access the subsystem specific driver data using the   &host1x_client.parent field and driver data associated with it (usually by   calling dev_get_drvdata()). ", "int host1x_device_exit(struct host1x_device *device)": "host1x_device_exit() - uninitialize host1x logical device   @device: host1x logical device     When the driver for a host1x logical device is unloaded, it can call this   function to tear down each of its clients. Typically this is done after a   subsystem-specific data structure is removed and the functionality can no   longer be used. ", "int host1x_driver_register_full(struct host1x_driver *driver,struct module *owner)": "host1x_driver_register_full() - register a host1x driver   @driver: host1x driver   @owner: owner module     Drivers for host1x logical devices call this function to register a driver   with the infrastructure. Note that since these drive logical devices, the   registration of the driver actually triggers tho logical device creation.   A logical device will be created for each host1x instance. ", "void host1x_driver_unregister(struct host1x_driver *driver)": "host1x_driver_unregister() - unregister a host1x driver   @driver: host1x driver     Unbinds the driver from each of the host1x logical devices that it is   bound to, effectively removing the subsystem devices that they represent. ", "void __host1x_client_init(struct host1x_client *client, struct lock_class_key *key)": "__host1x_client_init() - initialize a host1x client   @client: host1x client   @key: lock class key for the client-specific mutex ", "void host1x_client_exit(struct host1x_client *client)": "host1x_client_exit() - uninitialize a host1x client   @client: host1x client ", "int __host1x_client_register(struct host1x_client *client)": "__host1x_client_register() - register a host1x client   @client: host1x client     Registers a host1x client with each host1x controller instance. Note that   each client will only match their parent host1x controller and will only be   associated with that instance. Once all clients have been registered with   their parent host1x controller, the infrastructure will set up the logical   device and call host1x_device_init(), which will in turn call each client's   &host1x_client_ops.init implementation. ", "void host1x_client_unregister(struct host1x_client *client)": "host1x_client_unregister() - unregister a host1x client   @client: host1x client     Removes a host1x client from its host1x controller instance. If a logical   device has already been initialized, it will be torn down. ", "kref_get(&mapping->ref);}unlock:if (cache)mutex_unlock(&cache->lock);return mapping;}EXPORT_SYMBOL(host1x_bo_pin": "host1x_bo_pin(struct device  dev, struct host1x_bo  bo,enum dma_data_direction dir,struct host1x_bo_cache  cache){struct host1x_bo_mapping  mapping;if (cache) {mutex_lock(&cache->lock);list_for_each_entry(mapping, &cache->mappings, entry) {if (mapping->bo == bo && mapping->direction == dir) {kref_get(&mapping->ref);goto unlock;}}}mapping = bo->ops->pin(dev, bo, dir);if (IS_ERR(mapping))goto unlock;spin_lock(&mapping->bo->lock);list_add_tail(&mapping->list, &bo->mappings);spin_unlock(&mapping->bo->lock);if (cache) {INIT_LIST_HEAD(&mapping->entry);mapping->cache = cache;list_add_tail(&mapping->entry, &cache->mappings);  bump reference count to track the copy in the cache ", "if (mapping->cache)list_del(&mapping->entry);spin_lock(&mapping->bo->lock);list_del(&mapping->list);spin_unlock(&mapping->bo->lock);mapping->bo->ops->unpin(mapping);}void host1x_bo_unpin(struct host1x_bo_mapping *mapping)": "host1x_bo_unpin(struct kref  ref){struct host1x_bo_mapping  mapping = to_host1x_bo_mapping(ref);    When the last reference of the mapping goes away, make sure to remove the mapping from   the cache. ", "value = tegra_mipi_readl(device->mipi, MIPI_CAL_STATUS);tegra_mipi_writel(device->mipi, value, MIPI_CAL_STATUS);value = tegra_mipi_readl(device->mipi, MIPI_CAL_CTRL);value |= MIPI_CAL_CTRL_START;tegra_mipi_writel(device->mipi, value, MIPI_CAL_CTRL);/* * Wait for min 72uS to let calibration logic finish calibration * sequence codes before waiting for pads idle state to apply the * results. ": "tegra_mipi_start_calibration(struct tegra_mipi_device  device){const struct tegra_mipi_soc  soc = device->mipi->soc;unsigned int i;u32 value;int err;err = clk_enable(device->mipi->clk);if (err < 0)return err;mutex_lock(&device->mipi->lock);value = MIPI_CAL_BIAS_PAD_DRV_DN_REF(soc->pad_drive_down_ref) |MIPI_CAL_BIAS_PAD_DRV_UP_REF(soc->pad_drive_up_ref);tegra_mipi_writel(device->mipi, value, MIPI_CAL_BIAS_PAD_CFG1);value = tegra_mipi_readl(device->mipi, MIPI_CAL_BIAS_PAD_CFG2);value &= ~MIPI_CAL_BIAS_PAD_VCLAMP(0x7);value &= ~MIPI_CAL_BIAS_PAD_VAUXP(0x7);value |= MIPI_CAL_BIAS_PAD_VCLAMP(soc->pad_vclamp_level);value |= MIPI_CAL_BIAS_PAD_VAUXP(soc->pad_vauxp_level);tegra_mipi_writel(device->mipi, value, MIPI_CAL_BIAS_PAD_CFG2);for (i = 0; i < soc->num_pads; i++) {u32 clk = 0, data = 0;if (device->pads & BIT(i)) {data = MIPI_CAL_CONFIG_SELECT |       MIPI_CAL_CONFIG_HSPDOS(soc->hspdos) |       MIPI_CAL_CONFIG_HSPUOS(soc->hspuos) |       MIPI_CAL_CONFIG_TERMOS(soc->termos);clk = MIPI_CAL_CONFIG_SELECT |      MIPI_CAL_CONFIG_HSCLKPDOSD(soc->hsclkpdos) |      MIPI_CAL_CONFIG_HSCLKPUOSD(soc->hsclkpuos);}tegra_mipi_writel(device->mipi, data, soc->pads[i].data);if (soc->has_clk_lane && soc->pads[i].clk != 0)tegra_mipi_writel(device->mipi, clk, soc->pads[i].clk);}value = tegra_mipi_readl(device->mipi, MIPI_CAL_CTRL);value &= ~MIPI_CAL_CTRL_NOISE_FILTER(0xf);value &= ~MIPI_CAL_CTRL_PRESCALE(0x3);value |= MIPI_CAL_CTRL_NOISE_FILTER(0xa);value |= MIPI_CAL_CTRL_PRESCALE(0x2);if (!soc->clock_enable_override)value &= ~MIPI_CAL_CTRL_CLKEN_OVR;elsevalue |= MIPI_CAL_CTRL_CLKEN_OVR;tegra_mipi_writel(device->mipi, value, MIPI_CAL_CTRL);  clear any pending status bits ", "struct host1x_syncpt *host1x_syncpt_alloc(struct host1x *host,  unsigned long flags,  const char *name)": "host1x_syncpt_alloc() - allocate a syncpoint   @host: host1x device data   @flags: bitfield of HOST1X_SYNCPT_  flags   @name: name for the syncpoint for use in debug prints     Allocates a hardware syncpoint for the caller's use. The caller then has   the sole authority to mutate the syncpoint's value until it is freed again.     If no free syncpoints are available, or a NULL name was specified, returns   NULL. ", "u32 host1x_syncpt_id(struct host1x_syncpt *sp)": "host1x_syncpt_id() - retrieve syncpoint ID   @sp: host1x syncpoint     Given a pointer to a struct host1x_syncpt, retrieves its ID. This ID is   often used as a value to program into registers that control how hardware   blocks interact with syncpoints. ", "u32 host1x_syncpt_incr_max(struct host1x_syncpt *sp, u32 incrs)": "host1x_syncpt_incr_max() - update the value sent to hardware   @sp: host1x syncpoint   @incrs: number of increments ", "int host1x_syncpt_wait(struct host1x_syncpt *sp, u32 thresh, long timeout,       u32 *value)": "host1x_syncpt_wait() - wait for a syncpoint to reach a given value   @sp: host1x syncpoint   @thresh: threshold   @timeout: maximum time to wait for the syncpoint to reach the given value   @value: return location for the syncpoint value ", "struct host1x_syncpt *host1x_syncpt_request(struct host1x_client *client,    unsigned long flags)": "host1x_syncpt_put(). ", "u32 host1x_syncpt_read_max(struct host1x_syncpt *sp)": "host1x_syncpt_read_max() - read maximum syncpoint value   @sp: host1x syncpoint     The maximum syncpoint value indicates how many operations there are in   queue, either in channel or in a software thread. ", "u32 host1x_syncpt_read_min(struct host1x_syncpt *sp)": "host1x_syncpt_read_min() - read minimum syncpoint value   @sp: host1x syncpoint     The minimum syncpoint value is a shadow of the current sync point value in   hardware. ", "void host1x_syncpt_put(struct host1x_syncpt *sp)": "host1x_syncpt_read(sp));sp->locked = false;mutex_lock(&sp->host->syncpt_mutex);host1x_syncpt_base_free(sp->base);kfree(sp->name);sp->base = NULL;sp->name = NULL;sp->client_managed = false;mutex_unlock(&sp->host->syncpt_mutex);}     host1x_syncpt_put() - free a requested syncpoint   @sp: host1x syncpoint     Release a syncpoint previously allocated using host1x_syncpt_request(). A   host1x client driver should call this when the syncpoint is no longer in   use. ", "struct host1x_syncpt *host1x_syncpt_get_by_id(struct host1x *host,      unsigned int id)": "host1x_syncpt_get_by_id() - obtain a syncpoint by ID   @host: host1x controller   @id: syncpoint ID ", "struct host1x_syncpt *host1x_syncpt_get_by_id_noref(struct host1x *host,    unsigned int id)": "host1x_syncpt_get_by_id_noref() - obtain a syncpoint by ID but don't   increase the refcount.   @host: host1x controller   @id: syncpoint ID ", "struct host1x_syncpt_base *host1x_syncpt_get_base(struct host1x_syncpt *sp)": "host1x_syncpt_get_base() - obtain the wait base associated with a syncpoint   @sp: host1x syncpoint ", "u32 host1x_syncpt_base_id(struct host1x_syncpt_base *base)": "host1x_syncpt_base_id() - retrieve the ID of a syncpoint wait base   @base: host1x syncpoint wait base ", "void host1x_syncpt_release_vblank_reservation(struct host1x_client *client,      u32 syncpt_id)": "host1x_syncpt_release_vblank_reservation() - Make VBLANK syncpoint     available for allocation     @client: host1x bus client   @syncpt_id: syncpoint ID to make available     Makes VBLANK<i> syncpoint available for allocatation if it was   reserved at initialization time. This should be called by the display   driver after it has ensured that any VBLANK increment programming configured   by the boot chain has been disabled. ", "u64 host1x_get_dma_mask(struct host1x *host1x)": "host1x_get_dma_mask() - query the supported DMA mask for host1x   @host1x: host1x instance     Note that this returns the supported DMA mask for host1x, which can be   different from the applicable DMA mask under certain circumstances. ", "int vga_switcheroo_register_handler(  const struct vga_switcheroo_handler *handler,  enum vga_switcheroo_handler_flags_t handler_flags)": "vga_switcheroo_register_handler() - register handler   @handler: handler callbacks   @handler_flags: handler flags     Register handler. Enable vga_switcheroo if two vga clients have already   registered.     Return: 0 on success, -EINVAL if a handler was already registered. ", "void vga_switcheroo_unregister_handler(void)": "vga_switcheroo_unregister_handler() - unregister handler     Unregister handler. Disable vga_switcheroo. ", "static struct vgasr_priv vgasr_priv = ": "vga_switcheroo_handler_flags_t handler_flags;struct mutex mux_hw_lock;int old_ddc_owner;};#define ID_BIT_AUDIO0x100#define client_is_audio(c)((c)->id & ID_BIT_AUDIO)#define client_is_vga(c)(!client_is_audio(c))#define client_id(c)((c)->id & ~ID_BIT_AUDIO)static void vga_switcheroo_debugfs_init(struct vgasr_priv  priv);static void vga_switcheroo_debugfs_fini(struct vgasr_priv  priv);  only one switcheroo per system ", "int vga_switcheroo_register_client(struct pci_dev *pdev,   const struct vga_switcheroo_client_ops *ops,   bool driver_power_control)": "vga_switcheroo_client_probe_defer() shall be called   to ensure that all prerequisites are met.     Return: 0 on success, -ENOMEM on memory allocation error. ", "int vga_switcheroo_register_audio_client(struct pci_dev *pdev,const struct vga_switcheroo_client_ops *ops,struct pci_dev *vga_dev)": "vga_switcheroo_register_audio_client - register audio client   @pdev: client pci device   @ops: client callbacks   @vga_dev:  pci device which is bound to current audio client     Register audio client (audio device on a GPU). The client is assumed   to use runtime PM. Beforehand, vga_switcheroo_client_probe_defer()   shall be called to ensure that all prerequisites are met.     Return: 0 on success, -ENOMEM on memory allocation error, -EINVAL on getting   client id error. ", "enum vga_switcheroo_state vga_switcheroo_get_client_state(struct pci_dev *pdev)": "vga_switcheroo_get_client_state() - obtain power state of a given client   @pdev: client pci device     Obtain power state of a given client as seen from vga_switcheroo.   The function is only called from hda_intel.c.     Return: Power state. ", "void vga_switcheroo_unregister_client(struct pci_dev *pdev)": "vga_switcheroo_unregister_client() - unregister client   @pdev: client pci device     Unregister client. Disable vga_switcheroo if this is a vga client (GPU). ", "void vga_switcheroo_client_fb_set(struct pci_dev *pdev, struct fb_info *info)": "vga_switcheroo_client_fb_set() - set framebuffer of a given client   @pdev: client pci device   @info: framebuffer     Set framebuffer of a given client. The console will be remapped to this   on switching. ", "int vga_switcheroo_lock_ddc(struct pci_dev *pdev)": "vga_switcheroo_unlock_ddc(),   even if this function returns an error.     Return: Previous DDC owner on success or a negative int on error.   Specifically, %-ENODEV if no handler has registered or if the handler   does not support switching the DDC lines. Also, a negative value   returned by the handler is propagated back to the caller.   The return value has merely an informational purpose for any caller   which might be interested in it. It is acceptable to ignore the return   value and simply rely on the result of the subsequent EDID probe,   which will be %NULL if DDC switching failed. ", "int vga_switcheroo_process_delayed_switch(void)": "vga_switcheroo_process_delayed_switch() - helper for delayed switching     Process a delayed switch if one is pending. DRM drivers should call this   from their ->lastclose callback.     Return: 0 on success. -EINVAL if no delayed switch is pending, if the client   has unregistered in the meantime or if there are other clients blocking the   switch. If the actual switch fails, an error is reported and 0 is returned. ", "static void vga_switcheroo_power_switch(struct pci_dev *pdev,enum vga_switcheroo_state state)": "vga_switcheroo_init_domain_pm_ops(),   which augments the GPU's suspendresume functions by the requisite   calls to the handler.     When the audio device resumes, the GPU needs to be woken. This is achieved   by a PCI quirk which calls device_link_add() to declare a dependency on the   GPU. That way, the GPU is kept awake whenever and as long as the audio   device is in use.     On muxed machines, if the mux is initially switched to the discrete GPU,   the user ends up with a black screen when the GPU powers down after boot.   As a workaround, the mux is forced to the integrated GPU on runtime suspend,   cf. https:bugs.freedesktop.orgshow_bug.cgi?id=75917 ", "struct drm_crtc *drm_crtc_from_index(struct drm_device *dev, int idx)": "drm_crtc_from_index - find the registered CRTC at an index   @dev: DRM device   @idx: index of registered CRTC to find for     Given a CRTC index, return the registered CRTC from DRM device's   list of CRTCs with matching index. This is the inverse of drm_crtc_index().   It's useful in the vblank callbacks (like &drm_driver.enable_vblank or   &drm_driver.disable_vblank), since that still deals with indices instead   of pointers to &struct drm_crtc.\" ", "/** * drm_crtc_from_index - find the registered CRTC at an index * @dev: DRM device * @idx: index of registered CRTC to find for * * Given a CRTC index, return the registered CRTC from DRM device's * list of CRTCs with matching index. This is the inverse of drm_crtc_index(). * It's useful in the vblank callbacks (like &drm_driver.enable_vblank or * &drm_driver.disable_vblank), since that still deals with indices instead * of pointers to &struct drm_crtc.\" ": "drm_crtc_init_with_planes().     The CRTC is also the entry point for legacy modeset operations, see   &drm_crtc_funcs.set_config, legacy plane operations, see   &drm_crtc_funcs.page_flip and &drm_crtc_funcs.cursor_set2, and other legacy   operations like &drm_crtc_funcs.gamma_set. For atomic drivers all these   features are controlled through &drm_property and   &drm_mode_config_funcs.atomic_check. ", "int drm_crtc_init_with_planes(struct drm_device *dev, struct drm_crtc *crtc,      struct drm_plane *primary,      struct drm_plane *cursor,      const struct drm_crtc_funcs *funcs,      const char *name, ...)": "drm_crtc_cleanup() and kfree()   the crtc structure. The crtc structure should not be allocated with   devm_kzalloc().     The @primary and @cursor planes are only relevant for legacy uAPI, see   &drm_crtc.primary and &drm_crtc.cursor.     Note: consider using drmm_crtc_alloc_with_planes() or   drmm_crtc_init_with_planes() instead of drm_crtc_init_with_planes()   to let the DRM managed resource infrastructure take care of cleanup   and deallocation.     Returns:   Zero on success, error code on failure. ", "__printf(6, 0)static int __drm_crtc_init_with_planes(struct drm_device *dev, struct drm_crtc *crtc,       struct drm_plane *primary,       struct drm_plane *cursor,       const struct drm_crtc_funcs *funcs,       const char *name, va_list ap)": "drm_mode_set_config_internal(&set);}static unsigned int drm_num_crtcs(struct drm_device  dev){unsigned int num = 0;struct drm_crtc  tmp;drm_for_each_crtc(tmp, dev) {num++;}return num;}int drm_crtc_register_all(struct drm_device  dev){struct drm_crtc  crtc;int ret = 0;drm_for_each_crtc(crtc, dev) {drm_debugfs_crtc_add(crtc);if (crtc->funcs->late_register)ret = crtc->funcs->late_register(crtc);if (ret)return ret;}return 0;}void drm_crtc_unregister_all(struct drm_device  dev){struct drm_crtc  crtc;drm_for_each_crtc(crtc, dev) {if (crtc->funcs->early_unregister)crtc->funcs->early_unregister(crtc);drm_debugfs_crtc_remove(crtc);}}static int drm_crtc_crc_init(struct drm_crtc  crtc){#ifdef CONFIG_DEBUG_FSspin_lock_init(&crtc->crc.lock);init_waitqueue_head(&crtc->crc.wq);crtc->crc.source = kstrdup(\"auto\", GFP_KERNEL);if (!crtc->crc.source)return -ENOMEM;#endifreturn 0;}static void drm_crtc_crc_fini(struct drm_crtc  crtc){#ifdef CONFIG_DEBUG_FSkfree(crtc->crc.source);#endif}static const struct dma_fence_ops drm_crtc_fence_ops;static struct drm_crtc  fence_to_crtc(struct dma_fence  fence){BUG_ON(fence->ops != &drm_crtc_fence_ops);return container_of(fence->lock, struct drm_crtc, fence_lock);}static const char  drm_crtc_fence_get_driver_name(struct dma_fence  fence){struct drm_crtc  crtc = fence_to_crtc(fence);return crtc->dev->driver->name;}static const char  drm_crtc_fence_get_timeline_name(struct dma_fence  fence){struct drm_crtc  crtc = fence_to_crtc(fence);return crtc->timeline_name;}static const struct dma_fence_ops drm_crtc_fence_ops = {.get_driver_name = drm_crtc_fence_get_driver_name,.get_timeline_name = drm_crtc_fence_get_timeline_name,};struct dma_fence  drm_crtc_create_fence(struct drm_crtc  crtc){struct dma_fence  fence;fence = kzalloc(sizeof( fence), GFP_KERNEL);if (!fence)return NULL;dma_fence_init(fence, &drm_crtc_fence_ops, &crtc->fence_lock,       crtc->fence_context, ++crtc->fence_seqno);return fence;}     DOC: standard CRTC properties     DRM CRTCs have a few standardized properties:     ACTIVE:   Atomic property for setting the power state of the CRTC. When set to 1   the CRTC will actively display content. When set to 0 the CRTC will be   powered off. There is no expectation that user-space will reset CRTC   resources like the mode and planes when setting ACTIVE to 0.     User-space can rely on an ACTIVE change to 1 to never fail an atomic   test as long as no other property has changed. If a change to ACTIVE   fails an atomic test, this is a driver bug. For this reason setting   ACTIVE to 0 must not release internal resources (like reserved memory   bandwidth or clock generators).     Note that the legacy DPMS property on connectors is internally routed   to control this property for atomic drivers.   MODE_ID:   Atomic property for setting the CRTC display timings. The value is the   ID of a blob containing the DRM mode info. To disable the CRTC,   user-space must set this property to 0.     Setting MODE_ID to 0 will release reserved resources for the CRTC.   SCALING_FILTER:   Atomic property for setting the scaling filter for CRTC scaler     The value of this property can be one of the following:     Default:   Driver's default scaling filter   Nearest Neighbor:   Nearest Neighbor scaling filter ", "int drm_crtc_check_viewport(const struct drm_crtc *crtc,    int x, int y,    const struct drm_display_mode *mode,    const struct drm_framebuffer *fb)": "drm_crtc_check_viewport - Checks that a framebuffer is big enough for the       CRTC viewport   @crtc: CRTC that framebuffer will be displayed on   @x: x panning   @y: y panning   @mode: mode that framebuffer will be displayed under   @fb: framebuffer to check size of ", "int drm_crtc_create_scaling_filter_property(struct drm_crtc *crtc,    unsigned int supported_filters)": "drm_crtc_create_scaling_filter_property - create a new scaling filter   property     @crtc: drm CRTC   @supported_filters: bitmask of supported scaling filters, must include         BIT(DRM_SCALING_FILTER_DEFAULT).     This function lets driver to enable the scaling filter property on a given   CRTC.     RETURNS:   Zero for success or -errno ", "long drm_compat_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)": "drm_compat_ioctls[] = {#define DRM_IOCTL32_DEF(n, f) [DRM_IOCTL_NR(n##32)] = {.fn = f, .name = #n}DRM_IOCTL32_DEF(DRM_IOCTL_VERSION, compat_drm_version),DRM_IOCTL32_DEF(DRM_IOCTL_GET_UNIQUE, compat_drm_getunique),#if IS_ENABLED(CONFIG_DRM_LEGACY)DRM_IOCTL32_DEF(DRM_IOCTL_GET_MAP, compat_drm_getmap),#endifDRM_IOCTL32_DEF(DRM_IOCTL_GET_CLIENT, compat_drm_getclient),DRM_IOCTL32_DEF(DRM_IOCTL_GET_STATS, compat_drm_getstats),DRM_IOCTL32_DEF(DRM_IOCTL_SET_UNIQUE, compat_drm_setunique),#if IS_ENABLED(CONFIG_DRM_LEGACY)DRM_IOCTL32_DEF(DRM_IOCTL_ADD_MAP, compat_drm_addmap),DRM_IOCTL32_DEF(DRM_IOCTL_ADD_BUFS, compat_drm_addbufs),DRM_IOCTL32_DEF(DRM_IOCTL_MARK_BUFS, compat_drm_markbufs),DRM_IOCTL32_DEF(DRM_IOCTL_INFO_BUFS, compat_drm_infobufs),DRM_IOCTL32_DEF(DRM_IOCTL_MAP_BUFS, compat_drm_mapbufs),DRM_IOCTL32_DEF(DRM_IOCTL_FREE_BUFS, compat_drm_freebufs),DRM_IOCTL32_DEF(DRM_IOCTL_RM_MAP, compat_drm_rmmap),DRM_IOCTL32_DEF(DRM_IOCTL_SET_SAREA_CTX, compat_drm_setsareactx),DRM_IOCTL32_DEF(DRM_IOCTL_GET_SAREA_CTX, compat_drm_getsareactx),DRM_IOCTL32_DEF(DRM_IOCTL_RES_CTX, compat_drm_resctx),DRM_IOCTL32_DEF(DRM_IOCTL_DMA, compat_drm_dma),#if IS_ENABLED(CONFIG_AGP)DRM_IOCTL32_DEF(DRM_IOCTL_AGP_ENABLE, compat_drm_agp_enable),DRM_IOCTL32_DEF(DRM_IOCTL_AGP_INFO, compat_drm_agp_info),DRM_IOCTL32_DEF(DRM_IOCTL_AGP_ALLOC, compat_drm_agp_alloc),DRM_IOCTL32_DEF(DRM_IOCTL_AGP_FREE, compat_drm_agp_free),DRM_IOCTL32_DEF(DRM_IOCTL_AGP_BIND, compat_drm_agp_bind),DRM_IOCTL32_DEF(DRM_IOCTL_AGP_UNBIND, compat_drm_agp_unbind),#endif#endif#if IS_ENABLED(CONFIG_DRM_LEGACY)DRM_IOCTL32_DEF(DRM_IOCTL_SG_ALLOC, compat_drm_sg_alloc),DRM_IOCTL32_DEF(DRM_IOCTL_SG_FREE, compat_drm_sg_free),#endif#if defined(CONFIG_X86) || defined(CONFIG_IA64)DRM_IOCTL32_DEF(DRM_IOCTL_UPDATE_DRAW, compat_drm_update_draw),#endifDRM_IOCTL32_DEF(DRM_IOCTL_WAIT_VBLANK, compat_drm_wait_vblank),#if defined(CONFIG_X86) || defined(CONFIG_IA64)DRM_IOCTL32_DEF(DRM_IOCTL_MODE_ADDFB2, compat_drm_mode_addfb2),#endif};     drm_compat_ioctl - 32bit IOCTL compatibility handler for DRM drivers   @filp: file this ioctl is called on   @cmd: ioctl cmd number   @arg: user argument     Compatibility handler for 32 bit userspace running on 64 kernels. All actual   IOCTL handling is forwarded to drm_ioctl(), while marshalling structures as   appropriate. Note that this only handles DRM core IOCTLs, if the driver has   botched IOCTL itself, it must handle those by wrapping this function.     Returns:   Zero on success, negative error code on failure. ", "void__drm_atomic_helper_crtc_state_reset(struct drm_crtc_state *crtc_state,     struct drm_crtc *crtc)": "__drm_atomic_helper_crtc_state_reset - reset the CRTC state   @crtc_state: atomic CRTC state, must not be NULL   @crtc: CRTC object, must not be NULL     Initializes the newly allocated @crtc_state with default   values. This is useful for drivers that subclass the CRTC state. ", "void__drm_atomic_helper_crtc_reset(struct drm_crtc *crtc,       struct drm_crtc_state *crtc_state)": "drm_atomic_helper_crtc_reset - reset state on CRTC   @crtc: drm CRTC   @crtc_state: CRTC state to assign     Initializes the newly allocated @crtc_state and assigns it to   the &drm_crtc->state pointer of @crtc, usually required when   initializing the drivers or when called from the &drm_crtc_funcs.reset   hook.     This is useful for drivers that subclass the CRTC state. ", "void __drm_atomic_helper_crtc_duplicate_state(struct drm_crtc *crtc,      struct drm_crtc_state *state)": "drm_atomic_helper_crtc_duplicate_state - copy atomic CRTC state   @crtc: CRTC object   @state: atomic CRTC state     Copies atomic state from a CRTC's current state and resets inferred values.   This is useful for drivers that subclass the CRTC state. ", "void __drm_atomic_helper_crtc_destroy_state(struct drm_crtc_state *state)": "drm_atomic_helper_crtc_destroy_state - release CRTC state   @state: CRTC state object to release     Releases all resources stored in the CRTC state without actually freeing   the memory of the CRTC state. This is useful for drivers that subclass the   CRTC state. ", "void __drm_atomic_helper_plane_state_reset(struct drm_plane_state *plane_state,   struct drm_plane *plane)": "__drm_atomic_helper_plane_state_reset - resets plane state to default values   @plane_state: atomic plane state, must not be NULL   @plane: plane object, must not be NULL     Initializes the newly allocated @plane_state with default   values. This is useful for drivers that subclass the CRTC state. ", "void __drm_atomic_helper_plane_reset(struct drm_plane *plane,     struct drm_plane_state *plane_state)": "drm_atomic_helper_plane_reset - reset state on plane   @plane: drm plane   @plane_state: plane state to assign     Initializes the newly allocated @plane_state and assigns it to   the &drm_crtc->state pointer of @plane, usually required when   initializing the drivers or when called from the &drm_plane_funcs.reset   hook.     This is useful for drivers that subclass the plane state. ", "void __drm_atomic_helper_plane_duplicate_state(struct drm_plane *plane,       struct drm_plane_state *state)": "drm_atomic_helper_plane_destroy_state(plane->state);kfree(plane->state);plane->state = kzalloc(sizeof( plane->state), GFP_KERNEL);if (plane->state)__drm_atomic_helper_plane_reset(plane, plane->state);}EXPORT_SYMBOL(drm_atomic_helper_plane_reset);     __drm_atomic_helper_plane_duplicate_state - copy atomic plane state   @plane: plane object   @state: atomic plane state     Copies atomic state from a plane's current state. This is useful for   drivers that subclass the plane state. ", "void__drm_atomic_helper_connector_state_reset(struct drm_connector_state *conn_state,  struct drm_connector *connector)": "__drm_atomic_helper_connector_state_reset - reset the connector state   @conn_state: atomic connector state, must not be NULL   @connector: connectotr object, must not be NULL     Initializes the newly allocated @conn_state with default   values. This is useful for drivers that subclass the connector state. ", "void__drm_atomic_helper_connector_reset(struct drm_connector *connector,    struct drm_connector_state *conn_state)": "drm_atomic_helper_connector_reset - reset state on connector   @connector: drm connector   @conn_state: connector state to assign     Initializes the newly allocated @conn_state and assigns it to   the &drm_connector->state pointer of @connector, usually required when   initializing the drivers or when called from the &drm_connector_funcs.reset   hook.     This is useful for drivers that subclass the connector state. ", "void drm_atomic_helper_connector_tv_margins_reset(struct drm_connector *connector)": "drm_atomic_helper_connector_destroy_state(connector->state);kfree(connector->state);__drm_atomic_helper_connector_reset(connector, conn_state);}EXPORT_SYMBOL(drm_atomic_helper_connector_reset);     drm_atomic_helper_connector_tv_margins_reset - Resets TV connector properties   @connector: DRM connector     Resets the TV-related properties attached to a connector. ", "void drm_atomic_helper_connector_tv_reset(struct drm_connector *connector)": "drm_atomic_helper_connector_tv_reset - Resets Analog TV connector properties   @connector: DRM connector     Resets the analog TV properties attached to a connector ", "int drm_atomic_helper_connector_tv_check(struct drm_connector *connector, struct drm_atomic_state *state)": "drm_atomic_helper_connector_tv_check - Validate an analog TV connector state   @connector: DRM Connector   @state: the DRM State object     Checks the state object to see if the requested state is valid for an   analog TV connector.     Return:   %0 for success, a negative error code on error. ", "void__drm_atomic_helper_connector_duplicate_state(struct drm_connector *connector,    struct drm_connector_state *state)": "drm_atomic_helper_connector_duplicate_state - copy atomic connector state   @connector: connector object   @state: atomic connector state     Copies atomic state from a connector's current state. This is useful for   drivers that subclass the connector state. ", "void __drm_atomic_helper_private_obj_duplicate_state(struct drm_private_obj *obj,     struct drm_private_state *state)": "__drm_atomic_helper_private_obj_duplicate_state - copy atomic private state   @obj: CRTC object   @state: new private object state     Copies atomic state from a private objects's current state and resets inferred values.   This is useful for drivers that subclass the private state. ", "void __drm_atomic_helper_bridge_duplicate_state(struct drm_bridge *bridge,struct drm_bridge_state *state)": "drm_atomic_helper_bridge_duplicate_state() - Copy atomic bridge state   @bridge: bridge object   @state: atomic bridge state     Copies atomic state from a bridge's current state and resets inferred values.   This is useful for drivers that subclass the bridge state. ", "void drm_atomic_helper_bridge_destroy_state(struct drm_bridge *bridge,    struct drm_bridge_state *state)": "drm_atomic_helper_bridge_reset() or   &drm_atomic_helper_bridge_duplicate_state(). This helper is meant to be   used as a bridge &drm_bridge_funcs.atomic_destroy_state hook for bridges   that don't subclass the bridge state. ", "void __drm_atomic_helper_bridge_reset(struct drm_bridge *bridge,      struct drm_bridge_state *state)": "__drm_atomic_helper_bridge_reset() - Initialize a bridge state to its  default   @bridge: the bridge this state refers to   @state: bridge state to initialize     Initializes the bridge state to default values. This is meant to be called   by the bridge &drm_bridge_funcs.atomic_reset hook for bridges that subclass   the bridge state. ", "int drm_legacy_pci_init(const struct drm_driver *driver,struct pci_driver *pdriver)": "drm_legacy_pci_init - shadow-attach a legacy DRM PCI driver   @driver: DRM device driver   @pdriver: PCI device driver     This is only used by legacy dri1 drivers and deprecated.     Return: 0 on success or a negative error code on failure. ", "void drm_legacy_pci_exit(const struct drm_driver *driver, struct pci_driver *pdriver)": "drm_legacy_pci_exit - unregister shadow-attach legacy DRM driver   @driver: DRM device driver   @pdriver: PCI device driver     Unregister a DRM driver shadow-attached through drm_legacy_pci_init(). This   is deprecated and only used by dri1 drivers. ", "void drm_atomic_helper_check_plane_damage(struct drm_atomic_state *state,  struct drm_plane_state *plane_state)": "drm_atomic_helper_check_plane_damage - Verify plane damage on atomic_check.   @state: The driver state object.   @plane_state: Plane state for which to verify damage.     This helper function makes sure that damage from plane state is discarded   for full modeset. If there are more reasons a driver would want to do a full   plane update rather than processing individual damage regions, then those   cases should be taken care of here.     Note that &drm_plane_state.fb_damage_clips == NULL in plane state means that   full plane update should happen. It also ensure helper iterator will return   &drm_plane_state.src as damage. ", "int drm_atomic_helper_dirtyfb(struct drm_framebuffer *fb,      struct drm_file *file_priv, unsigned int flags,      unsigned int color, struct drm_clip_rect *clips,      unsigned int num_clips)": "drm_atomic_helper_dirtyfb - Helper for dirtyfb.   @fb: DRM framebuffer.   @file_priv: Drm file for the ioctl call.   @flags: Dirty fb annotate flags.   @color: Color for annotate fill.   @clips: Dirty region.   @num_clips: Count of clip in clips.     A helper to implement &drm_framebuffer_funcs.dirty using damage interface   during plane update. If num_clips is 0 then this helper will do a full plane   update. This is the same behaviour expected by DIRTFB IOCTL.     Note that this helper is blocking implementation. This is what current   drivers and userspace expect in their DIRTYFB IOCTL implementation, as a way   to rate-limit userspace and make sure its rendering doesn't get ahead of   uploading new data too much.     Return: Zero on success, negative errno on failure. ", "voiddrm_atomic_helper_damage_iter_init(struct drm_atomic_helper_damage_iter *iter,   const struct drm_plane_state *old_state,   const struct drm_plane_state *state)": "drm_atomic_helper_damage_iter_init - Initialize the damage iterator.   @iter: The iterator to initialize.   @old_state: Old plane state for validation.   @state: Plane state from which to iterate the damage clips.     Initialize an iterator, which clips plane damage   &drm_plane_state.fb_damage_clips to plane &drm_plane_state.src. This iterator   returns full plane src in case damage is not present because either   user-space didn't sent or driver discarded it (it want to do full plane   update). Currently this iterator returns full plane src in case plane src   changed but that can be changed in future to return damage.     For the case when plane is not visible or plane update should not happen the   first call to iter_next will return false. Note that this helper use clipped   &drm_plane_state.src, so driver calling this helper should have called   drm_atomic_helper_check_plane_state() earlier. ", "booldrm_atomic_helper_damage_iter_next(struct drm_atomic_helper_damage_iter *iter,   struct drm_rect *rect)": "drm_atomic_helper_damage_iter_next - Advance the damage iterator.   @iter: The iterator to advance.   @rect: Return a rectangle in fb coordinate clipped to plane src.     Since plane src is in 16.16 fixed point and damage clips are whole number,   this iterator round off clips that intersect with plane src. Round down for   x1y1 and round up for x2y2 for the intersected coordinate. Similar rounding   off for full plane src, in case it's returned as damage. This iterator will   skip damage clips outside of plane src.     Return: True if the output is valid, false if reached the end.     If the first call to iterator next returns false then it means no need to   update the plane. ", "bool drm_atomic_helper_damage_merged(const struct drm_plane_state *old_state,     struct drm_plane_state *state,     struct drm_rect *rect)": "drm_atomic_helper_damage_merged - Merged plane damage   @old_state: Old plane state for validation.   @state: Plane state from which to iterate the damage clips.   @rect: Returns the merged damage rectangle     This function merges any valid plane damage clips into one rectangle and   returns it in @rect.     For details see: drm_atomic_helper_damage_iter_init() and   drm_atomic_helper_damage_iter_next().     Returns:   True if there is valid plane damage otherwise false. ", "/** * devm_aperture_acquire_from_firmware - Acquires ownership of a firmware framebuffer *                                       on behalf of a DRM driver. * @dev:the DRM device to own the framebuffer memory * @base:the framebuffer's byte offset in physical memory * @size:the framebuffer size in bytes * * Installs the given device as the new owner of the framebuffer. The function * expects the framebuffer to be provided by a platform device that has been * set up by firmware. Firmware can be any generic interface, such as EFI, * VESA, VGA, etc. If the native hardware driver takes over ownership of the * framebuffer range, the firmware state gets lost. Aperture helpers will then * unregister the platform device automatically. Acquired apertures are * released automatically if the underlying device goes away. * * The function fails if the framebuffer range, or parts of it, is currently * owned by another driver. To evict current owners, callers should use * drm_aperture_remove_conflicting_framebuffers() et al. before calling this * function. The function also fails if the given device is not a platform * device. * * Returns: * 0 on success, or a negative errno value otherwise. ": "drm_aperture_remove_conflicting_pci_framebuffers() and let it detect the   framebuffer apertures automatically. Device drivers without knowledge of   the framebuffer's location shall call drm_aperture_remove_framebuffers(),   which removes all drivers for known framebuffer.     Drivers that are susceptible to being removed by other drivers, such as   generic EFI or VESA drivers, have to register themselves as owners of their   given framebuffer memory. Ownership of the framebuffer memory is achieved   by calling devm_aperture_acquire_from_firmware(). On success, the driver   is the owner of the framebuffer range. The function fails if the   framebuffer is already owned by another driver. See below for an example.     .. code-block:: c    static int acquire_framebuffers(struct drm_device  dev, struct platform_device  pdev)  {  resource_size_t base, size;    mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);  if (!mem)  return -EINVAL;  base = mem->start;  size = resource_size(mem);    return devm_acquire_aperture_from_firmware(dev, base, size);  }    static int probe(struct platform_device  pdev)  {  struct drm_device  dev;  int ret;     ... Initialize the device...  dev = devm_drm_dev_alloc();  ...     ... and acquire ownership of the framebuffer.  ret = acquire_framebuffers(dev, pdev);  if (ret)  return ret;    drm_dev_register(dev, 0);    return 0;  }     The generic driver is now subject to forced removal by other drivers. This   only works for platform drivers that support hot unplug.   When a driver calls drm_aperture_remove_conflicting_framebuffers() et al.   for the registered framebuffer range, the aperture helpers call   platform_device_unregister() and the generic driver unloads itself. It   may not access the device's registers, framebuffer memory, ROM, etc   afterwards. ", "void drm_gem_dma_print_info(const struct drm_gem_dma_object *dma_obj,    struct drm_printer *p, unsigned int indent)": "drm_gem_dma_print_info() - Print &drm_gem_dma_object info for debugfs   @dma_obj: DMA GEM object   @p: DRM printer   @indent: Tab indentation level     This function prints dma_addr and vaddr for use in e.g. debugfs output. ", "struct drm_gem_object *drm_gem_dma_prime_import_sg_table_vmap(struct drm_device *dev,       struct dma_buf_attachment *attach,       struct sg_table *sgt)": "drm_gem_dma_prime_import_sg_table_vmap - PRIME import another driver's  scattergather table and get the virtual address of the buffer   @dev: DRM device   @attach: DMA-BUF attachment   @sgt: Scattergather table of pinned pages     This function imports a scattergather table using   drm_gem_dma_prime_import_sg_table() and uses dma_buf_vmap() to get the kernel   virtual address. This ensures that a DMA GEM object always has its virtual   address set. This address is released when the object is freed.     This function can be used as the &drm_driver.gem_prime_import_sg_table   callback. The &DRM_GEM_DMA_DRIVER_OPS_VMAP macro provides a shortcut to set   the necessary DRM driver operations.     Returns:   A pointer to a newly created GEM object or an ERR_PTR-encoded negative   error code on failure. ", "struct drm_property *drm_property_create(struct drm_device *dev, u32 flags, const char *name, int num_values)": "drm_property_destroy(), which is done automatically when   calling drm_mode_config_cleanup().     Returns:   A pointer to the newly created property on success, NULL on failure. ", "struct drm_property *drm_property_create_enum(struct drm_device *dev,      u32 flags, const char *name,      const struct drm_prop_enum_list *props,      int num_values)": "drm_property_create_enum - create a new enumeration property type   @dev: drm device   @flags: flags specifying the property type   @name: name of the property   @props: enumeration lists with property values   @num_values: number of pre-defined values     This creates a new generic drm property which can then be attached to a drm   object with drm_object_attach_property(). The returned property object must   be freed with drm_property_destroy(), which is done automatically when   calling drm_mode_config_cleanup().     Userspace is only allowed to set one of the predefined values for enumeration   properties.     Returns:   A pointer to the newly created property on success, NULL on failure. ", "struct drm_property *drm_property_create_bitmask(struct drm_device *dev, u32 flags, const char *name, const struct drm_prop_enum_list *props, int num_props, uint64_t supported_bits)": "drm_property_add_enum(property,    props[i].type,    props[i].name);if (ret) {drm_property_destroy(dev, property);return NULL;}}return property;}EXPORT_SYMBOL(drm_property_create_enum);     drm_property_create_bitmask - create a new bitmask property type   @dev: drm device   @flags: flags specifying the property type   @name: name of the property   @props: enumeration lists with property bitflags   @num_props: size of the @props array   @supported_bits: bitmask of all supported enumeration values     This creates a new bitmask drm property which can then be attached to a drm   object with drm_object_attach_property(). The returned property object must   be freed with drm_property_destroy(), which is done automatically when   calling drm_mode_config_cleanup().     Compared to plain enumeration properties userspace is allowed to set any   or'ed together combination of the predefined property bitflag values     Returns:   A pointer to the newly created property on success, NULL on failure. ", "struct drm_property *drm_property_create_range(struct drm_device *dev,       u32 flags, const char *name,       uint64_t min, uint64_t max)": "drm_property_create_range - create a new unsigned ranged property type   @dev: drm device   @flags: flags specifying the property type   @name: name of the property   @min: minimum value of the property   @max: maximum value of the property     This creates a new generic drm property which can then be attached to a drm   object with drm_object_attach_property(). The returned property object must   be freed with drm_property_destroy(), which is done automatically when   calling drm_mode_config_cleanup().     Userspace is allowed to set any unsigned integer value in the (min, max)   range inclusive.     Returns:   A pointer to the newly created property on success, NULL on failure. ", "struct drm_property *drm_property_create_signed_range(struct drm_device *dev,      u32 flags, const char *name,      int64_t min, int64_t max)": "drm_property_create_signed_range - create a new signed ranged property type   @dev: drm device   @flags: flags specifying the property type   @name: name of the property   @min: minimum value of the property   @max: maximum value of the property     This creates a new generic drm property which can then be attached to a drm   object with drm_object_attach_property(). The returned property object must   be freed with drm_property_destroy(), which is done automatically when   calling drm_mode_config_cleanup().     Userspace is allowed to set any signed integer value in the (min, max)   range inclusive.     Returns:   A pointer to the newly created property on success, NULL on failure. ", "struct drm_property *drm_property_create_object(struct drm_device *dev,u32 flags, const char *name,uint32_t type)": "drm_property_create_object - create a new object property type   @dev: drm device   @flags: flags specifying the property type   @name: name of the property   @type: object type from DRM_MODE_OBJECT_  defines     This creates a new generic drm property which can then be attached to a drm   object with drm_object_attach_property(). The returned property object must   be freed with drm_property_destroy(), which is done automatically when   calling drm_mode_config_cleanup().     Userspace is only allowed to set this to any property value of the given   @type. Only useful for atomic properties, which is enforced.     Returns:   A pointer to the newly created property on success, NULL on failure. ", "struct drm_property *drm_property_create_bool(struct drm_device *dev,      u32 flags, const char *name)": "drm_property_create_bool - create a new boolean property type   @dev: drm device   @flags: flags specifying the property type   @name: name of the property     This creates a new generic drm property which can then be attached to a drm   object with drm_object_attach_property(). The returned property object must   be freed with drm_property_destroy(), which is done automatically when   calling drm_mode_config_cleanup().     This is implemented as a ranged property with only {0, 1} as valid values.     Returns:   A pointer to the newly created property on success, NULL on failure. ", "struct drm_property_blob *drm_property_create_blob(struct drm_device *dev, size_t length, const void *data)": "drm_property_create_blob - Create new blob property   @dev: DRM device to create property for   @length: Length to allocate for blob data   @data: If specified, copies data into blob     Creates a new blob property for a specified DRM device, optionally   copying data. Note that blob properties are meant to be invariant, hence the   data must be filled out before the blob is used as the value of any property.     Returns:   New blob property with a single reference on success, or an ERR_PTR   value on failure. ", "void drm_property_blob_put(struct drm_property_blob *blob)": "drm_property_blob_put - release a blob property reference   @blob: DRM blob property     Releases a reference to a blob property. May free the object. ", "struct drm_property_blob *drm_property_blob_get(struct drm_property_blob *blob)": "drm_property_blob_get - acquire blob property reference   @blob: DRM blob property     Acquires a reference to an existing blob property. Returns @blob, which   allows this to be used as a shorthand in assignments. ", "struct drm_property_blob *drm_property_lookup_blob(struct drm_device *dev,           uint32_t id)": "drm_property_lookup_blob - look up a blob property and take a reference   @dev: drm device   @id: id of the blob property     If successful, this takes an additional reference to the blob property.   callers need to make sure to eventually unreferenced the returned property   again, using drm_property_blob_put().     Return:   NULL on failure, pointer to the blob on success. ", "int drm_property_replace_global_blob(struct drm_device *dev,     struct drm_property_blob **replace,     size_t length,     const void *data,     struct drm_mode_object *obj_holds_id,     struct drm_property *prop_holds_id)": "drm_property_replace_global_blob - replace existing blob property   @dev: drm device   @replace: location of blob property pointer to be replaced   @length: length of data for new blob, or 0 for no data   @data: content for new blob, or NULL for no data   @obj_holds_id: optional object for property holding blob ID   @prop_holds_id: optional property holding blob ID   @return 0 on success or error on failure     This function will replace a global property in the blob list, optionally   updating a property which holds the ID of that property.     If length is 0 or data is NULL, no new blob will be created, and the holding   property, if specified, will be set to 0.     Access to the replace pointer is assumed to be protected by the caller, e.g.   by holding the relevant modesetting object lock for its parent.     For example, a drm_connector has a 'PATH' property, which contains the ID   of a blob property with the value of the MST path information. Calling this   function with replace pointing to the connector's path_blob_ptr, length and   data set for the new path information, obj_holds_id set to the connector's   base object, and prop_holds_id set to the path property name, will perform   a completely atomic update. The access to path_blob_ptr is protected by the   caller holding a lock on the connector. ", "bool drm_property_replace_blob(struct drm_property_blob **blob,       struct drm_property_blob *new_blob)": "drm_property_replace_blob - replace a blob property   @blob: a pointer to the member blob to be replaced   @new_blob: the new blob to replace with     Return: true if the blob was in fact replaced. ", "int drm_i2c_encoder_init(struct drm_device *dev, struct drm_encoder_slave *encoder, struct i2c_adapter *adap, const struct i2c_board_info *info)": "drm_i2c_encoder_init - Initialize an I2C slave encoder   @dev:DRM device.   @encoder:    Encoder to be attached to the I2C device. You aren't  required to have called drm_encoder_init() before.   @adap:I2C adapter that will be used to communicate with  the device.   @info:Information that will be used to create the I2C device.  Required fields are @addr and @type.     Create an I2C device on the specified bus (the module containing its   driver is transparently loaded) and attach it to the specified   &drm_encoder_slave. The @slave_funcs field will be initialized with   the hooks provided by the slave driver.     If @info.platform_data is non-NULL it will be used as the initial   slave config.     Returns 0 on success or a negative errno on failure, in particular,   -ENODEV is returned when no matching driver is found. ", "void drm_i2c_encoder_destroy(struct drm_encoder *drm_encoder)": "drm_i2c_encoder_destroy - Unregister the I2C device backing an encoder   @drm_encoder:Encoder to be unregistered.     This should be called from the @destroy method of an I2C slave   encoder driver once I2C access is no longer needed. ", "/** * drm_plane_create_alpha_property - create a new alpha property * @plane: drm plane * * This function creates a generic, mutable, alpha property and enables support * for it in the DRM core. It is attached to @plane. * * The alpha property will be allowed to be within the bounds of 0 * (transparent) to 0xffff (opaque). * * Returns: * 0 on success, negative error code on failure. ": "drm_plane_create_blend_mode_property().  It adds a blend mode for alpha blending equation selection, describing  how the pixels from the current plane are composited with the  background.     Three alpha blending equations are defined:     \"None\":   Blend formula that ignores the pixel alpha::     out.rgb = plane_alpha   fg.rgb +   (1 - plane_alpha)   bg.rgb     \"Pre-multiplied\":   Blend formula that assumes the pixel color values   have been already pre-multiplied with the alpha   channel values::     out.rgb = plane_alpha   fg.rgb +   (1 - (plane_alpha   fg.alpha))   bg.rgb     \"Coverage\":   Blend formula that assumes the pixel color values have not   been pre-multiplied and will do so when blending them to the   background color values::     out.rgb = plane_alpha   fg.alpha   fg.rgb +   (1 - (plane_alpha   fg.alpha))   bg.rgb     Using the following symbols:     \"fg.rgb\":   Each of the RGB component values from the plane's pixel   \"fg.alpha\":   Alpha component value from the plane's pixel. If the plane's   pixel format has no alpha component, then this is assumed to be   1.0. In these cases, this property has no effect, as all three   equations become equivalent.   \"bg.rgb\":   Each of the RGB component values from the background   \"plane_alpha\":   Plane alpha value set by the plane \"alpha\" property. If the   plane does not expose the \"alpha\" property, then this is   assumed to be 1.0     Note that all the property extensions described here apply either to the   plane or the CRTC (e.g. for the background color, which currently is not   exposed and assumed to be black).     SCALING_FILTER:       Indicates scaling filter to be used for plane scaler         The value of this property can be one of the following:         Default:               Driver's default scaling filter       Nearest Neighbor:               Nearest Neighbor scaling filter     Drivers can set up this property for a plane by calling   drm_plane_create_scaling_filter_property ", "int drm_plane_create_rotation_property(struct drm_plane *plane,       unsigned int rotation,       unsigned int supported_rotations)": "drm_rotation_simplify() to normalize values of this property.     The property exposed to userspace is a bitmask property (see   drm_property_create_bitmask()) called \"rotation\" and has the following   bitmask enumaration values:     DRM_MODE_ROTATE_0:   \"rotate-0\"   DRM_MODE_ROTATE_90:   \"rotate-90\"   DRM_MODE_ROTATE_180:   \"rotate-180\"   DRM_MODE_ROTATE_270:   \"rotate-270\"   DRM_MODE_REFLECT_X:   \"reflect-x\"   DRM_MODE_REFLECT_Y:   \"reflect-y\"     Rotation is the specified amount in degrees in counter clockwise direction,   the X and Y axis are within the source rectangle, i.e.  the XY axis before   rotation. After reflection, the rotation is applied to the image sampled from   the source rectangle, before scaling it to fit the destination rectangle. ", "int drm_plane_create_zpos_property(struct drm_plane *plane,   unsigned int zpos,   unsigned int min, unsigned int max)": "drm_atomic_normalize_zpos() helper during their implementation of   &drm_mode_config_funcs.atomic_check(), which will update the normalized zpos   values and store them in &drm_plane_state.normalized_zpos. Usually min   should be set to 0 and max to maximal number of planes for given crtc - 1.     If zpos of some planes cannot be changed (like fixed background or   cursortopmost planes), drivers shall adjust the minmax values and assign   those planes immutable zpos properties with lower or higher values (for more   information, see drm_plane_create_zpos_immutable_property() function). In such   case drivers shall also assign proper initial zpos values for all planes in   its plane_reset() callback, so the planes will be always sorted properly.     See also drm_atomic_normalize_zpos().     The property exposed to userspace is called \"zpos\".     Returns:   Zero on success, negative errno on failure. ", "void drm_helper_move_panel_connectors_to_head(struct drm_device *dev)": "drm_helper_move_panel_connectors_to_head() - move panels to the front in the   connector list   @dev: drm device to operate on     Some userspace presumes that the first connected connector is the main   display, where it's supposed to display e.g. the login screen. For   laptops, this should be the main panel. Use this function to sort all   (eDPLVDSDSI) panels to the front of the connector list, instead of   painstakingly trying to initialize them in the right order. ", "void drm_helper_mode_fill_fb_struct(struct drm_device *dev,    struct drm_framebuffer *fb,    const struct drm_mode_fb_cmd2 *mode_cmd)": "drm_helper_mode_fill_fb_struct - fill out framebuffer metadata   @dev: DRM device   @fb: drm_framebuffer object to fill out   @mode_cmd: metadata from the userspace fb creation request     This helper can be used in a drivers fb_create callback to pre-fill the fb's   metadata fields. ", "int drm_crtc_init(struct drm_device *dev, struct drm_crtc *crtc,  const struct drm_crtc_funcs *funcs)": "drm_crtc_init - Legacy CRTC initialization function   @dev: DRM device   @crtc: CRTC object to init   @funcs: callbacks for the new CRTC     Initialize a CRTC object with a default helper-provided primary plane and no   cursor plane.     Note that we make some assumptions about hardware limitations that may not be   true for all hardware:     1. Primary plane cannot be repositioned.   2. Primary plane cannot be scaled.   3. Primary plane must cover the entire CRTC.   4. Subpixel positioning is not supported.   5. The primary plane must always be on if the CRTC is enabled.     This is purely a backwards compatibility helper for old drivers. Drivers   should instead implement their own primary plane. Atomic drivers must do so.   Drivers with the above hardware restriction can look into using &struct   drm_simple_display_pipe, which encapsulates the above limitations into a nice   interface.     Returns:   Zero on success, error code on failure. ", "int drm_mode_config_helper_suspend(struct drm_device *dev)": "drm_mode_config_helper_suspend - Modeset suspend helper   @dev: DRM device     This helper function takes care of suspending the modeset side. It disables   output polling if initialized, suspends fbdev if used and finally calls   drm_atomic_helper_suspend().   If suspending fails, fbdev and polling is re-enabled.     Returns:   Zero on success, negative error code on error.     See also:   drm_kms_helper_poll_disable() and drm_fb_helper_set_suspend_unlocked(). ", "int drm_mode_config_helper_resume(struct drm_device *dev)": "drm_mode_config_helper_resume - Modeset resume helper   @dev: DRM device     This helper function takes care of resuming the modeset side. It calls   drm_atomic_helper_resume(), resumes fbdev if used and enables output polling   if initiaized.     Returns:   Zero on success, negative error code on error.     See also:   drm_fb_helper_set_suspend_unlocked() and drm_kms_helper_poll_enable(). ", "int mipi_dbi_command_read(struct mipi_dbi *dbi, u8 cmd, u8 *val)": "mipi_dbi_command_read - MIPI DCS read command   @dbi: MIPI DBI structure   @cmd: Command   @val: Value read     Send MIPI DCS read command to the controller.     Returns:   Zero on success, negative error code on failure. ", "int mipi_dbi_command_buf(struct mipi_dbi *dbi, u8 cmd, u8 *data, size_t len)": "mipi_dbi_command_buf(dbi, cmd, val, 1);}EXPORT_SYMBOL(mipi_dbi_command_read);     mipi_dbi_command_buf - MIPI DCS command with parameter(s) in an array   @dbi: MIPI DBI structure   @cmd: Command   @data: Parameter buffer   @len: Buffer length     Returns:   Zero on success, negative error code on failure. ", "int mipi_dbi_buf_copy(void *dst, struct iosys_map *src, struct drm_framebuffer *fb,      struct drm_rect *clip, bool swap)": "mipi_dbi_buf_copy - Copy a framebuffer, transforming it if necessary   @dst: The destination buffer   @src: The source buffer   @fb: The source framebuffer   @clip: Clipping rectangle of the area to be copied   @swap: When true, swap MSBLSB of 16-bit values     Returns:   Zero on success, negative error code on failure. ", "enum drm_mode_status mipi_dbi_pipe_mode_valid(struct drm_simple_display_pipe *pipe,      const struct drm_display_mode *mode)": "mipi_dbi_pipe_mode_valid - MIPI DBI mode-valid helper   @pipe: Simple display pipe   @mode: The mode to test     This function validates a given display mode against the MIPI DBI's hardware   display. Drivers can use this as their &drm_simple_display_pipe_funcs->mode_valid   callback. ", "void mipi_dbi_pipe_update(struct drm_simple_display_pipe *pipe,  struct drm_plane_state *old_state)": "mipi_dbi_pipe_update - Display pipe update helper   @pipe: Simple display pipe   @old_state: Old plane state     This function handles framebuffer flushing and vblank events. Drivers can use   this as their &drm_simple_display_pipe_funcs->update callback. ", "void mipi_dbi_enable_flush(struct mipi_dbi_dev *dbidev,   struct drm_crtc_state *crtc_state,   struct drm_plane_state *plane_state)": "mipi_dbi_enable_flush - MIPI DBI enable helper   @dbidev: MIPI DBI device structure   @crtc_state: CRTC state   @plane_state: Plane state     Flushes the whole framebuffer and enables the backlight. Drivers can use this   in their &drm_simple_display_pipe_funcs->enable callback.     Note: Drivers which don't use mipi_dbi_pipe_update() because they have custom   framebuffer flushing, can't use this function since they both use the same   flushing code. ", "void mipi_dbi_pipe_disable(struct drm_simple_display_pipe *pipe)": "mipi_dbi_pipe_disable - MIPI DBI pipe disable helper   @pipe: Display pipe     This function disables backlight if present, if not the display memory is   blanked. The regulator is disabled if in use. Drivers can use this as their   &drm_simple_display_pipe_funcs->disable callback. ", "int mipi_dbi_pipe_begin_fb_access(struct drm_simple_display_pipe *pipe,  struct drm_plane_state *plane_state)": "mipi_dbi_pipe_begin_fb_access - MIPI DBI pipe begin-access helper   @pipe: Display pipe   @plane_state: Plane state     This function implements struct &drm_simple_display_funcs.begin_fb_access.     See drm_gem_begin_shadow_fb_access() for details and mipi_dbi_pipe_cleanup_fb()   for cleanup.     Returns:   0 on success, or a negative errno code otherwise. ", "void mipi_dbi_pipe_end_fb_access(struct drm_simple_display_pipe *pipe, struct drm_plane_state *plane_state)": "mipi_dbi_pipe_end_fb_access - MIPI DBI pipe end-access helper   @pipe: Display pipe   @plane_state: Plane state     This function implements struct &drm_simple_display_funcs.end_fb_access.     See mipi_dbi_pipe_begin_fb_access(). ", "void mipi_dbi_pipe_reset_plane(struct drm_simple_display_pipe *pipe)": "mipi_dbi_pipe_reset_plane - MIPI DBI plane-reset helper   @pipe: Display pipe     This function implements struct &drm_simple_display_funcs.reset_plane   for MIPI DBI planes. ", "struct drm_plane_state *mipi_dbi_pipe_duplicate_plane_state(struct drm_simple_display_pipe *pipe)": "mipi_dbi_pipe_duplicate_plane_state - duplicates MIPI DBI plane state   @pipe: Display pipe     This function implements struct &drm_simple_display_funcs.duplicate_plane_state   for MIPI DBI planes.     See drm_gem_duplicate_shadow_plane_state() for additional details.     Returns:   A pointer to a new plane state on success, or NULL otherwise. ", "void mipi_dbi_pipe_destroy_plane_state(struct drm_simple_display_pipe *pipe,       struct drm_plane_state *plane_state)": "mipi_dbi_pipe_destroy_plane_state - cleans up MIPI DBI plane state   @pipe: Display pipe   @plane_state: Plane state     This function implements struct drm_simple_display_funcs.destroy_plane_state   for MIPI DBI planes.     See drm_gem_destroy_shadow_plane_state() for additional details. ", "int mipi_dbi_dev_init_with_formats(struct mipi_dbi_dev *dbidev,   const struct drm_simple_display_pipe_funcs *funcs,   const uint32_t *formats, unsigned int format_count,   const struct drm_display_mode *mode,   unsigned int rotation, size_t tx_buf_size)": "mipi_dbi_dev_init_with_formats - MIPI DBI device initialization with custom formats   @dbidev: MIPI DBI device structure to initialize   @funcs: Display pipe functions   @formats: Array of supported formats (DRM_FORMAT\\_\\ ).   @format_count: Number of elements in @formats   @mode: Display mode   @rotation: Initial rotation in degrees Counter Clock Wise   @tx_buf_size: Allocate a transmit buffer of this size.     This function sets up a &drm_simple_display_pipe with a &drm_connector that   has one fixed &drm_display_mode which is rotated according to @rotation.   This mode is used to set the mode config minmax widthheight properties.     Use mipi_dbi_dev_init() if you don't need custom formats.     Note:   Some of the helper functions expects RGB565 to be the default format and the   transmit buffer sized to fit that.     Returns:   Zero on success, negative error code on failure. ", "void mipi_dbi_hw_reset(struct mipi_dbi *dbi)": "mipi_dbi_hw_reset - Hardware reset of controller   @dbi: MIPI DBI structure     Reset controller if the &mipi_dbi->reset gpio is set. ", "bool mipi_dbi_display_is_on(struct mipi_dbi *dbi)": "mipi_dbi_display_is_on - Check if display is on   @dbi: MIPI DBI structure     This function checks the Power Mode register (if readable) to see if   display output is turned on. This can be used to see if the bootloader   has already turned on the display avoiding flicker when the pipeline is   enabled.     Returns:   true if the display can be verified to be on, false otherwise. ", "if (dbi->reset)usleep_range(5000, 20000);elsemsleep(120);return 0;}/** * mipi_dbi_poweron_reset - MIPI DBI poweron and reset * @dbidev: MIPI DBI device structure * * This function enables the regulator if used and does a hardware and software * reset. * * Returns: * Zero on success, or a negative error code. ": "mipi_dbi_poweron_reset_conditional(struct mipi_dbi_dev  dbidev, bool cond){struct device  dev = dbidev->drm.dev;struct mipi_dbi  dbi = &dbidev->dbi;int ret;if (dbidev->regulator) {ret = regulator_enable(dbidev->regulator);if (ret) {DRM_DEV_ERROR(dev, \"Failed to enable regulator (%d)\\n\", ret);return ret;}}if (dbidev->io_regulator) {ret = regulator_enable(dbidev->io_regulator);if (ret) {DRM_DEV_ERROR(dev, \"Failed to enable IO regulator (%d)\\n\", ret);if (dbidev->regulator)regulator_disable(dbidev->regulator);return ret;}}if (cond && mipi_dbi_display_is_on(dbi))return 1;mipi_dbi_hw_reset(dbi);ret = mipi_dbi_command(dbi, MIPI_DCS_SOFT_RESET);if (ret) {DRM_DEV_ERROR(dev, \"Failed to send reset command (%d)\\n\", ret);if (dbidev->regulator)regulator_disable(dbidev->regulator);if (dbidev->io_regulator)regulator_disable(dbidev->io_regulator);return ret;}    If we did a hw reset, we know the controller is in Sleep mode and   per MIPI DSC spec should wait 5ms after soft reset. If we didn't,   we assume worst case and wait 120ms. ", "int mipi_dbi_poweron_conditional_reset(struct mipi_dbi_dev *dbidev)": "mipi_dbi_poweron_conditional_reset - MIPI DBI poweron and conditional reset   @dbidev: MIPI DBI device structure     This function enables the regulator if used and if the display is off, it   does a hardware and software reset. If mipi_dbi_display_is_on() determines   that the display is on, no reset is performed.     Returns:   Zero if the controller was reset, 1 if the display was already on, or a   negative error code. ", "u32 mipi_dbi_spi_cmd_max_speed(struct spi_device *spi, size_t len)": "mipi_dbi_spi_cmd_max_speed - get the maximum SPI bus speed   @spi: SPI device   @len: The transfer buffer length.     Many controllers have a max speed of 10MHz, but can be pushed way beyond   that. Increase reliability by running pixel data at max speed and the rest   at 10MHz, preventing transfer glitches from messing up the init settings. ", "#define MIPI_DBI_DEBUG_COMMAND(cmd, data, len) \\(": "mipi_dbi_spi_init(). ", "int mipi_dbi_spi_init(struct spi_device *spi, struct mipi_dbi *dbi,      struct gpio_desc *dc)": "mipi_dbi_spi_transfer(spi, speed_hz, 8, cmd, 1);if (ret || !num)return ret;if ( cmd == MIPI_DCS_WRITE_MEMORY_START && !dbi->swap_bytes)bpw = 16;gpiod_set_value_cansleep(dbi->dc, 1);speed_hz = mipi_dbi_spi_cmd_max_speed(spi, num);return mipi_dbi_spi_transfer(spi, speed_hz, bpw, par, num);}     mipi_dbi_spi_init - Initialize MIPI DBI SPI interface   @spi: SPI device   @dbi: MIPI DBI structure to initialize   @dc: DC gpio (optional)     This function sets &mipi_dbi->command, enables &mipi_dbi->read_commands for the   usual read commands. It should be followed by a call to mipi_dbi_dev_init() or   a driver-specific init.     If @dc is set, a Type C Option 3 interface is assumed, if not   Type C Option 1.     If the SPI master driver doesn't support the necessary bits per word,   the following transformation is used:     - 9-bit: reorder buffer as 9x 8-bit words, padded with no-op command.   - 16-bit: if big endian send as 8-bit, if little endian swap bytes     Returns:   Zero on success, negative error code on failure. ", "void mipi_dbi_debugfs_init(struct drm_minor *minor)": "mipi_dbi_debugfs_init - Create debugfs entries   @minor: DRM minor     This function creates a 'command' debugfs file for sending commands to the   controller or getting the read command values.   Drivers can use this as their &drm_driver->debugfs_init callback.   ", "void drm_put_dev(struct drm_device *dev)": "drm_put_dev - Unregister and release a DRM device   @dev: DRM device     Called at module unload time or when a PCI device is unplugged.     Cleans up all DRM device, calling drm_lastclose().     Note: Use of this function is deprecated. It will eventually go away   completely.  Please use drm_dev_unregister() and drm_dev_put() explicitly   instead to make sure that the device isn't userspace accessible any more   while teardown is in progress, ensuring that userspace can't access an   inconsistent state. ", "/** * drm_put_dev - Unregister and release a DRM device * @dev: DRM device * * Called at module unload time or when a PCI device is unplugged. * * Cleans up all DRM device, calling drm_lastclose(). * * Note: Use of this function is deprecated. It will eventually go away * completely.  Please use drm_dev_unregister() and drm_dev_put() explicitly * instead to make sure that the device isn't userspace accessible any more * while teardown is in progress, ensuring that userspace can't access an * inconsistent state. ": "drm_dev_unregister(). Then clean up   any other resources allocated at device initialization and drop the driver's   reference to &drm_device using drm_dev_put().     Note that any allocation or resource which is visible to userspace must be   released only when the final drm_dev_put() is called, and not when the   driver is unbound from the underlying physical struct &device. Best to use   &drm_device managed resources with drmm_add_action(), drmm_kmalloc() and   related functions.     devres managed resources like devm_kmalloc() can only be used for resources   directly related to the underlying hardware device, and only used in code   paths fully protected by drm_dev_enter() and drm_dev_exit().     Display driver example   ~~~~~~~~~~~~~~~~~~~~~~     The following example shows a typical structure of a DRM display driver.   The example focus on the probe() function and the other functions that is   almost always present and serves as a demonstration of devm_drm_dev_alloc().     .. code-block:: c    struct driver_device {  struct drm_device drm;  void  userspace_facing;  struct clk  pclk;  };    static const struct drm_driver driver_drm_driver = {  [...]  };    static int driver_probe(struct platform_device  pdev)  {  struct driver_device  priv;  struct drm_device  drm;  int ret;    priv = devm_drm_dev_alloc(&pdev->dev, &driver_drm_driver,    struct driver_device, drm);  if (IS_ERR(priv))  return PTR_ERR(priv);  drm = &priv->drm;    ret = drmm_mode_config_init(drm);  if (ret)  return ret;    priv->userspace_facing = drmm_kzalloc(..., GFP_KERNEL);  if (!priv->userspace_facing)  return -ENOMEM;    priv->pclk = devm_clk_get(dev, \"PCLK\");  if (IS_ERR(priv->pclk))  return PTR_ERR(priv->pclk);     Further setup, display pipeline etc    platform_set_drvdata(pdev, drm);    drm_mode_config_reset(drm);    ret = drm_dev_register(drm);  if (ret)  return ret;    drm_fbdev_generic_setup(drm, 32);    return 0;  }     This function is called before the devm_ resources are released  static int driver_remove(struct platform_device  pdev)  {  struct drm_device  drm = platform_get_drvdata(pdev);    drm_dev_unregister(drm);  drm_atomic_helper_shutdown(drm)    return 0;  }     This function is called on kernel restart and shutdown  static void driver_shutdown(struct platform_device  pdev)  {  drm_atomic_helper_shutdown(platform_get_drvdata(pdev));  }    static int __maybe_unused driver_pm_suspend(struct device  dev)  {  return drm_mode_config_helper_suspend(dev_get_drvdata(dev));  }    static int __maybe_unused driver_pm_resume(struct device  dev)  {  drm_mode_config_helper_resume(dev_get_drvdata(dev));    return 0;  }    static const struct dev_pm_ops driver_pm_ops = {  SET_SYSTEM_SLEEP_PM_OPS(driver_pm_suspend, driver_pm_resume)  };    static struct platform_driver driver_driver = {  .driver = {  [...]  .pm = &driver_pm_ops,  },  .probe = driver_probe,  .remove = driver_remove,  .shutdown = driver_shutdown,  };  module_platform_driver(driver_driver);     Drivers that want to support device unplugging (USB, DT overlay unload) should   use drm_dev_unplug() instead of drm_dev_unregister(). The driver must protect   regions that is accessing device resources to prevent use after they're   released. This is done using drm_dev_enter() and drm_dev_exit(). There is one   shortcoming however, drm_dev_unplug() marks the drm_device as unplugged before   drm_atomic_helper_shutdown() is called. This means that if the disable code   paths are protected, they will not run on regular driver module unload,   possibly leaving the hardware enabled. ", "void drm_gem_ttm_print_info(struct drm_printer *p, unsigned int indent,    const struct drm_gem_object *gem)": "drm_gem_ttm_print_info() - Print &ttm_buffer_object info for debugfs   @p: DRM printer   @indent: Tab indentation level   @gem: GEM object     This function can be used as &drm_gem_object_funcs.print_info   callback. ", "int drm_gem_ttm_vmap(struct drm_gem_object *gem,     struct iosys_map *map)": "drm_gem_ttm_vmap() - vmap &ttm_buffer_object   @gem: GEM object.   @map: [out] returns the dma-buf mapping.     Maps a GEM object with ttm_bo_vmap(). This function can be used as   &drm_gem_object_funcs.vmap callback.     Returns:   0 on success, or a negative errno code otherwise. ", "void drm_gem_ttm_vunmap(struct drm_gem_object *gem,struct iosys_map *map)": "drm_gem_ttm_vunmap() - vunmap &ttm_buffer_object   @gem: GEM object.   @map: dma-buf mapping.     Unmaps a GEM object with ttm_bo_vunmap(). This function can be used as   &drm_gem_object_funcs.vmap callback. ", "int drm_gem_ttm_mmap(struct drm_gem_object *gem,     struct vm_area_struct *vma)": "drm_gem_ttm_mmap() - mmap &ttm_buffer_object   @gem: GEM object.   @vma: vm area.     This function can be used as &drm_gem_object_funcs.mmap   callback. ", "int drm_gem_ttm_dumb_map_offset(struct drm_file *file, struct drm_device *dev,uint32_t handle, uint64_t *offset)": "drm_gem_ttm_dumb_map_offset() - Implements struct &drm_driver.dumb_map_offset   @file:DRM file pointer.   @dev:DRM device.   @handle:GEM handle   @offset:Returns the mapping's memory offset on success     Provides an implementation of struct &drm_driver.dumb_map_offset for   TTM-based GEM drivers. TTM allocates the offset internally and   drm_gem_ttm_dumb_map_offset() returns it for dumb-buffer implementations.     See struct &drm_driver.dumb_map_offset.     Returns:   0 on success, or a negative errno code otherwise. ", "int drm_framebuffer_check_src_coords(uint32_t src_x, uint32_t src_y,     uint32_t src_w, uint32_t src_h,     const struct drm_framebuffer *fb)": "drm_framebuffer_unregister_private(). But doing this is not   recommended, and it's better to have a normal free-standing &struct   drm_framebuffer. ", "drm_framebuffer_put(fb);/* * we now own the reference that was stored in the fbs list * * drm_framebuffer_remove may fail with -EINTR on pending signals, * so run this in a separate stack as there's no way to correctly * handle this after the fb is already removed from the lookup table. ": "drm_framebuffer_lookup(dev, file_priv, fb_id);if (!fb)return -ENOENT;mutex_lock(&file_priv->fbs_lock);list_for_each_entry(fbl, &file_priv->fbs, filp_head)if (fb == fbl)found = 1;if (!found) {mutex_unlock(&file_priv->fbs_lock);goto fail_unref;}list_del_init(&fb->filp_head);mutex_unlock(&file_priv->fbs_lock);  drop the reference we picked up in framebuffer lookup ", "void drm_framebuffer_cleanup(struct drm_framebuffer *fb)": "drm_framebuffer_cleanup - remove a framebuffer object   @fb: framebuffer to remove     Cleanup framebuffer. This function is intended to be used from the drivers   &drm_framebuffer_funcs.destroy callback. It can also be used to clean up   driver private framebuffers embedded into a larger structure.     Note that this function does not remove the fb from active usage - if it is   still used anywhere, hilarity can ensue since userspace could call getfb on   the id and get back -EINVAL. Obviously no concern at driver unload time.     Also, the framebuffer will not be removed from the lookup idr - for   user-created framebuffers this will happen in the rmfb ioctl. For   driver-private objects (e.g. for fbdev) drivers need to explicitly call   drm_framebuffer_unregister_private. ", "int drm_mode_rmfb(struct drm_device *dev, u32 fb_id,  struct drm_file *file_priv)": "drm_framebuffer_remove(fb);}}     drm_mode_rmfb - remove an FB from the configuration   @dev: drm device   @fb_id: id of framebuffer to remove   @file_priv: drm file     Remove the specified FB.     Called by the user via ioctl, or by an in-kernel client.     Returns:   Zero on success, negative errno on failure. ", "int drm_framebuffer_plane_width(int width,const struct drm_framebuffer *fb, int plane)": "drm_framebuffer_plane_width - width of the plane given the first plane   @width: width of the first plane   @fb: the framebuffer   @plane: plane index     Returns:   The width of @plane, given that the width of the first plane is @width. ", "int drm_framebuffer_plane_height(int height, const struct drm_framebuffer *fb, int plane)": "drm_framebuffer_plane_height - height of the plane given the first plane   @height: height of the first plane   @fb: the framebuffer   @plane: plane index     Returns:   The height of @plane, given that the height of the first plane is @height. ", "struct drm_flip_task *drm_flip_work_allocate_task(void *data, gfp_t flags)": "drm_flip_work_allocate_task - allocate a flip-work task   @data: data associated to the task   @flags: allocator flags     Allocate a drm_flip_task object and attach private data to it. ", "void drm_flip_work_queue_task(struct drm_flip_work *work,      struct drm_flip_task *task)": "drm_flip_work_commit() is called. ", "void drm_flip_work_init(struct drm_flip_work *work,const char *name, drm_flip_func_t func)": "drm_flip_work_init - initialize flip-work   @work: the flip-work to initialize   @name: debug name   @func: the callback work function     Initializesallocates resources for the flip-work ", "void drm_flip_work_cleanup(struct drm_flip_work *work)": "drm_flip_work_cleanup - cleans up flip-work   @work: the flip-work to cleanup     Destroy resources allocated for the flip-work ", "list_for_each_entry_safe(r_list, list_t, &dev->maplist, head) ": "drm_legacy_rmmap_locked(struct drm_device  dev, struct drm_local_map  map){struct drm_map_list  r_list = NULL,  list_t;int found = 0;struct drm_master  master;  Find the list entry for the map and remove it ", "valid = 0;list_for_each_entry(agp_entry, &dev->agp->memory, head) ": "drm_legacy_addbufs_agp(struct drm_device  dev,   struct drm_buf_desc  request){struct drm_device_dma  dma = dev->dma;struct drm_buf_entry  entry;struct drm_agp_mem  agp_entry;struct drm_buf  buf;unsigned long offset;unsigned long agp_offset;int count;int order;int size;int alignment;int page_order;int total;int byte_count;int i, valid;struct drm_buf   temp_buflist;if (!dma)return -EINVAL;count = request->count;order = order_base_2(request->size);size = 1 << order;alignment = (request->flags & _DRM_PAGE_ALIGN)    ? PAGE_ALIGN(size) : size;page_order = order - PAGE_SHIFT > 0 ? order - PAGE_SHIFT : 0;total = PAGE_SIZE << page_order;byte_count = 0;agp_offset = dev->agp->base + request->agp_start;DRM_DEBUG(\"count:      %d\\n\", count);DRM_DEBUG(\"order:      %d\\n\", order);DRM_DEBUG(\"size:       %d\\n\", size);DRM_DEBUG(\"agp_offset: %lx\\n\", agp_offset);DRM_DEBUG(\"alignment:  %d\\n\", alignment);DRM_DEBUG(\"page_order: %d\\n\", page_order);DRM_DEBUG(\"total:      %d\\n\", total);if (order < DRM_MIN_ORDER || order > DRM_MAX_ORDER)return -EINVAL;  Make sure buffers are located in AGP memory that we own ", "}if (count < 0 || count > 4096) ": "drm_legacy_addbufs_pci(struct drm_device  dev,   struct drm_buf_desc  request){struct drm_device_dma  dma = dev->dma;int count;int order;int size;int total;int page_order;struct drm_buf_entry  entry;drm_dma_handle_t  dmah;struct drm_buf  buf;int alignment;unsigned long offset;int i;int byte_count;int page_count;unsigned long  temp_pagelist;struct drm_buf   temp_buflist;if (!drm_core_check_feature(dev, DRIVER_PCI_DMA))return -EOPNOTSUPP;if (!dma)return -EINVAL;if (!capable(CAP_SYS_ADMIN))return -EPERM;count = request->count;order = order_base_2(request->size);size = 1 << order;DRM_DEBUG(\"count=%d, size=%d (%d), order=%d\\n\",  request->count, request->size, size, order);if (order < DRM_MIN_ORDER || order > DRM_MAX_ORDER)return -EINVAL;alignment = (request->flags & _DRM_PAGE_ALIGN)    ? PAGE_ALIGN(size) : size;page_order = order - PAGE_SHIFT > 0 ? order - PAGE_SHIFT : 0;total = PAGE_SIZE << page_order;spin_lock(&dev->buf_lock);if (dev->buf_use) {spin_unlock(&dev->buf_lock);return -EBUSY;}atomic_inc(&dev->buf_alloc);spin_unlock(&dev->buf_lock);mutex_lock(&dev->struct_mutex);entry = &dma->bufs[order];if (entry->buf_count) {mutex_unlock(&dev->struct_mutex);atomic_dec(&dev->buf_alloc);return -ENOMEM;  May only call once for each order ", "intdrm_atomic_helper_check_modeset(struct drm_device *dev,struct drm_atomic_state *state)": "drm_atomic_helper_check_modeset - validate state object for modeset changes   @dev: DRM device   @state: the driver state object     Check the state object to see if the requested state is physically possible.   This does all the CRTC and connector related computations for an atomic   update and adds any additional connectors needed for full modesets. It calls   the various per-object callbacks in the follow order:     1. &drm_connector_helper_funcs.atomic_best_encoder for determining the new encoder.   2. &drm_connector_helper_funcs.atomic_check to validate the connector state.   3. If it's determined a modeset is needed then all connectors on the affected      CRTC are added and &drm_connector_helper_funcs.atomic_check is run on them.   4. &drm_encoder_helper_funcs.mode_valid, &drm_bridge_funcs.mode_valid and      &drm_crtc_helper_funcs.mode_valid are called on the affected components.   5. &drm_bridge_funcs.mode_fixup is called on all encoder bridges.   6. &drm_encoder_helper_funcs.atomic_check is called to validate any encoder state.      This function is only called when the encoder will be part of a configured CRTC,      it must not be used for implementing connector property validation.      If this function is NULL, &drm_atomic_encoder_helper_funcs.mode_fixup is called      instead.   7. &drm_crtc_helper_funcs.mode_fixup is called last, to fix up the mode with CRTC constraints.     &drm_crtc_state.mode_changed is set when the input mode is changed.   &drm_crtc_state.connectors_changed is set when a connector is added or   removed from the CRTC.  &drm_crtc_state.active_changed is set when   &drm_crtc_state.active changes, which is used for DPMS.   &drm_crtc_state.no_vblank is set from the result of drm_dev_has_vblank().   See also: drm_atomic_crtc_needs_modeset()     IMPORTANT:     Drivers which set &drm_crtc_state.mode_changed (e.g. in their   &drm_plane_helper_funcs.atomic_check hooks if a plane update can't be done   without a full modeset) _must_ call this function after that change. It is   permitted to call this function multiple times for the same update, e.g.   when the &drm_crtc_helper_funcs.atomic_check functions depend upon the   adjusted dotclock for fifo space allocation and watermark computation.     RETURNS:   Zero for success or -errno ", "intdrm_atomic_helper_check_wb_encoder_state(struct drm_encoder *encoder, struct drm_connector_state *conn_state)": "drm_atomic_helper_check_wb_encoder_state() - Check writeback encoder state   @encoder: encoder state to check   @conn_state: connector state to check     Checks if the writeback connector state is valid, and returns an error if it   isn't.     RETURNS:   Zero for success or -errno ", "int drm_atomic_helper_check_plane_state(struct drm_plane_state *plane_state,const struct drm_crtc_state *crtc_state,int min_scale,int max_scale,bool can_position,bool can_update_disabled)": "drm_atomic_helper_check_plane_state() - Check plane state for validity   @plane_state: plane state to check   @crtc_state: CRTC state to check   @min_scale: minimum @src:@dest scaling factor in 16.16 fixed point   @max_scale: maximum @src:@dest scaling factor in 16.16 fixed point   @can_position: is it legal to position the plane such that it                  doesn't cover the entire CRTC?  This will generally                  only be false for primary planes.   @can_update_disabled: can the plane be updated while the CRTC                         is disabled?     Checks that a desired plane update is valid, and updates various   bits of derived state (clipped coordinates etc.). Drivers that provide   their own plane handling rather than helper-provided implementations may   still wish to call this function to avoid duplication of error checking   code.     RETURNS:   Zero if update appears valid, error code on failure ", "int drm_atomic_helper_check_crtc_primary_plane(struct drm_crtc_state *crtc_state)": "drm_atomic_helper_check_crtc_primary_plane() - Check CRTC state for primary plane   @crtc_state: CRTC state to check     Checks that a CRTC has at least one primary plane attached to it, which is   a requirement on some hardware. Note that this only involves the CRTC side   of the test. To test if the primary plane is visible or if it can be updated   without the CRTC being enabled, use drm_atomic_helper_check_plane_state() in   the plane's atomic check.     RETURNS:   0 if a primary plane is attached to the CRTC, or an error code otherwise ", "intdrm_atomic_helper_check_planes(struct drm_device *dev,       struct drm_atomic_state *state)": "drm_atomic_helper_check_planes - validate state object for planes changes   @dev: DRM device   @state: the driver state object     Check the state object to see if the requested state is physically possible.   This does all the plane update related checks using by calling into the   &drm_crtc_helper_funcs.atomic_check and &drm_plane_helper_funcs.atomic_check   hooks provided by the driver.     It also sets &drm_crtc_state.planes_changed to indicate that a CRTC has   updated planes.     RETURNS:   Zero for success or -errno ", "static voiddrm_atomic_helper_plane_changed(struct drm_atomic_state *state,struct drm_plane_state *old_plane_state,struct drm_plane_state *plane_state,struct drm_plane *plane)": "drm_atomic_helper_set_config(),   drm_atomic_helper_disable_plane(), and the various functions to implement   set_property callbacks. New drivers must not implement these functions   themselves but must use the provided helpers.     The atomic helper uses the same function table structures as all other   modesetting helpers. See the documentation for &struct drm_crtc_helper_funcs,   struct &drm_encoder_helper_funcs and &struct drm_connector_helper_funcs. It   also shares the &struct drm_plane_helper_funcs function table with the plane   helpers. ", "voiddrm_atomic_helper_update_legacy_modeset_state(struct drm_device *dev,      struct drm_atomic_state *old_state)": "drm_atomic_helper_update_legacy_modeset_state - update legacy modeset state   @dev: DRM device   @old_state: atomic state object with old state structures     This function updates all the various legacy modeset state pointers in   connectors, encoders and CRTCs.     Drivers can use this for building their own atomic commit if they don't have   a pure helper-based modeset implementation.     Since these updates are not synchronized with lockings, only code paths   called from &drm_mode_config_helper_funcs.atomic_commit_tail can look at the   legacy state filled out by this helper. Defacto this means this helper and   the legacy state pointers are only really useful for transitioning an   existing driver to the atomic world. ", "void drm_atomic_helper_calc_timestamping_constants(struct drm_atomic_state *state)": "drm_atomic_helper_calc_timestamping_constants - update vblank timestamping constants   @state: atomic state object     Updates the timestamping constants used for precise vblank timestamps   by calling drm_calc_timestamping_constants() for all enabled crtcs in @state. ", "void drm_atomic_helper_commit_modeset_disables(struct drm_device *dev,       struct drm_atomic_state *old_state)": "drm_atomic_helper_commit_planes(), which is what the default commit function   does. But drivers with different needs can group the modeset commits together   and do the plane commits at the end. This is useful for drivers doing runtime   PM since planes updates then only happen when the CRTC is actually enabled. ", "void drm_atomic_helper_commit_modeset_enables(struct drm_device *dev,      struct drm_atomic_state *old_state)": "drm_atomic_helper_commit_modeset_enables - modeset commit to enable outputs   @dev: DRM device   @old_state: atomic state object with old state structures     This function enables all the outputs with the new configuration which had to   be turned off for the update.     For compatibility with legacy CRTC helpers this should be called after   drm_atomic_helper_commit_planes(), which is what the default commit function   does. But drivers with different needs can group the modeset commits together   and do the plane commits at the end. This is useful for drivers doing runtime   PM since planes updates then only happen when the CRTC is actually enabled. ", "int drm_atomic_helper_wait_for_fences(struct drm_device *dev,      struct drm_atomic_state *state,      bool pre_swap)": "drm_atomic_helper_swap_state() so it uses the current plane state (and   just uses the atomic state to find the changed planes)     Note that @pre_swap is needed since the point where we block for fences moves   around depending upon whether an atomic commit is blocking or   non-blocking. For non-blocking commit all waiting needs to happen after   drm_atomic_helper_swap_state() is called, but for blocking commits we want   to wait   before   we do anything that can't be easily rolled back. That is   before we call drm_atomic_helper_swap_state().     Returns zero if success or < 0 if dma_fence_wait() fails. ", "voiddrm_atomic_helper_wait_for_vblanks(struct drm_device *dev,struct drm_atomic_state *old_state)": "drm_atomic_helper_cleanup_planes()). It will only wait on CRTCs where the   framebuffers have actually changed to optimize for the legacy cursor and   plane update use-case.     Drivers using the nonblocking commit tracking support initialized by calling   drm_atomic_helper_setup_commit() should look at   drm_atomic_helper_wait_for_flip_done() as an alternative. ", "void drm_atomic_helper_commit_tail(struct drm_atomic_state *old_state)": "drm_atomic_helper_commit_tail_rpm().     Note that the default ordering of how the various stages are called is to   match the legacy modeset helper library closest. ", "if (!new_state)return drm_atomic_crtc_effectively_active(old_state);/* * We need to disable bridge(s) and CRTC if we're transitioning out of * self-refresh and changing CRTCs at the same time, because the * bridge tracks self-refresh status via CRTC state. ": "drm_atomic_helper_async_check(dev, state);drm_self_refresh_helper_alter_state(state);return ret;}EXPORT_SYMBOL(drm_atomic_helper_check);static boolcrtc_needs_disable(struct drm_crtc_state  old_state,   struct drm_crtc_state  new_state){    No new_state means the CRTC is off, so the only criteria is whether   it's currently active or in self refresh mode. ", "void drm_atomic_helper_async_commit(struct drm_device *dev,    struct drm_atomic_state *state)": "drm_atomic_helper_async_commit - commit state asynchronously   @dev: DRM device   @state: the driver state object     This function commits a state asynchronously, i.e., not vblank   synchronized. It should be used on a state only when   drm_atomic_async_check() succeeds. Async commits are not supposed to swap   the states like normal sync commits, but just do in-place changes on the   current state.     TODO: Implement full swap instead of doing in-place changes. ", "for_each_new_crtc_in_state(old_state, crtc, new_crtc_state, i)if (new_crtc_state->self_refresh_active)new_self_refresh_mask |= BIT(i);if (funcs && funcs->atomic_commit_tail)funcs->atomic_commit_tail(old_state);elsedrm_atomic_helper_commit_tail(old_state);commit_time_ms = ktime_ms_delta(ktime_get(), start);if (commit_time_ms > 0)drm_self_refresh_helper_update_avg_times(old_state, (unsigned long)commit_time_ms, new_self_refresh_mask);drm_atomic_helper_commit_cleanup_done(old_state);drm_atomic_state_put(old_state);}static void commit_work(struct work_struct *work)": "drm_atomic_helper_wait_for_dependencies(old_state);    We cannot safely access new_crtc_state after   drm_atomic_helper_commit_hw_done() so figure out which crtc's have   self-refresh active beforehand: ", "void drm_atomic_helper_commit_tail_rpm(struct drm_atomic_state *old_state)": "drm_atomic_helper_commit_hw_done(old_state);drm_atomic_helper_wait_for_vblanks(dev, old_state);drm_atomic_helper_cleanup_planes(dev, old_state);}EXPORT_SYMBOL(drm_atomic_helper_commit_tail);     drm_atomic_helper_commit_tail_rpm - commit atomic update to hardware   @old_state: new modeset state to be committed     This is an alternative implementation for the   &drm_mode_config_helper_funcs.atomic_commit_tail hook, for drivers   that support runtime_pm or need the CRTC to be enabled to perform a   commit. Otherwise, one should use the default implementation   drm_atomic_helper_commit_tail(). ", "int drm_atomic_helper_async_check(struct drm_device *dev,   struct drm_atomic_state *state)": "drm_atomic_helper_commit_cleanup_done(old_state);drm_atomic_state_put(old_state);}static void commit_work(struct work_struct  work){struct drm_atomic_state  state = container_of(work,      struct drm_atomic_state,      commit_work);commit_tail(state);}     drm_atomic_helper_async_check - check if state can be committed asynchronously   @dev: DRM device   @state: the driver state object     This helper will check if it is possible to commit the state asynchronously.   Async commits are not supposed to swap the states like normal sync commits   but just do in-place changes on the current state.     It will return 0 if the commit can happen in an asynchronous fashion or error   if not. Note that error just mean it can't be committed asynchronously, if it   fails the commit should be treated like a normal synchronous commit. ", "ret = drm_atomic_helper_swap_state(state, true);if (ret)goto err;/* * Everything below can be run asynchronously without the need to grab * any modeset locks at all under one condition: It must be guaranteed * that the asynchronous work has either been cancelled (if the driver * supports it, which at least requires that the framebuffers get * cleaned up with drm_atomic_helper_cleanup_planes()) or completed * before the new state gets committed on the software side with * drm_atomic_helper_swap_state(). * * This scheme allows new atomic state updates to be prepared and * checked in parallel to the asynchronous completion of the previous * update. Which is important since compositors need to figure out the * composition of the next frame right after having submitted the * current layout. * * NOTE: Commit work has multiple phases, first hardware commit, then * cleanup. We want them to overlap, hence need system_unbound_wq to * make sure work items don't artificially stall on each another. ": "drm_atomic_helper_prepare_planes(dev, state);if (ret)return ret;drm_atomic_helper_async_commit(dev, state);drm_atomic_helper_cleanup_planes(dev, state);return 0;}ret = drm_atomic_helper_setup_commit(state, nonblock);if (ret)return ret;INIT_WORK(&state->commit_work, commit_work);ret = drm_atomic_helper_prepare_planes(dev, state);if (ret)return ret;if (!nonblock) {ret = drm_atomic_helper_wait_for_fences(dev, state, true);if (ret)goto err;}    This is the point of no return - everything below never fails except   when the hw goes bonghits. Which means we can commit the new state on   the software side now. ", "void drm_atomic_helper_commit_planes(struct drm_device *dev,     struct drm_atomic_state *old_state,     uint32_t flags)": "drm_atomic_helper_commit_planes_on_crtc() instead.     Plane parameters can be updated by applications while the associated CRTC is   disabled. The DRMKMS core will store the parameters in the plane state,   which will be available to the driver when the CRTC is turned on. As a result   most drivers don't need to be immediately notified of plane updates for a   disabled CRTC.     Unless otherwise needed, drivers are advised to set the ACTIVE_ONLY flag in   @flags in order not to receive plane update notifications related to a   disabled CRTC. This avoids the need to manually ignore plane updates in   driver code when the driver andor hardware can't or just don't need to deal   with updates on disabled CRTCs, for example when supporting runtime PM.     Drivers may set the NO_DISABLE_AFTER_MODESET flag in @flags if the relevant   display controllers require to disable a CRTC's planes when the CRTC is   disabled. This function would skip the &drm_plane_helper_funcs.atomic_disable   call for a plane if the CRTC of the old plane state needs a modesetting   operation. Of course, the drivers need to disable the planes in their CRTC   disable callbacks since no one else would do that.     The drm_atomic_helper_commit() default implementation doesn't set the   ACTIVE_ONLY flag to most closely match the behaviour of the legacy helpers.   This should not be copied blindly by drivers. ", "voiddrm_atomic_helper_disable_planes_on_crtc(struct drm_crtc_state *old_crtc_state, bool atomic)": "drm_atomic_helper_disable_planes_on_crtc - helper to disable CRTC's planes   @old_crtc_state: atomic state object with the old CRTC state   @atomic: if set, synchronize with CRTC's atomic_beginflush hooks     Disables all planes associated with the given CRTC. This can be   used for instance in the CRTC helper atomic_disable callback to disable   all planes.     If the atomic-parameter is set the function calls the CRTC's   atomic_begin hook before and atomic_flush hook after disabling the   planes.     It is a bug to call this function without having implemented the   &drm_plane_helper_funcs.atomic_disable plane hook. ", "int drm_atomic_helper_update_plane(struct drm_plane *plane,   struct drm_crtc *crtc,   struct drm_framebuffer *fb,   int crtc_x, int crtc_y,   unsigned int crtc_w, unsigned int crtc_h,   uint32_t src_x, uint32_t src_y,   uint32_t src_w, uint32_t src_h,   struct drm_modeset_acquire_ctx *ctx)": "drm_atomic_helper_update_plane - Helper for primary plane update using atomic   @plane: plane object to update   @crtc: owning CRTC of owning plane   @fb: framebuffer to flip onto plane   @crtc_x: x offset of primary plane on @crtc   @crtc_y: y offset of primary plane on @crtc   @crtc_w: width of primary plane rectangle on @crtc   @crtc_h: height of primary plane rectangle on @crtc   @src_x: x offset of @fb for panning   @src_y: y offset of @fb for panning   @src_w: width of source rectangle in @fb   @src_h: height of source rectangle in @fb   @ctx: lock acquire context     Provides a default plane update handler using the atomic driver interface.     RETURNS:   Zero on success, error code on failure ", "int drm_atomic_helper_disable_all(struct drm_device *dev,  struct drm_modeset_acquire_ctx *ctx)": "drm_atomic_helper_suspend(), drm_atomic_helper_resume() and   drm_atomic_helper_shutdown(). ", "struct drm_atomic_state *drm_atomic_helper_duplicate_state(struct drm_device *dev,  struct drm_modeset_acquire_ctx *ctx)": "drm_atomic_helper_duplicate_state - duplicate an atomic state object   @dev: DRM device   @ctx: lock acquisition context     Makes a copy of the current atomic state by looping over all objects and   duplicating their respective states. This is used for example by suspend   resume support code to save the state prior to suspend such that it can   be restored upon resume.     Note that this treats atomic state as persistent between save and restore.   Drivers must make sure that this is possible and won't result in confusion   or erroneous behaviour.     Note that if callers haven't already acquired all modeset locks this might   return -EDEADLK, which must be handled by calling drm_modeset_backoff().     Returns:   A pointer to the copy of the atomic state object on success or an   ERR_PTR()-encoded error code on failure.     See also:   drm_atomic_helper_suspend(), drm_atomic_helper_resume() ", "struct drm_atomic_state *drm_atomic_helper_suspend(struct drm_device *dev)": "drm_atomic_helper_commit_duplicated_state() ", "if (!state->duplicated && drm_connector_is_unregistered(connector) &&    crtc_state->active) ": "drm_atomic_helper_resume() to fail. ", "int drm_atomic_helper_page_flip(struct drm_crtc *crtc,struct drm_framebuffer *fb,struct drm_pending_vblank_event *event,uint32_t flags,struct drm_modeset_acquire_ctx *ctx)": "drm_atomic_helper_page_flip_target() ", "u32 *drm_atomic_helper_bridge_propagate_bus_fmt(struct drm_bridge *bridge,struct drm_bridge_state *bridge_state,struct drm_crtc_state *crtc_state,struct drm_connector_state *conn_state,u32 output_fmt,unsigned int *num_input_fmts)": "drm_atomic_helper_bridge_propagate_bus_fmt() - Propagate output format to    the input end of a bridge   @bridge: bridge control structure   @bridge_state: new bridge state   @crtc_state: new CRTC state   @conn_state: new connector state   @output_fmt: tested output bus format   @num_input_fmts: will contain the size of the returned array     This helper is a pluggable implementation of the   &drm_bridge_funcs.atomic_get_input_bus_fmts operation for bridges that don't   modify the bus configuration between their input and their output. It   returns an array of input formats with a single element set to @output_fmt.     RETURNS:   a valid format array of size @num_input_fmts, or NULL if the allocation   failed ", "uint32_t drm_of_crtc_port_mask(struct drm_device *dev,    struct device_node *port)": "drm_of_crtc_port_mask - find the mask of a registered CRTC by port OF node   @dev: DRM device   @port: port OF node     Given a port OF node, return the possible mask of the corresponding   CRTC within a device's list of CRTCs.  Returns zero if not found. ", "uint32_t drm_of_find_possible_crtcs(struct drm_device *dev,    struct device_node *port)": "drm_of_find_possible_crtcs - find the possible CRTCs for an encoder port   @dev: DRM device   @port: encoder port to scan for endpoints     Scan all endpoints attached to a port, locate their attached CRTCs,   and generate the DRM mask of CRTCs which may be attached to this   encoder.     See Documentationdevicetreebindingsgraph.txt for the bindings. ", "int drm_of_component_probe(struct device *dev,   int (*compare_of)(struct device *, void *),   const struct component_master_ops *m_ops)": "drm_of_component_probe - Generic probe function for a component based master   @dev: master device containing the OF node   @compare_of: compare function used for matching components   @m_ops: component master ops to be used     Parse the platform device OF node and bind all the components associated   with the master. Interface ports are added before the encoders in order to   satisfy their .bind requirements   See Documentationdevicetreebindingsgraph.txt for the bindings.     Returns zero if successful, or one of the standard error codes if it fails. ", "EXPORT_SYMBOL(drm_legacy_agp_init": "drm_legacy_agp_init(struct drm_device  dev){struct pci_dev  pdev = to_pci_dev(dev->dev);struct drm_agp_head  head = NULL;head = kzalloc(sizeof( head), GFP_KERNEL);if (!head)return NULL;head->bridge = agp_find_bridge(pdev);if (!head->bridge) {head->bridge = agp_backend_acquire(pdev);if (!head->bridge) {kfree(head);return NULL;}agp_copy_info(head->bridge, &head->agp_info);agp_backend_release(head->bridge);} else {agp_copy_info(head->bridge, &head->agp_info);}if (head->agp_info.chipset == NOT_SUPPORTED) {kfree(head);return NULL;}INIT_LIST_HEAD(&head->memory);head->cant_use_aperture = head->agp_info.cant_use_aperture;head->page_mask = head->agp_info.page_mask;head->base = head->agp_info.aper_base;return head;}  Only exported for i810.ko ", "void drm_bridge_add(struct drm_bridge *bridge)": "drm_bridge_add - add the given bridge to the global bridge list     @bridge: bridge control structure ", "int devm_drm_bridge_add(struct device *dev, struct drm_bridge *bridge)": "drm_bridge_remove_void(void  bridge){drm_bridge_remove(bridge);}     devm_drm_bridge_add - devm managed version of drm_bridge_add()     @dev: device to tie the bridge lifetime to   @bridge: bridge control structure     This is the managed version of drm_bridge_add() which automatically   calls drm_bridge_remove() when @dev is unbound.     Return: 0 if no error or negative error code. ", "/** * DOC: special care dsi * * The interaction between the bridges and other frameworks involved in * the probing of the upstream driver and the bridge driver can be * challenging. Indeed, there's multiple cases that needs to be * considered: * * - The upstream driver doesn't use the component framework and isn't a *   MIPI-DSI host. In this case, the bridge driver will probe at some *   point and the upstream driver should try to probe again by returning *   EPROBE_DEFER as long as the bridge driver hasn't probed. * * - The upstream driver doesn't use the component framework, but is a *   MIPI-DSI host. The bridge device uses the MIPI-DCS commands to be *   controlled. In this case, the bridge device is a child of the *   display device and when it will probe it's assured that the display *   device (and MIPI-DSI host) is present. The upstream driver will be *   assured that the bridge driver is connected between the *   &mipi_dsi_host_ops.attach and &mipi_dsi_host_ops.detach operations. *   Therefore, it must run mipi_dsi_host_register() in its probe *   function, and then run drm_bridge_attach() in its *   &mipi_dsi_host_ops.attach hook. * * - The upstream driver uses the component framework and is a MIPI-DSI *   host. The bridge device uses the MIPI-DCS commands to be *   controlled. This is the same situation than above, and can run *   mipi_dsi_host_register() in either its probe or bind hooks. * * - The upstream driver uses the component framework and is a MIPI-DSI *   host. The bridge device uses a separate bus (such as I2C) to be *   controlled. In this case, there's no correlation between the probe *   of the bridge and upstream drivers, so care must be taken to avoid *   an endless EPROBE_DEFER loop, with each driver waiting for the *   other to probe. * * The ideal pattern to cover the last item (and all the others in the * MIPI-DSI host driver case) is to split the operations like this: * * - The MIPI-DSI host driver must run mipi_dsi_host_register() in its *   probe hook. It will make sure that the MIPI-DSI host sticks around, *   and that the driver's bind can be called. * * - In its probe hook, the bridge driver must try to find its MIPI-DSI *   host, register as a MIPI-DSI device and attach the MIPI-DSI device *   to its host. The bridge driver is now functional. * * - In its &struct mipi_dsi_host_ops.attach hook, the MIPI-DSI host can *   now add its component. Its bind hook will now be called and since *   the bridge driver is attached and registered, we can now look for *   and attach it. * * At this point, we're now certain that both the upstream driver and * the bridge driver are functional and we can't have a deadlock-like * situation when probing. ": "drm_atomic_bridge_chain_check()), mode   setting (through drm_bridge_chain_mode_set()), enable (through   drm_atomic_bridge_chain_pre_enable() and drm_atomic_bridge_chain_enable())   and disable (through drm_atomic_bridge_chain_disable() and   drm_atomic_bridge_chain_post_disable()). Those functions call the   corresponding operations provided in &drm_bridge_funcs in sequence for all   bridges in the chain.     For display drivers that use the atomic helpers   drm_atomic_helper_check_modeset(),   drm_atomic_helper_commit_modeset_enables() and   drm_atomic_helper_commit_modeset_disables() (either directly in hand-rolled   commit check and commit tail handlers, or through the higher-level   drm_atomic_helper_check() and drm_atomic_helper_commit_tail() or   drm_atomic_helper_commit_tail_rpm() helpers), this is done transparently and   requires no intervention from the driver. For other drivers, the relevant   DRM bridge chain functions shall be called manually.     Bridges also participate in implementing the &drm_connector at the end of   the bridge chain. Display drivers may use the drm_bridge_connector_init()   helper to create the &drm_connector, or implement it manually on top of the   connector-related operations exposed by the bridge (see the overview   documentation of bridge operations for more details). ", "bool drm_bridge_chain_mode_fixup(struct drm_bridge *bridge, const struct drm_display_mode *mode, struct drm_display_mode *adjusted_mode)": "drm_bridge_chain_mode_fixup - fixup proposed mode for all bridges in the   encoder chain   @bridge: bridge control structure   @mode: desired mode to be set for the bridge   @adjusted_mode: updated mode that works for this bridge     Calls &drm_bridge_funcs.mode_fixup for all the bridges in the   encoder chain, starting from the first bridge to the last.     Note: the bridge passed should be the one closest to the encoder     RETURNS:   true on success, false on failure ", "struct drm_bridge *of_drm_find_bridge(struct device_node *np)": "of_drm_find_bridge - find the bridge corresponding to the device node in  the global bridge list     @np: device node     RETURNS:   drm_bridge control struct on success, NULL on failure ", "static bool drm_is_current_master_locked(struct drm_file *fpriv)": "drm_is_current_master().     Clients can authenticate against the current master (if it matches their own)   using the GETMAGIC and AUTHMAGIC IOCTLs. Together with exchanging masters,   this allows controlled access to the device for an entire group of mutually   trusted clients. ", "static intdrm_master_check_perm(struct drm_device *dev, struct drm_file *file_priv)": "drm_master_put(&old_master);return 0;}    In the olden days the SETDROP_MASTER ioctls used to return EACCES when   CAP_SYS_ADMIN was not set. This was used to prevent rogue applications   from becoming master andor failing to release it.     At the same time, the first client (for a given VT) is _always_ master.   Thus in order for the ioctls to succeed, one had to _explicitly_ run the   application as root or flip the setuid bit.     If the CAP_SYS_ADMIN was missing, no other client could become master...   EVER :-( Leading to a) the graphics session dying badly or b) a completely   locked session.       As some point systemd-logind was introduced to orchestrate and delegate   master as applicable. It does so by opening the fd and passing it to users   while in itself logind a) does the setdrop master per users' request and   b)    implicitly drops master on VT switch.     Even though logind looks like the future, there are a few issues:    - some platforms don't have equivalent (Android, CrOS, some BSDs) so   root is required _solely_ for SETDROP MASTER.    - applications may not be updated to use it,    - any client which fails to drop master  can DoS the application using   logind, to a varying degree.       Either due missing CAP_SYS_ADMIN or simply not calling DROP_MASTER.       Here we implement the next best thing:    - ensure the logind style of fd passing works unchanged, and    - allow a client to dropset master, iff it iswas master at a given point   in time.     Note: DROP_MASTER cannot be free for all, as an arbitrator user could:    - DoScrash the arbitrator - details would be implementation specific    - open the node, become master implicitly and cause issues     As a result this fixes the following when using root-less build wo logind   - startx   - weston   - various compositors based on wlroots ", "struct drm_master *drm_file_get_master(struct drm_file *file_priv)": "drm_file_get_master - reference &drm_file.master of @file_priv   @file_priv: DRM file private     Increments the reference count of @file_priv's &drm_file.master and returns   the &drm_file.master. If @file_priv has no &drm_file.master, returns NULL.     Master pointers returned from this function should be unreferenced using   drm_master_put(). ", "voiddrm_clflush_pages(struct page *pages[], unsigned long num_pages)": "drm_clflush_pages - Flush dcache lines of a set of pages.   @pages: List of pages to be flushed.   @num_pages: Number of pages in the array.     Flush every data cache line entry that points to an address belonging   to a page in the array. ", "voiddrm_clflush_sg(struct sg_table *st)": "drm_clflush_sg - Flush dcache lines pointing to a scather-gather.   @st: struct sg_table.     Flush every data cache line entry that points to an address in the   sg. ", "voiddrm_clflush_virt_range(void *addr, unsigned long length)": "drm_clflush_virt_range - Flush dcache lines of a region   @addr: Initial kernel memory address.   @length: Region size.     Flush every data cache line entry that points to an address in the   region requested. ", "if (xen_pv_domain())return true;/* * Enforce dma_alloc_coherent when memory encryption is active as well * for the same reasons as for Xen paravirtual hosts. ": "drm_need_swiotlb(int dma_bits){struct resource  tmp;resource_size_t max_iomem = 0;    Xen paravirtual hosts require swiotlb regardless of requested dma   transfer size.     NOTE: Really, what it requires is use of the dma_alloc_coherent         allocator used in ttm_dma_populate() instead of         ttm_populate_and_map_pages(), which bounce buffers so much in         Xen it leads to swiotlb buffer exhaustion. ", "static void __drm_memcpy_from_wc(void *dst, const void *src, unsigned long len)": "drm_memcpy_from_wc copies @len bytes from @src to @dst using   non-temporal instructions where available. Note that all arguments   (@src, @dst) must be aligned to 16 bytes and @len must be a multiple   of 16. ", "u64 drm_color_ctm_s31_32_to_qm_n(u64 user_input, u32 m, u32 n)": "drm_color_ctm_s31_32_to_qm_n     @user_input: input value   @m: number of integer bits, only support m <= 32, include the sign-bit   @n: number of fractional bits, only support n <= 32     Convert and clamp S31.32 sign-magnitude to Qm.n (signed 2's complement).   The sign-bit BIT(m+n-1) and above are 0 for positive value and 1 for negative   the range of value is [-2^(m-1), 2^(m-1) - 2^-n]     For example   A Q3.12 format number:   - required bit: 3 + 12 = 15bits   - range: [-2^2, 2^2 - 2^\u221215]     NOTE: the m can be zero if all bit_precision are used to present fractional         bits like Q0.32 ", "/** * drm_color_ctm_s31_32_to_qm_n * * @user_input: input value * @m: number of integer bits, only support m <= 32, include the sign-bit * @n: number of fractional bits, only support n <= 32 * * Convert and clamp S31.32 sign-magnitude to Qm.n (signed 2's complement). * The sign-bit BIT(m+n-1) and above are 0 for positive value and 1 for negative * the range of value is [-2^(m-1), 2^(m-1) - 2^-n] * * For example * A Q3.12 format number: * - required bit: 3 + 12 = 15bits * - range: [-2^2, 2^2 - 2^\u221215] * * NOTE: the m can be zero if all bit_precision are used to present fractional *       bits like Q0.32 ": "drm_plane_create_color_properties().     \"COLOR_ENCODING\":   Optional plane enum property to support different non RGB   color encodings. The driver can provide a subset of standard   enum values supported by the DRM plane.     \"COLOR_RANGE\":   Optional plane enum property to support different non RGB   color parameter ranges. The driver can provide a subset of   standard enum values supported by the DRM plane. ", "int drm_color_lut_check(const struct drm_property_blob *lut, u32 tests)": "drm_color_lut_check - check validity of lookup table   @lut: property blob containing LUT to check   @tests: bitmask of tests to run     Helper to check whether a userspace-provided lookup table is valid and   satisfies hardware requirements.  Drivers pass a bitmask indicating which of   the tests in &drm_color_lut_tests should be performed.     Returns 0 on success, -EINVAL on failure. ", "uint32_t drm_mode_legacy_fb_format(uint32_t bpp, uint32_t depth)": "drm_mode_legacy_fb_format - compute drm fourcc code from legacy description   @bpp: bits per pixels   @depth: bit depth per pixel     Computes a drm fourcc pixel format code for the given @bpp@depth values.   Useful in fbdev emulation code, since that deals in those values. ", "uint32_t drm_driver_legacy_fb_format(struct drm_device *dev,     uint32_t bpp, uint32_t depth)": "drm_driver_legacy_fb_format - compute drm fourcc code from legacy description   @dev: DRM device   @bpp: bits per pixels   @depth: bit depth per pixel     Computes a drm fourcc pixel format code for the given @bpp@depth values.   Unlike drm_mode_legacy_fb_format() this looks at the drivers mode_config,   and depending on the &drm_mode_config.quirk_addfb_prefer_host_byte_order flag   it returns little endian byte order or host byte order framebuffer formats. ", "const struct drm_format_info *__drm_format_info(u32 format)": "drm_format_info() for the public API. ", "const struct drm_format_info *drm_get_format_info(struct drm_device *dev,    const struct drm_mode_fb_cmd2 *mode_cmd)": "drm_get_format_info - query information for a given framebuffer configuration   @dev: DRM device   @mode_cmd: metadata from the userspace fb creation request     Returns:   The instance of struct drm_format_info that describes the pixel format, or   NULL if the format is unsupported. ", "unsigned int drm_format_info_block_width(const struct drm_format_info *info, int plane)": "drm_format_info_block_width - width in pixels of block.   @info: pixel format info   @plane: plane index     Returns:   The width in pixels of a block, depending on the plane index. ", "unsigned int drm_format_info_block_height(const struct drm_format_info *info,  int plane)": "drm_format_info_block_height - height in pixels of a block   @info: pixel format info   @plane: plane index     Returns:   The height in pixels of a block, depending on the plane index. ", "unsigned int drm_format_info_bpp(const struct drm_format_info *info, int plane)": "drm_format_info_bpp - number of bits per pixel   @info: pixel format info   @plane: plane index     Returns:   The actual number of bits per pixel, depending on the plane index. ", "uint64_t drm_format_info_min_pitch(const struct drm_format_info *info,   int plane, unsigned int buffer_width)": "drm_format_info_min_pitch - computes the minimum required pitch in bytes   @info: pixel format info   @plane: plane index   @buffer_width: buffer width in pixels     Returns:   The minimum required pitch in bytes for a buffer by taking into consideration   the pixel format information and the buffer width. ", "static const struct drm_encoder_funcs drm_simple_encoder_funcs_cleanup = ": "drm_simple_display_pipe_init() initializes a simple display pipeline   which has only one full-screen scanout buffer feeding one output. The   pipeline is represented by &struct drm_simple_display_pipe and binds   together &drm_plane, &drm_crtc and &drm_encoder structures into one fixed   entity. Some flexibility for code reuse is provided through a separately   allocated &drm_connector object and supporting optional &drm_bridge   encoder drivers.     Many drivers require only a very simple encoder that fulfills the minimum   requirements of the display pipeline and does not add additional   functionality. The function drm_simple_encoder_init() provides an   implementation of such an encoder. ", "int drm_simple_display_pipe_attach_bridge(struct drm_simple_display_pipe *pipe,  struct drm_bridge *bridge)": "drm_simple_display_pipe_attach_bridge - Attach a bridge to the display pipe   @pipe: simple display pipe object   @bridge: bridge to attach     Makes it possible to still use the drm_simple_display_pipe helpers when   a DRM bridge has to be used.     Note that you probably want to initialize the pipe by passing a NULL   connector to drm_simple_display_pipe_init().     Returns:   Zero on success, negative error code on failure. ", "void drm_mode_config_reset(struct drm_device *dev)": "drm_mode_config_reset - call ->reset callbacks   @dev: drm device     This functions calls all the crtc's, encoder's and connector's ->reset   callback. Drivers can use this in e.g. their driver load or resume code to   reset hardware and software state. ", "int drmm_mode_config_init(struct drm_device *dev)": "drm_mode_config_cleanup(dev);}     drmm_mode_config_init - managed DRM mode_configuration structure   initialization   @dev: DRM device     Initialize @dev's mode_config structure, used for tracking the graphics   configuration of @dev.     Since this initializes the modeset locks, no locking is possible. Which is no   problem, since this should happen single threaded at init time. It is the   driver's problem to ensure this guarantee.     Cleanup is automatically handled through registering drm_mode_config_cleanup   with drmm_add_action().     Returns: 0 on success, negative error value on failure. ", "/** * DOC: standard plane properties * * DRM planes have a few standardized properties: * * type: *     Immutable property describing the type of the plane. * *     For user-space which has enabled the &DRM_CLIENT_CAP_ATOMIC capability, *     the plane type is just a hint and is mostly superseded by atomic *     test-only commits. The type hint can still be used to come up more *     easily with a plane configuration accepted by the driver. * *     The value of this property can be one of the following: * *     \"Primary\": *         To light up a CRTC, attaching a primary plane is the most likely to *         work if it covers the whole CRTC and doesn't have scaling or *         cropping set up. * *         Drivers may support more features for the primary plane, user-space *         can find out with test-only atomic commits. * *         Some primary planes are implicitly used by the kernel in the legacy *         IOCTLs &DRM_IOCTL_MODE_SETCRTC and &DRM_IOCTL_MODE_PAGE_FLIP. *         Therefore user-space must not mix explicit usage of any primary *         plane (e.g. through an atomic commit) with these legacy IOCTLs. * *     \"Cursor\": *         To enable this plane, using a framebuffer configured without scaling *         or cropping and with the following properties is the most likely to *         work: * *         - If the driver provides the capabilities &DRM_CAP_CURSOR_WIDTH and *           &DRM_CAP_CURSOR_HEIGHT, create the framebuffer with this size. *           Otherwise, create a framebuffer with the size 64x64. *         - If the driver doesn't support modifiers, create a framebuffer with *           a linear layout. Otherwise, use the IN_FORMATS plane property. * *         Drivers may support more features for the cursor plane, user-space *         can find out with test-only atomic commits. * *         Some cursor planes are implicitly used by the kernel in the legacy *         IOCTLs &DRM_IOCTL_MODE_CURSOR and &DRM_IOCTL_MODE_CURSOR2. *         Therefore user-space must not mix explicit usage of any cursor *         plane (e.g. through an atomic commit) with these legacy IOCTLs. * *         Some drivers may support cursors even if no cursor plane is exposed. *         In this case, the legacy cursor IOCTLs can be used to configure the *         cursor. * *     \"Overlay\": *         Neither primary nor cursor. * *         Overlay planes are the only planes exposed when the *         &DRM_CLIENT_CAP_UNIVERSAL_PLANES capability is disabled. * * IN_FORMATS: *     Blob property which contains the set of buffer format and modifier *     pairs supported by this plane. The blob is a struct *     drm_format_modifier_blob. Without this property the plane doesn't *     support buffers with modifiers. Userspace cannot change this property. * *     Note that userspace can check the &DRM_CAP_ADDFB2_MODIFIERS driver *     capability for general modifier support. If this flag is set then every *     plane will have the IN_FORMATS property, even when it only supports *     DRM_FORMAT_MOD_LINEAR. Before linux kernel release v5.1 there have been *     various bugs in this area with inconsistencies between the capability *     flag and per-plane properties. ": "drm_universal_plane_init().     Each plane has a type, see enum drm_plane_type. A plane can be compatible   with multiple CRTCs, see &drm_plane.possible_crtcs.     Each CRTC must have a unique primary plane userspace can attach to enable   the CRTC. In other words, userspace must be able to attach a different   primary plane to each CRTC at the same time. Primary planes can still be   compatible with multiple CRTCs. There must be exactly as many primary planes   as there are CRTCs.     Legacy uAPI doesn't expose the primary and cursor planes directly. DRM core   relies on the driver to set the primary and optionally the cursor plane used   for legacy IOCTLs. This is done by calling drm_crtc_init_with_planes(). All   drivers must provide one primary plane per CRTC to avoid surprising legacy   userspace too much. ", "int drm_universal_plane_init(struct drm_device *dev, struct drm_plane *plane,     uint32_t possible_crtcs,     const struct drm_plane_funcs *funcs,     const uint32_t *formats, unsigned int format_count,     const uint64_t *format_modifiers,     enum drm_plane_type type,     const char *name, ...)": "drm_plane_cleanup() and kfree() the plane structure. The plane   structure should not be allocated with devm_kzalloc().     Note: consider using drmm_universal_plane_alloc() instead of   drm_universal_plane_init() to let the DRM managed resource infrastructure   take care of cleanup and deallocation.     Drivers that only support the DRM_FORMAT_MOD_LINEAR modifier support may set   @format_modifiers to NULL. The plane will advertise the linear modifier.     Returns:   Zero on success, error code on failure. ", "struct drm_plane *drm_plane_from_index(struct drm_device *dev, int idx)": "drm_plane_from_index - find the registered plane at an index   @dev: DRM device   @idx: index of registered plane to find for     Given a plane index, return the registered plane from DRM device's   list of planes with matching index. This is the inverse of drm_plane_index(). ", "void drm_plane_force_disable(struct drm_plane *plane)": "drm_plane_force_disable - Forcibly disable a plane   @plane: plane to disable     Forces the plane to be disabled.     Used when the plane's current framebuffer is destroyed,   and when restoring fbdev mode.     Note that this function is not suitable for atomic drivers, since it doesn't   wire through the lock acquisition context properly and hence can't handle   retries or driver private locks. You probably want to use   drm_atomic_helper_disable_plane() or   drm_atomic_helper_disable_planes_on_crtc() instead. ", "int drm_mode_plane_set_obj_prop(struct drm_plane *plane,struct drm_property *property,uint64_t value)": "drm_mode_plane_set_obj_prop - set the value of a property   @plane: drm plane object to set property value for   @property: property to set   @value: value the property should be set to     This functions sets a given property on a given plane object. This function   calls the driver's ->set_property callback and changes the software state of   the property if the callback succeeds.     Returns:   Zero on success, error code on failure. ", "bool drm_any_plane_has_format(struct drm_device *dev,      u32 format, u64 modifier)": "drm_any_plane_has_format - Check whether any plane supports this format and modifier combination   @dev: DRM device   @format: pixel format (DRM_FORMAT_ )   @modifier: data layout modifier     Returns:   Whether at least one plane supports the specified format and modifier combination. ", "/** * drm_plane_enable_fb_damage_clips - Enables plane fb damage clips property. * @plane: Plane on which to enable damage clips property. * * This function lets driver to enable the damage clips property on a plane. ": "drm_plane_enable_fb_damage_clips().   Drivers implementing damage can use drm_atomic_helper_damage_iter_init() and   drm_atomic_helper_damage_iter_next() helper iterator function to get damage   rectangles clipped to &drm_plane_state.src. ", "unsigned intdrm_plane_get_damage_clips_count(const struct drm_plane_state *state)": "drm_plane_get_damage_clips_count - Returns damage clips count.   @state: Plane state.     Simple helper to get the number of &drm_mode_rect clips set by user-space   during plane update.     Return: Number of clips in plane fb_damage_clips blob property. ", "int drm_plane_create_scaling_filter_property(struct drm_plane *plane,     unsigned int supported_filters)": "drm_plane_create_scaling_filter_property - create a new scaling filter   property     @plane: drm plane   @supported_filters: bitmask of supported scaling filters, must include         BIT(DRM_SCALING_FILTER_DEFAULT).     This function lets driver to enable the scaling filter property on a given   plane.     RETURNS:   Zero for success or -errno ", "void drm_mode_debug_printmodeline(const struct drm_display_mode *mode)": "drm_mode_debug_printmodeline - print a mode to dmesg   @mode: mode to print     Describe @mode using DRM_DEBUG. ", "struct drm_display_mode *drm_mode_create(struct drm_device *dev)": "drm_mode_create - create a new display mode   @dev: DRM device     Create a new, cleared drm_display_mode with kzalloc, allocate an ID for it   and return it.     Returns:   Pointer to new mode on success, NULL on error. ", "void drm_mode_destroy(struct drm_device *dev, struct drm_display_mode *mode)": "drm_mode_destroy - remove a mode   @dev: DRM device   @mode: mode to remove     Release @mode's unique ID, then free it @mode structure itself using kfree. ", "void drm_mode_probed_add(struct drm_connector *connector, struct drm_display_mode *mode)": "drm_mode_probed_add - add a mode to a connector's probed_mode list   @connector: connector the new mode   @mode: mode data     Add @mode to @connector's probed_mode list for later use. This list should   then in a second step get filtered and all the modes actually supported by   the hardware moved to the @connector's modes list. ", "struct drm_display_mode *drm_analog_tv_mode(struct drm_device *dev,    enum drm_connector_tv_mode tv_mode,    unsigned long pixel_clock_hz,    unsigned int hdisplay,    unsigned int vdisplay,    bool interlace)": "drm_mode_set_name(mode);drm_dbg_kms(dev, \"Generated mode \" DRM_MODE_FMT \"\\n\", DRM_MODE_ARG(mode));return 0;}     drm_analog_tv_mode - create a display mode for an analog TV   @dev: drm device   @tv_mode: TV Mode standard to create a mode for. See DRM_MODE_TV_MODE_ .   @pixel_clock_hz: Pixel Clock Frequency, in Hertz   @hdisplay: hdisplay size   @vdisplay: vdisplay size   @interlace: whether to compute an interlaced mode     This function creates a struct drm_display_mode instance suited for   an analog TV output, for one of the usual analog TV mode.     Note that @hdisplay is larger than the usual constraints for the PAL   and NTSC timings, and we'll choose to ignore most timings constraints   to reach those resolutions.     Returns:     A pointer to the mode, allocated with drm_mode_create(). Returns NULL   on error. ", "struct drm_display_mode *drm_cvt_mode(struct drm_device *dev, int hdisplay,      int vdisplay, int vrefresh,      bool reduced, bool interlaced, bool margins)": "drm_cvt_mode -create a modeline based on the CVT algorithm   @dev: drm device   @hdisplay: hdisplay size   @vdisplay: vdisplay size   @vrefresh: vrefresh rate   @reduced: whether to use reduced blanking   @interlaced: whether to compute an interlaced mode   @margins: whether to add margins (borders)     This function is called to generate the modeline based on CVT algorithm   according to the hdisplay, vdisplay, vrefresh.   It is based from the VESA(TM) Coordinated Video Timing Generator by   Graham Loveridge April 9, 2003 available at   http:www.elo.utfsm.cl~elo212docsCVTd6r1.xls     And it is copied from xf86CVTmode in xserverhwxfree86modesxf86cvt.c.   What I have done is to translate it by using integer calculation.     Returns:   The modeline based on the CVT algorithm stored in a drm_display_mode object.   The display mode object is allocated with drm_mode_create(). Returns NULL   when no mode could be allocated. ", "struct drm_display_mode *drm_gtf_mode_complex(struct drm_device *dev, int hdisplay, int vdisplay,     int vrefresh, bool interlaced, int margins,     int GTF_M, int GTF_2C, int GTF_K, int GTF_2J)": "drm_gtf_mode_complex - create the modeline based on the full GTF algorithm   @dev: drm device   @hdisplay: hdisplay size   @vdisplay: vdisplay size   @vrefresh: vrefresh rate.   @interlaced: whether to compute an interlaced mode   @margins: desired margin (borders) size   @GTF_M: extended GTF formula parameters   @GTF_2C: extended GTF formula parameters   @GTF_K: extended GTF formula parameters   @GTF_2J: extended GTF formula parameters     GTF feature blocks specify C and J in multiples of 0.5, so we pass them   in here multiplied by two.  For a C of 40, pass in 80.     Returns:   The modeline based on the full GTF algorithm stored in a drm_display_mode object.   The display mode object is allocated with drm_mode_create(). Returns NULL   when no mode could be allocated. ", "int drm_mode_vrefresh(const struct drm_display_mode *mode)": "drm_mode_vrefresh - get the vrefresh of a mode   @mode: mode     Returns:   @modes's vrefresh rate in Hz, rounded to the nearest integer. Calculates the   value first if it is not yet set. ", "void drm_mode_get_hv_timing(const struct drm_display_mode *mode,    int *hdisplay, int *vdisplay)": "drm_mode_get_hv_timing - Fetches hdisplayvdisplay for given mode   @mode: mode to query   @hdisplay: hdisplay value to fill in   @vdisplay: vdisplay value to fill in     The vdisplay value will be doubled if the specified mode is a stereo mode of   the appropriate layout. ", "void drm_mode_set_crtcinfo(struct drm_display_mode *p, int adjust_flags)": "drm_mode_init(&adjusted, mode);drm_mode_set_crtcinfo(&adjusted, CRTC_STEREO_DOUBLE_ONLY); hdisplay = adjusted.crtc_hdisplay; vdisplay = adjusted.crtc_vdisplay;}EXPORT_SYMBOL(drm_mode_get_hv_timing);     drm_mode_set_crtcinfo - set CRTC modesetting timing parameters   @p: mode   @adjust_flags: a combination of adjustment flags     Setup the CRTC modesetting timing parameters for @p, adjusting if necessary.     - The CRTC_INTERLACE_HALVE_V flag can be used to halve vertical timings of     interlaced modes.   - The CRTC_STEREO_DOUBLE flag can be used to compute the timings for     buffers containing two eyes (only adjust the timings when needed, eg. for     \"frame packing\" or \"side by side full\").   - The CRTC_NO_DBLSCAN and CRTC_NO_VSCAN flags request that adjustment  not      be performed for doublescan and vscan > 1 modes respectively. ", "void drm_mode_copy(struct drm_display_mode *dst, const struct drm_display_mode *src)": "drm_mode_copy - copy the mode   @dst: mode to overwrite   @src: mode to copy     Copy an existing mode into another mode, preserving the   list head of the destination mode. ", "struct drm_display_mode *drm_mode_duplicate(struct drm_device *dev,    const struct drm_display_mode *mode)": "drm_mode_duplicate - allocate and duplicate an existing mode   @dev: drm_device to allocate the duplicated mode for   @mode: mode to duplicate     Just allocate a new mode, copy the existing mode into it, and return   a pointer to it.  Used to create new instances of established modes.     Returns:   Pointer to duplicated mode on success, NULL on error. ", "if (mode1->clock && mode2->clock)return KHZ2PICOS(mode1->clock) == KHZ2PICOS(mode2->clock);elsereturn mode1->clock == mode2->clock;}static bool drm_mode_match_flags(const struct drm_display_mode *mode1, const struct drm_display_mode *mode2)": "drm_mode_match_timings(const struct drm_display_mode  mode1,   const struct drm_display_mode  mode2){return mode1->hdisplay == mode2->hdisplay &&mode1->hsync_start == mode2->hsync_start &&mode1->hsync_end == mode2->hsync_end &&mode1->htotal == mode2->htotal &&mode1->hskew == mode2->hskew &&mode1->vdisplay == mode2->vdisplay &&mode1->vsync_start == mode2->vsync_start &&mode1->vsync_end == mode2->vsync_end &&mode1->vtotal == mode2->vtotal &&mode1->vscan == mode2->vscan;}static bool drm_mode_match_clock(const struct drm_display_mode  mode1,  const struct drm_display_mode  mode2){    do clock check convert to PICOS   so fb modes get matched the same ", "bool drm_mode_equal(const struct drm_display_mode *mode1,    const struct drm_display_mode *mode2)": "drm_mode_equal - test modes for equality   @mode1: first mode   @mode2: second mode     Check to see if @mode1 and @mode2 are equivalent.     Returns:   True if the modes are equal, false otherwise. ", "bool drm_mode_equal_no_clocks(const struct drm_display_mode *mode1,      const struct drm_display_mode *mode2)": "drm_mode_equal_no_clocks - test modes for equality   @mode1: first mode   @mode2: second mode     Check to see if @mode1 and @mode2 are equivalent, but   don't check the pixel clocks.     Returns:   True if the modes are equal, false otherwise. ", "bool drm_mode_equal_no_clocks_no_stereo(const struct drm_display_mode *mode1,const struct drm_display_mode *mode2)": "drm_mode_equal_no_clocks_no_stereo - test modes for equality   @mode1: first mode   @mode2: second mode     Check to see if @mode1 and @mode2 are equivalent, but   don't check the pixel clocks nor the stereo layout.     Returns:   True if the modes are equal, false otherwise. ", "enum drm_mode_statusdrm_mode_validate_driver(struct drm_device *dev,const struct drm_display_mode *mode)": "drm_mode_validate_driver - make sure the mode is somewhat sane   @dev: drm device   @mode: mode to check     First do basic validation on the mode, and then allow the driver   to check for devicedriver specific limitations via the optional   &drm_mode_config_helper_funcs.mode_valid hook.     Returns:   The mode status ", "enum drm_mode_statusdrm_mode_validate_size(const struct drm_display_mode *mode,       int maxX, int maxY)": "drm_mode_validate_size - make sure modes adhere to size constraints   @mode: mode to check   @maxX: maximum width   @maxY: maximum height     This function is a helper which can be used to validate modes against size   limitations of the DRM deviceconnector. If a mode is too big its status   member is updated with the appropriate validation failure code. The list   itself is not changed.     Returns:   The mode status ", "enum drm_mode_statusdrm_mode_validate_ycbcr420(const struct drm_display_mode *mode,   struct drm_connector *connector)": "drm_mode_validate_ycbcr420 - add 'ycbcr420-only' modes only when allowed   @mode: mode to check   @connector: drm connector under action     This function is a helper which can be used to filter out any YCBCR420   only mode, when the source doesn't support it.     Returns:   The mode status ", "void drm_mode_prune_invalid(struct drm_device *dev,    struct list_head *mode_list, bool verbose)": "drm_mode_is_420_only(&connector->display_info, mode))return MODE_NO_420;return MODE_OK;}EXPORT_SYMBOL(drm_mode_validate_ycbcr420);#define MODE_STATUS(status) [MODE_ ## status + 3] = #statusstatic const char   const drm_mode_status_names[] = {MODE_STATUS(OK),MODE_STATUS(HSYNC),MODE_STATUS(VSYNC),MODE_STATUS(H_ILLEGAL),MODE_STATUS(V_ILLEGAL),MODE_STATUS(BAD_WIDTH),MODE_STATUS(NOMODE),MODE_STATUS(NO_INTERLACE),MODE_STATUS(NO_DBLESCAN),MODE_STATUS(NO_VSCAN),MODE_STATUS(MEM),MODE_STATUS(VIRTUAL_X),MODE_STATUS(VIRTUAL_Y),MODE_STATUS(MEM_VIRT),MODE_STATUS(NOCLOCK),MODE_STATUS(CLOCK_HIGH),MODE_STATUS(CLOCK_LOW),MODE_STATUS(CLOCK_RANGE),MODE_STATUS(BAD_HVALUE),MODE_STATUS(BAD_VVALUE),MODE_STATUS(BAD_VSCAN),MODE_STATUS(HSYNC_NARROW),MODE_STATUS(HSYNC_WIDE),MODE_STATUS(HBLANK_NARROW),MODE_STATUS(HBLANK_WIDE),MODE_STATUS(VSYNC_NARROW),MODE_STATUS(VSYNC_WIDE),MODE_STATUS(VBLANK_NARROW),MODE_STATUS(VBLANK_WIDE),MODE_STATUS(PANEL),MODE_STATUS(INTERLACE_WIDTH),MODE_STATUS(ONE_WIDTH),MODE_STATUS(ONE_HEIGHT),MODE_STATUS(ONE_SIZE),MODE_STATUS(NO_REDUCED),MODE_STATUS(NO_STEREO),MODE_STATUS(NO_420),MODE_STATUS(STALE),MODE_STATUS(BAD),MODE_STATUS(ERROR),};#undef MODE_STATUSconst char  drm_get_mode_status_name(enum drm_mode_status status){int index = status + 3;if (WARN_ON(index < 0 || index >= ARRAY_SIZE(drm_mode_status_names)))return \"\";return drm_mode_status_names[index];}     drm_mode_prune_invalid - remove invalid modes from mode list   @dev: DRM device   @mode_list: list of modes to check   @verbose: be verbose about it     This helper function can be used to prune a display mode list after   validation has been completed. All modes whose status is not MODE_OK will be   removed from the list, and if @verbose the status code and mode name is also   printed to dmesg. ", "void drm_mode_sort(struct list_head *mode_list)": "drm_mode_sort - sort mode list   @mode_list: list of drm_display_mode structures to sort     Sort @mode_list by favorability, moving good modes to the head of the list. ", "void drm_connector_list_update(struct drm_connector *connector)": "drm_connector_list_update - update the mode list for the connector   @connector: the connector to update     This moves the modes from the @connector probed_modes list   to the actual mode list. It compares the probed mode against the current   list and only adds differentnew modes.     This is just a helper functions doesn't validate any modes itself and also   doesn't prune any invalid modes. Callers need to do that themselves. ", "bool drm_mode_parse_command_line_for_connector(const char *mode_option,       const struct drm_connector *connector,       struct drm_cmdline_mode *mode)": "drm_mode_parse_command_line_for_connector - parse command line modeline for connector   @mode_option: optional per connector mode option   @connector: connector to parse modeline for   @mode: preallocated drm_cmdline_mode structure to fill out     This parses @mode_option command line modeline for modes and options to   configure the connector.     This uses the same parameters as the fb modedb.c, except for an extra   force-enable, force-enable-digital and force-disable bit at the end::    <xres>x<yres>[M][R][-<bpp>][@<refresh>][i][m][eDd]     Additionals options can be provided following the mode, using a comma to   separate each option. Valid options can be found in   Documentationfbmodedb.rst.     The intermediate drm_cmdline_mode structure is required to store additional   options from the command line modline like the force-enabledisable flag.     Returns:   True if a valid modeline has been parsed, false otherwise. ", "struct drm_display_mode *drm_mode_create_from_cmdline_mode(struct drm_device *dev,  struct drm_cmdline_mode *cmd)": "drm_mode_create_from_cmdline_mode - convert a command line modeline into a DRM display mode   @dev: DRM device to create the new mode for   @cmd: input command line modeline     Returns:   Pointer to converted mode on success, NULL on error. ", "bool drm_mode_is_420_also(const struct drm_display_info *display,  const struct drm_display_mode *mode)": "drm_mode_is_420_also - if a given videomode can be supported in YCBCR420   output format also (along with RGBYCBCR444422)     @display: display under action.   @mode: video mode to be tested.     Returns:   true if the mode can be support YCBCR420 format   false if not. ", "void drm_fbdev_generic_setup(struct drm_device *dev, unsigned int preferred_bpp)": "drm_fbdev_generic_setup() - Setup generic fbdev emulation   @dev: DRM device   @preferred_bpp: Preferred bits per pixel for the device.     This function sets up generic fbdev emulation for drivers that supports   dumb buffers with a virtual address and that can be mmap'ed.   drm_fbdev_generic_setup() shall be called after the DRM driver registered   the new DRM device with drm_dev_register().     Restore, hotplug events and teardown are all taken care of. Drivers that do   suspendresume need to call drm_fb_helper_set_suspend_unlocked() themselves.   Simple drivers might use drm_mode_config_helper_suspend().     In order to provide fixed mmap-able memory ranges, generic fbdev emulation   uses a shadow buffer in system memory. The implementation blits the shadow   fbdev buffer onto the real buffer in regular intervals.     This function is safe to call even when there are no connectors present.   Setup will be retried on the next hotplug event.     The fbdev is destroyed by drm_dev_unregister(). ", "void drm_kms_helper_poll_enable(struct drm_device *dev)": "drm_kms_helper_poll_disable(), for example over   suspendresume.     Drivers can call this helper from their device resume implementation. It is   not an error to call this even when output polling isn't enabled.     Note that calls to enable and disable polling must be strictly ordered, which   is automatically the case when they're only call from suspendresume   callbacks. ", "intdrm_helper_probe_detect(struct drm_connector *connector,struct drm_modeset_acquire_ctx *ctx,bool force)": "drm_helper_probe_detect_ctx(struct drm_connector  connector, bool force){const struct drm_connector_helper_funcs  funcs = connector->helper_private;struct drm_modeset_acquire_ctx ctx;int ret;drm_modeset_acquire_init(&ctx, 0);retry:ret = drm_modeset_lock(&connector->dev->mode_config.connection_mutex, &ctx);if (!ret) {if (funcs->detect_ctx)ret = funcs->detect_ctx(connector, &ctx, force);else if (connector->funcs->detect)ret = connector->funcs->detect(connector, force);elseret = connector_status_connected;}if (ret == -EDEADLK) {drm_modeset_backoff(&ctx);goto retry;}if (WARN_ON(ret < 0))ret = connector_status_unknown;if (ret != connector->status)connector->epoch_counter += 1;drm_modeset_drop_locks(&ctx);drm_modeset_acquire_fini(&ctx);return ret;}     drm_helper_probe_detect - probe connector status   @connector: connector to probe   @ctx: acquire_ctx, or NULL to let this function handle locking.   @force: Whether destructive probe operations should be performed.     This function calls the detect callbacks of the connector.   This function returns &drm_connector_status, or   if @ctx is set, it might also return -EDEADLK. ", "static bool drm_kms_helper_poll = true;module_param_named(poll, drm_kms_helper_poll, bool, 0600);static enum drm_mode_statusdrm_mode_validate_flag(const struct drm_display_mode *mode,       int flags)": "drm_helper_probe_single_connector_modes().     It also provides support for polling connectors with a work item and for   generic hotplug interrupt handling where the driver doesn't or cannot keep   track of a per-connector hpd interrupt.     This helper library can be used independently of the modeset helper library.   Drivers can also overwrite different parts e.g. use their own hotplug   handling code to avoid probing unrelated outputs.     The probe helpers share the function table structures with other display   helper libraries. See &struct drm_connector_helper_funcs for the details. ", "void drm_kms_helper_hotplug_event(struct drm_device *dev)": "drm_helper_hpd_irq_event() does - this is assumed to be done by the   driver already.     This function must be called from process context with no mode   setting locks held.     If only a single connector has changed, consider calling   drm_kms_helper_connector_hotplug_event() instead. ", "bool drm_kms_helper_is_poll_worker(void)": "drm_kms_helper_is_poll_worker - is %current task an output poll worker?     Determine if %current task is an output poll worker.  This can be used   to select distinct code paths for output polling versus other contexts.     One use case is to avoid a deadlock between the output poll worker and   the autosuspend worker wherein the latter waits for polling to finish   upon calling drm_kms_helper_poll_disable(), while the former waits for   runtime suspend to finish upon calling pm_runtime_get_sync() in a   connector ->detect hook. ", "void drm_kms_helper_poll_init(struct drm_device *dev)": "drm_kms_helper_poll_init - initialize and enable output polling   @dev: drm_device     This function initializes and then also enables output polling support for   @dev. Drivers which do not have reliable hotplug support in hardware can use   this helper infrastructure to regularly poll such connectors for changes in   their connection state.     Drivers can control which connectors are polled by setting the   DRM_CONNECTOR_POLL_CONNECT and DRM_CONNECTOR_POLL_DISCONNECT flags. On   connectors where probing live outputs can result in visual distortion drivers   should not set the DRM_CONNECTOR_POLL_DISCONNECT flag to avoid this.   Connectors which have no flag or only DRM_CONNECTOR_POLL_HPD set are   completely ignored by the polling logic.     Note that a connector can be both polled and probed from the hotplug handler,   in case the hotplug interrupt is known to be unreliable. ", "void drm_kms_helper_poll_fini(struct drm_device *dev)": "drm_kms_helper_poll_fini - disable output polling and clean it up   @dev: drm_device ", "bool drm_connector_helper_hpd_irq_event(struct drm_connector *connector)": "drm_connector_helper_hpd_irq_event - hotplug processing   @connector: drm_connector     Drivers can use this helper function to run a detect cycle on a connector   which has the DRM_CONNECTOR_POLL_HPD flag set in its &polled member.     This helper function is useful for drivers which can track hotplug   interrupts for a single connector. Drivers that want to send a   hotplug event for all connectors or can't track hotplug interrupts   per connector need to use drm_helper_hpd_irq_event().     This function must be called from process context with no mode   setting locks held.     Note that a connector can be both polled and probed from the hotplug   handler, in case the hotplug interrupt is known to be unreliable.     Returns:   A boolean indicating whether the connector status changed or not ", "enum drm_mode_status drm_crtc_helper_mode_valid_fixed(struct drm_crtc *crtc,      const struct drm_display_mode *mode,      const struct drm_display_mode *fixed_mode)": "drm_crtc_helper_mode_valid_fixed - Validates a display mode   @crtc: the crtc   @mode: the mode to validate   @fixed_mode: the display hardware's mode     Returns:   MODE_OK on success, or another mode-status code otherwise. ", "int drm_connector_helper_get_modes_from_ddc(struct drm_connector *connector)": "drm_connector_helper_get_modes_from_ddc - Updates the connector's EDID                                             property from the connector's                                             DDC channel   @connector: The connector     Returns:   The number of detected display modes.     Uses a connector's DDC channel to retrieve EDID data and update the   connector's EDID property and display modes. Drivers can use this   function to implement struct &drm_connector_helper_funcs.get_modes   for connectors with a DDC channel. ", "int drm_connector_helper_get_modes_fixed(struct drm_connector *connector, const struct drm_display_mode *fixed_mode)": "drm_connector_helper_get_modes_fixed - Duplicates a display mode for a connector   @connector: the connector   @fixed_mode: the display hardware's mode     This function duplicates a display modes for a connector. Drivers for hardware   that only supports a single fixed mode can use this function in their connector's   get_modes helper.     Returns:   The number of created modes. ", "int drm_connector_helper_tv_get_modes(struct drm_connector *connector)": "drm_connector_helper_tv_get_modes - Fills the modes availables to a TV connector   @connector: The connector     Fills the available modes for a TV connector based on the supported   TV modes, and the default mode expressed by the kernel command line.     This can be used as the default TV connector helper .get_modes() hook   if the driver does not need any special processing.     Returns:   The number of modes added to the connector. ", "int drm_vblank_work_schedule(struct drm_vblank_work *work,     u64 count, bool nextonmiss)": "drm_vblank_work_schedule - schedule a vblank work   @work: vblank work to schedule   @count: target vblank count   @nextonmiss: defer until the next vblank if target vblank was missed     Schedule @work for execution once the crtc vblank count reaches @count.     If the crtc vblank count has already reached @count and @nextonmiss is   %false the work starts to execute immediately.     If the crtc vblank count has already reached @count and @nextonmiss is   %true the work is deferred until the next vblank (as if @count has been   specified as crtc vblank count + 1).     If @work is already scheduled, this function will reschedule said work   using the new @count. This can be used for self-rearming work items.     Returns:   %1 if @work was successfully (re)scheduled, %0 if it was either already   scheduled or cancelled, or a negative error code on failure. ", "bool drm_vblank_work_cancel_sync(struct drm_vblank_work *work)": "drm_vblank_work_cancel_sync - cancel a vblank work and wait for it to   finish executing   @work: vblank work to cancel     Cancel an already scheduled vblank work and wait for its   execution to finish.     On return, @work is guaranteed to no longer be scheduled or running, even   if it's self-arming.     Returns:   %True if the work was cancelled before it started to execute, %false   otherwise. ", "void drm_vblank_work_flush(struct drm_vblank_work *work)": "drm_vblank_work_flush - wait for a scheduled vblank work to finish   executing   @work: vblank work to flush     Wait until @work has finished executing once. ", "void drm_vblank_work_init(struct drm_vblank_work *work, struct drm_crtc *crtc,  void (*func)(struct kthread_work *work))": "drm_vblank_work_init - initialize a vblank work item   @work: vblank work item   @crtc: CRTC whose vblank will trigger the work execution   @func: work function to be executed     Initialize a vblank work item for a specific crtc. ", "int drm_gem_object_init(struct drm_device *dev,struct drm_gem_object *obj, size_t size)": "drm_gem_object_init - initialize an allocated shmem-backed GEM object   @dev: drm_device the object should be initialized for   @obj: drm_gem_object to initialize   @size: object size     Initialize an already allocated GEM object of the specified size with   shmfs backing store. ", "void drm_gem_private_object_init(struct drm_device *dev, struct drm_gem_object *obj, size_t size)": "drm_gem_private_object_init(dev, obj, size);filp = shmem_file_setup(\"drm mm object\", size, VM_NORESERVE);if (IS_ERR(filp))return PTR_ERR(filp);obj->filp = filp;return 0;}EXPORT_SYMBOL(drm_gem_object_init);     drm_gem_private_object_init - initialize an allocated private GEM object   @dev: drm_device the object should be initialized for   @obj: drm_gem_object to initialize   @size: object size     Initialize an already allocated GEM object of the specified size with   no GEM provided backing store. Instead the caller is responsible for   backing the object and handling it. ", "void drm_gem_private_object_fini(struct drm_gem_object *obj)": "drm_gem_private_object_fini - Finalize a failed drm_gem_object   @obj: drm_gem_object     Uninitialize an already allocated GEM object when it initialized failed ", "intdrm_gem_handle_delete(struct drm_file *filp, u32 handle)": "drm_gem_object_release_handle(int id, void  ptr, void  data){struct drm_file  file_priv = data;struct drm_gem_object  obj = ptr;if (obj->funcs->close)obj->funcs->close(obj, file_priv);drm_prime_remove_buf_handle(&file_priv->prime, id);drm_vma_node_revoke(&obj->vma_node, file_priv);drm_gem_object_handle_put_unlocked(obj);return 0;}     drm_gem_handle_delete - deletes the given file-private handle   @filp: drm file-private structure to use for the handle look up   @handle: userspace handle to delete     Removes the GEM handle from the @filp lookup table which has been added with   drm_gem_handle_create(). If this is the last handle also cleans up linked   resources like GEM names. ", "voiddrm_gem_free_mmap_offset(struct drm_gem_object *obj)": "drm_gem_free_mmap_offset - release a fake mmap offset for an object   @obj: obj in question     This routine frees fake offsets allocated by drm_gem_create_mmap_offset().     Note that drm_gem_object_release() already calls this function, so drivers   don't have to take care of releasing the mmap offset themselves when freeing   the GEM object. ", "intdrm_gem_create_mmap_offset_size(struct drm_gem_object *obj, size_t size)": "drm_gem_create_mmap_offset_size - create a fake mmap offset for an object   @obj: obj in question   @size: the virtual size     GEM memory mapping works by handing back to userspace a fake mmap offset   it can use in a subsequent mmap(2) call.  The DRM core code then looks   up the object based on the offset and sets up the various memory mapping   structures.     This routine allocates and attaches a fake offset for @obj, in cases where   the virtual size differs from the physical size (ie. &drm_gem_object.size).   Otherwise just use drm_gem_create_mmap_offset().     This function is idempotent and handles an already allocated mmap offset   transparently. Drivers do not need to check for this case. ", "intdrm_gem_handle_create_tail(struct drm_file *file_priv,   struct drm_gem_object *obj,   u32 *handlep)": "drm_gem_create_mmap_offset(obj);if (ret)goto out; offset = drm_vma_node_offset_addr(&obj->vma_node);out:drm_gem_object_put(obj);return ret;}EXPORT_SYMBOL_GPL(drm_gem_dumb_map_offset);     drm_gem_handle_create_tail - internal functions to create a handle   @file_priv: drm file-private structure to register the handle for   @obj: object to register   @handlep: pointer to return the created handle to the caller     This expects the &drm_device.object_name_lock to be held already and will   drop it before returning. Used to avoid races in establishing new handles   when importing an object from either an flink name or a dma-buf.     Handles must be release again through drm_gem_handle_delete(). This is done   when userspace closes @file_priv for all attached handles, or through the   GEM_CLOSE ioctl for individual handles. ", "struct page **drm_gem_get_pages(struct drm_gem_object *obj)": "drm_gem_put_pages() to release the array and unpin all pages.     This uses the GFP-mask set on the shmem-mapping (see mapping_set_gfp_mask()).   If you require other GFP-masks, you have to do those allocations yourself.     Note that you are not allowed to change gfp-zones during runtime. That is,   shmem_read_mapping_page_gfp() must be called with the same gfp_zone(gfp) as   set during initialization. If you have special zone constraints, set them   after drm_gem_object_init() via mapping_set_gfp_mask(). shmem-core takes care   to keep pages in the required zone during swap-in.     This function is only valid on objects initialized with   drm_gem_object_init(), but not for those initialized with   drm_gem_private_object_init() only. ", "int drm_gem_objects_lookup(struct drm_file *filp, void __user *bo_handles,   int count, struct drm_gem_object ***objs_out)": "drm_gem_objects_lookup - look up GEM objects from an array of handles   @filp: DRM file private date   @bo_handles: user pointer to array of userspace handle   @count: size of handle array   @objs_out: returned pointer to array of drm_gem_object pointers     Takes an array of userspace handles and returns a newly allocated array of   GEM objects.     For a single handle lookup, use drm_gem_object_lookup().     Returns:     @objs filled in with GEM object pointers. Returned GEM objects need to be   released with drm_gem_object_put(). -ENOENT is returned on a lookup   failure. 0 is returned on success.   ", "if (obj->import_attach) ": "drm_gem_object_lookup(file, handle);if (!obj)return -ENOENT;  Don't allow imported objects to be mapped ", "long drm_gem_dma_resv_wait(struct drm_file *filep, u32 handle,    bool wait_all, unsigned long timeout)": "drm_gem_dma_resv_wait - Wait on GEM object's reservation's objects   shared andor exclusive fences.   @filep: DRM file private date   @handle: userspace handle   @wait_all: if true, wait on all fences, else wait on just exclusive fence   @timeout: timeout value in jiffies or zero to return immediately     Returns:     Returns -ERESTARTSYS if interrupted, 0 if the wait timed out, or   greater than 0 on success. ", "static void drm_gem_object_handle_free(struct drm_gem_object *obj)": "drm_gem_object_free or we'll be touching   freed memory ", "void drm_gem_vm_open(struct vm_area_struct *vma)": "drm_gem_vm_close(). ", "int drm_gem_mmap_obj(struct drm_gem_object *obj, unsigned long obj_size,     struct vm_area_struct *vma)": "drm_gem_mmap_obj - memory map a GEM object   @obj: the GEM object to map   @obj_size: the object size to be mapped, in bytes   @vma: VMA for the area to be mapped     Set up the VMA to prepare mapping of the GEM object using the GEM object's   vm_ops. Depending on their requirements, GEM objects can either   provide a fault handler in their vm_ops (in which case any accesses to   the object will be trapped, to perform migration, GTT binding, surface   register allocation, or performance monitoring), or mmap the buffer memory   synchronously after calling drm_gem_mmap_obj.     This function is mainly intended to implement the DMABUF mmap operation, when   the GEM object is not looked up based on its fake offset. To implement the   DRM mmap operation, drivers should use the drm_gem_mmap() function.     drm_gem_mmap_obj() assumes the user is granted access to the buffer while   drm_gem_mmap() prevents unprivileged users from mapping random objects. So   callers must verify access restrictions before calling this helper.     Return 0 or success or -EINVAL if the object size is smaller than the VMA   size, or if no vm_ops are provided. ", "iosys_map_clear(map);}EXPORT_SYMBOL(drm_gem_vunmap": "drm_gem_vunmap(struct drm_gem_object  obj, struct iosys_map  map){dma_resv_assert_held(obj->resv);if (iosys_map_is_null(map))return;if (obj->funcs->vunmap)obj->funcs->vunmap(obj, map);  Always set the mapping to NULL. Callers may rely on this. ", "intdrm_gem_lock_reservations(struct drm_gem_object **objs, int count,  struct ww_acquire_ctx *acquire_ctx)": "drm_gem_unlock_reservations().     @objs: drm_gem_objects to lock   @count: Number of objects in @objs   @acquire_ctx: struct ww_acquire_ctx that will be initialized as   part of tracking this set of locked reservations. ", "voiddrm_gem_lru_init(struct drm_gem_lru *lru, struct mutex *lock)": "drm_gem_lru_init - initialize a LRU     @lru: The LRU to initialize   @lock: The lock protecting the LRU ", "voiddrm_gem_object_free(struct kref *kref)": "drm_gem_lru_remove(obj);}EXPORT_SYMBOL(drm_gem_object_release);     drm_gem_object_free - free a GEM object   @kref: kref of the object to free     Called after the last reference to the object has been lost.     Frees the object ", "voiddrm_gem_lru_move_tail_locked(struct drm_gem_lru *lru, struct drm_gem_object *obj)": "drm_gem_lru_move_tail_locked - move the object to the tail of the LRU     Like &drm_gem_lru_move_tail but lru lock must be held     @lru: The LRU to move the object into.   @obj: The GEM object to move into this LRU ", "unsigned longdrm_gem_lru_scan(struct drm_gem_lru *lru, unsigned int nr_to_scan, unsigned long *remaining, bool (*shrink)(struct drm_gem_object *obj))": "drm_gem_lru_scan - helper to implement shrinker.scan_objects     If the shrink callback succeeds, it is expected that the driver   move the object out of this LRU.     If the LRU possibly contain active buffers, it is the responsibility   of the shrink callback to check for this (ie. dma_resv_test_signaled())   or if necessary block until the buffer becomes idle.     @lru: The LRU to scan   @nr_to_scan: The number of pages to try to reclaim   @remaining: The number of pages left to reclaim, should be initialized by caller   @shrink: Callback to try to shrinkreclaim the object. ", "int drm_gem_evict(struct drm_gem_object *obj)": "drm_gem_evict - helper to evict backing pages for a GEM object   @obj: obj in question ", "void drm_gem_fb_destroy(struct drm_framebuffer *fb)": "drm_gem_fb_destroy - Free GEM backed framebuffer   @fb: Framebuffer     Frees a GEM backed framebuffer with its backing buffer(s) and the structure   itself. Drivers can use this as their &drm_framebuffer_funcs->destroy   callback. ", "int drm_gem_fb_create_handle(struct drm_framebuffer *fb, struct drm_file *file,     unsigned int *handle)": "drm_gem_fb_create_handle - Create handle for GEM backed framebuffer   @fb: Framebuffer   @file: DRM file to register the handle for   @handle: Pointer to return the created handle     This function creates a handle for the GEM object backing the framebuffer.   Drivers can use this as their &drm_framebuffer_funcs->create_handle   callback. The GETFB IOCTL calls into this callback.     Returns:   0 on success or a negative error code on failure. ", "int drm_gem_fb_vmap(struct drm_framebuffer *fb, struct iosys_map *map,    struct iosys_map *data)": "drm_gem_fb_vunmap() for unmapping.     Returns:   0 on success, or a negative errno code otherwise. ", "int drm_gem_fb_begin_cpu_access(struct drm_framebuffer *fb, enum dma_data_direction dir)": "drm_gem_fb_end_cpu_access(struct drm_framebuffer  fb, enum dma_data_direction dir,unsigned int num_planes){struct dma_buf_attachment  import_attach;struct drm_gem_object  obj;int ret;while (num_planes) {--num_planes;obj = drm_gem_fb_get_obj(fb, num_planes);if (!obj)continue;import_attach = obj->import_attach;if (!import_attach)continue;ret = dma_buf_end_cpu_access(import_attach->dmabuf, dir);if (ret)drm_err(fb->dev, \"dma_buf_end_cpu_access(%u, %d) failed: %d\\n\",ret, num_planes, dir);}}     drm_gem_fb_begin_cpu_access - prepares GEM buffer objects for CPU access   @fb: the framebuffer   @dir: access mode     Prepares a framebuffer's GEM buffer objects for CPU access. This function   must be called before accessing the BO data within the kernel. For imported   BOs, the function calls dma_buf_begin_cpu_access().     See drm_gem_fb_end_cpu_access() for signalling the end of CPU access.     Returns:   0 on success, or a negative errno code otherwise. ", "struct drmres_node ": "drmm_kmalloc() and the related functions. Everything   will be released on the final drm_dev_put() in reverse order of how the   release actions have been added and memory has been allocated since driver   loading started with devm_drm_dev_alloc().     Note that release actions and managed memory can also be added and removed   during the lifetime of the driver, all the functions are fully concurrent   safe. But it is recommended to use managed resources only for resources that   change rarely, if ever, during the lifetime of the &drm_device instance. ", "void *drmm_kmalloc(struct drm_device *dev, size_t size, gfp_t gfp)": "drmm_kfree(). ", "int drm_mm_reserve_node(struct drm_mm *mm, struct drm_mm_node *node)": "drm_mm_reserve_node - insert an pre-initialized node   @mm: drm_mm allocator to insert @node into   @node: drm_mm_node to insert     This functions inserts an already set-up &drm_mm_node into the allocator,   meaning that start, size and color must be set by the caller. All other   fields must be cleared to 0. This is useful to initialize the allocator with   preallocated objects which must be set-up before the range allocator can be   set-up, e.g. when taking over a firmware framebuffer.     Returns:   0 on success, -ENOSPC if there's no hole where @node is. ", "int drm_mm_insert_node_in_range(struct drm_mm * const mm,struct drm_mm_node * const node,u64 size, u64 alignment,unsigned long color,u64 range_start, u64 range_end,enum drm_mm_insert_mode mode)": "drm_mm_insert_node_in_range - ranged search for space and insert @node   @mm: drm_mm to allocate from   @node: preallocate node to insert   @size: size of the allocation   @alignment: alignment of the allocation   @color: opaque tag value to use for this node   @range_start: start of the allowed range for this node   @range_end: end of the allowed range for this node   @mode: fine-tune the allocation search and placement     The preallocated @node must be cleared to 0.     Returns:   0 on success, -ENOSPC if there's no suitable hole. ", "void drm_mm_remove_node(struct drm_mm_node *node)": "drm_mm_remove_node - Remove a memory node from the allocator.   @node: drm_mm_node to remove     This just removes a node from its drm_mm allocator. The node does not need to   be cleared again before it can be re-inserted into this or any other drm_mm   allocator. It is a bug to call this function on a unallocated node. ", "void drm_mm_replace_node(struct drm_mm_node *old, struct drm_mm_node *new)": "drm_mm_replace_node - move an allocation from @old to @new   @old: drm_mm_node to remove from the allocator   @new: drm_mm_node which should inherit @old's allocation     This is useful for when drivers embed the drm_mm_node structure and hence   can't move allocations by reassigning pointers. It's a combination of remove   and insert with the guarantee that the allocation start will match. ", "/** * drm_mm_scan_init_with_range - initialize range-restricted lru scanning * @scan: scan state * @mm: drm_mm to scan * @size: size of the allocation * @alignment: alignment of the allocation * @color: opaque tag value to use for the allocation * @start: start of the allowed range for the allocation * @end: end of the allowed range for the allocation * @mode: fine-tune the allocation search and placement * * This simply sets up the scanning routines with the parameters for the desired * hole. * * Warning: * As long as the scan list is non-empty, no other operations than * adding/removing nodes to/from the scan list are allowed. ": "drm_mm_scan_color_evict()). Adding and removing an object is O(1), and   since freeing a node is also O(1) the overall complexity is   O(scanned_objects). So like the free stack which needs to be walked before a   scan operation even begins this is linear in the number of objects. It   doesn't seem to hurt too badly. ", "void drm_mm_init(struct drm_mm *mm, u64 start, u64 size)": "drm_mm_init - initialize a drm-mm allocator   @mm: the drm_mm structure to initialize   @start: start of the range managed by @mm   @size: end of the range managed by @mm     Note that @mm must be cleared to 0 before calling this function. ", "void drm_mm_takedown(struct drm_mm *mm)": "drm_mm_takedown - clean up a drm_mm allocator   @mm: drm_mm allocator to clean up     Note that it is a bug to call this function on an allocator which is not   clean. ", "void drm_mm_print(const struct drm_mm *mm, struct drm_printer *p)": "drm_mm_print - print allocator state   @mm: drm_mm allocator to print   @p: DRM printer to use ", "int drm_get_panel_orientation_quirk(int width, int height)": "drm_get_panel_orientation_quirk - Check for panel orientation quirks   @width: width in pixels of the panel   @height: height in pixels of the panel     This function checks for platform specific (e.g. DMI based) quirks   providing info on panel_orientation for systems where this cannot be   probed from the hard-firm-ware. To avoid false-positive this function   takes the panel resolution as argument and checks that against the   resolution expected by the quirk-table entry.     Note this function is also used outside of the drm-subsys, by for example   the efifb code. Because of this this function gets compiled into its own   kernel-module when built as a module.     Returns:   A DRM_MODE_PANEL_ORIENTATION_  value if there is a quirk for this system,   or DRM_MODE_PANEL_ORIENTATION_UNKNOWN if there is no quirk. ", "void drm_debugfs_create_files(const struct drm_info_list *files, int count,      struct dentry *root, struct drm_minor *minor)": "drm_debugfs_create_files - Initialize a given set of debugfs files for DRM   minor   @files: The array of files to create   @count: The number of files given   @root: DRI debugfs dir entry.   @minor: device minor number     Create a given set of debugfs files represented by an array of   &struct drm_info_list in the given root directory. These files will be removed   automatically on drm_debugfs_cleanup(). ", "void drm_debugfs_add_file(struct drm_device *dev, const char *name,  int (*show)(struct seq_file*, void*), void *data)": "drm_debugfs_add_files(minor->dev, drm_debugfs_list, DRM_DEBUGFS_ENTRIES);if (drm_drv_uses_atomic_modeset(dev)) {drm_atomic_debugfs_init(minor);}if (drm_core_check_feature(dev, DRIVER_MODESET)) {drm_framebuffer_debugfs_init(minor);drm_client_debugfs_init(minor);}if (dev->driver->debugfs_init)dev->driver->debugfs_init(minor);list_for_each_entry_safe(entry, tmp, &dev->debugfs_list, list) {debugfs_create_file(entry->file.name, 0444,    minor->debugfs_root, entry, &drm_debugfs_entry_fops);list_del(&entry->list);}return 0;}void drm_debugfs_late_register(struct drm_device  dev){struct drm_minor  minor = dev->primary;struct drm_debugfs_entry  entry,  tmp;if (!minor)return;list_for_each_entry_safe(entry, tmp, &dev->debugfs_list, list) {debugfs_create_file(entry->file.name, 0444,    minor->debugfs_root, entry, &drm_debugfs_entry_fops);list_del(&entry->list);}}int drm_debugfs_remove_files(const struct drm_info_list  files, int count,     struct drm_minor  minor){struct list_head  pos,  q;struct drm_info_node  tmp;int i;mutex_lock(&minor->debugfs_lock);for (i = 0; i < count; i++) {list_for_each_safe(pos, q, &minor->debugfs_list) {tmp = list_entry(pos, struct drm_info_node, list);if (tmp->info_ent == &files[i]) {debugfs_remove(tmp->dent);list_del(pos);kfree(tmp);}}}mutex_unlock(&minor->debugfs_lock);return 0;}EXPORT_SYMBOL(drm_debugfs_remove_files);static void drm_debugfs_remove_all_files(struct drm_minor  minor){struct drm_info_node  node,  tmp;mutex_lock(&minor->debugfs_lock);list_for_each_entry_safe(node, tmp, &minor->debugfs_list, list) {debugfs_remove(node->dent);list_del(&node->list);kfree(node);}mutex_unlock(&minor->debugfs_lock);}void drm_debugfs_cleanup(struct drm_minor  minor){if (!minor->debugfs_root)return;drm_debugfs_remove_all_files(minor);debugfs_remove_recursive(minor->debugfs_root);minor->debugfs_root = NULL;}     drm_debugfs_add_file - Add a given file to the DRM device debugfs file list   @dev: drm device for the ioctl   @name: debugfs file name   @show: show callback   @data: driver-private data, should not be device-specific     Add a given file entry to the DRM device debugfs file list to be created on   drm_debugfs_init. ", "bool drm_rect_intersect(struct drm_rect *r1, const struct drm_rect *r2)": "drm_rect_intersect - intersect two rectangles   @r1: first rectangle   @r2: second rectangle     Calculate the intersection of rectangles @r1 and @r2.   @r1 will be overwritten with the intersection.     RETURNS:   %true if rectangle @r1 is still visible after the operation,   %false otherwise. ", "bool drm_rect_clip_scaled(struct drm_rect *src, struct drm_rect *dst,  const struct drm_rect *clip)": "drm_rect_clip_scaled - perform a scaled clip operation   @src: source window rectangle   @dst: destination window rectangle   @clip: clip rectangle     Clip rectangle @dst by rectangle @clip. Clip rectangle @src by   the corresponding amounts, retaining the vertical and horizontal scaling   factors from @src to @dst.     RETURNS:     %true if rectangle @dst is still visible after being clipped,   %false otherwise. ", "int drm_rect_calc_hscale(const struct drm_rect *src, const struct drm_rect *dst, int min_hscale, int max_hscale)": "drm_rect_calc_hscale - calculate the horizontal scaling factor   @src: source window rectangle   @dst: destination window rectangle   @min_hscale: minimum allowed horizontal scaling factor   @max_hscale: maximum allowed horizontal scaling factor     Calculate the horizontal scaling factor as   (@src width)  (@dst width).     If the scale is below 1 << 16, round down. If the scale is above   1 << 16, round up. This will calculate the scale with the most   pessimistic limit calculation.     RETURNS:   The horizontal scaling factor, or errno of out of limits. ", "int drm_rect_calc_vscale(const struct drm_rect *src, const struct drm_rect *dst, int min_vscale, int max_vscale)": "drm_rect_calc_vscale - calculate the vertical scaling factor   @src: source window rectangle   @dst: destination window rectangle   @min_vscale: minimum allowed vertical scaling factor   @max_vscale: maximum allowed vertical scaling factor     Calculate the vertical scaling factor as   (@src height)  (@dst height).     If the scale is below 1 << 16, round down. If the scale is above   1 << 16, round up. This will calculate the scale with the most   pessimistic limit calculation.     RETURNS:   The vertical scaling factor, or errno of out of limits. ", "void drm_rect_debug_print(const char *prefix, const struct drm_rect *r, bool fixed_point)": "drm_rect_debug_print - print the rectangle information   @prefix: prefix string   @r: rectangle to print   @fixed_point: rectangle is in 16.16 fixed point format ", "void drm_rect_rotate(struct drm_rect *r,     int width, int height,     unsigned int rotation)": "drm_rect_rotate - Rotate the rectangle   @r: rectangle to be rotated   @width: Width of the coordinate space   @height: Height of the coordinate space   @rotation: Transformation to be applied     Apply @rotation to the coordinates of rectangle @r.     @width and @height combined with @rotation define   the location of the new origin.     @width correcsponds to the horizontal and @height   to the vertical axis of the untransformed coordinate   space. ", "void drm_rect_rotate_inv(struct drm_rect *r, int width, int height, unsigned int rotation)": "drm_rect_rotate_inv - Inverse rotate the rectangle   @r: rectangle to be rotated   @width: Width of the coordinate space   @height: Height of the coordinate space   @rotation: Transformation whose inverse is to be applied     Apply the inverse of @rotation to the coordinates   of rectangle @r.     @width and @height combined with @rotation define   the location of the new origin.     @width correcsponds to the horizontal and @height   to the vertical axis of the original untransformed   coordinate space, so that you never have to flip   them when doing a rotatation and its inverse.   That is, if you do ::         drm_rect_rotate(&r, width, height, rotation);       drm_rect_rotate_inv(&r, width, height, rotation);     you will always get back the original rectangle. ", "unsigned long __drm_debug;EXPORT_SYMBOL(__drm_debug": "__drm_debug: Enable debug output.   Bitmask of DRM_UT_x. See includedrmdrm_print.h for details. ", "memcpy(iterator->data,str + (iterator->start - iterator->offset), copy);iterator->offset = iterator->start + copy;iterator->remain -= copy;} else ": "drm_puts_coredump(struct drm_printer  p, const char  str){struct drm_print_iterator  iterator = p->arg;ssize_t len;if (!iterator->remain)return;if (iterator->offset < iterator->start) {ssize_t copy;len = strlen(str);if (iterator->offset + len <= iterator->start) {iterator->offset += len;return;}copy = len - (iterator->start - iterator->offset);if (copy > iterator->remain)copy = iterator->remain;  Copy out the bit of the string that we need ", "len = snprintf(NULL, 0, \"%pV\", vaf);/* This is the easiest path, we've already advanced beyond the offset ": "drm_printfn_coredump(struct drm_printer  p, struct va_format  vaf){struct drm_print_iterator  iterator = p->arg;size_t len;char  buf;if (!iterator->remain)return;  Figure out how big the string will be ", "printk(KERN_DEBUG \"%s %pV\", p->prefix, vaf);}EXPORT_SYMBOL(__drm_printfn_debug": "__drm_printfn_debug(struct drm_printer  p, struct va_format  vaf){  pr_debug callsite decorations are unhelpful here ", "void drm_print_bits(struct drm_printer *p, unsigned long value,    const char * const bits[], unsigned int nbits)": "drm_print_bits - print bits to a &drm_printer stream     Print bits (in flag fields for example) in human readable form.     @p: the &drm_printer   @value: field value.   @bits: Array with bit names.   @nbits: Size of bit names array. ", "va_start(args, format);vaf.fmt = format;vaf.va = &args;if (dev)dev_printk(KERN_DEBUG, dev, \"[\" DRM_NAME \":%ps] %pV\",   __builtin_return_address(0), &vaf);elseprintk(KERN_DEBUG \"[\" DRM_NAME \":%ps] %pV\",       __builtin_return_address(0), &vaf);va_end(args);}EXPORT_SYMBOL(__drm_dev_dbg": "__drm_dev_dbg(struct _ddebug  desc, const struct device  dev,   enum drm_debug_category category, const char  format, ...){struct va_format vaf;va_list args;if (!__drm_debug_enabled(category))return;  we know we are printing for either syslog, tracefs, or both ", "void drm_print_regset32(struct drm_printer *p, struct debugfs_regset32 *regset)": "drm_print_regset32 - print the contents of registers to a   &drm_printer stream.     @p: the &drm printer   @regset: the list of registers to print.     Often in driver debug, it's useful to be able to either capture the   contents of registers in the steady state using debugfs or at   specific points during operation.  This lets the driver have a   single list of registers for both. ", "if (drm_dev_has_vblank(dev)) ": "drm_legacy_irq_uninstall(struct drm_device  dev){unsigned long irqflags;bool irq_enabled;int i;irq_enabled = dev->irq_enabled;dev->irq_enabled = false;    Wake up any waiters so they don't hang. This is just to paper over   issues for UMS drivers which aren't in full control of their   vblankirq handling. KMS drivers must ensure that vblanks are all   disabled when uninstalling the irq handler. ", "int drm_client_init(struct drm_device *dev, struct drm_client_dev *client,    const char *name, const struct drm_client_funcs *funcs)": "drm_client_release().     Returns:   Zero on success or negative error code on failure. ", "void drm_client_dev_hotplug(struct drm_device *dev)": "drm_client_dev_hotplug - Send hotplug event to clients   @dev: DRM device     This function calls the &drm_client_funcs.hotplug callback on the attached clients.     drm_kms_helper_hotplug_event() calls this function, so drivers that use it   don't need to call this function themselves. ", "intdrm_client_buffer_vmap(struct drm_client_buffer *buffer,       struct iosys_map *map_copy)": "drm_client_buffer_vunmap(); or the client buffer should be mapped   throughout its lifetime.     The returned address is a copy of the internal value. In contrast to   other vmap interfaces, you don't need it for the client's vunmap   function. So you can modify it at will during blit and draw operations.     Returns:  0 on success, or a negative errno code otherwise. ", "struct drm_client_buffer *drm_client_framebuffer_create(struct drm_client_dev *client, u32 width, u32 height, u32 format)": "drm_client_framebuffer_delete() to free the buffer.     Returns:   Pointer to a client buffer or an error pointer on failure. ", "int drm_client_framebuffer_flush(struct drm_client_buffer *buffer, struct drm_rect *rect)": "drm_client_framebuffer_flush - Manually flush client framebuffer   @buffer: DRM client buffer (can be NULL)   @rect: Damage rectangle (if NULL flushes all)     This calls &drm_framebuffer_funcs->dirty (if present) to flush buffer changes   for drivers that need it.     Returns:   Zero on success or negative error code on failure. ", "void drm_privacy_screen_lookup_add(struct drm_privacy_screen_lookup *lookup)": "drm_privacy_screen_lookup_remove(). ", "struct drm_privacy_screen *drm_privacy_screen_get(struct device *dev,  const char *con_id)": "drm_privacy_screen_get_by_name(const char  name){struct drm_privacy_screen  priv;struct device  dev = NULL;mutex_lock(&drm_privacy_screen_devs_lock);list_for_each_entry(priv, &drm_privacy_screen_devs, list) {if (strcmp(dev_name(&priv->dev), name) == 0) {dev = get_device(&priv->dev);break;}}mutex_unlock(&drm_privacy_screen_devs_lock);return dev ? to_drm_privacy_screen(dev) : NULL;}     drm_privacy_screen_get - get a privacy-screen provider   @dev: consumer-device for which to get a privacy-screen provider   @con_id: (video)connector name for which to get a privacy-screen provider     Get a privacy-screen provider for a privacy-screen attached to the   display described by the @dev and @con_id parameters.     Return:     A pointer to a &struct drm_privacy_screen on success.     ERR_PTR(-ENODEV) if no matching privacy-screen is found     ERR_PTR(-EPROBE_DEFER) if there is a matching privacy-screen,                            but it has not been registered yet. ", "void drm_privacy_screen_put(struct drm_privacy_screen *priv)": "drm_privacy_screen_put - release a privacy-screen reference   @priv: privacy screen reference to release     Release a privacy-screen provider reference gotten through   drm_privacy_screen_get(). May be called with a NULL or ERR_PTR,   in which case it is a no-op. ", "int drm_privacy_screen_set_sw_state(struct drm_privacy_screen *priv,    enum drm_privacy_screen_status sw_state)": "drm_privacy_screen_set_sw_state - set a privacy-screen's sw-state   @priv: privacy screen to set the sw-state for   @sw_state: new sw-state value to set     Set the sw-state of a privacy screen. If the privacy-screen is not   in a locked hw-state, then the actual and hw-state of the privacy-screen   will be immediately updated to the new value. If the privacy-screen is   in a locked hw-state, then the new sw-state will be remembered as the   requested state to put the privacy-screen in when it becomes unlocked.     Return: 0 on success, negative error code on failure. ", "void drm_privacy_screen_get_state(struct drm_privacy_screen *priv,  enum drm_privacy_screen_status *sw_state_ret,  enum drm_privacy_screen_status *hw_state_ret)": "drm_privacy_screen_get_state - get privacy-screen's current state   @priv: privacy screen to get the state for   @sw_state_ret: address where to store the privacy-screens current sw-state   @hw_state_ret: address where to store the privacy-screens current hw-state     Get the current state of a privacy-screen, both the sw-state and the   hw-state. ", "int drm_privacy_screen_register_notifier(struct drm_privacy_screen *priv, struct notifier_block *nb)": "drm_privacy_screen_register_notifier - register a notifier   @priv: Privacy screen to register the notifier with   @nb: Notifier-block for the notifier to register     Register a notifier with the privacy-screen to be notified of changes made   to the privacy-screen state from outside of the privacy-screen class.   E.g. the state may be changed by the hardware itself in response to a   hotkey press.     The notifier is called with no locks held. The new hw_state and sw_state   can be retrieved using the drm_privacy_screen_get_state() function.   A pointer to the drm_privacy_screen's struct is passed as the ``void  data``   argument of the notifier_block's notifier_call.     The notifier will NOT be called when changes are made through   drm_privacy_screen_set_sw_state(). It is only called for external changes.     Return: 0 on success, negative error code on failure. ", "int drm_privacy_screen_unregister_notifier(struct drm_privacy_screen *priv,   struct notifier_block *nb)": "drm_privacy_screen_unregister_notifier - unregister a notifier   @priv: Privacy screen to register the notifier with   @nb: Notifier-block for the notifier to register     Unregister a notifier registered with drm_privacy_screen_register_notifier().     Return: 0 on success, negative error code on failure. ", "void drm_privacy_screen_call_notifier_chain(struct drm_privacy_screen *priv)": "drm_privacy_screen_call_notifier_chain - notify consumers of state change   @priv: Privacy screen to register the notifier with     A privacy-screen provider driver can call this functions upon external   changes to the privacy-screen state. E.g. the state may be changed by the   hardware itself in response to a hotkey press.   This function must be called without holding the privacy-screen lock.   the driver must update sw_state and hw_state to reflect the new state before   calling this function.   The expected behavior from the driver upon receiving an external state   change event is: 1. Take the lock; 2. Update sw_state and hw_state;   3. Release the lock. 4. Call drm_privacy_screen_call_notifier_chain(). ", "struct drm_prime_member ": "drm_gem_prime_import() already. ", "/** * drm_gem_map_attach - dma_buf attach implementation for GEM * @dma_buf: buffer to attach device to * @attach: buffer attachment data * * Calls &drm_gem_object_funcs.pin for device specific handling. This can be * used as the &dma_buf_ops.attach callback. Must be used together with * drm_gem_map_detach(). * * Returns 0 on success, negative error code on failure. ": "drm_gem_dmabuf_mmap().     Note that these export helpers can only be used if the underlying backing   storage is fully coherent and either permanently pinned, or it is safe to pin   it indefinitely.     FIXME: The underlying helper functions are named rather inconsistently.     Importing buffers   ~~~~~~~~~~~~~~~~~     Importing dma-bufs using drm_gem_prime_import() relies on   &drm_driver.gem_prime_import_sg_table.     Note that similarly to the export helpers this permanently pins the   underlying backing storage. Which is ok for scanout, but is not the best   option for sharing lots of buffers for rendering. ", "int drm_gem_prime_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)": "drm_gem_prime_mmap - PRIME mmap function for GEM drivers   @obj: GEM object   @vma: Virtual address range     This function sets up a userspace mapping for PRIME exported buffers using   the same codepath that is used for regular GEM buffer mapping on the DRM fd.   The fake GEM offset is added to vma->vm_pgoff and &drm_driver->fops->mmap is   called to set up the mapping.     Drivers can use this as their &drm_driver.gem_prime_mmap callback. ", "struct sg_table *drm_prime_pages_to_sg(struct drm_device *dev,       struct page **pages, unsigned int nr_pages)": "drm_prime_pages_to_sg - converts a page array into an sg list   @dev: DRM device   @pages: pointer to the array of page pointers to convert   @nr_pages: length of the page vector     This helper creates an sg table object from a set of pages   the driver is responsible for mapping the pages into the   importers address space for use with dma_buf itself.     This is useful for implementing &drm_gem_object_funcs.get_sg_table. ", "unsigned long drm_prime_get_contiguous_size(struct sg_table *sgt)": "drm_prime_get_contiguous_size - returns the contiguous size of the buffer   @sgt: sg_table describing the buffer to check     This helper calculates the contiguous size in the DMA address space   of the buffer described by the provided sg_table.     This is useful for implementing   &drm_gem_object_funcs.gem_prime_import_sg_table. ", "return dmabuf;}/* * Note that callers do not need to clean up the export cache * since the check for obj->handle_count guarantees that someone * will clean it up. ": "drm_gem_prime_export(obj, flags);if (IS_ERR(dmabuf)) {  normally the created dma-buf takes ownership of the ref,   but if that fails then drop the ref ", "struct drm_gem_object *drm_gem_prime_import_dev(struct drm_device *dev,    struct dma_buf *dma_buf,    struct device *attach_dev)": "drm_prime_gem_destroy() from their   &drm_gem_object_funcs.free hook when using this function. ", "int __deprecated drm_prime_sg_to_page_array(struct sg_table *sgt,    struct page **pages,    int max_entries)": "drm_prime_sg_to_page_array - convert an sg table into a page array   @sgt: scatter-gather table to convert   @pages: array of page pointers to store the pages in   @max_entries: size of the passed-in array     Exports an sg table into an array of pages.     This function is deprecated and strongly discouraged to be used.   The page array is only useful for page faults and those can corrupt fields   in the struct page if they are not handled by the exporting driver. ", "int drm_prime_sg_to_dma_addr_array(struct sg_table *sgt, dma_addr_t *addrs,   int max_entries)": "drm_prime_sg_to_dma_addr_array - convert an sg table into a dma addr array   @sgt: scatter-gather table to convert   @addrs: array to store the dma bus address of each page   @max_entries: size of both the passed-in arrays     Exports an sg table into an array of addresses.     Drivers should use this in their &drm_driver.gem_prime_import_sg_table   implementation. ", "int drm_atomic_set_mode_for_crtc(struct drm_crtc_state *state, const struct drm_display_mode *mode)": "drm_atomic_set_mode_for_crtc - set mode for CRTC   @state: the CRTC whose incoming state to update   @mode: kernel-internal mode to use for the CRTC, or NULL to disable     Set a mode (originating from the kernel) on the desired CRTC state and update   the enable property.     RETURNS:   Zero on success, error code on failure. Cannot return -EDEADLK. ", "int drm_atomic_set_mode_prop_for_crtc(struct drm_crtc_state *state,      struct drm_property_blob *blob)": "drm_atomic_set_mode_prop_for_crtc - set mode for CRTC   @state: the CRTC whose incoming state to update   @blob: pointer to blob property to use for mode     Set a mode (originating from a blob property) on the desired CRTC state.   This function will take a reference on the blob property for the CRTC state,   and release the reference held on the state's existing mode property, if any   was set.     RETURNS:   Zero on success, error code on failure. Cannot return -EDEADLK. ", "intdrm_atomic_set_crtc_for_plane(struct drm_plane_state *plane_state,      struct drm_crtc *crtc)": "drm_atomic_set_crtc_for_plane - set CRTC for plane   @plane_state: the plane whose incoming state to update   @crtc: CRTC to use for the plane     Changing the assigned CRTC for a plane requires us to grab the lock and state   for the new CRTC, as needed. This function takes care of all these details   besides updating the pointer in the state object itself.     Returns:   0 on success or can fail with -EDEADLK or -ENOMEM. When the error is EDEADLK   then the ww mutex code has detected a deadlock and the entire atomic   sequence must be restarted. All other errors are fatal. ", "voiddrm_atomic_set_fb_for_plane(struct drm_plane_state *plane_state,    struct drm_framebuffer *fb)": "drm_atomic_set_fb_for_plane - set framebuffer for plane   @plane_state: atomic state object for the plane   @fb: fb to use for the plane     Changing the assigned framebuffer for a plane requires us to grab a reference   to the new fb and drop the reference to the old fb, if there is one. This   function takes care of all these details besides updating the pointer in the   state object itself. ", "intdrm_atomic_set_crtc_for_connector(struct drm_connector_state *conn_state,  struct drm_crtc *crtc)": "drm_atomic_set_crtc_for_connector - set CRTC for connector   @conn_state: atomic state object for the connector   @crtc: CRTC to use for the connector     Changing the assigned CRTC for a connector requires us to grab the lock and   state for the new CRTC, as needed. This function takes care of all these   details besides updating the pointer in the state object itself.     Returns:   0 on success or can fail with -EDEADLK or -ENOMEM. When the error is EDEADLK   then the ww mutex code has detected a deadlock and the entire atomic   sequence must be restarted. All other errors are fatal. ", "int drm_plane_helper_update_primary(struct drm_plane *plane, struct drm_crtc *crtc,    struct drm_framebuffer *fb,    int crtc_x, int crtc_y,    unsigned int crtc_w, unsigned int crtc_h,    uint32_t src_x, uint32_t src_y,    uint32_t src_w, uint32_t src_h,    struct drm_modeset_acquire_ctx *ctx)": "drm_plane_helper_update_primary - Helper for updating primary planes   @plane: plane to update   @crtc: the plane's new CRTC   @fb: the plane's new framebuffer   @crtc_x: x coordinate within CRTC   @crtc_y: y coordinate within CRTC   @crtc_w: width coordinate within CRTC   @crtc_h: height coordinate within CRTC   @src_x: x coordinate within source   @src_y: y coordinate within source   @src_w: width coordinate within source   @src_h: height coordinate within source   @ctx: modeset locking context     This helper validates the given parameters and updates the primary plane.     This function is only useful for non-atomic modesetting. Don't use   it in new drivers.     Returns:   Zero on success, or an errno code otherwise. ", "int drm_plane_helper_disable_primary(struct drm_plane *plane,     struct drm_modeset_acquire_ctx *ctx)": "drm_plane_helper_disable_primary - Helper for disabling primary planes   @plane: plane to disable   @ctx: modeset locking context     This helper returns an error when trying to disable the primary   plane.     This function is only useful for non-atomic modesetting. Don't use   it in new drivers.     Returns:   An errno code. ", "void drm_plane_helper_destroy(struct drm_plane *plane)": "drm_plane_helper_destroy() - Helper for primary plane destruction   @plane: plane to destroy     Provides a default plane destroy handler for primary planes.  This handler   is called during CRTC destruction.  We disable the primary plane, remove   it from the DRM plane list, and deallocate the plane structure. ", "int drm_plane_helper_atomic_check(struct drm_plane *plane, struct drm_atomic_state *state)": "drm_plane_helper_atomic_check() - Helper to check plane atomic-state   @plane: plane to check   @state: atomic state object     Provides a default plane-state check handler for planes whose atomic-state   scale and positioning are not expected to change since the plane is always   a fullscreen scanout buffer.     This is often the case for the primary plane of simple framebuffers. See   also drm_crtc_helper_atomic_check() for the respective CRTC-state check   helper function.     RETURNS:   Zero on success, or an errno code otherwise. ", "voiddrm_self_refresh_helper_update_avg_times(struct drm_atomic_state *state, unsigned int commit_time_ms, unsigned int new_self_refresh_mask)": "drm_self_refresh_helper_update_avg_times - Updates a crtc's SR time averages   @state: the state which has just been applied to hardware   @commit_time_ms: the amount of time in ms that this commit took to complete   @new_self_refresh_mask: bitmask of crtc's that have self_refresh_active in      new state     Called after &drm_mode_config_funcs.atomic_commit_tail, this function will   update the average entryexit self refresh times on self refresh transitions.   These averages will be used when calculating how long to delay before   entering self refresh mode after activity. ", "void drm_self_refresh_helper_alter_state(struct drm_atomic_state *state)": "drm_self_refresh_helper_alter_state - Alters the atomic state for SR exit   @state: the state currently being checked     Called at the end of atomic check. This function checks the state for flags   incompatible with self refresh exit and changes them. This is a bit   disingenuous since userspace is expecting one thing and we're giving it   another. However in order to keep self refresh entirely hidden from   userspace, this is required.     At the end, we queue up the self refresh entry work so we can enter PSR after   the desired delay. ", "#define SELF_REFRESH_AVG_SEED_MS 200DECLARE_EWMA(psr_time, 4, 4)struct drm_self_refresh_data ": "drm_self_refresh_helper_cleanup).   The connector is responsible for setting   &drm_connector_state.self_refresh_aware to true at runtime if it is SR-aware   (meaning it knows how to initiate self refresh on the panel).     Once a crtc has enabled SR using &drm_self_refresh_helper_init, the   helpers will monitor activity and call back into the driver to enabledisable   SR as appropriate. The best way to think about this is that it's a DPMS   onoff request with &drm_crtc_state.self_refresh_active set in crtc state   that tells you to disableenable SR on the panel instead of power-cycling it.     During SR, drivers may choose to fully disable their crtcencoderbridge   hardware (in which case no driver changes are necessary), or they can inspect   &drm_crtc_state.self_refresh_active if they want to enter low power mode   without full disable (in case full disableenable is too slow).     SR will be deactivated if there are any atomic updates affecting the   pipe that is in SR mode. If a crtc is driving multiple connectors, all   connectors must be SR aware and all will enterexit SR mode at the same time.     If the crtc and connector are SR aware, but the panel connected does not   support it (or is otherwise unable to enter SR), the driver should fail   atomic_check when &drm_crtc_state.self_refresh_active is true. ", "static DEFINE_WW_CLASS(crtc_ww_class);#if IS_ENABLED(CONFIG_DRM_DEBUG_MODESET_LOCK)static noinline depot_stack_handle_t __drm_stack_depot_save(void)": "drm_modeset_lock_all_ctx().     If all that is needed is a single modeset lock, then the &struct   drm_modeset_acquire_ctx is not needed and the locking can be simplified   by passing a NULL instead of ctx in the drm_modeset_lock() call or   calling  drm_modeset_lock_single_interruptible(). To unlock afterwards   call drm_modeset_unlock().     On top of these per-object locks using &ww_mutex there's also an overall   &drm_mode_config.mutex, for protecting everything else. Mostly this means   probe state of connectors, and preventing hotplug addremoval of connectors.     Finally there's a bunch of dedicated locks to protect drm core internal   lists and lookup data structures. ", "void drm_modeset_lock_all(struct drm_device *dev)": "drm_modeset_unlock_all() function.     This function is deprecated. It allocates a lock acquisition context and   stores it in &drm_device.mode_config. This facilitate conversion of   existing code because it removes the need to manually deal with the   acquisition context, but it is also brittle because the context is global   and care must be taken not to nest calls. New code should use the   drm_modeset_lock_all_ctx() function and pass in the context explicitly. ", "void drm_modeset_unlock_all(struct drm_device *dev)": "drm_warn_on_modeset_not_all_locked(dev);}EXPORT_SYMBOL(drm_modeset_lock_all);     drm_modeset_unlock_all - drop all modeset locks   @dev: DRM device     This function drops all modeset locks taken by a previous call to the   drm_modeset_lock_all() function.     This function is deprecated. It uses the lock acquisition context stored   in &drm_device.mode_config. This facilitates conversion of existing   code because it removes the need to manually deal with the acquisition   context, but it is also brittle because the context is global and care must   be taken not to nest calls. New code should pass the acquisition context   directly to the drm_modeset_drop_locks() function. ", "void drm_modeset_lock_init(struct drm_modeset_lock *lock)": "drm_modeset_lock_init - initialize lock   @lock: lock to init ", "struct drm_syncobj *drm_syncobj_find(struct drm_file *file_private,     u32 handle)": "drm_syncobj_find - lookup and reference a sync object.   @file_private: drm file private pointer   @handle: sync object handle to lookup.     Returns a reference to the syncobj pointed to by handle or NULL. The   reference must be released by calling drm_syncobj_put(). ", "void drm_syncobj_add_point(struct drm_syncobj *syncobj,   struct dma_fence_chain *chain,   struct dma_fence *fence,   uint64_t point)": "drm_syncobj_add_point - add new timeline point to the syncobj   @syncobj: sync object to add timeline point do   @chain: chain node to use to add the point   @fence: fence to encapsulate in the chain node   @point: sequence number to use for the point     Add the chain node as new timeline point to the syncobj. ", "void drm_syncobj_replace_fence(struct drm_syncobj *syncobj,       struct dma_fence *fence)": "drm_syncobj_replace_fence - replace fence in a sync object.   @syncobj: Sync object to replace fence in   @fence: fence to install in sync file.     This replaces the fence on a sync object. ", "int drm_syncobj_find_fence(struct drm_file *file_private,   u32 handle, u64 point, u64 flags,   struct dma_fence **fence)": "drm_syncobj_find_fence - lookup and reference the fence in a sync object   @file_private: drm file private pointer   @handle: sync object handle to lookup.   @point: timeline point   @flags: DRM_SYNCOBJ_WAIT_FLAGS_WAIT_FOR_SUBMIT or not   @fence: out parameter for the fence     This is just a convenience function that combines drm_syncobj_find() and   drm_syncobj_fence_get().     Returns 0 on success or a negative error value on failure. On success @fence   contains a reference to the fence, which must be released by calling   dma_fence_put(). ", "void drm_syncobj_free(struct kref *kref)": "drm_syncobj_free - free a sync object.   @kref: kref to free.     Only to be called from kref_put in drm_syncobj_put. ", "int drm_syncobj_create(struct drm_syncobj **out_syncobj, uint32_t flags,       struct dma_fence *fence)": "drm_syncobj_get_fd().     Returns 0 on success or a negative error value on failure. ", "signed long drm_timeout_abs_to_jiffies(int64_t timeout_nsec)": "drm_timeout_abs_to_jiffies - calculate jiffies timeout from absolute value     @timeout_nsec: timeout nsec component in ns, 0 for poll     Calculate the timeout in jiffies from an absolute time in secnsec. ", "static const struct drm_prop_enum_list drm_encoder_enum_list[] = ": "drm_encoder_cleanup(). ", "int drm_encoder_init(struct drm_device *dev,     struct drm_encoder *encoder,     const struct drm_encoder_funcs *funcs,     int encoder_type, const char *name, ...)": "drmm_encoder_init()   instead of drm_encoder_init() to let the DRM managed resource   infrastructure take care of cleanup and deallocation.     Returns:   Zero on success, error code on failure. ", "/** * drm_file_alloc - allocate file context * @minor: minor to allocate on * * This allocates a new DRM file context. It is not linked into any context and * can be used by the caller freely. Note that the context keeps a pointer to * @minor, so it must be freed before @minor is. * * RETURNS: * Pointer to newly allocated context, ERR_PTR on failure. ": "drm_send_event() as the main starting points.     The memory mapping implementation will vary depending on how the driver   manages memory. Legacy drivers will use the deprecated drm_legacy_mmap()   function, modern drivers should use one of the provided memory-manager   specific implementations. For GEM-based drivers this is drm_gem_mmap().     No other file operations are supported by the DRM userspace API. Overall the   following is an example &file_operations structure::         static const example_drm_fops = {               .owner = THIS_MODULE,               .open = drm_open,               .release = drm_release,               .unlocked_ioctl = drm_ioctl,               .compat_ioctl = drm_compat_ioctl,  NULL if CONFIG_COMPAT=n               .poll = drm_poll,               .read = drm_read,               .llseek = no_llseek,               .mmap = drm_gem_mmap,       };     For plain GEM based drivers there is the DEFINE_DRM_GEM_FOPS() macro, and for   DMA based drivers there is the DEFINE_DRM_GEM_DMA_FOPS() macro to make this   simpler.     The driver's &file_operations must be stored in &drm_driver.fops.     For driver-private IOCTL handling see the more detailed discussion in   :ref:`IOCTL support in the userland interfaces chapter<drm_driver_ioctl>`. ", "int drm_release_noglobal(struct inode *inode, struct file *filp)": "drm_release_noglobal - release method for DRM file   @inode: device inode   @filp: file pointer.     This function may be used by drivers as their &file_operations.release   method. It frees any resources associated with the open file prior to taking   the drm_global_mutex, which then calls the &drm_driver.postclose driver   callback. If this is the last open file for the DRM device also proceeds to   call the &drm_driver.lastclose driver callback.     RETURNS:     Always succeeds and returns 0. ", "int drm_event_reserve_init_locked(struct drm_device *dev,  struct drm_file *file_priv,  struct drm_pending_event *p,  struct drm_event *e)": "drm_send_event_locked() to signal completion of the   asynchronous event to userspace.     If callers embedded @p into a larger structure it must be allocated with   kmalloc and @p must be the first member element.     This is the locked version of drm_event_reserve_init() for callers which   already hold &drm_device.event_lock.     RETURNS:     0 on success or a negative error code on failure. ", "void drm_send_event_timestamp_locked(struct drm_device *dev,     struct drm_pending_event *e, ktime_t timestamp)": "drm_send_event_timestamp_locked - send DRM event to file descriptor   @dev: DRM device   @e: DRM event to deliver   @timestamp: timestamp to set for the fence event in kernel's CLOCK_MONOTONIC   time domain     This function sends the event @e, initialized with drm_event_reserve_init(),   to its associated userspace DRM file. Callers must already hold   &drm_device.event_lock.     Note that the core will take care of unlinking and disarming events when the   corresponding DRM file is closed. Drivers need not worry about whether the   DRM file for this event still exists and can call this function upon   completion of the asynchronous work unconditionally. ", "void drm_print_memory_stats(struct drm_printer *p,    const struct drm_memory_stats *stats,    enum drm_gem_object_status supported_status,    const char *region)": "drm_print_memory_stats - A helper to print memory stats   @p: The printer to print output to   @stats: The collected memory stats   @supported_status: Bitmask of optional stats which are available   @region: The memory region   ", "void drm_show_memory_stats(struct drm_printer *p, struct drm_file *file)": "drm_show_memory_stats - Helper to collect and show standard fdinfo memory stats   @p: the printer to print output to   @file: the DRM file     Helper to iterate over GEM objects with a handle allocated in the specified   file. ", "void drm_show_fdinfo(struct seq_file *m, struct file *f)": "drm_show_fdinfo - helper for drm file fops   @m: output stream   @f: the device file instance     Helper to implement fdinfo, for userspace to query usage stats, etc, of a   process using the GPU.  See also &drm_driver.show_fdinfo.     For text output format description please see Documentationgpudrm-usage-stats.rst ", "int drm_noop(struct drm_device *dev, void *data,     struct drm_file *file_priv)": "drm_noop - DRM no-op ioctl implementation   @dev: DRM device for the ioctl   @data: data pointer for the ioctl   @file_priv: DRM file for the ioctl call     This no-op implementation for drm ioctls is useful for deprecated   functionality where we can't return a failure code because existing userspace   checks the result of the ioctl, but doesn't care about the action.     Always returns successfully with 0. ", "/* * Get the bus id. * * \\param inode device inode. * \\param file_priv DRM file private. * \\param cmd command. * \\param arg user argument, pointing to a drm_unique structure. * \\return zero on success or a negative number on failure. * * Copies the bus id from drm_device::unique into user space. ": "drm_ioctl.h>#include <drmdrm_print.h>#include \"drm_crtc_internal.h\"#include \"drm_internal.h\"#include \"drm_legacy.h\"     DOC: getunique and setversion story     BEWARE THE DRAGONS! MIND THE TRAPDOORS!     In an attempt to warn anyone else who's trying to figure out what's going   on here, I'll try to summarize the story. First things first, let's clear up   the names, because the kernel internals, libdrm and the ioctls are all named   differently:      - GET_UNIQUE ioctl, implemented by drm_getunique is wrapped up in libdrm      through the drmGetBusid function.    - The libdrm drmSetBusid function is backed by the SET_UNIQUE ioctl. All      that code is nerved in the kernel with drm_invalid_op().    - The internal set_busid kernel functions and driver callbacks are      exclusively use by the SET_VERSION ioctl, because only drm 1.0 (which is      nerved) allowed userspace to set the busid through the above ioctl.    - Other ioctls and functions involved are named consistently.     For anyone wondering what's the difference between drm 1.1 and 1.4: Correctly   handling pci domains in the busid on ppc. Doing this correctly was only   implemented in libdrm in 2010, hence can't be nerved yet. No one knows what's   special with drm 1.2 and 1.3.     Now the actual horror story of how device lookup in drm works. At large,   there's 2 different ways, either by busid, or by device driver name.     Opening by busid is fairly simple:     1. First call SET_VERSION to make sure pci domains are handled properly. As a      side-effect this fills out the unique name in the master structure.   2. Call GET_UNIQUE to read out the unique name from the master structure,      which matches the busid thanks to step 1. If it doesn't, proceed to try      the next device node.     Opening by name is slightly different:     1. Directly call VERSION to get the version and to match against the driver      name returned by that ioctl. Note that SET_VERSION is not called, which      means the unique name for the master node just opening is _not_ filled      out. This despite that with current drm device nodes are always bound to      one device, and can't be runtime assigned like with drm 1.0.   2. Match driver name. If it mismatches, proceed to the next device node.   3. Call GET_UNIQUE, and check whether the unique name has length zero (by      checking that the first byte in the string is 0). If that's not the case      libdrm skips and proceeds to the next device node. Probably this is just      copypasta from drm 1.0 times where a set unique name meant that the driver      was in use already, but that's just conjecture.     Long story short: To keep the open by name logic working, GET_UNIQUE must   _not_ return a unique string when SET_VERSION hasn't been called yet,   otherwise libdrm breaks. Even when that unique string can't ever change, and   is totally irrelevant for actually opening the device because runtime   assignable device instances were only support in drm 1.0, which is long dead.   But the libdrm code in drmOpenByName somehow survived, hence this can't be   broken. ", "if (likely(!drm_core_check_feature(dev, DRIVER_LEGACY)) ||    (flags & DRM_UNLOCKED))retcode = func(dev, kdata, file_priv);else ": "drm_ioctl_kernel(struct file  file, drm_ioctl_t  func, void  kdata,      u32 flags){struct drm_file  file_priv = file->private_data;struct drm_device  dev = file_priv->minor->dev;int retcode;if (drm_dev_is_unplugged(dev))return -ENODEV;retcode = drm_ioctl_permit(flags, file_priv);if (unlikely(retcode))return retcode;  Enforce sane locking for modern driver ioctls. ", "bool drm_ioctl_flags(unsigned int nr, unsigned int *flags)": "drm_ioctl_flags - Check for core ioctl and return ioctl permission flags   @nr: ioctl number   @flags: where to return the ioctl permission flags     This ioctl is only used by the vmwgfx driver to augment the access checks   done by the drm core and insofar a pretty decent layering violation. This   shouldn't be used by any drivers.     Returns:   True if the @nr corresponds to a DRM core ioctl number, false otherwise. ", "void drm_fbdev_dma_setup(struct drm_device *dev, unsigned int preferred_bpp)": "drm_fbdev_dma_setup() - Setup fbdev emulation for GEM DMA helpers   @dev: DRM device   @preferred_bpp: Preferred bits per pixel for the device.                   32 is used if this is zero.     This function sets up fbdev emulation for GEM DMA drivers that support   dumb buffers with a virtual address and that can be mmap'ed.   drm_fbdev_dma_setup() shall be called after the DRM driver registered   the new DRM device with drm_dev_register().     Restore, hotplug events and teardown are all taken care of. Drivers that do   suspendresume need to call drm_fb_helper_set_suspend_unlocked() themselves.   Simple drivers might use drm_mode_config_helper_suspend().     This function is safe to call even when there are no connectors present.   Setup will be retried on the next hotplug event.     The fbdev is destroyed by drm_dev_unregister(). ", "struct drm_mode_object *drm_mode_object_find(struct drm_device *dev,struct drm_file *file_priv,uint32_t id, uint32_t type)": "drm_mode_object_put(). ", "void drm_mode_object_put(struct drm_mode_object *obj)": "drm_mode_object_get(). ", "void drm_object_attach_property(struct drm_mode_object *obj,struct drm_property *property,uint64_t init_val)": "drm_object_attach_property - attach a property to a modeset object   @obj: drm modeset object   @property: property to attach   @init_val: initial value of the property     This attaches the given property to the modeset object with the given initial   value. Currently this function cannot fail since the properties are stored in   a statically sized array.     Note that all properties must be attached before the object itself is   registered and accessible from userspace. ", "int drm_object_property_set_value(struct drm_mode_object *obj,  struct drm_property *property, uint64_t val)": "drm_object_property_set_value - set the value of a property   @obj: drm mode object to set property value for   @property: property to set   @val: value the property should be set to     This function sets a given property on a given object. This function only   changes the software state of the property, it does not call into the   driver's ->set_property callback.     Note that atomic drivers should not have any need to call this, the core will   ensure consistency of values reported back to userspace through the   appropriate ->atomic_get_property callback. Only legacy drivers should call   this function to update the tracked value (after clamping and other   restrictions have been applied).     Returns:   Zero on success, error code on failure. ", "if (drm_drv_uses_atomic_modeset(property->dev) &&!(property->flags & DRM_MODE_PROP_IMMUTABLE))return drm_atomic_get_property(obj, property, val);return __drm_object_property_get_prop_value(obj, property, val);}/** * drm_object_property_get_value - retrieve the value of a property * @obj: drm mode object to get property value from * @property: property to retrieve * @val: storage for the property value * * This function retrieves the softare state of the given property for the given * property. Since there is no driver callback to retrieve the current property * value this might be out of sync with the hardware, depending upon the driver * and property. * * Atomic drivers should never call this function directly, the core will read * out property values through the various ->atomic_get_property callbacks. * * Returns: * Zero on success, error code on failure. ": "drm_object_property_get_value(struct drm_mode_object  obj,   struct drm_property  property,   uint64_t  val){  read-only properties bypass atomic mechanism and still store   their value in obj->properties->values[].. mostly to avoid   having to deal w EDID and similar props in atomic paths: ", "int drm_object_property_get_default_value(struct drm_mode_object *obj,  struct drm_property *property,  uint64_t *val)": "drm_object_property_get_default_value - retrieve the default value of a   property when in atomic mode.   @obj: drm mode object to get property value from   @property: property to retrieve   @val: storage for the property value     This function retrieves the default state of the given property as passed in   to drm_object_attach_property     Only atomic drivers should call this function directly, as for non-atomic   drivers it will return the current value.     Returns:   Zero on success, error code on failure. ", "#ifdef CONFIG_X86if (shmem->map_wc)set_pages_array_wc(pages, obj->size >> PAGE_SHIFT);#endifshmem->pages = pages;return 0;}/* * drm_gem_shmem_get_pages - Allocate backing pages for a shmem GEM object * @shmem: shmem GEM object * * This function makes sure that backing pages exists for the shmem GEM object * and increases the use count. * * Returns: * 0 on success or a negative error code on failure. ": "drm_gem_shmem_put_pages(shmem);}drm_WARN_ON(obj->dev, shmem->pages_use_count);drm_gem_object_release(obj);mutex_destroy(&shmem->pages_lock);mutex_destroy(&shmem->vmap_lock);kfree(shmem);}EXPORT_SYMBOL_GPL(drm_gem_shmem_free);static int drm_gem_shmem_get_pages_locked(struct drm_gem_shmem_object  shmem){struct drm_gem_object  obj = &shmem->base;struct page   pages;if (shmem->pages_use_count++ > 0)return 0;pages = drm_gem_get_pages(obj);if (IS_ERR(pages)) {drm_dbg_kms(obj->dev, \"Failed to get pages (%ld)\\n\",    PTR_ERR(pages));shmem->pages_use_count = 0;return PTR_ERR(pages);}    TODO: Allocating WC pages which are correctly flushed is only   supported on x86. Ideal solution would be a GFP_WC flag, which also   ttm_pool.c could use. ", "int drm_gem_shmem_pin(struct drm_gem_shmem_object *shmem)": "drm_gem_shmem_pin - Pin backing pages for a shmem GEM object   @shmem: shmem GEM object     This function makes sure the backing pages are pinned in memory while the   buffer is exported.     Returns:   0 on success or a negative error code on failure. ", "void drm_gem_shmem_unpin(struct drm_gem_shmem_object *shmem)": "drm_gem_shmem_unpin - Unpin backing pages for a shmem GEM object   @shmem: shmem GEM object     This function removes the requirement that the backing pages are pinned in   memory. ", "static const struct drm_gem_object_funcs drm_gem_shmem_funcs = ": "drm_gem_shmem_vmap()). These helpers perform the necessary type conversion. ", "int drm_gem_shmem_vmap(struct drm_gem_shmem_object *shmem,       struct iosys_map *map)": "drm_gem_shmem_vunmap().     Returns:   0 on success or a negative error code on failure. ", "shmem_truncate_range(file_inode(obj->filp), 0, (loff_t)-1);invalidate_mapping_pages(file_inode(obj->filp)->i_mapping, 0, (loff_t)-1);}EXPORT_SYMBOL(drm_gem_shmem_purge_locked": "drm_gem_shmem_purge_locked(struct drm_gem_shmem_object  shmem){struct drm_gem_object  obj = &shmem->base;struct drm_device  dev = obj->dev;drm_WARN_ON(obj->dev, !drm_gem_shmem_is_purgeable(shmem));dma_unmap_sgtable(dev->dev, shmem->sgt, DMA_BIDIRECTIONAL, 0);sg_free_table(shmem->sgt);kfree(shmem->sgt);shmem->sgt = NULL;drm_gem_shmem_put_pages_locked(shmem);shmem->madv = -1;drm_vma_node_unmap(&obj->vma_node, dev->anon_inode->i_mapping);drm_gem_free_mmap_offset(obj);  Our goal here is to return as much of the memory as   is possible back to the system as we are called from OOM.   To do this we must instruct the shmfs to drop all of its   backing pages,  now . ", "shmem_truncate_range(file_inode(obj->filp), 0, (loff_t)-1);invalidate_mapping_pages(file_inode(obj->filp)->i_mapping, 0, (loff_t)-1);}EXPORT_SYMBOL(drm_gem_shmem_purge": "drm_gem_shmem_purge_locked(struct drm_gem_shmem_object  shmem){struct drm_gem_object  obj = &shmem->base;struct drm_device  dev = obj->dev;drm_WARN_ON(obj->dev, !drm_gem_shmem_is_purgeable(shmem));dma_unmap_sgtable(dev->dev, shmem->sgt, DMA_BIDIRECTIONAL, 0);sg_free_table(shmem->sgt);kfree(shmem->sgt);shmem->sgt = NULL;drm_gem_shmem_put_pages_locked(shmem);shmem->madv = -1;drm_vma_node_unmap(&obj->vma_node, dev->anon_inode->i_mapping);drm_gem_free_mmap_offset(obj);  Our goal here is to return as much of the memory as   is possible back to the system as we are called from OOM.   To do this we must instruct the shmfs to drop all of its   backing pages,  now . ", "void drm_gem_shmem_print_info(const struct drm_gem_shmem_object *shmem,      struct drm_printer *p, unsigned int indent)": "drm_gem_shmem_print_info() - Print &drm_gem_shmem_object info for debugfs   @shmem: shmem GEM object   @p: DRM printer   @indent: Tab indentation level ", "int drm_crtc_commit_wait(struct drm_crtc_commit *commit)": "drm_crtc_commit_wait - Waits for a commit to complete   @commit: &drm_crtc_commit to wait for     Waits for a given &drm_crtc_commit to be programmed into the   hardware and flipped to.     Returns:     0 on success, a negative error code otherwise. ", "void drm_atomic_state_default_release(struct drm_atomic_state *state)": "drm_atomic_state_init   @state: atomic state     Free all the memory allocated by drm_atomic_state_init.   This should only be used by drivers which are still subclassing   &drm_atomic_state and haven't switched to &drm_private_state yet. ", "struct drm_atomic_state *drm_atomic_state_alloc(struct drm_device *dev)": "drm_atomic_state_alloc - allocate atomic state   @dev: DRM device     This allocates an empty atomic state to track updates. ", "void drm_atomic_state_default_clear(struct drm_atomic_state *state)": "drm_atomic_state_default_clear - clear base atomic state   @state: atomic state     Default implementation for clearing atomic state.   This should only be used by drivers which are still subclassing   &drm_atomic_state and haven't switched to &drm_private_state yet. ", "void drm_atomic_state_clear(struct drm_atomic_state *state)": "drm_atomic_state_clear - clear state object   @state: atomic state     When the ww mutex algorithm detects a deadlock we need to back off and drop   all locks. So someone else could sneak in and change the current modeset   configuration. Which means that all the state assembled in @state is no   longer an atomic update to the current state, but to some arbitrary earlier   state. Which could break assumptions the driver's   &drm_mode_config_funcs.atomic_check likely relies on.     Hence we must clear all cached state and completely start over, using this   function. ", "void __drm_atomic_state_free(struct kref *ref)": "__drm_atomic_state_free - free all memory for an atomic state   @ref: This atomic state to deallocate     This frees all memory associated with an atomic state, including all the   per-object state for planes, CRTCs and connectors. ", "struct drm_crtc_state *drm_atomic_get_crtc_state(struct drm_atomic_state *state,  struct drm_crtc *crtc)": "drm_atomic_get_crtc_state - get CRTC state   @state: global atomic state object   @crtc: CRTC to get state object for     This function returns the CRTC state for the given CRTC, allocating it if   needed. It will also grab the relevant CRTC lock to make sure that the state   is consistent.     WARNING: Drivers may only add new CRTC states to a @state if   drm_atomic_state.allow_modeset is set, or if it's a driver-internal commit   not created by userspace through an IOCTL call.     Returns:     Either the allocated state or the error code encoded into the pointer. When   the error is EDEADLK then the ww mutex code has detected a deadlock and the   entire atomic sequence must be restarted. All other errors are fatal. ", "struct drm_plane_state *drm_atomic_get_plane_state(struct drm_atomic_state *state,  struct drm_plane *plane)": "drm_atomic_get_plane_state - get plane state   @state: global atomic state object   @plane: plane to get state object for     This function returns the plane state for the given plane, allocating it if   needed. It will also grab the relevant plane lock to make sure that the state   is consistent.     Returns:     Either the allocated state or the error code encoded into the pointer. When   the error is EDEADLK then the ww mutex code has detected a deadlock and the   entire atomic sequence must be restarted. All other errors are fatal. ", "voiddrm_atomic_private_obj_init(struct drm_device *dev,    struct drm_private_obj *obj,    struct drm_private_state *state,    const struct drm_private_state_funcs *funcs)": "drm_atomic_private_obj_init - initialize private object   @dev: DRM device this object will be attached to   @obj: private object   @state: initial private object state   @funcs: pointer to the struct of function pointers that identify the object   type     Initialize the private object, which can be embedded into any   driver private object that needs its own atomic state. ", "voiddrm_atomic_private_obj_fini(struct drm_private_obj *obj)": "drm_atomic_private_obj_fini - finalize private object   @obj: private object     Finalize the private object. ", "/** * drm_atomic_private_obj_init - initialize private object * @dev: DRM device this object will be attached to * @obj: private object * @state: initial private object state * @funcs: pointer to the struct of function pointers that identify the object * type * * Initialize the private object, which can be embedded into any * driver private object that needs its own atomic state. ": "drm_atomic_get_private_obj_state(). This also takes care   of locking, hence drivers should not have a need to call drm_modeset_lock()   directly. Sequence of the actual hardware state commit is not handled,   drivers might need to keep track of struct drm_crtc_commit within subclassed   structure of &drm_private_state as necessary, e.g. similar to   &drm_plane_state.commit. See also &drm_atomic_state.fake_commit.     All private state structures contained in a &drm_atomic_state update can be   iterated using for_each_oldnew_private_obj_in_state(),   for_each_new_private_obj_in_state() and for_each_old_private_obj_in_state().   Drivers are recommended to wrap these for each type of driver private state   object they have, filtering on &drm_private_obj.funcs using for_each_if(), at   least if they want to iterate over all objects of a given type.     An earlier way to handle driver private state was by subclassing struct   &drm_atomic_state. But since that encourages non-standard ways to implement   the checkcommit split atomic requires (by using e.g. \"check and rollback or   commit instead\" of \"duplicate state, check, then either commit or release   duplicated state) it is deprecated in favour of using &drm_private_state. ", "struct drm_private_state *drm_atomic_get_old_private_obj_state(const struct drm_atomic_state *state,     struct drm_private_obj *obj)": "drm_atomic_get_old_private_obj_state   @state: global atomic state object   @obj: private_obj to grab     This function returns the old private object state for the given private_obj,   or NULL if the private_obj is not part of the global atomic state. ", "struct drm_private_state *drm_atomic_get_new_private_obj_state(const struct drm_atomic_state *state,     struct drm_private_obj *obj)": "drm_atomic_get_new_private_obj_state   @state: global atomic state object   @obj: private_obj to grab     This function returns the new private object state for the given private_obj,   or NULL if the private_obj is not part of the global atomic state. ", "struct drm_connector *drm_atomic_get_old_connector_for_encoder(const struct drm_atomic_state *state, struct drm_encoder *encoder)": "drm_atomic_get_old_connector_for_encoder - Get old connector for an encoder   @state: Atomic state   @encoder: The encoder to fetch the connector state for     This function finds and returns the connector that was connected to @encoder   as specified by the @state.     If there is no connector in @state which previously had @encoder connected to   it, this function will return NULL. While this may seem like an invalid use   case, it is sometimes useful to differentiate commits which had no prior   connectors attached to @encoder vs ones that did (and to inspect their   state). This is especially true in enable hooks because the pipeline has   changed.     Returns: The old connector connected to @encoder, or NULL if the encoder is   not connected. ", "struct drm_connector *drm_atomic_get_new_connector_for_encoder(const struct drm_atomic_state *state, struct drm_encoder *encoder)": "drm_atomic_get_new_connector_for_encoder - Get new connector for an encoder   @state: Atomic state   @encoder: The encoder to fetch the connector state for     This function finds and returns the connector that will be connected to   @encoder as specified by the @state.     If there is no connector in @state which will have @encoder connected to it,   this function will return NULL. While this may seem like an invalid use case,   it is sometimes useful to differentiate commits which have no connectors   attached to @encoder vs ones that do (and to inspect their state). This is   especially true in disable hooks because the pipeline will change.     Returns: The new connector connected to @encoder, or NULL if the encoder is   not connected. ", "struct drm_crtc *drm_atomic_get_old_crtc_for_encoder(struct drm_atomic_state *state,    struct drm_encoder *encoder)": "drm_atomic_get_old_crtc_for_encoder - Get old crtc for an encoder   @state: Atomic state   @encoder: The encoder to fetch the crtc state for     This function finds and returns the crtc that was connected to @encoder   as specified by the @state.     Returns: The old crtc connected to @encoder, or NULL if the encoder is   not connected. ", "struct drm_crtc *drm_atomic_get_new_crtc_for_encoder(struct drm_atomic_state *state,    struct drm_encoder *encoder)": "drm_atomic_get_new_crtc_for_encoder - Get new crtc for an encoder   @state: Atomic state   @encoder: The encoder to fetch the crtc state for     This function finds and returns the crtc that will be connected to @encoder   as specified by the @state.     Returns: The new crtc connected to @encoder, or NULL if the encoder is   not connected. ", "struct drm_connector_state *drm_atomic_get_connector_state(struct drm_atomic_state *state,  struct drm_connector *connector)": "drm_atomic_get_connector_state - get connector state   @state: global atomic state object   @connector: connector to get state object for     This function returns the connector state for the given connector,   allocating it if needed. It will also grab the relevant connector lock to   make sure that the state is consistent.     Returns:     Either the allocated state or the error code encoded into the pointer. When   the error is EDEADLK then the ww mutex code has detected a deadlock and the   entire atomic sequence must be restarted. All other errors are fatal. ", "struct drm_bridge_state *drm_atomic_get_bridge_state(struct drm_atomic_state *state,    struct drm_bridge *bridge)": "drm_atomic_get_bridge_state - get bridge state   @state: global atomic state object   @bridge: bridge to get state object for     This function returns the bridge state for the given bridge, allocating it   if needed. It will also grab the relevant bridge lock to make sure that the   state is consistent.     Returns:     Either the allocated state or the error code encoded into the pointer. When   the error is EDEADLK then the ww mutex code has detected a deadlock and the   entire atomic sequence must be restarted. ", "struct drm_bridge_state *drm_atomic_get_old_bridge_state(const struct drm_atomic_state *state,struct drm_bridge *bridge)": "drm_atomic_get_old_bridge_state - get old bridge state, if it exists   @state: global atomic state object   @bridge: bridge to grab     This function returns the old bridge state for the given bridge, or NULL if   the bridge is not part of the global atomic state. ", "struct drm_bridge_state *drm_atomic_get_new_bridge_state(const struct drm_atomic_state *state,struct drm_bridge *bridge)": "drm_atomic_get_new_bridge_state - get new bridge state, if it exists   @state: global atomic state object   @bridge: bridge to grab     This function returns the new bridge state for the given bridge, or NULL if   the bridge is not part of the global atomic state. ", "intdrm_atomic_add_encoder_bridges(struct drm_atomic_state *state,       struct drm_encoder *encoder)": "drm_atomic_add_encoder_bridges - add bridges attached to an encoder   @state: atomic state   @encoder: DRM encoder     This function adds all bridges attached to @encoder. This is needed to add   bridge states to @state and make them available when   &drm_bridge_funcs.atomic_check(), &drm_bridge_funcs.atomic_pre_enable(),   &drm_bridge_funcs.atomic_enable(),   &drm_bridge_funcs.atomic_disable_post_disable() are called.     Returns:   0 on success or can fail with -EDEADLK or -ENOMEM. When the error is EDEADLK   then the ww mutex code has detected a deadlock and the entire atomic   sequence must be restarted. All other errors are fatal. ", "intdrm_atomic_add_affected_connectors(struct drm_atomic_state *state,   struct drm_crtc *crtc)": "drm_atomic_add_affected_connectors - add connectors for CRTC   @state: atomic state   @crtc: DRM CRTC     This function walks the current configuration and adds all connectors   currently using @crtc to the atomic configuration @state. Note that this   function must acquire the connection mutex. This can potentially cause   unneeded serialization if the update is just for the planes on one CRTC. Hence   drivers and helpers should only call this when really needed (e.g. when a   full modeset needs to happen due to some change).     Returns:   0 on success or can fail with -EDEADLK or -ENOMEM. When the error is EDEADLK   then the ww mutex code has detected a deadlock and the entire atomic   sequence must be restarted. All other errors are fatal. ", "intdrm_atomic_add_affected_planes(struct drm_atomic_state *state,       struct drm_crtc *crtc)": "drm_atomic_add_affected_planes - add planes for CRTC   @state: atomic state   @crtc: DRM CRTC     This function walks the current configuration and adds all planes   currently used by @crtc to the atomic configuration @state. This is useful   when an atomic commit also needs to check all currently enabled plane on   @crtc, e.g. when changing the mode. It's also useful when re-enabling a CRTC   to avoid special code to force-enable all planes.     Since acquiring a plane state will always also acquire the ww mutex of the   current CRTC for that plane (if there is any) adding all the plane states for   a CRTC will not reduce parallelism of atomic updates.     Returns:   0 on success or can fail with -EDEADLK or -ENOMEM. When the error is EDEADLK   then the ww mutex code has detected a deadlock and the entire atomic   sequence must be restarted. All other errors are fatal. ", "int drm_atomic_check_only(struct drm_atomic_state *state)": "drm_atomic_check_only - check whether a given config would work   @state: atomic configuration to check     Note that this function can return -EDEADLK if the driver needed to acquire   more locks but encountered a deadlock. The caller must then do the usual ww   backoff dance and restart. All other errors are fatal.     Returns:   0 on success, negative error code on failure. ", "int drm_atomic_commit(struct drm_atomic_state *state)": "drm_atomic_commit - commit configuration atomically   @state: atomic configuration to check     Note that this function can return -EDEADLK if the driver needed to acquire   more locks but encountered a deadlock. The caller must then do the usual ww   backoff dance and restart. All other errors are fatal.     This function will take its own reference on @state.   Callers should always release their reference with drm_atomic_state_put().     Returns:   0 on success, negative error code on failure. ", "int drm_atomic_nonblocking_commit(struct drm_atomic_state *state)": "drm_atomic_print_new_state(state, &p);ret = drm_atomic_check_only(state);if (ret)return ret;drm_dbg_atomic(state->dev, \"committing %p\\n\", state);return config->funcs->atomic_commit(state->dev, state, false);}EXPORT_SYMBOL(drm_atomic_commit);     drm_atomic_nonblocking_commit - atomic nonblocking commit   @state: atomic configuration to check     Note that this function can return -EDEADLK if the driver needed to acquire   more locks but encountered a deadlock. The caller must then do the usual ww   backoff dance and restart. All other errors are fatal.     This function will take its own reference on @state.   Callers should always release their reference with drm_atomic_state_put().     Returns:   0 on success, negative error code on failure. ", "void drm_state_dump(struct drm_device *dev, struct drm_printer *p)": "drm_state_dump(struct drm_device  dev, struct drm_printer  p,     bool take_locks){struct drm_mode_config  config = &dev->mode_config;struct drm_plane  plane;struct drm_crtc  crtc;struct drm_connector  connector;struct drm_connector_list_iter conn_iter;if (!drm_drv_uses_atomic_modeset(dev))return;list_for_each_entry(plane, &config->plane_list, head) {if (take_locks)drm_modeset_lock(&plane->mutex, NULL);drm_atomic_plane_print_state(p, plane->state);if (take_locks)drm_modeset_unlock(&plane->mutex);}list_for_each_entry(crtc, &config->crtc_list, head) {if (take_locks)drm_modeset_lock(&crtc->mutex, NULL);drm_atomic_crtc_print_state(p, crtc->state);if (take_locks)drm_modeset_unlock(&crtc->mutex);}drm_connector_list_iter_begin(dev, &conn_iter);if (take_locks)drm_modeset_lock(&dev->mode_config.connection_mutex, NULL);drm_for_each_connector_iter(connector, &conn_iter)drm_atomic_connector_print_state(p, connector->state);if (take_locks)drm_modeset_unlock(&dev->mode_config.connection_mutex);drm_connector_list_iter_end(&conn_iter);}     drm_state_dump - dump entire device atomic state   @dev: the drm device   @p: where to print the state to     Just for debugging.  Drivers might want an option to dump state   to dmesg in case of error irq's.  (Hint, you probably want to   ratelimit this!)     The caller must wrap this drm_modeset_lock_all_ctx() and   drm_modeset_drop_locks(). If this is called from error irq handler, it should   not be enabled by default - if you are debugging errors you might   not care that this is racey, but calling this without all modeset locks held   is inherently unsafe. ", "void drm_suballoc_manager_init(struct drm_suballoc_manager *sa_manager,       size_t size, size_t align)": "drm_suballoc_manager_init() - Initialise the drm_suballoc_manager   @sa_manager: pointer to the sa_manager   @size: number of bytes we want to suballocate   @align: alignment for each suballocated chunk     Prepares the suballocation manager for suballocations. ", "void drm_suballoc_manager_fini(struct drm_suballoc_manager *sa_manager)": "drm_suballoc_free() must be signaled, or we cannot clean up   the entire manager. ", "struct drm_suballoc *drm_suballoc_new(struct drm_suballoc_manager *sa_manager, size_t size, gfp_t gfp, bool intr, size_t align)": "drm_suballoc_new() - Make a suballocation.   @sa_manager: pointer to the sa_manager   @size: number of bytes we want to suballocate.   @gfp: gfp flags used for memory allocation. Typically GFP_KERNEL but         the argument is provided for suballocations from reclaim context or         where the caller wants to avoid pipelining rather than wait for         reclaim.   @intr: Whether to perform waits interruptible. This should typically          always be true, unless the caller needs to propagate a          non-interruptible context from above layers.   @align: Alignment. Must not exceed the default manager alignment.           If @align is zero, then the manager alignment is used.     Try to make a suballocation of size @size, which will be rounded   up to the alignment specified in specified in drm_suballoc_manager_init().     Return: a new suballocated bo, or an ERR_PTR. ", "int drm_writeback_connector_init(struct drm_device *dev, struct drm_writeback_connector *wb_connector, const struct drm_connector_funcs *con_funcs, const struct drm_encoder_helper_funcs *enc_helper_funcs, const u32 *formats, int n_formats, u32 possible_crtcs)": "drm_writeback_connector_init - Initialize a writeback connector and its properties   @dev: DRM device   @wb_connector: Writeback connector to initialize   @con_funcs: Connector funcs vtable   @enc_helper_funcs: Encoder helper funcs vtable to be used by the internal encoder   @formats: Array of supported pixel formats for the writeback engine   @n_formats: Length of the formats array   @possible_crtcs: possible crtcs for the internal writeback encoder     This function creates the writeback-connector-specific properties if they   have not been already created, initializes the connector as   type DRM_MODE_CONNECTOR_WRITEBACK, and correctly initializes the property   values. It will also create an internal encoder associated with the   drm_writeback_connector and set it to use the @enc_helper_funcs vtable for   the encoder helper.     Drivers should always use this function instead of drm_connector_init() to   set up writeback connectors.     Returns: 0 on success, or a negative error code ", "int drm_writeback_connector_init_with_encoder(struct drm_device *dev,struct drm_writeback_connector *wb_connector, struct drm_encoder *enc,const struct drm_connector_funcs *con_funcs, const u32 *formats,int n_formats)": "drm_writeback_connector_init_with_encoder(dev, wb_connector, &wb_connector->encoder,con_funcs, formats, n_formats);if (ret)drm_encoder_cleanup(&wb_connector->encoder);return ret;}EXPORT_SYMBOL(drm_writeback_connector_init);     drm_writeback_connector_init_with_encoder - Initialize a writeback connector with   a custom encoder     @dev: DRM device   @wb_connector: Writeback connector to initialize   @enc: handle to the already initialized drm encoder   @con_funcs: Connector funcs vtable   @formats: Array of supported pixel formats for the writeback engine   @n_formats: Length of the formats array     This function creates the writeback-connector-specific properties if they   have not been already created, initializes the connector as   type DRM_MODE_CONNECTOR_WRITEBACK, and correctly initializes the property   values.     This function assumes that the drm_writeback_connector's encoder has already been   created and initialized before invoking this function.     In addition, this function also assumes that callers of this API will manage   assigning the encoder helper functions, possible_crtcs and any other encoder   specific operation.     Drivers should always use this function instead of drm_connector_init() to   set up writeback connectors if they want to manage themselves the lifetime of the   associated encoder.     Returns: 0 on success, or a negative error code ", "void drm_writeback_queue_job(struct drm_writeback_connector *wb_connector,     struct drm_connector_state *conn_state)": "drm_writeback_signal_completion).     For every call to drm_writeback_queue_job() there must be exactly one call to   drm_writeback_signal_completion()     See also: drm_writeback_signal_completion() ", "int drm_edid_header_is_valid(const void *_edid)": "drm_edid_header_is_valid - sanity check the header of the base EDID block   @_edid: pointer to raw base EDID block     Sanity check the header of the base EDID block.     Return: 8 if the header is perfect, down to 0 if it's totally wrong. ", "bool drm_edid_are_equal(const struct edid *edid1, const struct edid *edid2)": "drm_edid_are_equal - compare two edid blobs.   @edid1: pointer to first blob   @edid2: pointer to second blob   This helper can be used during probing to determine if   edid had changed. ", "bool drm_edid_block_valid(u8 *_block, int block_num, bool print_bad_edid,  bool *edid_corrupt)": "drm_edid_block_valid - Sanity check the EDID block (base or extension)   @_block: pointer to raw EDID block   @block_num: type of block to validate (0 for base, extension otherwise)   @print_bad_edid: if true, dump bad EDID blocks to the console   @edid_corrupt: if true, the header or checksum is invalid     Validate a base or extension EDID block and optionally dump bad blocks to   the console.     Return: True if the block is valid, false otherwise. ", "bool drm_edid_is_valid(struct edid *edid)": "drm_edid_is_valid - sanity check EDID data   @edid: EDID data     Sanity-check an entire EDID record (including extensions)     Return: True if the EDID data is valid, false otherwise. ", "bool drm_edid_valid(const struct drm_edid *drm_edid)": "drm_edid_valid - sanity check EDID data   @drm_edid: EDID data     Sanity check an EDID. Cross check block count against allocated size and   checksum the blocks.     Return: True if the EDID data is valid, false otherwise. ", "int drm_edid_override_connector_update(struct drm_connector *connector)": "drm_get_edid() and caused the overridefirmware EDID to be   skipped.     Return: The number of modes added or 0 if we couldn't find any. ", "const struct edid *drm_edid_raw(const struct drm_edid *drm_edid)": "drm_edid_raw - Get a pointer to the raw EDID data.   @drm_edid: drm_edid container     Get a pointer to the raw EDID data.     This is for transition only. Avoid using this like the plague.     Return: Pointer to raw EDID data. ", "int drm_edid_override_reset(struct drm_connector *connector)": "drm_edid_free(drm_edid);return -EINVAL;}drm_dbg_kms(connector->dev, \"[CONNECTOR:%d:%s] EDID override set\\n\",    connector->base.id, connector->name);mutex_lock(&connector->edid_override_mutex);drm_edid_free(connector->edid_override);connector->edid_override = drm_edid;mutex_unlock(&connector->edid_override_mutex);return 0;}  For debugfs edid_override implementation ", "int drm_edid_override_show(struct drm_connector *connector, struct seq_file *m)": "drm_edid_dup(connector->edid_override);mutex_unlock(&connector->edid_override_mutex);if (!override)override = drm_edid_load_firmware(connector);return IS_ERR(override) ? NULL : override;}  For debugfs edid_override implementation ", "booldrm_probe_ddc(struct i2c_adapter *adapter)": "drm_probe_ddc() - probe DDC presence   @adapter: I2C adapter to probe     Return: True on success, false on failure. ", "const struct drm_edid *drm_edid_read_custom(struct drm_connector *connector,    read_block_fn read_block,    void *context)": "drm_connector_update_edid_property(connector, edid);return edid;}EXPORT_SYMBOL(drm_get_edid);     drm_edid_read_custom - Read EDID data using given EDID block read function   @connector: Connector to use   @read_block: EDID block read function   @context: Private data passed to the block read function     When the I2C adapter connected to the DDC bus is hidden behind a device that   exposes a different interface to read EDID blocks this function can be used   to get EDID data using a custom block read function.     As in the general case the DDC bus is accessible by the kernel at the I2C   level, drivers must make all reasonable efforts to expose it as an I2C   adapter and use drm_edid_read() or drm_edid_read_ddc() instead of abusing   this function.     The EDID may be overridden using debugfs override_edid or firmware EDID   (drm_edid_load_firmware() and drm.edid_firmware parameter), in this priority   order. Having either of them bypasses actual EDID reads.     The returned pointer must be freed using drm_edid_free().     Return: Pointer to EDID, or NULL if proberead failed. ", "u32 drm_edid_get_panel_id(struct i2c_adapter *adapter)": "drm_edid_get_panel_id - Get a panel's ID through DDC   @adapter: I2C adapter to use for DDC     This function reads the first block of the EDID of a panel and (assuming   that the EDID is valid) extracts the ID out of it. The ID is a 32-bit value   (16 bits of manufacturer ID and 16 bits of per-manufacturer ID) that's   supposed to be different for each different modem of panel.     This function is intended to be used during early probing on devices where   more than one panel might be present. Because of its intended use it must   assume that the EDID of the panel is correct, at least as far as the ID   is concerned (in other words, we don't process any overrides here).     NOTE: it's expected that this function and drm_do_get_edid() will both   be read the EDID, but there is no caching between them. Since we're only   reading the first block, hopefully this extra overhead won't be too big.     Return: A 32-bit ID that should be different for each makemodel of panel.           See the functions drm_edid_encode_panel_id() and           drm_edid_decode_panel_id() for some details on the structure of this           ID. ", "struct edid *drm_get_edid_switcheroo(struct drm_connector *connector,     struct i2c_adapter *adapter)": "drm_get_edid_switcheroo - get EDID data for a vga_switcheroo output   @connector: connector we're probing   @adapter: I2C adapter to use for DDC     Wrapper around drm_get_edid() for laptops with dual GPUs using one set of   outputs. The wrapper adds the requisite vga_switcheroo calls to temporarily   switch DDC to the GPU which is retrieving EDID.     Return: Pointer to valid EDID or %NULL if we couldn't find any. ", "const struct drm_edid *drm_edid_read_switcheroo(struct drm_connector *connector,struct i2c_adapter *adapter)": "drm_edid_read_switcheroo - get EDID data for a vga_switcheroo output   @connector: connector we're probing   @adapter: I2C adapter to use for DDC     Wrapper around drm_edid_read_ddc() for laptops with dual GPUs using one set   of outputs. The wrapper adds the requisite vga_switcheroo calls to   temporarily switch DDC to the GPU which is retrieving EDID.     Return: Pointer to valid EDID or %NULL if we couldn't find any. ", "struct edid *drm_edid_duplicate(const struct edid *edid)": "drm_edid_duplicate - duplicate an EDID and the extensions   @edid: EDID to duplicate     Return: Pointer to duplicated EDID or NULL on allocation failure. ", "struct drm_display_mode *drm_mode_find_dmt(struct drm_device *dev,   int hsize, int vsize, int fresh,   bool rb)": "drm_mode_find_dmt - Create a copy of a mode if present in DMT   @dev: Device to duplicate against   @hsize: Mode width   @vsize: Mode height   @fresh: Mode refresh rate   @rb: Mode reduced-blanking-ness     Walk the DMT mode list looking for a match for the given parameters.     Return: A newly allocated copy of the mode, or NULL if not found. ", "clock1 = cea_mode.clock;clock2 = cea_mode_alternate_clock(&cea_mode);if (abs(to_match->clock - clock1) > clock_tolerance &&    abs(to_match->clock - clock2) > clock_tolerance)continue;do ": "drm_match_cea_mode_clock_tolerance(const struct drm_display_mode  to_match,     unsigned int clock_tolerance){unsigned int match_flags = DRM_MODE_MATCH_TIMINGS | DRM_MODE_MATCH_FLAGS;u8 vic;if (!to_match->clock)return 0;if (to_match->picture_aspect_ratio)match_flags |= DRM_MODE_MATCH_ASPECT_RATIO;for (vic = 1; vic < cea_num_vics(); vic = cea_next_vic(vic)) {struct drm_display_mode cea_mode;unsigned int clock1, clock2;drm_mode_init(&cea_mode, cea_mode_for_vic(vic));  Check both 60Hz and 59.94Hz ", "static int do_y420vdb_modes(struct drm_connector *connector,    const u8 *svds, u8 svds_len)": "drm_display_mode_from_cea_vic(dev, info->vics[vic_index]);}    do_y420vdb_modes - Parse YCBCR 420 only modes   @connector: connector corresponding to the HDMI sink   @svds: start of the data block of CEA YCBCR 420 VDB   @len: length of the CEA YCBCR 420 VDB     Parse the CEA-861-F YCBCR 420 Video Data Block (Y420VDB)   which contains modes which can be supported in YCBCR 420   output format only. ", "void drm_edid_get_monitor_name(const struct edid *edid, char *name, int bufsize)": "drm_edid_get_monitor_name - fetch the monitor name from the edid   @edid: monitor EDID information   @name: pointer to a character array to hold the name of the monitor   @bufsize: The size of the name buffer (should be at least 14 chars.)   ", "*sads = kcalloc(count, sizeof(**sads), GFP_KERNEL);if (!*sads)return -ENOMEM;for (j = 0; j < count; j++) ": "drm_edid_to_sad(const struct drm_edid  drm_edid,    struct cea_sad   sads){const struct cea_db  db;struct cea_db_iter iter;int count = 0;cea_db_iter_edid_begin(drm_edid, &iter);cea_db_iter_for_each(db, &iter) {if (cea_db_tag(db) == CTA_DB_AUDIO) {int j;count = cea_db_payload_len(db)  3;   SAD is 3B ", "int drm_edid_to_speaker_allocation(const struct edid *edid, u8 **sadb)": "drm_edid_to_speaker_allocation(const struct drm_edid  drm_edid,   u8   sadb){const struct cea_db  db;struct cea_db_iter iter;int count = 0;cea_db_iter_edid_begin(drm_edid, &iter);cea_db_iter_for_each(db, &iter) {if (cea_db_tag(db) == CTA_DB_SPEAKER &&    cea_db_payload_len(db) == 3) { sadb = kmemdup(db->data, cea_db_payload_len(db),GFP_KERNEL);if (! sadb)return -ENOMEM;count = cea_db_payload_len(db);break;}}cea_db_iter_end(&iter);DRM_DEBUG_KMS(\"Found %d Speaker Allocation Data Blocks\\n\", count);return count;}     drm_edid_to_speaker_allocation - extracts Speaker Allocation Data Blocks from EDID   @edid: EDID to parse   @sadb: pointer to the speaker block     Looks for CEA EDID block and extracts the Speaker Allocation Data Block from it.     Note: The returned pointer needs to be freed using kfree().     Return: The number of found Speaker Allocation Blocks or negative number on   error. ", "int drm_av_sync_delay(struct drm_connector *connector,      const struct drm_display_mode *mode)": "drm_av_sync_delay - compute the HDMIDP sink audio-video sync delay   @connector: connector associated with the HDMIDP sink   @mode: the display mode     Return: The HDMIDP sink's audio-video sync delay in milliseconds or 0 if   the sink doesn't support audio or video. ", "cea_db_iter_edid_begin(drm_edid, &iter);cea_db_iter_for_each(db, &iter) ": "drm_detect_hdmi_monitor(const struct drm_edid  drm_edid){const struct cea_db  db;struct cea_db_iter iter;bool hdmi = false;    Because HDMI identifier is in Vendor Specific Block,   search it from all data blocks of CEA extension. ", "bool drm_detect_monitor_audio(const struct edid *edid)": "drm_detect_monitor_audio(const struct drm_edid  drm_edid){struct drm_edid_iter edid_iter;const struct cea_db  db;struct cea_db_iter iter;const u8  edid_ext;bool has_audio = false;drm_edid_iter_begin(drm_edid, &edid_iter);drm_edid_iter_for_each(edid_ext, &edid_iter) {if (edid_ext[0] == CEA_EXT) {has_audio = edid_ext[3] & EDID_BASIC_AUDIO;if (has_audio)break;}}drm_edid_iter_end(&edid_iter);if (has_audio) {DRM_DEBUG_KMS(\"Monitor has basic audio support\\n\");goto end;}cea_db_iter_edid_begin(drm_edid, &iter);cea_db_iter_for_each(db, &iter) {if (cea_db_tag(db) == CTA_DB_AUDIO) {const u8  data = cea_db_data(db);int i;for (i = 0; i < cea_db_payload_len(db); i += 3)DRM_DEBUG_KMS(\"CEA audio format %d\\n\",      (data[i] >> 3) & 0xf);has_audio = true;break;}}cea_db_iter_end(&iter);end:return has_audio;}     drm_detect_monitor_audio - check monitor audio capability   @edid: EDID block to scan     Monitor should have CEA extension block.   If monitor has 'basic audio', but no CEA audio blocks, it's 'basic   audio' only. If there is any audio extension block and supported   audio format, assume at least 'basic audio' support, even if 'basic   audio' is not defined in EDID.     Return: True if the monitor supports audio, false otherwise. ", "enum hdmi_quantization_rangedrm_default_rgb_quant_range(const struct drm_display_mode *mode)": "drm_default_rgb_quant_range - default RGB quantization range   @mode: display mode     Determine the default RGB quantization range for the mode,   as specified in CEA-861.     Return: The default RGB quantization range for the mode ", "status = edid_block_check(block, is_base_block);if (status == EDID_BLOCK_OK)status = EDID_BLOCK_HEADER_FIXED;}if (edid_block_status_valid(status, edid_block_tag(block)))break;/* Fail early for unrepairable base block all zeros. ": "drm_edid_connector_update(connector, override);drm_edid_free(override);drm_dbg_kms(connector->dev,    \"[CONNECTOR:%d:%s] adding %d modes via fallback overridefirmware EDID\\n\",    connector->base.id, connector->name, num_modes);}return num_modes;}EXPORT_SYMBOL(drm_edid_override_connector_update);typedef int read_block_fn(void  context, u8  buf, unsigned int block, size_t len);static enum edid_block_status edid_block_read(void  block, unsigned int block_num,      read_block_fn read_block,      void  context){enum edid_block_status status;bool is_base_block = block_num == 0;int try;for (try = 0; try < 4; try++) {if (read_block(context, block, block_num, EDID_LENGTH))return EDID_BLOCK_READ_FAIL;status = edid_block_check(block, is_base_block);if (status == EDID_BLOCK_HEADER_REPAIR) {edid_header_fix(block);  Retry with fixed header, update status if that worked. ", "num_modes += add_detailed_modes(connector, drm_edid);num_modes += add_cvt_modes(connector, drm_edid);num_modes += add_standard_modes(connector, drm_edid);num_modes += add_established_modes(connector, drm_edid);num_modes += add_cea_modes(connector, drm_edid);num_modes += add_alternate_cea_modes(connector, drm_edid);num_modes += add_displayid_detailed_modes(connector, drm_edid);if (drm_edid->edid->features & DRM_EDID_FEATURE_CONTINUOUS_FREQ)num_modes += add_inferred_modes(connector, drm_edid);if (info->quirks & (EDID_QUIRK_PREFER_LARGE_60 | EDID_QUIRK_PREFER_LARGE_75))edid_fixup_preferred(connector);return num_modes;}static void _drm_update_tile_info(struct drm_connector *connector,  const struct drm_edid *drm_edid);static int _drm_edid_connector_property_update(struct drm_connector *connector,       const struct drm_edid *drm_edid)": "drm_edid_connector_add_modes(struct drm_connector  connector, const struct drm_edid  drm_edid){const struct drm_display_info  info = &connector->display_info;int num_modes = 0;if (!drm_edid)return 0;    EDID spec says modes should be preferred in this order:   - preferred detailed mode   - other detailed modes from base block   - detailed modes from extension blocks   - CVT 3-byte code modes   - standard timing codes   - established timing codes   - modes inferred from GTF or CVT range information     We get this pretty much right.     XXX order for additional mode types in extension blocks? ", "int drm_add_edid_modes(struct drm_connector *connector, struct edid *edid)": "drm_add_edid_modes - add modes from EDID data, if available   @connector: connector we're probing   @edid: EDID data     Add the specified modes to the connector's mode list. Also fills out the   &drm_display_info structure and ELD in @connector with any information which   can be derived from the edid.     This function is deprecated. Use drm_edid_connector_add_modes() instead.     Return: The number of modes added or 0 if we couldn't find any. ", "int drm_add_modes_noedid(struct drm_connector *connector,int hdisplay, int vdisplay)": "drm_add_modes_noedid - add modes for the connectors without EDID   @connector: connector we're probing   @hdisplay: the horizontal display limit   @vdisplay: the vertical display limit     Add the specified modes to the connector's mode list. Only when the   hdisplayvdisplay is not beyond the given limit, it will be added.     Return: The number of modes added or 0 if we couldn't find any. ", "void drm_set_preferred_mode(struct drm_connector *connector,   int hpref, int vpref)": "drm_set_preferred_mode - Sets the preferred mode of a connector   @connector: connector whose mode list should be processed   @hpref: horizontal resolution of preferred mode   @vpref: vertical resolution of preferred mode     Marks a mode as preferred if it matches the resolution specified by @hpref   and @vpref. ", "intdrm_hdmi_avi_infoframe_from_display_mode(struct hdmi_avi_infoframe *frame, const struct drm_connector *connector, const struct drm_display_mode *mode)": "drm_hdmi_avi_infoframe_from_display_mode() - fill an HDMI AVI infoframe with                                                data from a DRM display mode   @frame: HDMI AVI infoframe   @connector: the connector   @mode: DRM display mode     Return: 0 on success or a negative error code on failure. ", "voiddrm_hdmi_avi_infoframe_quant_range(struct hdmi_avi_infoframe *frame,   const struct drm_connector *connector,   const struct drm_display_mode *mode,   enum hdmi_quantization_range rgb_quant_range)": "drm_hdmi_avi_infoframe_quant_range() - fill the HDMI AVI infoframe                                          quantization range information   @frame: HDMI AVI infoframe   @connector: the connector   @mode: DRM display mode   @rgb_quant_range: RGB quantization range (Q) ", "intdrm_hdmi_vendor_infoframe_from_display_mode(struct hdmi_vendor_infoframe *frame,    const struct drm_connector *connector,    const struct drm_display_mode *mode)": "drm_hdmi_vendor_infoframe_from_display_mode() - fill an HDMI infoframe with   data from a DRM display mode   @frame: HDMI vendor infoframe   @connector: the connector   @mode: DRM display mode     Note that there's is a need to send HDMI vendor infoframes only when using a   4k or stereoscopic 3D mode. So when giving any other mode as input this   function will return -EINVAL, error that can be safely ignored.     Return: 0 on success or a negative error code on failure. ", "unsigned int drm_fb_clip_offset(unsigned int pitch, const struct drm_format_info *format,const struct drm_rect *clip)": "drm_fb_clip_offset - Returns the clipping rectangles byte-offset in a framebuffer   @pitch: Framebuffer line pitch in byte   @format: Framebuffer format   @clip: Clip rectangle     Returns:   The byte offset of the clip rectangle's top-left corner within the framebuffer. ", "void drm_fb_memcpy(struct iosys_map *dst, const unsigned int *dst_pitch,   const struct iosys_map *src, const struct drm_framebuffer *fb,   const struct drm_rect *clip)": "drm_fb_memcpy - Copy clip buffer   @dst: Array of destination buffers   @dst_pitch: Array of numbers of bytes between the start of two consecutive scanlines               within @dst; can be NULL if scanlines are stored next to each other.   @src: Array of source buffers   @fb: DRM framebuffer   @clip: Clip rectangle area to copy     This function copies parts of a framebuffer to display memory. Destination and   framebuffer formats must match. No conversion takes place. The parameters @dst,   @dst_pitch and @src refer to arrays. Each array must have at least as many entries   as there are planes in @fb's format. Each entry stores the value for the format's   respective color plane at the same index.     This function does not apply clipping on @dst (i.e. the destination is at the   top-left corner). ", "void drm_fb_swab(struct iosys_map *dst, const unsigned int *dst_pitch, const struct iosys_map *src, const struct drm_framebuffer *fb, const struct drm_rect *clip, bool cached)": "drm_fb_swab16_line(void  dbuf, const void  sbuf, unsigned int pixels){u16  dbuf16 = dbuf;const u16  sbuf16 = sbuf;const u16  send16 = sbuf16 + pixels;while (sbuf16 < send16) dbuf16++ = swab16( sbuf16++);}static void drm_fb_swab32_line(void  dbuf, const void  sbuf, unsigned int pixels){u32  dbuf32 = dbuf;const u32  sbuf32 = sbuf;const u32  send32 = sbuf32 + pixels;while (sbuf32 < send32) dbuf32++ = swab32( sbuf32++);}     drm_fb_swab - Swap bytes into clip buffer   @dst: Array of destination buffers   @dst_pitch: Array of numbers of bytes between the start of two consecutive scanlines               within @dst; can be NULL if scanlines are stored next to each other.   @src: Array of source buffers   @fb: DRM framebuffer   @clip: Clip rectangle area to copy   @cached: Source buffer is mapped cached (eg. not write-combined)     This function copies parts of a framebuffer to display memory and swaps per-pixel   bytes during the process. Destination and framebuffer formats must match. The   parameters @dst, @dst_pitch and @src refer to arrays. Each array must have at   least as many entries as there are planes in @fb's format. Each entry stores the   value for the format's respective color plane at the same index. If @cached is   false a temporary buffer is used to cache one pixel line at a time to speed up   slow uncached reads.     This function does not apply clipping on @dst (i.e. the destination is at the   top-left corner). ", "void drm_fb_xrgb8888_to_rgb332(struct iosys_map *dst, const unsigned int *dst_pitch,       const struct iosys_map *src, const struct drm_framebuffer *fb,       const struct drm_rect *clip)": "drm_fb_xrgb8888_to_rgb332_line(void  dbuf, const void  sbuf, unsigned int pixels){u8  dbuf8 = dbuf;const __le32  sbuf32 = sbuf;unsigned int x;u32 pix;for (x = 0; x < pixels; x++) {pix = le32_to_cpu(sbuf32[x]);dbuf8[x] = ((pix & 0x00e00000) >> 16) |   ((pix & 0x0000e000) >> 11) |   ((pix & 0x000000c0) >> 6);}}     drm_fb_xrgb8888_to_rgb332 - Convert XRGB8888 to RGB332 clip buffer   @dst: Array of RGB332 destination buffers   @dst_pitch: Array of numbers of bytes between the start of two consecutive scanlines               within @dst; can be NULL if scanlines are stored next to each other.   @src: Array of XRGB8888 source buffers   @fb: DRM framebuffer   @clip: Clip rectangle area to copy     This function copies parts of a framebuffer to display memory and converts the   color format during the process. Destination and framebuffer formats must match. The   parameters @dst, @dst_pitch and @src refer to arrays. Each array must have at   least as many entries as there are planes in @fb's format. Each entry stores the   value for the format's respective color plane at the same index.     This function does not apply clipping on @dst (i.e. the destination is at the   top-left corner).     Drivers can use this function for RGB332 devices that don't support XRGB8888 natively. ", "static void drm_fb_xrgb8888_to_rgb565_swab_line(void *dbuf, const void *sbuf,unsigned int pixels)": "drm_fb_xrgb8888_to_rgb565_line(void  dbuf, const void  sbuf, unsigned int pixels){__le16  dbuf16 = dbuf;const __le32  sbuf32 = sbuf;unsigned int x;u16 val16;u32 pix;for (x = 0; x < pixels; x++) {pix = le32_to_cpu(sbuf32[x]);val16 = ((pix & 0x00F80000) >> 8) |((pix & 0x0000FC00) >> 5) |((pix & 0x000000F8) >> 3);dbuf16[x] = cpu_to_le16(val16);}}  TODO: implement this helper as conversion to RGB565|BIG_ENDIAN ", "void drm_fb_xrgb8888_to_xrgb1555(struct iosys_map *dst, const unsigned int *dst_pitch, const struct iosys_map *src, const struct drm_framebuffer *fb, const struct drm_rect *clip)": "drm_fb_xrgb8888_to_xrgb1555_line(void  dbuf, const void  sbuf, unsigned int pixels){__le16  dbuf16 = dbuf;const __le32  sbuf32 = sbuf;unsigned int x;u16 val16;u32 pix;for (x = 0; x < pixels; x++) {pix = le32_to_cpu(sbuf32[x]);val16 = ((pix & 0x00f80000) >> 9) |((pix & 0x0000f800) >> 6) |((pix & 0x000000f8) >> 3);dbuf16[x] = cpu_to_le16(val16);}}     drm_fb_xrgb8888_to_xrgb1555 - Convert XRGB8888 to XRGB1555 clip buffer   @dst: Array of XRGB1555 destination buffers   @dst_pitch: Array of numbers of bytes between the start of two consecutive scanlines               within @dst; can be NULL if scanlines are stored next to each other.   @src: Array of XRGB8888 source buffer   @fb: DRM framebuffer   @clip: Clip rectangle area to copy     This function copies parts of a framebuffer to display memory and converts   the color format during the process. The parameters @dst, @dst_pitch and   @src refer to arrays. Each array must have at least as many entries as   there are planes in @fb's format. Each entry stores the value for the   format's respective color plane at the same index.     This function does not apply clipping on @dst (i.e. the destination is at the   top-left corner).     Drivers can use this function for XRGB1555 devices that don't support   XRGB8888 natively. ", "((pix & 0x00f80000) >> 9) |((pix & 0x0000f800) >> 6) |((pix & 0x000000f8) >> 3);dbuf16[x] = cpu_to_le16(val16);}}/** * drm_fb_xrgb8888_to_argb1555 - Convert XRGB8888 to ARGB1555 clip buffer * @dst: Array of ARGB1555 destination buffers * @dst_pitch: Array of numbers of bytes between the start of two consecutive scanlines *             within @dst; can be NULL if scanlines are stored next to each other. * @src: Array of XRGB8888 source buffer * @fb: DRM framebuffer * @clip: Clip rectangle area to copy * * This function copies parts of a framebuffer to display memory and converts * the color format during the process. The parameters @dst, @dst_pitch and * @src refer to arrays. Each array must have at least as many entries as * there are planes in @fb's format. Each entry stores the value for the * format's respective color plane at the same index. * * This function does not apply clipping on @dst (i.e. the destination is at the * top-left corner). * * Drivers can use this function for ARGB1555 devices that don't support * XRGB8888 natively. It sets an opaque alpha channel as part of the conversion. ": "drm_fb_xrgb8888_to_argb1555_line(void  dbuf, const void  sbuf, unsigned int pixels){__le16  dbuf16 = dbuf;const __le32  sbuf32 = sbuf;unsigned int x;u16 val16;u32 pix;for (x = 0; x < pixels; x++) {pix = le32_to_cpu(sbuf32[x]);val16 = BIT(15) |   set alpha bit ", "dbuf16[x] = cpu_to_le16(val16);}}/** * drm_fb_xrgb8888_to_rgba5551 - Convert XRGB8888 to RGBA5551 clip buffer * @dst: Array of RGBA5551 destination buffers * @dst_pitch: Array of numbers of bytes between the start of two consecutive scanlines *             within @dst; can be NULL if scanlines are stored next to each other. * @src: Array of XRGB8888 source buffer * @fb: DRM framebuffer * @clip: Clip rectangle area to copy * * This function copies parts of a framebuffer to display memory and converts * the color format during the process. The parameters @dst, @dst_pitch and * @src refer to arrays. Each array must have at least as many entries as * there are planes in @fb's format. Each entry stores the value for the * format's respective color plane at the same index. * * This function does not apply clipping on @dst (i.e. the destination is at the * top-left corner). * * Drivers can use this function for RGBA5551 devices that don't support * XRGB8888 natively. It sets an opaque alpha channel as part of the conversion. ": "drm_fb_xrgb8888_to_rgba5551_line(void  dbuf, const void  sbuf, unsigned int pixels){__le16  dbuf16 = dbuf;const __le32  sbuf32 = sbuf;unsigned int x;u16 val16;u32 pix;for (x = 0; x < pixels; x++) {pix = le32_to_cpu(sbuf32[x]);val16 = ((pix & 0x00f80000) >> 8) |((pix & 0x0000f800) >> 5) |((pix & 0x000000f8) >> 2) |BIT(0);   set alpha bit ", "*dbuf8++ = (pix & 0x000000FF) >>  0;*dbuf8++ = (pix & 0x0000FF00) >>  8;*dbuf8++ = (pix & 0x00FF0000) >> 16;}}/** * drm_fb_xrgb8888_to_rgb888 - Convert XRGB8888 to RGB888 clip buffer * @dst: Array of RGB888 destination buffers * @dst_pitch: Array of numbers of bytes between the start of two consecutive scanlines *             within @dst; can be NULL if scanlines are stored next to each other. * @src: Array of XRGB8888 source buffers * @fb: DRM framebuffer * @clip: Clip rectangle area to copy * * This function copies parts of a framebuffer to display memory and converts the * color format during the process. Destination and framebuffer formats must match. The * parameters @dst, @dst_pitch and @src refer to arrays. Each array must have at * least as many entries as there are planes in @fb's format. Each entry stores the * value for the format's respective color plane at the same index. * * This function does not apply clipping on @dst (i.e. the destination is at the * top-left corner). * * Drivers can use this function for RGB888 devices that don't natively * support XRGB8888. ": "drm_fb_xrgb8888_to_rgb888_line(void  dbuf, const void  sbuf, unsigned int pixels){u8  dbuf8 = dbuf;const __le32  sbuf32 = sbuf;unsigned int x;u32 pix;for (x = 0; x < pixels; x++) {pix = le32_to_cpu(sbuf32[x]);  write blue-green-red to output in little endianness ", "dbuf32[x] = cpu_to_le32(pix);}}/** * drm_fb_xrgb8888_to_argb8888 - Convert XRGB8888 to ARGB8888 clip buffer * @dst: Array of ARGB8888 destination buffers * @dst_pitch: Array of numbers of bytes between the start of two consecutive scanlines *             within @dst; can be NULL if scanlines are stored next to each other. * @src: Array of XRGB8888 source buffer * @fb: DRM framebuffer * @clip: Clip rectangle area to copy * * This function copies parts of a framebuffer to display memory and converts the * color format during the process. The parameters @dst, @dst_pitch and @src refer * to arrays. Each array must have at least as many entries as there are planes in * @fb's format. Each entry stores the value for the format's respective color plane * at the same index. * * This function does not apply clipping on @dst (i.e. the destination is at the * top-left corner). * * Drivers can use this function for ARGB8888 devices that don't support XRGB8888 * natively. It sets an opaque alpha channel as part of the conversion. ": "drm_fb_xrgb8888_to_argb8888_line(void  dbuf, const void  sbuf, unsigned int pixels){__le32  dbuf32 = dbuf;const __le32  sbuf32 = sbuf;unsigned int x;u32 pix;for (x = 0; x < pixels; x++) {pix = le32_to_cpu(sbuf32[x]);pix |= GENMASK(31, 24);   fill alpha bits ", "void drm_fb_xrgb8888_to_xrgb2101010(struct iosys_map *dst, const unsigned int *dst_pitch,    const struct iosys_map *src, const struct drm_framebuffer *fb,    const struct drm_rect *clip)": "drm_fb_xrgb8888_to_xrgb2101010_line(void  dbuf, const void  sbuf, unsigned int pixels){__le32  dbuf32 = dbuf;const __le32  sbuf32 = sbuf;unsigned int x;u32 val32;u32 pix;for (x = 0; x < pixels; x++) {pix = le32_to_cpu(sbuf32[x]);val32 = ((pix & 0x000000FF) << 2) |((pix & 0x0000FF00) << 4) |((pix & 0x00FF0000) << 6);pix = val32 | ((val32 >> 8) & 0x00300C03); dbuf32++ = cpu_to_le32(pix);}}     drm_fb_xrgb8888_to_xrgb2101010 - Convert XRGB8888 to XRGB2101010 clip buffer   @dst: Array of XRGB2101010 destination buffers   @dst_pitch: Array of numbers of bytes between the start of two consecutive scanlines               within @dst; can be NULL if scanlines are stored next to each other.   @src: Array of XRGB8888 source buffers   @fb: DRM framebuffer   @clip: Clip rectangle area to copy     This function copies parts of a framebuffer to display memory and converts the   color format during the process. Destination and framebuffer formats must match. The   parameters @dst, @dst_pitch and @src refer to arrays. Each array must have at   least as many entries as there are planes in @fb's format. Each entry stores the   value for the format's respective color plane at the same index.     This function does not apply clipping on @dst (i.e. the destination is at the   top-left corner).     Drivers can use this function for XRGB2101010 devices that don't support XRGB8888   natively. ", "      val32 | ((val32 >> 8) & 0x00300c03);*dbuf32++ = cpu_to_le32(pix);}}/** * drm_fb_xrgb8888_to_argb2101010 - Convert XRGB8888 to ARGB2101010 clip buffer * @dst: Array of ARGB2101010 destination buffers * @dst_pitch: Array of numbers of bytes between the start of two consecutive scanlines *             within @dst; can be NULL if scanlines are stored next to each other. * @src: Array of XRGB8888 source buffers * @fb: DRM framebuffer * @clip: Clip rectangle area to copy * * This function copies parts of a framebuffer to display memory and converts * the color format during the process. The parameters @dst, @dst_pitch and * @src refer to arrays. Each array must have at least as many entries as * there are planes in @fb's format. Each entry stores the value for the * format's respective color plane at the same index. * * This function does not apply clipping on @dst (i.e. the destination is at the * top-left corner). * * Drivers can use this function for ARGB2101010 devices that don't support XRGB8888 * natively. ": "drm_fb_xrgb8888_to_argb2101010_line(void  dbuf, const void  sbuf, unsigned int pixels){__le32  dbuf32 = dbuf;const __le32  sbuf32 = sbuf;unsigned int x;u32 val32;u32 pix;for (x = 0; x < pixels; x++) {pix = le32_to_cpu(sbuf32[x]);val32 = ((pix & 0x000000ff) << 2) |((pix & 0x0000ff00) << 4) |((pix & 0x00ff0000) << 6);pix = GENMASK(31, 30) |   set alpha bits ", "*dbuf8++ = (3 * r + 6 * g + b) / 10;}}/** * drm_fb_xrgb8888_to_gray8 - Convert XRGB8888 to grayscale * @dst: Array of 8-bit grayscale destination buffers * @dst_pitch: Array of numbers of bytes between the start of two consecutive scanlines *             within @dst; can be NULL if scanlines are stored next to each other. * @src: Array of XRGB8888 source buffers * @fb: DRM framebuffer * @clip: Clip rectangle area to copy * * This function copies parts of a framebuffer to display memory and converts the * color format during the process. Destination and framebuffer formats must match. The * parameters @dst, @dst_pitch and @src refer to arrays. Each array must have at * least as many entries as there are planes in @fb's format. Each entry stores the * value for the format's respective color plane at the same index. * * This function does not apply clipping on @dst (i.e. the destination is at the * top-left corner). * * DRM doesn't have native monochrome or grayscale support. Drivers can use this * function for grayscale devices that don't support XRGB8888 natively.Such * drivers can announce the commonly supported XR24 format to userspace and use * this function to convert to the native format. Monochrome drivers will use the * most significant bit, where 1 means foreground color and 0 background color. * ITU BT.601 is being used for the RGB -> luma (brightness) conversion. ": "drm_fb_xrgb8888_to_gray8_line(void  dbuf, const void  sbuf, unsigned int pixels){u8  dbuf8 = dbuf;const __le32  sbuf32 = sbuf;unsigned int x;for (x = 0; x < pixels; x++) {u32 pix = le32_to_cpu(sbuf32[x]);u8 r = (pix & 0x00ff0000) >> 16;u8 g = (pix & 0x0000ff00) >> 8;u8 b =  pix & 0x000000ff;  ITU BT.601: Y = 0.299 R + 0.587 G + 0.114 B ", "int drm_fb_blit(struct iosys_map *dst, const unsigned int *dst_pitch, uint32_t dst_format,const struct iosys_map *src, const struct drm_framebuffer *fb,const struct drm_rect *clip)": "drm_fb_blit - Copy parts of a framebuffer to display memory   @dst:Array of display-memory addresses to copy to   @dst_pitch: Array of numbers of bytes between the start of two consecutive scanlines               within @dst; can be NULL if scanlines are stored next to each other.   @dst_format:FOURCC code of the display's color format   @src:The framebuffer memory to copy from   @fb:The framebuffer to copy from   @clip:Clip rectangle area to copy     This function copies parts of a framebuffer to display memory. If the   formats of the display and the framebuffer mismatch, the blit function   will attempt to convert between them during the process. The parameters @dst,   @dst_pitch and @src refer to arrays. Each array must have at least as many   entries as there are planes in @dst_format's format. Each entry stores the   value for the format's respective color plane at the same index.     This function does not apply clipping on @dst (i.e. the destination is at the   top-left corner).     Returns:   0 on success, or   -EINVAL if the color-format conversion failed, or   a negative error code otherwise. ", "void drm_fb_xrgb8888_to_mono(struct iosys_map *dst, const unsigned int *dst_pitch,     const struct iosys_map *src, const struct drm_framebuffer *fb,     const struct drm_rect *clip)": "drm_fb_xrgb8888_to_mono - Convert XRGB8888 to monochrome   @dst: Array of monochrome destination buffers (0=black, 1=white)   @dst_pitch: Array of numbers of bytes between the start of two consecutive scanlines               within @dst; can be NULL if scanlines are stored next to each other.   @src: Array of XRGB8888 source buffers   @fb: DRM framebuffer   @clip: Clip rectangle area to copy     This function copies parts of a framebuffer to display memory and converts the   color format during the process. Destination and framebuffer formats must match. The   parameters @dst, @dst_pitch and @src refer to arrays. Each array must have at   least as many entries as there are planes in @fb's format. Each entry stores the   value for the format's respective color plane at the same index.     This function does not apply clipping on @dst (i.e. the destination is at the   top-left corner). The first pixel (upper left corner of the clip rectangle) will   be converted and copied to the first bit (LSB) in the first byte of the monochrome   destination buffer. If the caller requires that the first pixel in a byte must   be located at an x-coordinate that is a multiple of 8, then the caller must take   care itself of supplying a suitable clip rectangle.     DRM doesn't have native monochrome support. Drivers can use this function for   monochrome devices that don't support XRGB8888 natively. Such drivers can   announce the commonly supported XR24 format to userspace and use this function   to convert to the native format.     This function uses drm_fb_xrgb8888_to_gray8() to convert to grayscale and   then the result is converted from grayscale to monochrome. ", "size_t drm_fb_build_fourcc_list(struct drm_device *dev,const u32 *native_fourccs, size_t native_nfourccs,u32 *fourccs_out, size_t nfourccs_out)": "drm_fb_build_fourcc_list - Filters a list of supported color formats against                              the device's native formats   @dev: DRM device   @native_fourccs: 4CC codes of natively supported color formats   @native_nfourccs: The number of entries in @native_fourccs   @fourccs_out: Returns 4CC codes of supported color formats   @nfourccs_out: The number of available entries in @fourccs_out     This function create a list of supported color format from natively   supported formats and additional emulated formats.   At a minimum, most userspace programs expect at least support for   XRGB8888 on the primary plane. Devices that have to emulate the   format, and possibly others, can use drm_fb_build_fourcc_list() to   create a list of supported color formats. The returned list can   be handed over to drm_universal_plane_init() et al. Native formats   will go before emulated formats. Native formats with alpha channel   will be replaced by such without, as primary planes usually don't   support alpha. Other heuristics might be applied   to optimize the order. Formats near the beginning of the list are   usually preferred over formats near the end of the list.     Returns:   The number of color-formats 4CC codes returned in @fourccs_out. ", "void drm_vma_offset_manager_init(struct drm_vma_offset_manager *mgr, unsigned long page_offset, unsigned long size)": "drm_vma_offset_manager_init - Initialize new offset-manager   @mgr: Manager object   @page_offset: Offset of available memory area (page-based)   @size: Size of available address space range (page-based)     Initialize a new offset-manager. The offset and area size available for the   manager are given as @page_offset and @size. Both are interpreted as   page-numbers, not bytes.     Addingremoving nodes from the manager is locked internally and protected   against concurrent access. However, node allocation and destruction is left   for the caller. While calling into the vma-manager, a given node must   always be guaranteed to be referenced. ", "void drm_vma_offset_manager_destroy(struct drm_vma_offset_manager *mgr)": "drm_vma_offset_manager_destroy() - Destroy offset manager   @mgr: Manager object     Destroy an object manager which was previously created via   drm_vma_offset_manager_init(). The caller must remove all allocated nodes   before destroying the manager. Otherwise, drm_mm will refuse to free the   requested resources.     The manager must not be accessed after this function is called. ", "struct drm_vma_offset_node *drm_vma_offset_lookup_locked(struct drm_vma_offset_manager *mgr, unsigned long start, unsigned long pages)": "drm_vma_offset_lookup_locked() - Find node in offset space   @mgr: Manager object   @start: Start address for object (page-based)   @pages: Size of object (page-based)     Find a node given a start address and object size. This returns the _best_   match for the given node. That is, @start may point somewhere into a valid   region and the given node will be returned, as long as the node spans the   whole requested area (given the size in number of pages as @pages).     Note that before lookup the vma offset manager lookup lock must be acquired   with drm_vma_offset_lock_lookup(). See there for an example. This can then be   used to implement weakly referenced lookups using kref_get_unless_zero().     Example:     ::         drm_vma_offset_lock_lookup(mgr);       node = drm_vma_offset_lookup_locked(mgr);       if (node)           kref_get_unless_zero(container_of(node, sth, entr));       drm_vma_offset_unlock_lookup(mgr);     RETURNS:   Returns NULL if no suitable node can be found. Otherwise, the best match   is returned. It's the caller's responsibility to make sure the node doesn't   get destroyed before the caller can access it. ", "int drm_vma_offset_add(struct drm_vma_offset_manager *mgr,       struct drm_vma_offset_node *node, unsigned long pages)": "drm_vma_offset_remove(), anyway. However, no cleanup is required in that   case.     @pages is not required to be the same size as the underlying memory object   that you want to map. It only limits the size that user-space can map into   their address space.     RETURNS:   0 on success, negative error code on failure. ", "/** * drm_vma_offset_manager_init - Initialize new offset-manager * @mgr: Manager object * @page_offset: Offset of available memory area (page-based) * @size: Size of available address space range (page-based) * * Initialize a new offset-manager. The offset and area size available for the * manager are given as @page_offset and @size. Both are interpreted as * page-numbers, not bytes. * * Adding/removing nodes from the manager is locked internally and protected * against concurrent access. However, node allocation and destruction is left * for the caller. While calling into the vma-manager, a given node must * always be guaranteed to be referenced. ": "drm_vma_node_revoke(). However, the caller is responsible   for destroying already existing mappings, if required. ", "int drm_vma_node_allow_once(struct drm_vma_offset_node *node, struct drm_file *tag)": "drm_vma_node_allow_once - Add open-file to list of allowed users   @node: Node to modify   @tag: Tag of file to remove     Add @tag to the list of allowed open-files for this node.     The list of allowed-users is preserved across drm_vma_offset_add() and   drm_vma_offset_remove() calls. You may even call it if the node is currently   not added to any offset-manager.     This is not ref-counted unlike drm_vma_node_allow() hence drm_vma_node_revoke()   should only be called once after this.     This is locked against concurrent access internally.     RETURNS:   0 on success, negative error code on internal failure (out-of-mem) ", "bool drm_vma_node_is_allowed(struct drm_vma_offset_node *node,     struct drm_file *tag)": "drm_vma_node_is_allowed - Check whether an open-file is granted access   @node: Node to check   @tag: Tag of file to remove     Search the list in @node whether @tag is currently on the list of allowed   open-files (see drm_vma_node_allow()).     This is locked against concurrent access internally.     RETURNS:   true if @filp is on the list ", "bool drm_helper_encoder_in_use(struct drm_encoder *encoder)": "drm_helper_encoder_in_use - check if a given encoder is in use   @encoder: encoder to check     Checks whether @encoder is with the current mode setting output configuration   in use by any connector. This doesn't mean that it is actually enabled since   the DPMS state is tracked separately.     Returns:   True if @encoder is used, false otherwise. ", "bool drm_helper_crtc_in_use(struct drm_crtc *crtc)": "drm_helper_crtc_in_use - check if a given CRTC is in a mode_config   @crtc: CRTC to check     Checks whether @crtc is with the current mode setting output configuration   in use by any connector. This doesn't mean that it is actually enabled since   the DPMS state is tracked separately.     Returns:   True if @crtc is used, false otherwise. ", "encoder->crtc = NULL;}}drm_for_each_crtc(crtc, dev) ": "drm_helper_disable_unused_functions(struct drm_device  dev){struct drm_encoder  encoder;struct drm_crtc  crtc;drm_warn_on_modeset_not_all_locked(dev);drm_for_each_encoder(encoder, dev) {if (!drm_helper_encoder_in_use(encoder)) {drm_encoder_disable(encoder);  disconnect encoder from any connector ", "bool drm_crtc_helper_set_mode(struct drm_crtc *crtc,      struct drm_display_mode *mode,      int x, int y,      struct drm_framebuffer *old_fb)": "drm_crtc_helper_set_mode - internal helper to set a mode   @crtc: CRTC to program   @mode: mode to use   @x: horizontal offset into the surface   @y: vertical offset into the surface   @old_fb: old framebuffer, for cleanup     Try to set @mode on @crtc.  Give @crtc and its associated connectors a chance   to fixup or reject the mode prior to trying to set it. This is an internal   helper that drivers could e.g. use to update properties that require the   entire output pipe to be disabled and re-enabled in a new configuration. For   example for changing whether audio is enabled on a hdmi link or for changing   panel fitter or dither attributes. It is also called by the   drm_crtc_helper_set_config() helper function to drive the mode setting   sequence.     Returns:   True if the mode was set successfully, false otherwise. ", "int drm_crtc_helper_atomic_check(struct drm_crtc *crtc, struct drm_atomic_state *state)": "drm_crtc_helper_atomic_check() - Helper to check CRTC atomic-state   @crtc: CRTC to check   @state: atomic state object     Provides a default CRTC-state check handler for CRTCs that only have   one primary plane attached to it.     This is often the case for the CRTC of simple framebuffers. See also   drm_plane_helper_atomic_check() for the respective plane-state check   helper function.     RETURNS:   Zero on success, or an errno code otherwise. ", "/** * drm_helper_encoder_in_use - check if a given encoder is in use * @encoder: encoder to check * * Checks whether @encoder is with the current mode setting output configuration * in use by any connector. This doesn't mean that it is actually enabled since * the DPMS state is tracked separately. * * Returns: * True if @encoder is used, false otherwise. ": "drm_helper_resume_force_mode().     Note that this helper library doesn't track the current power state of CRTCs   and encoders. It can call callbacks like &drm_encoder_helper_funcs.dpms even   though the hardware is already in the desired state. This deficiency has been   fixed in the atomic helpers.     The driver callbacks are mostly compatible with the atomic modeset helpers,   except for the handling of the primary plane: Atomic helpers require that the   primary plane is implemented as a real standalone plane and not directly tied   to the CRTC state. For easier transition this library provides functions to   implement the old semantics required by the CRTC helpers using the new plane   and atomic helper callbacks.     Drivers are strongly urged to convert to the atomic helpers (by way of first   converting to the plane helpers). New drivers must not use these functions   but need to implement the atomic interface instead, potentially using the   atomic helpers for that.     These legacy modeset helpers use the same function table structures as   all other modesetting helpers. See the documentation for struct   &drm_crtc_helper_funcs, &struct drm_encoder_helper_funcs and struct   &drm_connector_helper_funcs. ", "int drm_helper_connector_dpms(struct drm_connector *connector, int mode)": "drm_helper_connector_dpms() - connector dpms helper implementation   @connector: affected connector   @mode: DPMS mode     The drm_helper_connector_dpms() helper function implements the   &drm_connector_funcs.dpms callback for drivers using the legacy CRTC   helpers.     This is the main helper function provided by the CRTC helper framework for   implementing the DPMS connector attribute. It computes the new desired DPMS   state for all encoders and CRTCs in the output mesh and calls the   &drm_crtc_helper_funcs.dpms and &drm_encoder_helper_funcs.dpms callbacks   provided by the driver.     This function is deprecated.  New drivers must implement atomic modeset   support, where DPMS is handled in the DRM core.     Returns:   Always returns 0. ", "int drm_helper_force_disable_all(struct drm_device *dev)": "drm_helper_force_disable_all - Forcibly turn off all enabled CRTCs   @dev: DRM device whose CRTCs to turn off     Drivers may want to call this on unload to ensure that all displays are   unlit and the GPU is in a consistent, low power state. Takes modeset locks.     Note: This should only be used by non-atomic legacy drivers. For an atomic   version look at drm_atomic_helper_shutdown().     Returns:   Zero on success, error code on failure. ", "int drm_client_modeset_probe(struct drm_client_dev *client, unsigned int width, unsigned int height)": "drm_client_modeset_probe() - Probe for displays   @client: DRM client   @width: Maximum display mode width (optional)   @height: Maximum display mode height (optional)     This function sets up display pipelines for enabled connectors and stores the   config in the client's modeset array.     Returns:   Zero on success or negative error code on failure. ", "bool drm_client_rotation(struct drm_mode_set *modeset, unsigned int *rotation)": "drm_client_rotation() - Check the initial rotation value   @modeset: DRM modeset   @rotation: Returned rotation value     This function checks if the primary plane in @modeset can hw rotate   to match the rotation needed on its connector.     Note: Currently only 0 and 180 degrees are supported.     Return:   True if the plane can do the rotation, false otherwise. ", "int drm_client_modeset_check(struct drm_client_dev *client)": "drm_client_modeset_check() - Check modeset configuration   @client: DRM client     Check modeset configuration.     Returns:   Zero on success or negative error code on failure. ", "int drm_client_modeset_commit_locked(struct drm_client_dev *client)": "drm_client_modeset_commit_locked() - Force commit CRTC configuration   @client: DRM client     Commit modeset configuration to crtcs without checking if there is a DRM   master. The assumption is that the caller already holds an internal DRM   master reference acquired with drm_master_internal_acquire().     Returns:   Zero on success or negative error code on failure. ", "if (plane->type == DRM_PLANE_TYPE_PRIMARY)continue;ret = __drm_atomic_helper_disable_plane(plane, plane_state);if (ret != 0)goto out_state;}drm_client_for_each_modeset(mode_set, client) ": "drm_client_modeset_commit_atomic(struct drm_client_dev  client, bool active, bool check){struct drm_device  dev = client->dev;struct drm_plane  plane;struct drm_atomic_state  state;struct drm_modeset_acquire_ctx ctx;struct drm_mode_set  mode_set;int ret;drm_modeset_acquire_init(&ctx, 0);state = drm_atomic_state_alloc(dev);if (!state) {ret = -ENOMEM;goto out_ctx;}state->acquire_ctx = &ctx;retry:drm_for_each_plane(plane, dev) {struct drm_plane_state  plane_state;plane_state = drm_atomic_get_plane_state(state, plane);if (IS_ERR(plane_state)) {ret = PTR_ERR(plane_state);goto out_state;}plane_state->rotation = DRM_MODE_ROTATE_0;  disable non-primary: ", "int drm_client_modeset_dpms(struct drm_client_dev *client, int mode)": "drm_client_modeset_dpms_legacy(struct drm_client_dev  client, int dpms_mode){struct drm_device  dev = client->dev;struct drm_connector  connector;struct drm_mode_set  modeset;struct drm_modeset_acquire_ctx ctx;int j;int ret;DRM_MODESET_LOCK_ALL_BEGIN(dev, ctx, 0, ret);drm_client_for_each_modeset(modeset, client) {if (!modeset->crtc->enabled)continue;for (j = 0; j < modeset->num_connectors; j++) {connector = modeset->connectors[j];connector->funcs->dpms(connector, dpms_mode);drm_object_property_set_value(&connector->base,dev->mode_config.dpms_property, dpms_mode);}}DRM_MODESET_LOCK_ALL_END(dev, ctx, ret);}     drm_client_modeset_dpms() - Set DPMS mode   @client: DRM client   @mode: DPMS mode     Note: For atomic drivers @mode is reduced to onoff.     Returns:   Zero on success or negative error code on failure. ", "const char *drm_get_connector_type_name(unsigned int type)": "drm_get_connector_type_name - return a string for connector type   @type: The connector type (DRM_MODE_CONNECTOR_ )     Returns: the name of the connector type, or NULL if the type is not valid. ", "/* * Global connector list for drm_connector_find_by_fwnode(). * Note drm_connector_[un]register() first take connector->lock and then * take the connector_list_lock. ": "drm_connector_register().     Connectors must be attached to an encoder to be used. For devices that map   connectors to encoders 1:1, the connector should be attached at   initialization time with a call to drm_connector_attach_encoder(). The   driver must also set the &drm_connector.encoder field to point to the   attached encoder.     For connectors which are not fixed (like built-in panels) the driver needs to   support hotplug notifications. The simplest way to do that is by using the   probe helpers, see drm_kms_helper_poll_init() for connectors which don't have   hardware support for hotplug interrupts. Connectors with hardware hotplug   support can instead use e.g. drm_helper_hpd_irq_event(). ", "int drm_connector_init_with_ddc(struct drm_device *dev,struct drm_connector *connector,const struct drm_connector_funcs *funcs,int connector_type,struct i2c_adapter *ddc)": "drm_connector_init_with_ddc - Init a preallocated connector   @dev: DRM device   @connector: the connector to init   @funcs: callbacks for this connector   @connector_type: user visible type of the connector   @ddc: pointer to the associated ddc adapter     Initialises a preallocated connector. Connectors should be   subclassed as part of driver connector objects.     At driver unload time the driver's &drm_connector_funcs.destroy hook   should call drm_connector_cleanup() and free the connector structure.   The connector structure should not be allocated with devm_kzalloc().     Ensures that the ddc field of the connector is correctly set.     Note: consider using drmm_connector_init() instead of   drm_connector_init_with_ddc() to let the DRM managed resource   infrastructure take care of cleanup and deallocation.     Returns:   Zero on success, error code on failure. ", "int drm_connector_init(struct drm_device *dev,       struct drm_connector *connector,       const struct drm_connector_funcs *funcs,       int connector_type)": "drm_connector_cleanup() and free the connector structure.   The connector structure should not be allocated with devm_kzalloc().     Note: consider using drmm_connector_init() instead of   drm_connector_init() to let the DRM managed resource infrastructure   take care of cleanup and deallocation.     Returns:   Zero on success, error code on failure. ", "bool drm_connector_has_possible_encoder(struct drm_connector *connector,struct drm_encoder *encoder)": "drm_connector_has_possible_encoder - check if the connector and encoder are   associated with each other   @connector: the connector   @encoder: the encoder     Returns:   True if @encoder is one of the possible encoders for @connector. ", "int drm_connector_register(struct drm_connector *connector)": "drm_mode_put_tile_group(dev, connector->tile_group);connector->tile_group = NULL;}list_for_each_entry_safe(mode, t, &connector->probed_modes, head)drm_mode_remove(connector, mode);list_for_each_entry_safe(mode, t, &connector->modes, head)drm_mode_remove(connector, mode);ida_free(&drm_connector_enum_list[connector->connector_type].ida,  connector->connector_type_id);ida_free(&dev->mode_config.connector_ida, connector->index);kfree(connector->display_info.bus_formats);kfree(connector->display_info.vics);drm_mode_object_unregister(dev, &connector->base);kfree(connector->name);connector->name = NULL;fwnode_handle_put(connector->fwnode);connector->fwnode = NULL;spin_lock_irq(&dev->mode_config.connector_list_lock);list_del(&connector->head);dev->mode_config.num_connector--;spin_unlock_irq(&dev->mode_config.connector_list_lock);WARN_ON(connector->state && !connector->funcs->atomic_destroy_state);if (connector->state && connector->funcs->atomic_destroy_state)connector->funcs->atomic_destroy_state(connector,       connector->state);mutex_destroy(&connector->mutex);memset(connector, 0, sizeof( connector));if (dev->registered)drm_sysfs_hotplug_event(dev);}EXPORT_SYMBOL(drm_connector_cleanup);     drm_connector_register - register a connector   @connector: the connector to register     Register userspace interfaces for a connector. Only call this for connectors   which can be hotplugged after drm_dev_register() has been called already,   e.g. DP MST connectors. All other connectors will be registered automatically   when calling drm_dev_register().     When the connector is no longer available, callers must call   drm_connector_unregister().     Returns:   Zero on success, error code on failure. ", "const char *drm_get_connector_status_name(enum drm_connector_status status)": "drm_connector_list_iter_end(&conn_iter);}int drm_connector_register_all(struct drm_device  dev){struct drm_connector  connector;struct drm_connector_list_iter conn_iter;int ret = 0;drm_connector_list_iter_begin(dev, &conn_iter);drm_for_each_connector_iter(connector, &conn_iter) {ret = drm_connector_register(connector);if (ret)break;}drm_connector_list_iter_end(&conn_iter);if (ret)drm_connector_unregister_all(dev);return ret;}     drm_get_connector_status_name - return a string for connector status   @status: connector status to compute name of     In contrast to the other drm_get_ _name functions this one here returns a   const pointer and hence is threadsafe.     Returns: connector status string ", "void drm_connector_list_iter_begin(struct drm_device *dev,   struct drm_connector_list_iter *iter)": "drm_connector_list_iter_next() or   drm_for_each_connector_iter(). ", "const char *drm_get_subpixel_order_name(enum subpixel_order order)": "drm_get_subpixel_order_name - return a string for a given subpixel enum   @order: enum of subpixel_order     Note you could abuse this and return something out of bounds, but that   would be a caller error.  No unscrubbed user data should make it here.     Returns: string describing an enumerated subpixel property ", "int drm_display_info_set_bus_formats(struct drm_display_info *info,     const u32 *formats,     unsigned int num_formats)": "drm_display_info_set_bus_formats - set the supported bus formats   @info: display info to store bus formats in   @formats: array containing the supported bus formats   @num_formats: the number of entries in the fmts array     Store the supported bus formats in display info structure.   See MEDIA_BUS_FMT_  definitions in includeuapilinuxmedia-bus-format.h for   a full list of available formats.     Returns:   0 on success or a negative error code on failure. ", "int drm_get_tv_mode_from_name(const char *name, size_t len)": "drm_get_tv_mode_from_name - Translates a TV mode name into its enum value   @name: TV Mode name we want to convert   @len: Length of @name     Translates @name into an enum drm_connector_tv_mode.     Returns: the enum value on success, a negative errno otherwise. ", "int drm_mode_create_dvi_i_properties(struct drm_device *dev)": "drm_mode_create_dvi_i_properties - create DVI-I specific connector properties   @dev: DRM device     Called by a driver the first time a DVI-I connector is made.     Returns: %0 ", "void drm_connector_attach_dp_subconnector_property(struct drm_connector *connector)": "drm_connector_attach_dp_subconnector_property - create subconnector property for DP   @connector: drm_connector to attach property     Called by a driver when DP connector is created. ", "/* * TODO: Document the properties: *   - brightness *   - contrast *   - flicker reduction *   - hue *   - mode *   - overscan *   - saturation *   - select subconnector ": "drm_connector_attach_content_type_property(). Decoding to  infoframe values is done through drm_hdmi_avi_infoframe_content_type(). ", "void drm_connector_attach_tv_margin_properties(struct drm_connector *connector)": "drm_mode_create_content_type_property(connector->dev))drm_object_attach_property(&connector->base,   connector->dev->mode_config.content_type_property,   DRM_MODE_CONTENT_TYPE_NO_DATA);return 0;}EXPORT_SYMBOL(drm_connector_attach_content_type_property);     drm_connector_attach_tv_margin_properties - attach TV connector margin   properties   @connector: DRM connector     Called by a driver when it needs to attach TV margin props to a connector.   Typically used on SDTV and HDMI connectors. ", "int drm_connector_create_standard_properties(struct drm_device *dev)": "drm_connector_set_panel_orientation_with_quirk()     scaling mode:  This property defines how a non-native mode is upscaled to the native  mode of an LCD panel:    None:  No upscaling happens, scaling is left to the panel. Not all  drivers expose this mode.  Full:  The output is upscaled to the full resolution of the panel,  ignoring the aspect ratio.  Center:  No upscaling happens, the output is centered within the native  resolution the panel.  Full aspect:  The output is upscaled to maximize either the width or height  while retaining the aspect ratio.    This property should be set up by calling  drm_connector_attach_scaling_mode_property(). Note that drivers  can also expose this property to external outputs, in which case they  must support \"None\", which should be the default (since external screens  have a built-in scaler).     subconnector:  This property is used by DVI-I, TVout and DisplayPort to indicate different  connector subtypes. Enum values more or less match with those from main  connector types.  For DVI-I and TVout there is also a matching property \"select subconnector\"  allowing to switch between signal types.  DP subconnector corresponds to a downstream port.     privacy-screen sw-state, privacy-screen hw-state:  These 2 optional properties can be used to query the state of the  electronic privacy screen that is available on some displays; and in  some cases also control the state. If a driver implements these  properties then both properties must be present.    \"privacy-screen hw-state\" is read-only and reflects the actual state  of the privacy-screen, possible values: \"Enabled\", \"Disabled,  \"Enabled-locked\", \"Disabled-locked\". The locked states indicate  that the state cannot be changed through the DRM API. E.g. there  might be devices where the firmware-setup options, or a hardware  slider-switch, offer always on  off modes.    \"privacy-screen sw-state\" can be set to change the privacy-screen state  when not locked. In this case the driver must update the hw-state  property to reflect the new state on completion of the commit of the  sw-state property. Setting the sw-state property when the hw-state is  locked must be interpreted by the driver as a request to change the  state to the set state when the hw-state becomes unlocked. E.g. if  \"privacy-screen hw-state\" is \"Enabled-locked\" and the sw-state  gets set to \"Disabled\" followed by the user unlocking the state by  changing the slider-switch position, then the driver must set the  state to \"Disabled\" upon receiving the unlock event.    In some cases the privacy-screen's actual state might change outside of  control of the DRM code. E.g. there might be a firmware handled hotkey  which toggles the actual state, or the actual state might be changed  through another userspace API such as writing procacpiibmlcdshadow.  In this case the driver must update both the hw-state and the sw-state  to reflect the new value, overwriting any pending state requests in the  sw-state. Any pending sw-state requests are thus discarded.    Note that the ability for the state to change outside of control of  the DRM master process means that userspace must not cache the value  of the sw-state. Caching the sw-state value and including it in later  atomic commits may lead to overriding a state change done through e.g.  a firmware handled hotkey. Therefor userspace must not include the  privacy-screen sw-state in an atomic commit unless it wants to change  its value.     left margin, right margin, top margin, bottom margin:  Add margins to the connector's viewport. This is typically used to  mitigate overscan on TVs.    The value is the size in pixels of the black border which will be  added. The attached CRTC's content will be scaled to fill the whole  area inside the margin.    The margins configuration might be sent to the sink, e.g. via HDMI AVI  InfoFrames.    Drivers can set up these properties by calling  drm_mode_create_tv_margin_properties(). ", "int drm_mode_create_tv_margin_properties(struct drm_device *dev)": "drm_mode_create_tv_properties_legacy().     Returns:   0 on success or a negative error code on failure. ", "/** * drm_connector_attach_content_type_property - attach content-type property * @connector: connector to attach content type property on. * * Called by a driver the first time a HDMI connector is made. * * Returns: %0 ": "drm_mode_create_tv_properties(). ", "int drm_mode_create_scaling_mode_property(struct drm_device *dev)": "drm_mode_create_scaling_mode_property - create scaling mode property   @dev: DRM device     Called by a driver the first time it's needed, must be attached to desired   connectors.     Atomic drivers should use drm_connector_attach_scaling_mode_property()   instead to correctly assign &drm_connector_state.scaling_mode   in the atomic state.     Returns: %0 ", "/** * drm_connector_attach_vrr_capable_property - creates the * vrr_capable property * @connector: connector to create the vrr_capable property on. * * This is used by atomic drivers to add support for querying * variable refresh rate capability for a connector. * * Returns: * Zero on success, negative errno on failure. ": "drm_connector_set_vrr_capable_property().    Absence of the property should indicate absence of support.     \"VRR_ENABLED\":  Default &drm_crtc boolean property that notifies the driver that the  content on the CRTC is suitable for variable refresh rate presentation.  The driver will take this property as a hint to enable variable  refresh rate support if the receiver supports it, ie. if the  \"vrr_capable\" property is true on the &drm_connector object. The  vertical front porch duration will be extended until page-flip or  timeout when enabled.    The minimum vertical front porch duration is defined as the vertical  front porch duration for the current mode.    The maximum vertical front porch duration is greater than or equal to  the minimum vertical front porch duration. The duration is derived  from the minimum supported variable refresh rate for the connector.    The driver may place further restrictions within these minimum  and maximum bounds. ", "int drm_mode_create_aspect_ratio_property(struct drm_device *dev)": "drm_mode_create_aspect_ratio_property - create aspect ratio property   @dev: DRM device     Called by a driver the first time it's needed, must be attached to desired   connectors.     Returns:   Zero on success, negative errno on failure. ", "static int drm_mode_create_colorspace_property(struct drm_connector *connector,u32 supported_colorspaces)": "drm_mode_create_dp_colorspace_property() is used for DP connector. ", "int drm_mode_create_suggested_offset_properties(struct drm_device *dev)": "drm_mode_create_suggested_offset_properties - create suggests offset properties   @dev: DRM device     Create the suggested xy offset property for connectors.     Returns:   0 on success or a negative error code on failure. ", "int drm_connector_attach_hdr_output_metadata_property(struct drm_connector *connector)": "drm_connector_attach_hdr_output_metadata_property - attach \"HDR_OUTPUT_METADA\" property   @connector: connector to attach the property on.     This is used to allow the userspace to send HDR Metadata to the   driver.     Returns:   Zero on success, negative errno on failure. ", "int drm_connector_attach_colorspace_property(struct drm_connector *connector)": "drm_connector_attach_colorspace_property - attach \"Colorspace\" property   @connector: connector to attach the property on.     This is used to allow the userspace to signal the output colorspace   to the driver.     Returns:   Zero on success, negative errno on failure. ", "bool drm_connector_atomic_hdr_metadata_equal(struct drm_connector_state *old_state,     struct drm_connector_state *new_state)": "drm_connector_atomic_hdr_metadata_equal - checks if the hdr metadata changed   @old_state: old connector state to compare   @new_state: new connector state to compare     This is used by HDR-enabled drivers to test whether the HDR metadata   have changed between two different connector state (and thus probably   requires a full blown mode change).     Returns:   True if the metadata are equal, False otherwise ", "ret = ida_alloc_max(&config->connector_ida, 31, GFP_KERNEL);if (ret < 0) ": "drm_connector_set_panel_orientation(connector,    mode->panel_orientation);}DRM_DEBUG_KMS(\"cmdline mode for connector %s %s %dx%d@%dHz%s%s%s\\n\",      connector->name, mode->name,      mode->xres, mode->yres,      mode->refresh_specified ? mode->refresh : 60,      mode->rb ? \" reduced blanking\" : \"\",      mode->margins ? \" with margins\" : \"\",      mode->interlace ?  \" interlaced\" : \"\");}static void drm_connector_free(struct kref  kref){struct drm_connector  connector =container_of(kref, struct drm_connector, base.refcount);struct drm_device  dev = connector->dev;drm_mode_object_unregister(dev, &connector->base);connector->funcs->destroy(connector);}void drm_connector_free_work_fn(struct work_struct  work){struct drm_connector  connector,  n;struct drm_device  dev =container_of(work, struct drm_device, mode_config.connector_free_work);struct drm_mode_config  config = &dev->mode_config;unsigned long flags;struct llist_node  freed;spin_lock_irqsave(&config->connector_list_lock, flags);freed = llist_del_all(&config->connector_free_list);spin_unlock_irqrestore(&config->connector_list_lock, flags);llist_for_each_entry_safe(connector, n, freed, free_node) {drm_mode_object_unregister(dev, &connector->base);connector->funcs->destroy(connector);}}static int __drm_connector_init(struct drm_device  dev,struct drm_connector  connector,const struct drm_connector_funcs  funcs,int connector_type,struct i2c_adapter  ddc){struct drm_mode_config  config = &dev->mode_config;int ret;struct ida  connector_ida =&drm_connector_enum_list[connector_type].ida;WARN_ON(drm_drv_uses_atomic_modeset(dev) &&(!funcs->atomic_destroy_state || !funcs->atomic_duplicate_state));ret = __drm_mode_object_add(dev, &connector->base,    DRM_MODE_OBJECT_CONNECTOR,    false, drm_connector_free);if (ret)return ret;connector->base.properties = &connector->properties;connector->dev = dev;connector->funcs = funcs;  connector index is used with 32bit bitmasks ", "int drm_connector_set_orientation_from_panel(struct drm_connector *connector,struct drm_panel *panel)": "drm_connector_set_orientation_from_panel -  set the connector's panel_orientation from panel's callback.   @connector: connector for which to init the panel-orientation property.   @panel: panel that can provide orientation information.     Drm drivers should call this function before drm_dev_register().   Orientation is obtained from panel's .get_orientation() callback.     Returns:   Zero on success, negative errno on failure. ", "voiddrm_connector_create_privacy_screen_properties(struct drm_connector *connector)": "drm_connector_create_privacy_screen_properties - create the drm connecter's      privacy-screen properties.   @connector: connector for which to create the privacy-screen properties     This function creates the \"privacy-screen sw-state\" and \"privacy-screen   hw-state\" properties for the connector. They are not attached. ", "voiddrm_connector_attach_privacy_screen_properties(struct drm_connector *connector)": "drm_connector_attach_privacy_screen_properties - attach the drm connecter's      privacy-screen properties.   @connector: connector on which to attach the privacy-screen properties     This function attaches the \"privacy-screen sw-state\" and \"privacy-screen   hw-state\" properties to the connector. The initial state of both is set   to \"Disabled\". ", "void drm_connector_attach_privacy_screen_provider(struct drm_connector *connector, struct drm_privacy_screen *priv)": "drm_connector_update_privacy_screen_properties(struct drm_connector  connector, bool set_sw_state){enum drm_privacy_screen_status sw_state, hw_state;drm_privacy_screen_get_state(connector->privacy_screen,     &sw_state, &hw_state);if (set_sw_state)connector->state->privacy_screen_sw_state = sw_state;drm_object_property_set_value(&connector->base,connector->privacy_screen_hw_state_property, hw_state);}static int drm_connector_privacy_screen_notifier(struct notifier_block  nb, unsigned long action, void  data){struct drm_connector  connector =container_of(nb, struct drm_connector, privacy_screen_notifier);struct drm_device  dev = connector->dev;drm_modeset_lock(&dev->mode_config.connection_mutex, NULL);drm_connector_update_privacy_screen_properties(connector, true);drm_modeset_unlock(&dev->mode_config.connection_mutex);drm_sysfs_connector_status_event(connector,connector->privacy_screen_sw_state_property);drm_sysfs_connector_status_event(connector,connector->privacy_screen_hw_state_property);return NOTIFY_DONE;}     drm_connector_attach_privacy_screen_provider - attach a privacy-screen to      the connector   @connector: connector to attach the privacy-screen to   @priv: drm_privacy_screen to attach     Create and attach the standard privacy-screen properties and register   a generic notifier for generating sysfs-connector-status-events   on external changes to the privacy-screen status.   This function takes ownership of the passed in drm_privacy_screen and will   call drm_privacy_screen_put() on it when the connector is destroyed. ", "void drm_connector_oob_hotplug_event(struct fwnode_handle *connector_fwnode)": "drm_connector_oob_hotplug_event - Report out-of-band hotplug event to connector   @connector_fwnode: fwnode_handle to report the event on     On some hardware a hotplug event notification may come from outside the display   driver  device. An example of this is some USB Type-C setups where the hardware   muxes the DisplayPort data and aux-lines but does not pass the altmode HPD   status bit to the GPU's DP HPD pin.     This function can be used to report these out-of-band events after obtaining   a drm_connector reference through calling drm_connector_find_by_fwnode(). ", "static void drm_tile_group_free(struct kref *kref)": "drm_mode_create_tile_group(), drm_mode_put_tile_group() and   drm_mode_get_tile_group(). But this is only needed for internal panels where   the tile group information is exposed through a non-standard way. ", "void__drm_gem_duplicate_shadow_plane_state(struct drm_plane *plane,       struct drm_shadow_plane_state *new_shadow_plane_state)": "drm_gem_duplicate_shadow_plane_state - duplicates shadow-buffered plane state   @plane: the plane   @new_shadow_plane_state: the new shadow-buffered plane state     This function duplicates shadow-buffered plane state. This is helpful for drivers   that subclass struct drm_shadow_plane_state.     The function does not duplicate existing mappings of the shadow buffers.   Mappings are maintained during the atomic commit by the plane's prepare_fb   and cleanup_fb helpers. See drm_gem_prepare_shadow_fb() and drm_gem_cleanup_shadow_fb()   for corresponding helpers. ", "void __drm_gem_destroy_shadow_plane_state(struct drm_shadow_plane_state *shadow_plane_state)": "drm_gem_destroy_shadow_plane_state - cleans up shadow-buffered plane state   @shadow_plane_state: the shadow-buffered plane state     This function cleans up shadow-buffered plane state. Helpful for drivers that   subclass struct drm_shadow_plane_state. ", "void __drm_gem_reset_shadow_plane(struct drm_plane *plane,  struct drm_shadow_plane_state *shadow_plane_state)": "drm_gem_reset_shadow_plane - resets a shadow-buffered plane   @plane: the plane   @shadow_plane_state: the shadow-buffered plane state     This function resets state for shadow-buffered planes. Helpful   for drivers that subclass struct drm_shadow_plane_state. ", "int drm_gem_begin_shadow_fb_access(struct drm_plane *plane, struct drm_plane_state *plane_state)": "drm_gem_end_shadow_fb_access() for cleanup.     Returns:   0 on success, or a negative errno code otherwise. ", "int drm_gem_simple_kms_begin_shadow_fb_access(struct drm_simple_display_pipe *pipe,      struct drm_plane_state *plane_state)": "drm_gem_simple_kms_begin_shadow_fb_access - prepares shadow framebuffers for CPU access   @pipe: the simple display pipe   @plane_state: the plane state of type struct drm_shadow_plane_state     This function implements struct drm_simple_display_funcs.begin_fb_access.     See drm_gem_begin_shadow_fb_access() for details and   drm_gem_simple_kms_cleanup_shadow_fb() for cleanup.     Returns:   0 on success, or a negative errno code otherwise. ", "void drm_gem_simple_kms_end_shadow_fb_access(struct drm_simple_display_pipe *pipe,     struct drm_plane_state *plane_state)": "drm_gem_simple_kms_end_shadow_fb_access - releases shadow framebuffers from CPU access   @pipe: the simple display pipe   @plane_state: the plane state of type struct drm_shadow_plane_state     This function implements struct drm_simple_display_funcs.end_fb_access.   It undoes all effects of drm_gem_simple_kms_begin_shadow_fb_access() in   reverse order.     See drm_gem_simple_kms_begin_shadow_fb_access(). ", "void drm_gem_simple_kms_reset_shadow_plane(struct drm_simple_display_pipe *pipe)": "drm_gem_simple_kms_reset_shadow_plane - resets a shadow-buffered plane   @pipe: the simple display pipe     This function implements struct drm_simple_display_funcs.reset_plane   for shadow-buffered planes. ", "struct drm_plane_state *drm_gem_simple_kms_duplicate_shadow_plane_state(struct drm_simple_display_pipe *pipe)": "drm_gem_simple_kms_duplicate_shadow_plane_state - duplicates shadow-buffered plane state   @pipe: the simple display pipe     This function implements struct drm_simple_display_funcs.duplicate_plane_state   for shadow-buffered planes. It does not duplicate existing mappings of the shadow   buffers. Mappings are maintained during the atomic commit by the plane's prepare_fb   and cleanup_fb helpers.     Returns:   A pointer to a new plane state on success, or NULL otherwise. ", "void drm_gem_simple_kms_destroy_shadow_plane_state(struct drm_simple_display_pipe *pipe,   struct drm_plane_state *plane_state)": "drm_gem_simple_kms_destroy_shadow_plane_state - resets shadow-buffered plane state   @pipe: the simple display pipe   @plane_state: the plane state of type struct drm_shadow_plane_state     This function implements struct drm_simple_display_funcs.destroy_plane_state   for shadow-buffered planes. It expects that mappings of shadow buffers   have been released already. ", "void drm_panel_init(struct drm_panel *panel, struct device *dev,    const struct drm_panel_funcs *funcs, int connector_type)": "drm_panel_add(). ", "void drm_panel_remove(struct drm_panel *panel)": "drm_panel_remove - remove a panel from the global registry   @panel: DRM panel     Removes a panel from the global registry. ", "int drm_panel_prepare(struct drm_panel *panel)": "drm_panel_prepare - power on a panel   @panel: DRM panel     Calling this function will enable power and deassert any reset signals to   the panel. After this has completed it is possible to communicate with any   integrated circuitry via a command bus.     Return: 0 on success or a negative error code on failure. ", "int drm_panel_unprepare(struct drm_panel *panel)": "drm_panel_unprepare - power off a panel   @panel: DRM panel     Calling this function will completely power off a panel (assert the panel's   reset, turn off power supplies, ...). After this function has completed, it   is usually no longer possible to communicate with the panel until another   call to drm_panel_prepare().     Return: 0 on success or a negative error code on failure. ", "int drm_panel_enable(struct drm_panel *panel)": "drm_panel_enable - enable a panel   @panel: DRM panel     Calling this function will cause the panel display drivers to be turned on   and the backlight to be enabled. Content will be visible on screen after   this call completes.     Return: 0 on success or a negative error code on failure. ", "int drm_panel_disable(struct drm_panel *panel)": "drm_panel_disable - disable a panel   @panel: DRM panel     This will typically turn off the panel's backlight or disable the display   drivers. For smart panels it should still be possible to communicate with   the integrated circuitry via any command bus after this call.     Return: 0 on success or a negative error code on failure. ", "int drm_panel_get_modes(struct drm_panel *panel,struct drm_connector *connector)": "drm_panel_get_modes - probe the available display modes of a panel   @panel: DRM panel   @connector: DRM connector     The modes probed from the panel are automatically added to the connector   that the panel is attached to.     Return: The number of modes available from the panel on success or a   negative error code on failure. ", "struct drm_panel *of_drm_find_panel(const struct device_node *np)": "of_drm_find_panel - look up a panel using a device tree node   @np: device tree node of the panel     Searches the set of registered panels for one that matches the given device   tree node. If a matching panel is found, return a pointer to it.     Return: A pointer to the panel registered for the specified device tree   node or an ERR_PTR() if no panel matching the device tree node can be found.     Possible error codes returned by this function:     - EPROBE_DEFER: the panel device has not been probed yet, and the caller     should retry later   - ENODEV: the device is not available (status != \"okay\" or \"ok\") ", "int of_drm_get_panel_orientation(const struct device_node *np, enum drm_panel_orientation *orientation)": "of_drm_get_panel_orientation - look up the orientation of the panel through   the \"rotation\" binding from a device tree node   @np: device tree node of the panel   @orientation: orientation enum to be filled in     Looks up the rotation of a panel in the device tree. The orientation of the   panel is expressed as a property name \"rotation\" in the device tree. The   rotation in the device tree is counter clockwise.     Return: 0 when a valid rotation value (0, 90, 180, or 270) is read or the   rotation property doesn't exist. Return a negative error code on failure. ", "int drm_panel_of_backlight(struct drm_panel *panel)": "drm_panel_of_backlight - use backlight device node for backlight   @panel: DRM panel     Use this function to enable backlight handling if your panel   uses device tree and has a backlight phandle.     When the panel is enabled backlight will be enabled after a   successful call to &drm_panel_funcs.enable()     When the panel is disabled backlight will be disabled before the   call to &drm_panel_funcs.disable().     A typical implementation for a panel driver supporting device tree   will call this function at probe time. Backlight will then be handled   transparently without requiring any intervention from the driver.   drm_panel_of_backlight() must be called after the call to drm_panel_init().     Return: 0 on success or a negative error code on failure. ", "int drm_buddy_init(struct drm_buddy *mm, u64 size, u64 chunk_size)": "drm_buddy_init - init memory manager     @mm: DRM buddy manager to initialize   @size: size in bytes to manage   @chunk_size: minimum page size in bytes for our allocations     Initializes the memory manager and its resources.     Returns:   0 on success, error code on failure. ", "void drm_buddy_fini(struct drm_buddy *mm)": "drm_buddy_fini - tear down the memory manager     @mm: DRM buddy manager to free     Cleanup memory manager resources and the freelist ", "struct drm_buddy_block *drm_get_buddy(struct drm_buddy_block *block)": "drm_get_buddy - get buddy address     @block: DRM buddy block     Returns the corresponding buddy block for @block, or NULL   if this is a root block and can't be merged further.   Requires some kind of locking to protect against   any concurrent allocate and free operations. ", "void drm_buddy_free_block(struct drm_buddy *mm,  struct drm_buddy_block *block)": "drm_buddy_free_block - free a block     @mm: DRM buddy manager   @block: block to be freed ", "void drm_buddy_free_list(struct drm_buddy *mm, struct list_head *objects)": "drm_buddy_free_list - free blocks     @mm: DRM buddy manager   @objects: input list head to free blocks ", "int drm_buddy_block_trim(struct drm_buddy *mm, u64 new_size, struct list_head *blocks)": "drm_buddy_block_trim - free unused pages     @mm: DRM buddy manager   @new_size: original size requested   @blocks: Input and output list of allocated blocks.   MUST contain single block as input to be trimmed.   On success will contain the newly allocated blocks   making up the @new_size. Blocks always appear in   ascending order     For contiguous allocation, we round up the size to the nearest   power of two value, drivers consume  actual  size, so remaining   portions are unused and can be optionally freed with this function     Returns:   0 on success, error code on failure. ", "int drm_buddy_alloc_blocks(struct drm_buddy *mm,   u64 start, u64 end, u64 size,   u64 min_page_size,   struct list_head *blocks,   unsigned long flags)": "drm_buddy_alloc_blocks - allocate power-of-two blocks     @mm: DRM buddy manager to allocate from   @start: start of the allowed range for this block   @end: end of the allowed range for this block   @size: size of the allocation   @min_page_size: alignment of the allocation   @blocks: output list head to add allocated blocks   @flags: DRM_BUDDY_ _ALLOCATION flags     alloc_range_bias() called on range limitations, which traverses   the tree and returns the desired block.     alloc_from_freelist() called when  no  range restrictions   are enforced, which picks the block from the freelist.     Returns:   0 on success, error code on failure. ", "void drm_buddy_block_print(struct drm_buddy *mm,   struct drm_buddy_block *block,   struct drm_printer *p)": "drm_buddy_block_print - print block information     @mm: DRM buddy manager   @block: DRM buddy block   @p: DRM printer to use ", "void drm_buddy_print(struct drm_buddy *mm, struct drm_printer *p)": "drm_buddy_print - print allocator state     @mm: DRM buddy manager   @p: DRM printer to use ", "void drm_sysfs_hotplug_event(struct drm_device *dev)": "drm_sysfs_connector_status_event()   for uevents on connector status change. ", "void drm_sysfs_connector_hotplug_event(struct drm_connector *connector)": "drm_sysfs_connector_hotplug_event - generate a DRM uevent for any connector   change   @connector: connector which has changed     Send a uevent for the DRM connector specified by @connector. This will send   a uevent with the properties HOTPLUG=1 and CONNECTOR. ", "int drm_fb_helper_debug_enter(struct fb_info *info)": "drm_fb_helper_debug_enter - implementation for &fb_ops.fb_debug_enter   @info: fbdev registered by the helper ", "int drm_fb_helper_debug_leave(struct fb_info *info)": "drm_fb_helper_debug_leave - implementation for &fb_ops.fb_debug_leave   @info: fbdev registered by the helper ", "ret = drm_client_modeset_commit_locked(&fb_helper->client);} else ": "drm_fb_helper_set_par(). ", "int drm_fb_helper_blank(int blank, struct fb_info *info)": "drm_fb_helper_blank - implementation for &fb_ops.fb_blank   @blank: desired blanking state   @info: fbdev registered by the helper ", "static void drm_fb_helper_restore_lut_atomic(struct drm_crtc *crtc)": "drm_fb_helper_output_poll_changed() as their   &drm_mode_config_funcs.output_poll_changed callback. New implementations   of fbdev should be build on top of struct &drm_client_funcs, which handles   this automatically. Setting the old callbacks should be avoided.     For suspendresume consider using drm_mode_config_helper_suspend() and   drm_mode_config_helper_resume() which takes care of fbdev as well.     All other functions exported by the fb helper library can be used to   implement the fbdev driver interface by the driver.     It is possible, though perhaps somewhat tricky, to implement race-free   hotplug detection using the fbdev helpers. The drm_fb_helper_prepare()   helper must be called first to initialize the minimum required to make   hotplug detection work. Drivers also need to make sure to properly set up   the &drm_mode_config.funcs member. After calling drm_kms_helper_poll_init()   it is safe to enable interrupts and start processing hotplug events. At the   same time, drivers should initialize all modeset objects such as CRTCs,   encoders and connectors. To finish up the fbdev helper initialization, the   drm_fb_helper_init() function is called. To probe for all attached displays   and set up an initial configuration using the detected hardware, drivers   should call drm_fb_helper_initial_config().     If &drm_framebuffer_funcs.dirty is set, the   drm_fb_helper_{cfb,sys}_{write,fillrect,copyarea,imageblit} functions will   accumulate changes and schedule &drm_fb_helper.dirty_work to run right   away. This worker then calls the dirty() function ensuring that it will   always run in process context since the fb_ () function could be running in   atomic context. If drm_fb_helper_deferred_io() is used as the deferred_io   callback it will also schedule dirty_work with the damage collected from the   mmap page writes.     Deferred IO is not compatible with SHMEM. Such drivers should request an   fbdev shadow buffer and call drm_fbdev_generic_setup() instead. ", "void drm_fb_helper_unprepare(struct drm_fb_helper *fb_helper)": "drm_fb_helper_unprepare - clean up a drm_fb_helper structure   @fb_helper: driver-allocated fbdev helper structure to set up     Cleans up the framebuffer helper. Inverse of drm_fb_helper_prepare(). ", "struct fb_info *drm_fb_helper_alloc_info(struct drm_fb_helper *fb_helper)": "drm_fb_helper_fini().     RETURNS:   fb_info pointer if things went okay, pointer containing error code   otherwise ", "void drm_fb_helper_release_info(struct drm_fb_helper *fb_helper)": "drm_fb_helper_release_info - release fb_info and its members   @fb_helper: driver-allocated fbdev helper     A helper to release fb_info and the member cmap.  Drivers do not   need to release the allocated fb_info structure themselves, this is   automatically done when calling drm_fb_helper_fini(). ", "void drm_fb_helper_unregister_info(struct drm_fb_helper *fb_helper)": "drm_fb_helper_unregister_info - unregister fb_info framebuffer device   @fb_helper: driver-allocated fbdev helper, can be NULL     A wrapper around unregister_framebuffer, to release the fb_info   framebuffer device. This must be called before releasing all resources for   @fb_helper by calling drm_fb_helper_fini(). ", "void drm_fb_helper_set_suspend(struct drm_fb_helper *fb_helper, bool suspend)": "drm_fb_helper_set_suspend_unlocked() if you don't need to take   the lock yourself ", "int drm_fb_helper_setcmap(struct fb_cmap *cmap, struct fb_info *info)": "drm_fb_helper_setcmap - implementation for &fb_ops.fb_setcmap   @cmap: cmap to set   @info: fbdev registered by the helper ", "int drm_fb_helper_ioctl(struct fb_info *info, unsigned int cmd,unsigned long arg)": "drm_fb_helper_ioctl - legacy ioctl implementation   @info: fbdev registered by the helper   @cmd: ioctl command   @arg: ioctl argument     A helper to implement the standard fbdev ioctl. Only   FBIO_WAITFORVSYNC is implemented for now. ", "int drm_fb_helper_check_var(struct fb_var_screeninfo *var,    struct fb_info *info)": "drm_fb_helper_check_var - implementation for &fb_ops.fb_check_var   @var: screeninfo to check   @info: fbdev registered by the helper ", "int drm_fb_helper_pan_display(struct fb_var_screeninfo *var,      struct fb_info *info)": "drm_fb_helper_pan_display - implementation for &fb_ops.fb_pan_display   @var: updated screen information   @info: fbdev registered by the helper ", "void drm_fb_helper_fill_info(struct fb_info *info,     struct drm_fb_helper *fb_helper,     struct drm_fb_helper_surface_size *sizes)": "drm_fb_helper_fill_info - initializes fbdev information   @info: fbdev instance to set up   @fb_helper: fb helper instance to use as template   @sizes: describes fbdev size and scanout surface size     Sets up the variable and fixed fbdev metainformation from the given fb helper   instance and the drm framebuffer allocated in &drm_fb_helper.fb.     Drivers should call this (or their equivalent setup code) from their   &drm_fb_helper_funcs.fb_probe callback after having allocated the fbdev   backing storage framebuffer. ", "int drm_fb_helper_restore_fbdev_mode_unlocked(struct drm_fb_helper *fb_helper)": "drm_fb_helper_hotplug_event(fb_helper);return ret;}     drm_fb_helper_restore_fbdev_mode_unlocked - restore fbdev configuration   @fb_helper: driver-allocated fbdev helper, can be NULL     This should be called from driver's drm &drm_driver.lastclose callback   when implementing an fbcon on top of kms using this helper. This ensures that   the user isn't greeted with a black screen when e.g. X dies.     RETURNS:   Zero if everything went ok, negative error code otherwise. ", "struct drm_gem_vram_object *drm_gem_vram_create(struct drm_device *dev,size_t size,unsigned long pg_align)": "drm_gem_vram_create() - Creates a VRAM-backed GEM object   @dev:the DRM device   @size:the buffer size in bytes   @pg_align:the buffer's alignment in multiples of the page size     GEM objects are allocated by calling struct drm_driver.gem_create_object,   if set. Otherwise kzalloc() will be used. Drivers can set their own GEM   object functions in struct drm_driver.gem_create_object. If no functions   are set, the new GEM object will use the default functions from GEM VRAM   helpers.     Returns:   A new instance of &struct drm_gem_vram_object on success, or   an ERR_PTR()-encoded error code otherwise. ", "void drm_gem_vram_put(struct drm_gem_vram_object *gbo)": "drm_gem_vram_put() - Releases a reference to a VRAM-backed GEM object   @gbo:the GEM VRAM object     See ttm_bo_put() for more information. ", "/* * Buffer-objects helpers ": "drmm_vram_helper_init().   The function allocates and initializes an instance of &struct drm_vram_mm   in &struct drm_device.vram_mm . Use &DRM_GEM_VRAM_DRIVER to initialize   &struct drm_driver and  &DRM_VRAM_MM_FILE_OPERATIONS to initialize   &struct file_operations; as illustrated below.     .. code-block:: c    struct file_operations fops ={  .owner = THIS_MODULE,  DRM_VRAM_MM_FILE_OPERATION  };  struct drm_driver drv = {  .driver_feature = DRM_ ... ,  .fops = &fops,  DRM_GEM_VRAM_DRIVER  };    int init_drm_driver()  {  struct drm_device  dev;  uint64_t vram_base;  unsigned long vram_size;  int ret;     setup device, vram base and size   ...    ret = drmm_vram_helper_init(dev, vram_base, vram_size);  if (ret)  return ret;  return 0;  }     This creates an instance of &struct drm_vram_mm, exports DRM userspace   interfaces for GEM buffer management and initializes file operations to   allow for accessing created GEM buffers. With this setup, the DRM driver   manages an area of video RAM with VRAM MM and provides GEM VRAM objects   to userspace.     You don't have to clean up the instance of VRAM MM.   drmm_vram_helper_init() is a managed interface that installs a   clean-up handler to run during the DRM device's release.     For drawing or scanout operations, rsp. buffer objects have to be pinned   in video RAM. Call drm_gem_vram_pin() with &DRM_GEM_VRAM_PL_FLAG_VRAM or   &DRM_GEM_VRAM_PL_FLAG_SYSTEM to pin a buffer object in video RAM or system   memory. Call drm_gem_vram_unpin() to release the pinned object afterwards.     A buffer object that is pinned in video RAM has a fixed address within that   memory region. Call drm_gem_vram_offset() to retrieve this value. Typically   it's used to program the hardware's scanout engine for framebuffers, set   the cursor overlay's image for a mouse cursor, or use it as input to the   hardware's drawing engine.     To access a buffer object's memory from the DRM driver, call   drm_gem_vram_vmap(). It maps the buffer into kernel address   space and returns the memory address. Use drm_gem_vram_vunmap() to   release the mapping. ", "int drm_gem_vram_fill_create_dumb(struct drm_file *file,  struct drm_device *dev,  unsigned long pg_align,  unsigned long pitch_align,  struct drm_mode_create_dumb *args)": "drm_gem_vram_fill_create_dumb() - \\Helper for implementing &struct drm_driver.dumb_create   @file:the DRM file   @dev:the DRM device   @pg_align:the buffer's alignment in multiples of the page size   @pitch_align:the scanline's alignment in powers of 2   @args:the arguments as provided to \\&struct drm_driver.dumb_create     This helper function fills &struct drm_mode_create_dumb, which is used   by &struct drm_driver.dumb_create. Implementations of this interface   should forwards their arguments to this helper, plus the driver-specific   parameters.     Returns:   0 on success, or   a negative error code otherwise. ", "int drm_gem_vram_driver_dumb_create(struct drm_file *file,    struct drm_device *dev,    struct drm_mode_create_dumb *args)": "drm_gem_vram_driver_dumb_create() - \\Implements &struct drm_driver.dumb_create   @file:the DRM file   @dev:the DRM device   @args:the arguments as provided to \\&struct drm_driver.dumb_create     This function requires the driver to use @drm_device.vram_mm for its   instance of VRAM MM.     Returns:   0 on success, or   a negative error code otherwise. ", "intdrm_gem_vram_plane_helper_prepare_fb(struct drm_plane *plane,     struct drm_plane_state *new_state)": "drm_gem_vram_plane_helper_cleanup_fb(struct drm_plane  plane,   struct drm_plane_state  state,   unsigned int num_planes){struct drm_gem_object  obj;struct drm_gem_vram_object  gbo;struct drm_framebuffer  fb = state->fb;while (num_planes) {--num_planes;obj = drm_gem_fb_get_obj(fb, num_planes);if (!obj)continue;gbo = drm_gem_vram_of_gem(obj);drm_gem_vram_unpin(gbo);}}     drm_gem_vram_plane_helper_prepare_fb() - \\  Implements &struct drm_plane_helper_funcs.prepare_fb   @plane:a DRM plane   @new_state:the plane's new state     During plane updates, this function sets the plane's fence and   pins the GEM VRAM objects of the plane's new framebuffer to VRAM.   Call drm_gem_vram_plane_helper_cleanup_fb() to unpin them.     Returns:  0 on success, or  a negative errno code otherwise. ", "int drm_gem_vram_simple_display_pipe_prepare_fb(struct drm_simple_display_pipe *pipe,struct drm_plane_state *new_state)": "drm_gem_vram_simple_display_pipe_cleanup_fb() to unpin them.     Returns:  0 on success, or  a negative errno code otherwise. ", "void drm_vram_mm_debugfs_init(struct drm_minor *minor)": "drm_vram_mm_debugfs_init() - Register VRAM MM debugfs file.     @minor: drm minor device.   ", "enum drm_mode_statusdrm_vram_helper_mode_valid(struct drm_device *dev,   const struct drm_display_mode *mode)": "drm_vram_helper_mode_valid_internal(struct drm_device  dev,    const struct drm_display_mode  mode,    unsigned long max_bpp){struct drm_vram_mm  vmm = dev->vram_mm;unsigned long fbsize, fbpages, max_fbpages;if (WARN_ON(!dev->vram_mm))return MODE_BAD;max_fbpages = (vmm->vram_size  2) >> PAGE_SHIFT;fbsize = mode->hdisplay   mode->vdisplay   max_bpp;fbpages = DIV_ROUND_UP(fbsize, PAGE_SIZE);if (fbpages > max_fbpages)return MODE_MEM;return MODE_OK;}     drm_vram_helper_mode_valid - Tests if a display mode's  framebuffer fits into the available video memory.   @dev:the DRM device   @mode:the mode to test     This function tests if enough video memory is available for using the   specified display mode. Atomic modesetting requires importing the   designated framebuffer into video memory before evicting the active   one. Hence, any framebuffer may consume at most half of the available   VRAM. Display modes that require a larger framebuffer can not be used,   even if the CRTC does support them. Each framebuffer is assumed to   have 32-bit color depth.     Note:   The function can only test if the display mode is supported in   general. If there are too many framebuffers pinned to video memory,   a display mode may still not be usable in practice. The color depth of   32-bit fits all current use case. A more flexible test can be added   when necessary.     Returns:   MODE_OK if the display mode is supported, or an error code of type   enum drm_mode_status otherwise. ", "struct mipi_dsi_device *of_find_mipi_dsi_device_by_node(struct device_node *np)": "of_find_mipi_dsi_device_by_node() - find the MIPI DSI device matching a      device tree node   @np: device tree node     Return: A pointer to the MIPI DSI device corresponding to @np or NULL if no      such device exists (or has not been registered yet). ", "struct mipi_dsi_device *mipi_dsi_device_register_full(struct mipi_dsi_host *host,      const struct mipi_dsi_device_info *info)": "mipi_dsi_device_register_full(host, &info);}#elsestatic struct mipi_dsi_device  of_mipi_dsi_device_add(struct mipi_dsi_host  host, struct device_node  node){return ERR_PTR(-ENODEV);}#endif     mipi_dsi_device_register_full - create a MIPI DSI device   @host: DSI host to which this device is connected   @info: pointer to template containing DSI device information     Create a MIPI DSI device by using the device information provided by   mipi_dsi_device_info template     Returns:   A pointer to the newly created MIPI DSI device, or, a pointer encoded   with an error ", "void mipi_dsi_device_unregister(struct mipi_dsi_device *dsi)": "mipi_dsi_device_unregister - unregister MIPI DSI device   @dsi: DSI peripheral device ", "struct mipi_dsi_host *of_find_mipi_dsi_host_by_node(struct device_node *node)": "of_find_mipi_dsi_host_by_node() - find the MIPI DSI host matching a       device tree node   @node: device tree node     Returns:   A pointer to the MIPI DSI host corresponding to @node or NULL if no   such device exists (or has not been registered yet). ", "if (!of_property_present(node, \"reg\"))continue;of_mipi_dsi_device_add(host, node);}mutex_lock(&host_lock);list_add_tail(&host->list, &host_list);mutex_unlock(&host_lock);return 0;}EXPORT_SYMBOL(mipi_dsi_host_register": "mipi_dsi_host_register(struct mipi_dsi_host  host){struct device_node  node;for_each_available_child_of_node(host->dev->of_node, node) {  skip nodes without reg property ", "int mipi_dsi_attach(struct mipi_dsi_device *dsi)": "mipi_dsi_detach(dsi);mipi_dsi_device_unregister(dsi);return 0;}void mipi_dsi_host_unregister(struct mipi_dsi_host  host){device_for_each_child(host->dev, NULL, mipi_dsi_remove_device_fn);mutex_lock(&host_lock);list_del_init(&host->list);mutex_unlock(&host_lock);}EXPORT_SYMBOL(mipi_dsi_host_unregister);     mipi_dsi_attach - attach a DSI device to its DSI host   @dsi: DSI peripheral ", "bool mipi_dsi_packet_format_is_short(u8 type)": "mipi_dsi_packet_format_is_short - check if a packet is of the short format   @type: MIPI DSI data type of the packet     Return: true if the packet for the given data type is a short packet, false   otherwise. ", "bool mipi_dsi_packet_format_is_long(u8 type)": "mipi_dsi_packet_format_is_long - check if a packet is of the long format   @type: MIPI DSI data type of the packet     Return: true if the packet for the given data type is a long packet, false   otherwise. ", "int mipi_dsi_create_packet(struct mipi_dsi_packet *packet,   const struct mipi_dsi_msg *msg)": "mipi_dsi_create_packet - create a packet from a message according to the       DSI protocol   @packet: pointer to a DSI packet structure   @msg: message to translate into a packet     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_shutdown_peripheral(struct mipi_dsi_device *dsi)": "mipi_dsi_shutdown_peripheral() - sends a Shutdown Peripheral command   @dsi: DSI peripheral device     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_turn_on_peripheral(struct mipi_dsi_device *dsi)": "mipi_dsi_turn_on_peripheral() - sends a Turn On Peripheral command   @dsi: DSI peripheral device     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_set_maximum_return_packet_size(struct mipi_dsi_device *dsi,    u16 value)": "mipi_dsi_set_maximum_return_packet_size() - specify the maximum size of      the payload in a long packet transmitted from the peripheral back to the      host processor   @dsi: DSI peripheral device   @value: the maximum size of the payload     Return: 0 on success or a negative error code on failure. ", "ssize_t mipi_dsi_compression_mode(struct mipi_dsi_device *dsi, bool enable)": "mipi_dsi_compression_mode() - enabledisable DSC on the peripheral   @dsi: DSI peripheral device   @enable: Whether to enable or disable the DSC     Enable or disable Display Stream Compression on the peripheral using the   default Picture Parameter Set and VESA DSC 1.1 algorithm.     Return: 0 on success or a negative error code on failure. ", "ssize_t mipi_dsi_picture_parameter_set(struct mipi_dsi_device *dsi,       const struct drm_dsc_picture_parameter_set *pps)": "mipi_dsi_picture_parameter_set() - transmit the DSC PPS to the peripheral   @dsi: DSI peripheral device   @pps: VESA DSC 1.1 Picture Parameter Set     Transmit the VESA DSC 1.1 Picture Parameter Set to the peripheral.     Return: 0 on success or a negative error code on failure. ", "ssize_t mipi_dsi_generic_write(struct mipi_dsi_device *dsi, const void *payload,       size_t size)": "mipi_dsi_generic_write() - transmit data using a generic write packet   @dsi: DSI peripheral device   @payload: buffer containing the payload   @size: size of payload buffer     This function will automatically choose the right data type depending on   the payload length.     Return: The number of bytes transmitted on success or a negative error code   on failure. ", "ssize_t mipi_dsi_generic_read(struct mipi_dsi_device *dsi, const void *params,      size_t num_params, void *data, size_t size)": "mipi_dsi_generic_read() - receive data using a generic read packet   @dsi: DSI peripheral device   @params: buffer containing the request parameters   @num_params: number of request parameters   @data: buffer in which to return the received data   @size: size of receive buffer     This function will automatically choose the right data type depending on   the number of parameters passed in.     Return: The number of bytes successfully read or a negative error code on   failure. ", "ssize_t mipi_dsi_dcs_write_buffer(struct mipi_dsi_device *dsi,  const void *data, size_t len)": "mipi_dsi_dcs_write_buffer() - transmit a DCS command with payload   @dsi: DSI peripheral device   @data: buffer containing data to be transmitted   @len: size of transmission buffer     This function will automatically choose the right data type depending on   the command payload length.     Return: The number of bytes successfully transmitted or a negative error   code on failure. ", "ssize_t mipi_dsi_dcs_read(struct mipi_dsi_device *dsi, u8 cmd, void *data,  size_t len)": "mipi_dsi_dcs_read() - send DCS read request command   @dsi: DSI peripheral device   @cmd: DCS command   @data: buffer in which to receive data   @len: size of receive buffer     Return: The number of bytes read or a negative error code on failure. ", "int mipi_dsi_dcs_nop(struct mipi_dsi_device *dsi)": "mipi_dsi_dcs_nop() - send DCS nop packet   @dsi: DSI peripheral device     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_dcs_soft_reset(struct mipi_dsi_device *dsi)": "mipi_dsi_dcs_soft_reset() - perform a software reset of the display module   @dsi: DSI peripheral device     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_dcs_get_power_mode(struct mipi_dsi_device *dsi, u8 *mode)": "mipi_dsi_dcs_get_power_mode() - query the display module's current power      mode   @dsi: DSI peripheral device   @mode: return location for the current power mode     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_dcs_get_pixel_format(struct mipi_dsi_device *dsi, u8 *format)": "mipi_dsi_dcs_get_pixel_format() - gets the pixel format for the RGB image      data used by the interface   @dsi: DSI peripheral device   @format: return location for the pixel format     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_dcs_enter_sleep_mode(struct mipi_dsi_device *dsi)": "mipi_dsi_dcs_enter_sleep_mode() - disable all unnecessary blocks inside the      display module except interface communication   @dsi: DSI peripheral device     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_dcs_exit_sleep_mode(struct mipi_dsi_device *dsi)": "mipi_dsi_dcs_exit_sleep_mode() - enable all blocks inside the display      module   @dsi: DSI peripheral device     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_dcs_set_display_off(struct mipi_dsi_device *dsi)": "mipi_dsi_dcs_set_display_off() - stop displaying the image data on the      display device   @dsi: DSI peripheral device     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_dcs_set_display_on(struct mipi_dsi_device *dsi)": "mipi_dsi_dcs_set_display_on() - start displaying the image data on the      display device   @dsi: DSI peripheral device     Return: 0 on success or a negative error code on failure ", "int mipi_dsi_dcs_set_column_address(struct mipi_dsi_device *dsi, u16 start,    u16 end)": "mipi_dsi_dcs_set_column_address() - define the column extent of the frame      memory accessed by the host processor   @dsi: DSI peripheral device   @start: first column of frame memory   @end: last column of frame memory     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_dcs_set_page_address(struct mipi_dsi_device *dsi, u16 start,  u16 end)": "mipi_dsi_dcs_set_page_address() - define the page extent of the frame      memory accessed by the host processor   @dsi: DSI peripheral device   @start: first page of frame memory   @end: last page of frame memory     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_dcs_set_tear_off(struct mipi_dsi_device *dsi)": "mipi_dsi_dcs_set_tear_off() - turn off the display module's Tearing Effect      output signal on the TE signal line   @dsi: DSI peripheral device     Return: 0 on success or a negative error code on failure ", "int mipi_dsi_dcs_set_tear_on(struct mipi_dsi_device *dsi,     enum mipi_dsi_dcs_tear_mode mode)": "mipi_dsi_dcs_set_tear_on() - turn on the display module's Tearing Effect      output signal on the TE signal line.   @dsi: DSI peripheral device   @mode: the Tearing Effect Output Line mode     Return: 0 on success or a negative error code on failure ", "int mipi_dsi_dcs_set_pixel_format(struct mipi_dsi_device *dsi, u8 format)": "mipi_dsi_dcs_set_pixel_format() - sets the pixel format for the RGB image      data used by the interface   @dsi: DSI peripheral device   @format: pixel format     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_dcs_set_tear_scanline(struct mipi_dsi_device *dsi, u16 scanline)": "mipi_dsi_dcs_set_tear_scanline() - set the scanline to use as trigger for      the Tearing Effect output signal of the display module   @dsi: DSI peripheral device   @scanline: scanline to use as trigger     Return: 0 on success or a negative error code on failure ", "int mipi_dsi_dcs_set_display_brightness(struct mipi_dsi_device *dsi,u16 brightness)": "mipi_dsi_dcs_set_display_brightness() - sets the brightness value of the      display   @dsi: DSI peripheral device   @brightness: brightness value     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_dcs_get_display_brightness(struct mipi_dsi_device *dsi,u16 *brightness)": "mipi_dsi_dcs_get_display_brightness() - gets the current brightness value      of the display   @dsi: DSI peripheral device   @brightness: brightness value     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_dcs_set_display_brightness_large(struct mipi_dsi_device *dsi,     u16 brightness)": "mipi_dsi_dcs_set_display_brightness_large() - sets the 16-bit brightness value      of the display   @dsi: DSI peripheral device   @brightness: brightness value     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_dcs_get_display_brightness_large(struct mipi_dsi_device *dsi,     u16 *brightness)": "mipi_dsi_dcs_get_display_brightness_large() - gets the current 16-bit      brightness value of the display   @dsi: DSI peripheral device   @brightness: brightness value     Return: 0 on success or a negative error code on failure. ", "int mipi_dsi_driver_register_full(struct mipi_dsi_driver *drv,  struct module *owner)": "mipi_dsi_driver_register_full() - register a driver for DSI devices   @drv: DSI driver structure   @owner: owner module     Return: 0 on success or a negative error code on failure. ", "void mipi_dsi_driver_unregister(struct mipi_dsi_driver *drv)": "mipi_dsi_driver_unregister() - unregister a driver for DSI devices   @drv: DSI driver structure     Return: 0 on success or a negative error code on failure. ", "u64 drm_crtc_accurate_vblank_count(struct drm_crtc *crtc)": "drm_crtc_vblank_count() but this function   interpolates to handle a race with vblank interrupts using the high precision   timestamping support.     This is mostly useful for hardware that can obtain the scanout position, but   doesn't have a hardware frame counter. ", "/* Retry timestamp calculation up to 3 times to satisfy * drm_timestamp_precision before giving up. ": "drm_crtc_handle_vblank() in its vblank interrupt handler for working vblank   support.     Vertical blanking interrupts can be enabled by the DRM core or by drivers   themselves (for instance to handle page flipping operations).  The DRM core   maintains a vertical blanking use count to ensure that the interrupts are not   disabled while a user still needs them. To increment the use count, drivers   call drm_crtc_vblank_get() and release the vblank reference again with   drm_crtc_vblank_put(). In between these two calls vblank interrupts are   guaranteed to be enabled.     On many hardware disabling the vblank interrupt cannot be done in a race-free   manner, see &drm_driver.vblank_disable_immediate and   &drm_driver.max_vblank_count. In that case the vblank core only disables the   vblanks after a timer has expired, which can be configured through the   ``vblankoffdelay`` module parameter.     Drivers for hardware without support for vertical-blanking interrupts   must not call drm_vblank_init(). For such drivers, atomic helpers will   automatically generate fake vblank events as part of the display update.   This functionality also can be controlled by the driver by enabling and   disabling struct drm_crtc_state.no_vblank. ", "bool drm_dev_has_vblank(const struct drm_device *dev)": "drm_dev_has_vblank - test if vblanking has been initialized for                        a device   @dev: the device     Drivers may call this function to test if vblank support is   initialized for a device. For most hardware this means that vblanking   can also be enabled.     Atomic helpers use this function to initialize   &drm_crtc_state.no_vblank. See also drm_atomic_helper_check_modeset().     Returns:   True if vblanking has been initialized for the given device, false   otherwise. ", "wait_queue_head_t *drm_crtc_vblank_waitqueue(struct drm_crtc *crtc)": "drm_crtc_vblank_waitqueue - get vblank waitqueue for the CRTC   @crtc: which CRTC's vblank waitqueue to retrieve     This function returns a pointer to the vblank waitqueue for the CRTC.   Drivers can use this to implement vblank waits using wait_event() and related   functions. ", "void drm_calc_timestamping_constants(struct drm_crtc *crtc,     const struct drm_display_mode *mode)": "drm_crtc_vblank_helper_get_vblank_timestamp(). They are derived from   CRTC's true scanout timing, so they take things like panel scaling or   other adjustments into account. ", "booldrm_crtc_vblank_helper_get_vblank_timestamp_internal(struct drm_crtc *crtc, int *max_error, ktime_t *vblank_time,bool in_vblank_irq,drm_vblank_get_scanout_position_func get_scanout_position)": "drm_crtc_vblank_helper_get_vblank_timestamp_internal - precise vblank                                                          timestamp helper   @crtc: CRTC whose vblank timestamp to retrieve   @max_error: Desired maximum allowable error in timestamps (nanosecs)               On return contains true maximum error of timestamp   @vblank_time: Pointer to time which should receive the timestamp   @in_vblank_irq:       True when called from drm_crtc_handle_vblank().  Some drivers       need to apply some workarounds for gpu-specific vblank irq quirks       if flag is set.   @get_scanout_position:       Callback function to retrieve the scanout position. See       @struct drm_crtc_helper_funcs.get_scanout_position.     Implements calculation of exact vblank timestamps from given drm_display_mode   timings and current video scanout position of a CRTC.     The current implementation only handles standard video modes. For double scan   and interlaced modes the driver is supposed to adjust the hardware mode   (taken from &drm_crtc_state.adjusted mode for atomic modeset drivers) to   match the scanout position reported.     Note that atomic drivers must call drm_calc_timestamping_constants() before   enabling a CRTC. The atomic helpers already take care of that in   drm_atomic_helper_calc_timestamping_constants().     Returns:     Returns true on success, and false on failure, i.e. when no accurate   timestamp could be acquired. ", "u64 drm_crtc_vblank_count(struct drm_crtc *crtc)": "drm_crtc_vblank_count_and_time()   provide a barrier: Any writes done before calling   drm_crtc_handle_vblank() will be visible to callers of the later   functions, if the vblank count is the same or a later one.     See also &drm_vblank_crtc.count.     Returns:   The software vblank counter. ", "int drm_crtc_next_vblank_start(struct drm_crtc *crtc, ktime_t *vblanktime)": "drm_crtc_next_vblank_start - calculate the time of the next vblank   @crtc: the crtc for which to calculate next vblank time   @vblanktime: pointer to time to receive the next vblank timestamp.     Calculate the expected time of the start of the next vblank period,   based on time of previous vblank and frame duration ", "void drm_crtc_arm_vblank_event(struct drm_crtc *crtc,       struct drm_pending_vblank_event *e)": "drm_crtc_send_vblank_event() and make sure that there's no   possible race with the hardware committing the atomic update.     Caller must hold a vblank reference for the event @e acquired by a   drm_crtc_vblank_get(), which will be dropped when the next vblank arrives. ", "void drm_wait_one_vblank(struct drm_device *dev, unsigned int pipe)": "drm_crtc_wait_one_vblank(). ", "drm_update_vblank_count(dev, pipe, false);__disable_vblank(dev, pipe);vblank->enabled = false;out:spin_unlock_irqrestore(&dev->vblank_time_lock, irqflags);}static void vblank_disable_fn(struct timer_list *t)": "drm_crtc_vblank_off(). ", "void drm_crtc_vblank_reset(struct drm_crtc *crtc)": "drm_crtc_vblank_reset - reset vblank state to off on a CRTC   @crtc: CRTC in question     Drivers can use this function to reset the vblank state to off at load time.   Drivers should use this together with the drm_crtc_vblank_off() and   drm_crtc_vblank_on() functions. The difference compared to   drm_crtc_vblank_off() is that this function doesn't save the vblank counter   and hence doesn't need to call any driver hooks.     This is useful for recovering driver state e.g. on driver load, or on resume. ", "void drm_crtc_set_max_vblank_count(struct drm_crtc *crtc,   u32 max_vblank_count)": "drm_crtc_set_max_vblank_count - configure the hw max vblank counter value   @crtc: CRTC in question   @max_vblank_count: max hardware vblank counter value     Update the maximum hardware vblank counter value for @crtc   at runtime. Useful for hardware where the operation of the   hardware vblank counter depends on the currently active   display configuration.     For example, if the hardware vblank counter does not work   when a specific connector is active the maximum can be set   to zero. And when that specific connector isn't active the   maximum can again be set to the appropriate non-zero value.     If used, must be called before drm_vblank_on(). ", "static void drm_reset_vblank_timestamp(struct drm_device *dev, unsigned int pipe)": "drm_crtc_vblank_on().     Note: caller must hold &drm_device.vbl_lock since this reads & writes   device vblank fields. ", "void drm_crtc_vblank_restore(struct drm_crtc *crtc)": "drm_crtc_vblank_restore - estimate missed vblanks and update vblank count.   @crtc: CRTC in question     Power manamement features can cause frame counter resets between vblank   disable and enable. Drivers can use this function in their   &drm_crtc_funcs.enable_vblank implementation to estimate missed vblanks since   the last &drm_crtc_funcs.disable_vblank using timestamps and update the   vblank counter.     Note that drivers must have race-free high-precision timestamping support,   i.e.  &drm_crtc_funcs.get_vblank_timestamp must be hooked up and   &drm_driver.vblank_disable_immediate must be set to indicate the   time-stamping functions are race-free against vblank hardware counter   increments. ", "#define DRM_REDUNDANT_VBLIRQ_THRESH_NS 1000000static booldrm_get_last_vbltimestamp(struct drm_device *dev, unsigned int pipe,  ktime_t *tvblank, bool in_vblank_irq);static unsigned int drm_timestamp_precision = 20;  /* Default to 20 usecs. ": "drm_handle_vblank(). 1 msec should be ok. ", "rgb->encoder.possible_crtcs = drm_crtc_mask(&tcon->crtc->crtc);if (rgb->panel) ": "sun4i_rgb_init(struct drm_device  drm, struct sun4i_tcon  tcon){struct drm_encoder  encoder;struct sun4i_rgb  rgb;int ret;rgb = devm_kzalloc(drm->dev, sizeof( rgb), GFP_KERNEL);if (!rgb)return -ENOMEM;rgb->tcon = tcon;encoder = &rgb->encoder;ret = drm_of_find_panel_or_bridge(tcon->dev->of_node, 1, 0,  &rgb->panel, &rgb->bridge);if (ret) {dev_info(drm->dev, \"No panel or bridge found... RGB output disabled\\n\");return 0;}drm_encoder_helper_add(&rgb->encoder,       &sun4i_rgb_enc_helper_funcs);ret = drm_simple_encoder_init(drm, &rgb->encoder,      DRM_MODE_ENCODER_NONE);if (ret) {dev_err(drm->dev, \"Couldn't initialise the rgb encoder\\n\");goto err_out;}  The RGB encoder can only work with the TCON channel 0 ", "lvds->encoder.possible_crtcs = drm_crtc_mask(&tcon->crtc->crtc);if (lvds->panel) ": "sun4i_lvds_init(struct drm_device  drm, struct sun4i_tcon  tcon){struct drm_encoder  encoder;struct drm_bridge  bridge;struct sun4i_lvds  lvds;int ret;lvds = devm_kzalloc(drm->dev, sizeof( lvds), GFP_KERNEL);if (!lvds)return -ENOMEM;encoder = &lvds->encoder;ret = drm_of_find_panel_or_bridge(tcon->dev->of_node, 1, 0,  &lvds->panel, &bridge);if (ret) {dev_info(drm->dev, \"No panel or bridge found... LVDS output disabled\\n\");return 0;}drm_encoder_helper_add(&lvds->encoder,       &sun4i_lvds_enc_helper_funcs);ret = drm_simple_encoder_init(drm, &lvds->encoder,      DRM_MODE_ENCODER_LVDS);if (ret) {dev_err(drm->dev, \"Couldn't initialise the lvds encoder\\n\");goto err_out;}  The LVDS encoder can only work with the TCON channel 0 ", "offset = (width - 1) & (32 - 1);regmap_write(frontend->regs, SUN4I_FRONTEND_TB_OFF0_REG,     SUN4I_FRONTEND_TB_OFF_X1(offset));if (fb->format->num_planes > 1) ": "sun4i_frontend_update_buffer(struct sun4i_frontend  frontend,  struct drm_plane  plane){struct drm_plane_state  state = plane->state;struct drm_framebuffer  fb = state->fb;unsigned int strides[3] = {};dma_addr_t dma_addr;bool swap;if (fb->modifier == DRM_FORMAT_MOD_ALLWINNER_TILED) {unsigned int width = state->src_w >> 16;unsigned int offset;strides[0] = SUN4I_FRONTEND_LINESTRD_TILED(fb->pitches[0]);    The X1 offset is the offset to the bottom-right point in the   end tile, which is the final pixel (at offset width - 1)   within the end tile (with a 32-byte mask). ", "ch1_phase_idx = (format->num_planes > 1) ? 1 : 0;regmap_write(frontend->regs, SUN4I_FRONTEND_CH0_HORZPHASE_REG,     frontend->data->ch_phase[0]);regmap_write(frontend->regs, SUN4I_FRONTEND_CH1_HORZPHASE_REG,     frontend->data->ch_phase[ch1_phase_idx]);regmap_write(frontend->regs, SUN4I_FRONTEND_CH0_VERTPHASE0_REG,     frontend->data->ch_phase[0]);regmap_write(frontend->regs, SUN4I_FRONTEND_CH1_VERTPHASE0_REG,     frontend->data->ch_phase[ch1_phase_idx]);regmap_write(frontend->regs, SUN4I_FRONTEND_CH0_VERTPHASE1_REG,     frontend->data->ch_phase[0]);regmap_write(frontend->regs, SUN4I_FRONTEND_CH1_VERTPHASE1_REG,     frontend->data->ch_phase[ch1_phase_idx]);/* * Checking the input format is sufficient since we currently only * support RGB output formats to the backend. If YUV output formats * ever get supported, an YUV input and output would require bypassing * the CSC engine too. ": "sun4i_frontend_update_formats(struct sun4i_frontend  frontend,  struct drm_plane  plane, uint32_t out_fmt){struct drm_plane_state  state = plane->state;struct drm_framebuffer  fb = state->fb;const struct drm_format_info  format = fb->format;uint64_t modifier = fb->modifier;unsigned int ch1_phase_idx;u32 out_fmt_val;u32 in_fmt_val, in_mod_val, in_ps_val;unsigned int i;u32 bypass;int ret;ret = sun4i_frontend_drm_format_to_input_fmt(format, &in_fmt_val);if (ret) {DRM_DEBUG_DRIVER(\"Invalid input format\\n\");return ret;}ret = sun4i_frontend_drm_format_to_input_mode(format, modifier,      &in_mod_val);if (ret) {DRM_DEBUG_DRIVER(\"Invalid input mode\\n\");return ret;}ret = sun4i_frontend_drm_format_to_input_sequence(format, &in_ps_val);if (ret) {DRM_DEBUG_DRIVER(\"Invalid pixel sequence\\n\");return ret;}ret = sun4i_frontend_drm_format_to_output_fmt(out_fmt, &out_fmt_val);if (ret) {DRM_DEBUG_DRIVER(\"Invalid output format\\n\");return ret;}    I have no idea what this does exactly, but it seems to be   related to the scaler FIR filter phase parameters. ", "DRM_DEBUG_DRIVER(\"Frontend size W: %u H: %u\\n\", state->crtc_w, state->crtc_h);luma_width = state->src_w >> 16;luma_height = state->src_h >> 16;chroma_width = DIV_ROUND_UP(luma_width, fb->format->hsub);chroma_height = DIV_ROUND_UP(luma_height, fb->format->vsub);regmap_write(frontend->regs, SUN4I_FRONTEND_CH0_INSIZE_REG,     SUN4I_FRONTEND_INSIZE(luma_height, luma_width));regmap_write(frontend->regs, SUN4I_FRONTEND_CH1_INSIZE_REG,     SUN4I_FRONTEND_INSIZE(chroma_height, chroma_width));regmap_write(frontend->regs, SUN4I_FRONTEND_CH0_OUTSIZE_REG,     SUN4I_FRONTEND_OUTSIZE(state->crtc_h, state->crtc_w));regmap_write(frontend->regs, SUN4I_FRONTEND_CH1_OUTSIZE_REG,     SUN4I_FRONTEND_OUTSIZE(state->crtc_h, state->crtc_w));regmap_write(frontend->regs, SUN4I_FRONTEND_CH0_HORZFACT_REG,     (luma_width << 16) / state->crtc_w);regmap_write(frontend->regs, SUN4I_FRONTEND_CH1_HORZFACT_REG,     (chroma_width << 16) / state->crtc_w);regmap_write(frontend->regs, SUN4I_FRONTEND_CH0_VERTFACT_REG,     (luma_height << 16) / state->crtc_h);regmap_write(frontend->regs, SUN4I_FRONTEND_CH1_VERTFACT_REG,     (chroma_height << 16) / state->crtc_h);regmap_write_bits(frontend->regs, SUN4I_FRONTEND_FRM_CTRL_REG,  SUN4I_FRONTEND_FRM_CTRL_REG_RDY,  SUN4I_FRONTEND_FRM_CTRL_REG_RDY);}EXPORT_SYMBOL(sun4i_frontend_update_coord": "sun4i_frontend_update_coord(struct sun4i_frontend  frontend, struct drm_plane  plane){struct drm_plane_state  state = plane->state;struct drm_framebuffer  fb = state->fb;uint32_t luma_width, luma_height;uint32_t chroma_width, chroma_height;  Set height and width ", "sun4i_tcon0_mode_set_cpu(tcon, encoder, mode);break;case DRM_MODE_ENCODER_LVDS:sun4i_tcon0_mode_set_lvds(tcon, encoder, mode);break;case DRM_MODE_ENCODER_NONE:sun4i_tcon0_mode_set_rgb(tcon, encoder, mode);sun4i_tcon_set_mux(tcon, 0, encoder);break;case DRM_MODE_ENCODER_TVDAC:case DRM_MODE_ENCODER_TMDS:sun4i_tcon1_mode_set(tcon, mode);sun4i_tcon_set_mux(tcon, 1, encoder);break;default:DRM_DEBUG_DRIVER(\"Unknown encoder type, doing nothing...\\n\");}}EXPORT_SYMBOL(sun4i_tcon_mode_set": "sun4i_tcon_mode_set(struct sun4i_tcon  tcon, const struct drm_encoder  encoder, const struct drm_display_mode  mode){switch (encoder->encoder_type) {case DRM_MODE_ENCODER_DSI:  DSI is tied to special case of CPU interface ", "writel(0, regs + TCON_TOP_PORT_SEL_REG);writel(0, regs + TCON_TOP_GATE_SRC_REG);/* * TCON TOP has two muxes, which select parent clock for each TCON TV * channel clock. Parent could be either TCON TV or TVE clock. For now * we leave this fixed to TCON TV, since TVE driver for R40 is not yet * implemented. Once it is, graph needs to be traversed to determine * if TVE is active on each TCON TV. If it is, mux should be switched * to TVE clock parent. ": "sun8i_tcon_top_of_table, node);}int sun8i_tcon_top_set_hdmi_src(struct device  dev, int tcon){struct sun8i_tcon_top  tcon_top = dev_get_drvdata(dev);unsigned long flags;u32 val;if (!sun8i_tcon_top_node_is_tcon_top(dev->of_node)) {dev_err(dev, \"Device is not TCON TOP!\\n\");return -EINVAL;}if (tcon < 2 || tcon > 3) {dev_err(dev, \"TCON index must be 2 or 3!\\n\");return -EINVAL;}spin_lock_irqsave(&tcon_top->reg_lock, flags);val = readl(tcon_top->regs + TCON_TOP_GATE_SRC_REG);val &= ~TCON_TOP_HDMI_SRC_MSK;val |= FIELD_PREP(TCON_TOP_HDMI_SRC_MSK, tcon - 1);writel(val, tcon_top->regs + TCON_TOP_GATE_SRC_REG);spin_unlock_irqrestore(&tcon_top->reg_lock, flags);return 0;}EXPORT_SYMBOL(sun8i_tcon_top_set_hdmi_src);int sun8i_tcon_top_de_config(struct device  dev, int mixer, int tcon){struct sun8i_tcon_top  tcon_top = dev_get_drvdata(dev);unsigned long flags;u32 reg;if (!sun8i_tcon_top_node_is_tcon_top(dev->of_node)) {dev_err(dev, \"Device is not TCON TOP!\\n\");return -EINVAL;}if (mixer > 1) {dev_err(dev, \"Mixer index is too high!\\n\");return -EINVAL;}if (tcon > 3) {dev_err(dev, \"TCON index is too high!\\n\");return -EINVAL;}spin_lock_irqsave(&tcon_top->reg_lock, flags);reg = readl(tcon_top->regs + TCON_TOP_PORT_SEL_REG);if (mixer == 0) {reg &= ~TCON_TOP_PORT_DE0_MSK;reg |= FIELD_PREP(TCON_TOP_PORT_DE0_MSK, tcon);} else {reg &= ~TCON_TOP_PORT_DE1_MSK;reg |= FIELD_PREP(TCON_TOP_PORT_DE1_MSK, tcon);}writel(reg, tcon_top->regs + TCON_TOP_PORT_SEL_REG);spin_unlock_irqrestore(&tcon_top->reg_lock, flags);return 0;}EXPORT_SYMBOL(sun8i_tcon_top_de_config);static struct clk_hw  sun8i_tcon_top_register_gate(struct device  dev,   const char  parent,   void __iomem  regs,   spinlock_t  lock,   u8 bit, int name_index){const char  clk_name,  parent_name;int ret, index;index = of_property_match_string(dev->of_node, \"clock-names\", parent);if (index < 0)return ERR_PTR(index);parent_name = of_clk_get_parent_name(dev->of_node, index);ret = of_property_read_string_index(dev->of_node,    \"clock-output-names\", name_index,    &clk_name);if (ret)return ERR_PTR(ret);return clk_hw_register_gate(dev, clk_name, parent_name,    CLK_SET_RATE_PARENT,    regs + TCON_TOP_GATE_SRC_REG,    bit, 0, lock);};static int sun8i_tcon_top_bind(struct device  dev, struct device  master,       void  data){struct platform_device  pdev = to_platform_device(dev);struct clk_hw_onecell_data  clk_data;struct sun8i_tcon_top  tcon_top;const struct sun8i_tcon_top_quirks  quirks;void __iomem  regs;int ret, i;quirks = of_device_get_match_data(&pdev->dev);tcon_top = devm_kzalloc(dev, sizeof( tcon_top), GFP_KERNEL);if (!tcon_top)return -ENOMEM;clk_data = devm_kzalloc(dev, struct_size(clk_data, hws, CLK_NUM),GFP_KERNEL);if (!clk_data)return -ENOMEM;tcon_top->clk_data = clk_data;spin_lock_init(&tcon_top->reg_lock);tcon_top->rst = devm_reset_control_get(dev, NULL);if (IS_ERR(tcon_top->rst)) {dev_err(dev, \"Couldn't get our reset line\\n\");return PTR_ERR(tcon_top->rst);}tcon_top->bus = devm_clk_get(dev, \"bus\");if (IS_ERR(tcon_top->bus)) {dev_err(dev, \"Couldn't get the bus clock\\n\");return PTR_ERR(tcon_top->bus);}regs = devm_platform_ioremap_resource(pdev, 0);tcon_top->regs = regs;if (IS_ERR(regs))return PTR_ERR(regs);ret = reset_control_deassert(tcon_top->rst);if (ret) {dev_err(dev, \"Could not deassert ctrl reset control\\n\");return ret;}ret = clk_prepare_enable(tcon_top->bus);if (ret) {dev_err(dev, \"Could not enable bus clock\\n\");goto err_assert_reset;}    At least on H6, some registers have some bits set by default   which may cause issues. Clear them here. ", "struct ttm_kmap_iter *ttm_kmap_iter_tt_init(struct ttm_kmap_iter_tt *iter_tt,      struct ttm_tt *tt)": "ttm_kmap_iter_tt_init - Initialize a struct ttm_kmap_iter_tt   @iter_tt: The struct ttm_kmap_iter_tt to initialize.   @tt: Struct ttm_tt holding page pointers of the struct ttm_resource.     Return: Pointer to the embedded struct ttm_kmap_iter. ", "vm_fault_t ttm_bo_vm_reserve(struct ttm_buffer_object *bo,     struct vm_fault *vmf)": "ttm_bo_vm_reserve - Reserve a buffer object in a retryable vm callback   @bo: The buffer object   @vmf: The fault structure handed to the callback     vm callbacks like fault() and  _mkwrite() allow for the mmap_lock to be dropped   during long waits, and after the wait the callback will be restarted. This   is to allow other threads using the same virtual memory space concurrent   access to map(), unmap() completely unrelated buffer objects. TTM buffer   object reservations sometimes wait for GPU and should therefore be   considered long waits. This function reserves the buffer object interruptibly   taking this into account. Starvation is avoided by the vm system not   allowing too many repeated restarts.   This function is intended to be used in customized fault() and _mkwrite()   handlers.     Return:      0 on success and the bo was reserved.      VM_FAULT_RETRY if blocking wait.      VM_FAULT_NOPAGE if blocking wait and retrying was not allowed. ", "vm_fault_t ttm_bo_vm_fault_reserved(struct vm_fault *vmf,    pgprot_t prot,    pgoff_t num_prefault)": "ttm_bo_vm_fault_reserved - TTM fault helper   @vmf: The struct vm_fault given as argument to the fault callback   @prot: The page protection to be used for this memory area.   @num_prefault: Maximum number of prefault pages. The caller may want to   specify this based on madvice settings and the size of the GPU object   backed by the memory.     This function inserts one or more page table entries pointing to the   memory backing the buffer object, and then returns a return code   instructing the caller to retry the page access.     Return:     VM_FAULT_NOPAGE on success or pending signal     VM_FAULT_SIGBUS on unspecified error     VM_FAULT_OOM on out-of-memory     VM_FAULT_RETRY if retryable wait ", "page = alloc_page(GFP_KERNEL | __GFP_ZERO);if (!page)return VM_FAULT_OOM;/* Set the page to be freed using drmm release action ": "ttm_bo_vm_dummy_page(struct vm_fault  vmf, pgprot_t prot){struct vm_area_struct  vma = vmf->vma;struct ttm_buffer_object  bo = vma->vm_private_data;struct drm_device  ddev = bo->base.dev;vm_fault_t ret = VM_FAULT_NOPAGE;unsigned long address;unsigned long pfn;struct page  page;  Allocate new dummy page to map all the VA range in this VMA to it", "if (dma_resv_test_signaled(bo->base.resv, DMA_RESV_USAGE_KERNEL))return 0;/* * If possible, avoid waiting for GPU with mmap_lock * held.  We only do this if the fault allows retry and this * is the first attempt. ": "ttm_bo_vm_fault_idle(struct ttm_buffer_object  bo,struct vm_fault  vmf){long err = 0;    Quick non-stalling check for idle. ", "offset -= page << PAGE_SHIFT;do ": "ttm_bo_vm_access_kmap(struct ttm_buffer_object  bo, unsigned long offset, uint8_t  buf, int len, int write){unsigned long page = offset >> PAGE_SHIFT;unsigned long bytes_left = len;int ret;  Copy a page at a time, that way no extra virtual address   mapping is needed ", "int ttm_bo_mmap_obj(struct vm_area_struct *vma, struct ttm_buffer_object *bo)": "ttm_bo_mmap_obj - mmap memory backed by a ttm buffer object.     @vma:       vma as input from the fbdev mmap method.   @bo:        The bo backing the address space.     Maps a buffer object. ", "ttm_eu_backoff_reservation_reverse(list, entry);if (ret == -EDEADLK) ": "ttm_eu_reserve_buffers(struct ww_acquire_ctx  ticket,   struct list_head  list, bool intr,   struct list_head  dups){struct ttm_validate_buffer  entry;int ret;if (list_empty(list))return 0;if (ticket)ww_acquire_init(ticket, &reservation_ww_class);list_for_each_entry(entry, list, head) {struct ttm_buffer_object  bo = entry->bo;unsigned int num_fences;ret = ttm_bo_reserve(bo, intr, (ticket == NULL), ticket);if (ret == -EALREADY && dups) {struct ttm_validate_buffer  safe = entry;entry = list_prev_entry(entry, head);list_del(&safe->head);list_add(&safe->head, dups);continue;}num_fences = max(entry->num_shared, 1u);if (!ret) {ret = dma_resv_reserve_fences(bo->base.resv,      num_fences);if (!ret)continue;}  uh oh, we lost out, drop every reservation and try   to only reserve this buffer, then start over if   this succeeds. ", "int ttm_range_man_init_nocheck(struct ttm_device *bdev,       unsigned type, bool use_tt,       unsigned long p_size)": "ttm_range_man_init_nocheck - Initialise a generic range manager for the   selected memory type.     @bdev: ttm device   @type: memory manager type   @use_tt: if the memory manager uses tt   @p_size: size of area to be managed in pages.     The range manager is installed for this device in the type slot.     Return: %0 on success or a negative error code on failure ", "int ttm_range_man_fini_nocheck(struct ttm_device *bdev,       unsigned type)": "ttm_range_man_fini_nocheck - Remove the generic range manager from a slot   and tear it down.     @bdev: ttm device   @type: memory manager type     Return: %0 on success or a negative error code on failure ", "static DEFINE_MUTEX(ttm_global_mutex);static unsigned ttm_glob_use_count;struct ttm_global ttm_glob;EXPORT_SYMBOL(ttm_glob": "ttm_global_mutex - protecting the global state ", "if (!ret)return num_pages;if (ret != -EBUSY)return ret;}}spin_unlock(&bdev->lru_lock);return 0;}EXPORT_SYMBOL(ttm_device_swapout": "ttm_device_swapout(bdev, ctx, gfp_flags);if (ret > 0) {list_move_tail(&bdev->device_list, &glob->device_list);break;}}mutex_unlock(&ttm_global_mutex);return ret;}int ttm_device_swapout(struct ttm_device  bdev, struct ttm_operation_ctx  ctx,       gfp_t gfp_flags){struct ttm_resource_cursor cursor;struct ttm_resource_manager  man;struct ttm_resource  res;unsigned i;int ret;spin_lock(&bdev->lru_lock);for (i = TTM_PL_SYSTEM; i < TTM_NUM_MEM_TYPES; ++i) {man = ttm_manager_type(bdev, i);if (!man || !man->use_tt)continue;ttm_resource_manager_for_each_res(man, &cursor, res) {struct ttm_buffer_object  bo = res->bo;uint32_t num_pages;if (!bo || bo->resource != res)continue;num_pages = PFN_UP(bo->base.size);ret = ttm_bo_swapout(bo, ctx, gfp_flags);  ttm_bo_swapout has dropped the lru_lock ", "int ttm_device_init(struct ttm_device *bdev, const struct ttm_device_funcs *funcs,    struct device *dev, struct address_space *mapping,    struct drm_vma_offset_manager *vma_manager,    bool use_dma_alloc, bool use_dma32)": "ttm_device_init     @bdev: A pointer to a struct ttm_device to initialize.   @funcs: Function table for the device.   @dev: The core kernel device pointer for DMA mappings and allocations.   @mapping: The address space to use for this bo.   @vma_manager: A pointer to a vma manager.   @use_dma_alloc: If coherent DMA allocation API should be used.   @use_dma32: If we should use GFP_DMA32 for device memory allocations.     Initializes a struct ttm_device:   Returns:   !0: Failure. ", "if (order)gfp_flags |= __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN |__GFP_KSWAPD_RECLAIM;if (!pool->use_dma_alloc) ": "ttm_pool_alloc_page(struct ttm_pool  pool, gfp_t gfp_flags,unsigned int order){unsigned long attr = DMA_ATTR_FORCE_CONTIGUOUS;struct ttm_pool_dma  dma;struct page  p;void  vaddr;  Don't set the __GFP_COMP flag for higher order allocations.   Mapping pages directly into an userspace process and calling   put_page() on a TTM allocated page is illegal. ", "if (caching != ttm_cached && !PageHighMem(p))set_pages_wb(p, 1 << order);#endifif (!pool || !pool->use_dma_alloc) ": "ttm_pool_free_page(struct ttm_pool  pool, enum ttm_caching caching,       unsigned int order, struct page  p){unsigned long attr = DMA_ATTR_FORCE_CONTIGUOUS;struct ttm_pool_dma  dma;void  vaddr;#ifdef CONFIG_X86  We don't care that set_pages_wb is inefficient here. This is only   used when we have to shrink and CPU overhead is irrelevant then. ", "void ttm_pool_init(struct ttm_pool *pool, struct device *dev,   int nid, bool use_dma_alloc, bool use_dma32)": "ttm_pool_init - Initialize a pool     @pool: the pool to initialize   @dev: device for DMA allocations and mappings   @nid: NUMA node to use for allocations   @use_dma_alloc: true if coherent DMA alloc should be used   @use_dma32: true if GFP_DMA32 should be used     Initialize the pool and its pool types. ", "void ttm_pool_fini(struct ttm_pool *pool)": "ttm_pool_fini - Cleanup a pool     @pool: the pool to clean up     Free all pages in the pool and unregister the types from the global   shrinker. ", "static void ttm_pool_debugfs_orders(struct ttm_pool_type *pt,    struct seq_file *m)": "ttm_pool_debugfs_header(struct seq_file  m){unsigned int i;seq_puts(m, \"\\t \");for (i = 0; i <= MAX_ORDER; ++i)seq_printf(m, \" ---%2u---\", i);seq_puts(m, \"\\n\");}  Dump information about the different pool types ", "void ttm_move_memcpy(bool clear,     u32 num_pages,     struct ttm_kmap_iter *dst_iter,     struct ttm_kmap_iter *src_iter)": "ttm_move_memcpy - Helper to perform a memcpy ttm move operation.   @clear: Whether to clear rather than copy.   @num_pages: Number of pages of the operation.   @dst_iter: A struct ttm_kmap_iter representing the destination resource.   @src_iter: A struct ttm_kmap_iter representing the source resource.     This function is intended to be able to move out async under a   dma-fence if desired. ", "int ttm_bo_move_memcpy(struct ttm_buffer_object *bo,       struct ttm_operation_ctx *ctx,       struct ttm_resource *dst_mem)": "ttm_bo_move_memcpy     @bo: A pointer to a struct ttm_buffer_object.   @ctx: operation context   @dst_mem: struct ttm_resource indicating where to move.     Fallback move function for a mappable buffer object in mappable memory.   The function will, if successful,   free any old aperture space, and set (@new_mem)->mm_node to NULL,   and update the (@bo)->mem placement flags. If unsuccessful, the old   data remains untouched, and it's up to the caller to free the   memory space indicated by @new_mem.   Returns:   !0: Failure. ", "pgprot_t ttm_io_prot(struct ttm_buffer_object *bo, struct ttm_resource *res,     pgprot_t tmp)": "ttm_io_prot     @bo: ttm buffer object   @res: ttm resource object   @tmp: Page protection flag for a normal, cached mapping.     Utility function that returns the pgprot_t that should be used for   setting up a PTE with the caching model indicated by @c_state. ", "map->bo_kmap_type = ttm_bo_map_kmap;map->page = ttm->pages[start_page];map->virtual = kmap(map->page);} else ": "ttm_bo_kmap_obj  map){struct ttm_resource  mem = bo->resource;if (bo->resource->bus.addr) {map->bo_kmap_type = ttm_bo_map_premapped;map->virtual = ((u8  )bo->resource->bus.addr) + offset;} else {resource_size_t res = bo->resource->bus.offset + offset;map->bo_kmap_type = ttm_bo_map_iomap;if (mem->bus.caching == ttm_write_combined)map->virtual = ioremap_wc(res, size);#ifdef CONFIG_X86else if (mem->bus.caching == ttm_cached)map->virtual = ioremap_cache(res, size);#endifelsemap->virtual = ioremap(res, size);}return (!map->virtual) ? -ENOMEM : 0;}static int ttm_bo_kmap_ttm(struct ttm_buffer_object  bo,   unsigned long start_page,   unsigned long num_pages,   struct ttm_bo_kmap_obj  map){struct ttm_resource  mem = bo->resource;struct ttm_operation_ctx ctx = {.interruptible = false,.no_wait_gpu = false};struct ttm_tt  ttm = bo->ttm;pgprot_t prot;int ret;BUG_ON(!ttm);ret = ttm_tt_populate(bo->bdev, ttm, &ctx);if (ret)return ret;if (num_pages == 1 && ttm->caching == ttm_cached) {    We're mapping a single page, and the desired   page protection is consistent with the bo. ", "void ttm_bo_kunmap(struct ttm_bo_kmap_obj *map)": "ttm_bo_kunmap     @map: Object describing the map to unmap.     Unmaps a kernel map set up by ttm_bo_kmap. ", "int ttm_bo_vmap(struct ttm_buffer_object *bo, struct iosys_map *map)": "ttm_bo_vunmap().     Returns   -ENOMEM: Out of memory.   -EINVAL: Invalid range. ", "int ttm_bo_move_accel_cleanup(struct ttm_buffer_object *bo,      struct dma_fence *fence,      bool evict,      bool pipeline,      struct ttm_resource *new_mem)": "ttm_bo_move_accel_cleanup - cleanup helper for hw copies     @bo: A pointer to a struct ttm_buffer_object.   @fence: A fence object that signals when moving is complete.   @evict: This is an evict move. Don't return until the buffer is idle.   @pipeline: evictions are to be pipelined.   @new_mem: struct ttm_resource indicating where to move.     Accelerated move function to be called when an accelerated move   has been scheduled. The function will create a new temporary buffer object   representing the old placement, and put the sync object on both buffer   objects. After that the newly created buffer object is unref'd to be   destroyed when the move is complete. This will help pipeline   buffer moves. ", "static int ttm_buffer_object_transfer(struct ttm_buffer_object *bo,      struct ttm_buffer_object **new_obj)": "ttm_bo_move_sync_cleanup(bo, dst_mem);out_src_iter:if (!dst_iter->ops->maps_tt)ttm_kmap_iter_linear_io_fini(&_dst_iter.io, bdev, dst_mem);return ret;}EXPORT_SYMBOL(ttm_bo_move_memcpy);static void ttm_transfered_destroy(struct ttm_buffer_object  bo){struct ttm_transfer_obj  fbo;fbo = container_of(bo, struct ttm_transfer_obj, base);dma_resv_fini(&fbo->base.base._resv);ttm_bo_put(fbo->bo);kfree(fbo);}     ttm_buffer_object_transfer     @bo: A pointer to a struct ttm_buffer_object.   @new_obj: A pointer to a pointer to a newly created ttm_buffer_object,   holding the data of @bo with the old placement.     This is a utility function that may be called after an accelerated move   has been scheduled. A new buffer object is created as a placeholder for   the old data while it's being copied. When that buffer object is idle,   it can be destroyed, releasing the space of the old placement.   Returns:   !0: Failure. ", "void ttm_lru_bulk_move_init(struct ttm_lru_bulk_move *bulk)": "ttm_lru_bulk_move_init - initialize a bulk move structure   @bulk: the structure to init     For now just memset the structure to zero. ", "void ttm_lru_bulk_move_tail(struct ttm_lru_bulk_move *bulk)": "ttm_lru_bulk_move_tail - bulk move range of resources to the LRU tail.     @bulk: bulk move structure     Bulk move BOs to the LRU tail, only valid to use when driver makes sure that   resource order never changes. Should be called with &ttm_device.lru_lock held. ", "void ttm_resource_init(struct ttm_buffer_object *bo,                       const struct ttm_place *place,                       struct ttm_resource *res)": "ttm_resource_fini(). ", "void ttm_resource_manager_init(struct ttm_resource_manager *man,       struct ttm_device *bdev,       uint64_t size)": "ttm_resource_manager_init     @man: memory manager object to init   @bdev: ttm device this manager belongs to   @size: size of managed resources in arbitrary units     Initialise core parts of a manager object. ", "int ttm_resource_manager_evict_all(struct ttm_device *bdev,   struct ttm_resource_manager *man)": "ttm_resource_manager_evict_all     @bdev - device to use   @man - manager to use     Evict all the objects out of a memory manager until it is empty.   Part of memory manager cleanup sequence. ", "uint64_t ttm_resource_manager_usage(struct ttm_resource_manager *man)": "ttm_resource_manager_usage     @man: A memory manager object.     Return how many resources are currently used. ", "void ttm_resource_manager_debug(struct ttm_resource_manager *man,struct drm_printer *p)": "ttm_resource_manager_debug     @man: manager type to dump.   @p: printer to use for debug. ", "struct ttm_kmap_iter *ttm_kmap_iter_iomap_init(struct ttm_kmap_iter_iomap *iter_io, struct io_mapping *iomap, struct sg_table *st, resource_size_t start)": "ttm_kmap_iter_iomap_init - Initialize a struct ttm_kmap_iter_iomap   @iter_io: The struct ttm_kmap_iter_iomap to initialize.   @iomap: The struct io_mapping representing the underlying linear io_memory.   @st: sg_table into @iomap, representing the memory of the struct   ttm_resource.   @start: Offset that needs to be subtracted from @st to make   sg_dma_address(st->sgl) - @start == 0 for @iomap start.     Return: Pointer to the embedded struct ttm_kmap_iter. ", "void ttm_resource_manager_create_debugfs(struct ttm_resource_manager *man, struct dentry * parent, const char *name)": "ttm_resource_manager_create_debugfs - Create debugfs entry for specified   resource manager.   @man: The TTM resource manager for which the debugfs stats file be creates   @parent: debugfs directory in which the file will reside   @name: The filename to create.     This function setups up a debugfs file that can be used to look   at debug statistics of the specified ttm_resource_manager. ", "void ttm_bo_move_to_lru_tail(struct ttm_buffer_object *bo)": "ttm_bo_mem_space_debug(struct ttm_buffer_object  bo,struct ttm_placement  placement){struct drm_printer p = drm_debug_printer(TTM_PFX);struct ttm_resource_manager  man;int i, mem_type;for (i = 0; i < placement->num_placement; i++) {mem_type = placement->placement[i].mem_type;drm_printf(&p, \"  placement[%d]=0x%08X (%d)\\n\",   i, placement->placement[i].flags, mem_type);man = ttm_manager_type(bo->bdev, mem_type);ttm_resource_manager_debug(man, &p);}}     ttm_bo_move_to_lru_tail     @bo: The buffer object.     Move this BO to the tail of all lru lists used to lookup and reserve an   object. This function must be called with struct ttm_global::lru_lock   held, and is used to make a BO less likely to be considered for eviction. ", "void ttm_bo_set_bulk_move(struct ttm_buffer_object *bo,  struct ttm_lru_bulk_move *bulk)": "ttm_bo_set_bulk_move - update BOs bulk move object     @bo: The buffer object.   @bulk: bulk move structure     Update the BOs bulk move object, making sure that resources are addedremoved   as well. A bulk move allows to move many resource on the LRU at once,   resulting in much less overhead of maintaining the LRU.   The only requirement is that the resources stay together on the LRU and are   never separated. This is enforces by setting the bulk_move structure on a BO.   ttm_lru_bulk_move_tail() should be used to move all resources to the tail of   their LRU list. ", "dma_resv_wait_timeout(bo->base.resv,      DMA_RESV_USAGE_BOOKKEEP, false,      30 * HZ);}if (bo->bdev->funcs->release_notify)bo->bdev->funcs->release_notify(bo);drm_vma_offset_remove(bdev->vma_manager, &bo->base.vma_node);ttm_mem_io_free(bdev, bo->resource);if (!dma_resv_test_signaled(bo->base.resv,    DMA_RESV_USAGE_BOOKKEEP) ||    !dma_resv_trylock(bo->base.resv)) ": "ttm_bo_put(bo);}static void ttm_bo_release(struct kref  kref){struct ttm_buffer_object  bo =    container_of(kref, struct ttm_buffer_object, kref);struct ttm_device  bdev = bo->bdev;int ret;WARN_ON_ONCE(bo->pin_count);WARN_ON_ONCE(bo->bulk_move);if (!bo->deleted) {ret = ttm_bo_individualize_resv(bo);if (ret) {  Last resort, if we fail to allocate memory for the   fences block for the BO to become idle ", "bool ttm_bo_eviction_valuable(struct ttm_buffer_object *bo,      const struct ttm_place *place)": "ttm_bo_eviction_valuable     @bo: The buffer object to evict   @place: the placement we need to make room for     Check if it is valuable to evict the BO to make room for the given placement. ", "void ttm_bo_pin(struct ttm_buffer_object *bo)": "ttm_bo_unpin(). ", "int ttm_bo_validate(struct ttm_buffer_object *bo,    struct ttm_placement *placement,    struct ttm_operation_ctx *ctx)": "ttm_bo_validate     @bo: The buffer object.   @placement: Proposed placement for the buffer object.   @ctx: validation parameters.     Changes placement and caching policy of the buffer object   according proposed placement.   Returns   -EINVAL on invalid proposed placement.   -ENOMEM on out-of-memory condition.   -EBUSY if no_wait is true and buffer busy.   -ERESTARTSYS if interrupted by a signal. ", "int ttm_bo_init_reserved(struct ttm_device *bdev, struct ttm_buffer_object *bo, enum ttm_bo_type type, struct ttm_placement *placement, uint32_t alignment, struct ttm_operation_ctx *ctx, struct sg_table *sg, struct dma_resv *resv, void (*destroy) (struct ttm_buffer_object *))": "ttm_bo_init_reserved     @bdev: Pointer to a ttm_device struct.   @bo: Pointer to a ttm_buffer_object to be initialized.   @type: Requested type of buffer object.   @placement: Initial placement for buffer object.   @alignment: Data alignment in pages.   @ctx: TTM operation context for memory allocation.   @sg: Scatter-gather table.   @resv: Pointer to a dma_resv, or NULL to let ttm allocate one.   @destroy: Destroy function. Use NULL for kfree().     This function initializes a pre-allocated struct ttm_buffer_object.   As this object may be part of a larger structure, this function,   together with the @destroy function, enables driver-specific objects   derived from a ttm_buffer_object.     On successful return, the caller owns an object kref to @bo. The kref and   list_kref are usually set to 1, but note that in some situations, other   tasks may already be holding references to @bo as well.   Furthermore, if resv == NULL, the buffer's reservation lock will be held,   and it is the caller's responsibility to call ttm_bo_unreserve.     If a failure occurs, the function will call the @destroy function. Thus,   after a failure, dereferencing @bo is illegal and will likely cause memory   corruption.     Returns   -ENOMEM: Out of memory.   -EINVAL: Invalid placement flags.   -ERESTARTSYS: Interrupted by signal while sleeping waiting for resources. ", "int ttm_bo_init_validate(struct ttm_device *bdev, struct ttm_buffer_object *bo, enum ttm_bo_type type, struct ttm_placement *placement, uint32_t alignment, bool interruptible, struct sg_table *sg, struct dma_resv *resv, void (*destroy) (struct ttm_buffer_object *))": "ttm_bo_init_validate     @bdev: Pointer to a ttm_device struct.   @bo: Pointer to a ttm_buffer_object to be initialized.   @type: Requested type of buffer object.   @placement: Initial placement for buffer object.   @alignment: Data alignment in pages.   @interruptible: If needing to sleep to wait for GPU resources,   sleep interruptible.   pinned in physical memory. If this behaviour is not desired, this member   holds a pointer to a persistent shmem object. Typically, this would   point to the shmem object backing a GEM object if TTM is used to back a   GEM user interface.   @sg: Scatter-gather table.   @resv: Pointer to a dma_resv, or NULL to let ttm allocate one.   @destroy: Destroy function. Use NULL for kfree().     This function initializes a pre-allocated struct ttm_buffer_object.   As this object may be part of a larger structure, this function,   together with the @destroy function,   enables driver-specific objects derived from a ttm_buffer_object.     On successful return, the caller owns an object kref to @bo. The kref and   list_kref are usually set to 1, but note that in some situations, other   tasks may already be holding references to @bo as well.     If a failure occurs, the function will call the @destroy function, Thus,   after a failure, dereferencing @bo is illegal and will likely cause memory   corruption.     Returns   -ENOMEM: Out of memory.   -EINVAL: Invalid placement flags.   -ERESTARTSYS: Interrupted by signal while sleeping waiting for resources. ", "if (new_use_tt) ": "ttm_bo_unmap_virtual(bo);    Create and bind a ttm if required. ", "return ttm_bo_pipeline_gutting(bo);}ret = ttm_bo_mem_space(bo, &placement, &evict_mem, ctx);if (ret) ": "ttm_bo_wait_ctx(bo, ctx);if (ret)return ret;    Since we've already synced, this frees backing store   immediately. ", "void drm_sched_fault(struct drm_gpu_scheduler *sched)": "drm_sched_fault - immediately start timeout handler     @sched: scheduler where the timeout handling should be started.     Start timeout handling immediately when the driver detects a hardware fault. ", "unsigned long drm_sched_suspend_timeout(struct drm_gpu_scheduler *sched)": "drm_sched_suspend_timeout - Suspend scheduler job timeout     @sched: scheduler instance for which to suspend the timeout     Suspend the delayed work timeout for the scheduler. This is done by   modifying the delayed work timeout to an arbitrary large value,   MAX_SCHEDULE_TIMEOUT in this case.     Returns the timeout remaining   ", "void drm_sched_resume_timeout(struct drm_gpu_scheduler *sched,unsigned long remaining)": "drm_sched_resume_timeout - Resume scheduler job timeout     @sched: scheduler instance for which to resume the timeout   @remaining: remaining timeout     Resume the delayed work timeout for the scheduler. ", "if (sched->free_guilty) ": "drm_sched_stop doc. ", "static void drm_sched_start_timeout(struct drm_gpu_scheduler *sched)": "drm_sched_start_timeout - start timeout for reset worker     @sched: scheduler instance to start the worker for     Start the timeout for the given scheduler. ", "void drm_sched_resubmit_jobs(struct drm_gpu_scheduler *sched)": "drm_sched_resubmit_jobs - Deprecated, don't use in new code!     @sched: scheduler instance     Re-submitting jobs was a concept AMD came up as cheap way to implement   recovery after a job timeout.     This turned out to be not working very well. First of all there are many   problem with the dma_fence implementation and requirements. Either the   implementation is risking deadlocks with core memory management or violating   documented implementation details of the dma_fence object.     Drivers can still save and restore their state for recovery operations, but   we shouldn't make this a general scheduler feature around the dma_fence   interface. ", "int drm_sched_job_init(struct drm_sched_job *job,       struct drm_sched_entity *entity,       void *owner)": "drm_sched_job_cleanup() if this function returns   successfully, even when @job is aborted before drm_sched_job_arm() is called.     WARNING: amdgpu abuses &drm_sched.ready to signal when the hardware   has died, which can mean that there's no valid runqueue for a @entity.   This function returns -ENOENT in this case (which probably should be -EIO as   a more meanigful return value).     Returns 0 for success, negative error code otherwise. ", "int drm_sched_job_add_dependency(struct drm_sched_job *job, struct dma_fence *fence)": "drm_sched_job_add_dependency - adds the fence as a job dependency   @job: scheduler job to add the dependencies to   @fence: the dma_fence to add to the list of dependencies.     Note that @fence is consumed in both the success and error cases.     Returns:   0 on success, or an error on failing to expand the array. ", "int drm_sched_job_add_syncobj_dependency(struct drm_sched_job *job, struct drm_file *file, u32 handle, u32 point)": "drm_sched_job_add_syncobj_dependency - adds a syncobj's fence as a job dependency   @job: scheduler job to add the dependencies to   @file: drm file private pointer   @handle: syncobj handle to lookup   @point: timeline point     This adds the fence matching the given syncobj to @job.     Returns:   0 on success, or an error on failing to expand the array. ", "int drm_sched_job_add_resv_dependencies(struct drm_sched_job *job,struct dma_resv *resv,enum dma_resv_usage usage)": "drm_sched_job_add_resv_dependencies - add all fences from the resv to the job   @job: scheduler job to add the dependencies to   @resv: the dma_resv object to get the fences from   @usage: the dma_resv_usage to use to filter the fences     This adds all fences matching the given usage from @resv to @job.   Must be called with the @resv lock held.     Returns:   0 on success, or an error on failing to expand the array. ", "int drm_sched_job_add_implicit_dependencies(struct drm_sched_job *job,    struct drm_gem_object *obj,    bool write)": "drm_sched_job_add_implicit_dependencies - adds implicit dependencies as job     dependencies   @job: scheduler job to add the dependencies to   @obj: the gem object to add new dependencies from.   @write: whether the job might write the object (so we need to depend on   shared fences in the reservation object).     This should be called after drm_gem_lock_reservations() on your array of   GEM objects used in the job but before updating the reservations with your   own fences.     Returns:   0 on success, or an error on failing to expand the array. ", "struct drm_gpu_scheduler *drm_sched_pick_best(struct drm_gpu_scheduler **sched_list,     unsigned int num_sched_list)": "drm_sched_pick_best - Get a drm sched from a sched_list with the least load   @sched_list: list of drm_gpu_schedulers   @num_sched_list: number of drm_gpu_schedulers in the sched_list     Returns pointer of the sched with the least load or NULL if none of the   drm_gpu_schedulers are ready ", "int drm_sched_init(struct drm_gpu_scheduler *sched,   const struct drm_sched_backend_ops *ops,   unsigned hw_submission, unsigned hang_limit,   long timeout, struct workqueue_struct *timeout_wq,   atomic_t *score, const char *name, struct device *dev)": "drm_sched_init - Init a gpu scheduler instance     @sched: scheduler instance   @ops: backend operations for this scheduler   @hw_submission: number of hw submissions that can be in flight   @hang_limit: number of times to allow a job to hang before dropping it   @timeout: timeout value in jiffies for the scheduler   @timeout_wq: workqueue to use for timeout work. If NULL, the system_wq is  used   @score: optional score atomic shared with other schedulers   @name: name used for debugging   @dev: target &struct device     Return 0 on success, otherwise error code. ", "void drm_sched_fini(struct drm_gpu_scheduler *sched)": "drm_sched_fini - Destroy a gpu scheduler     @sched: scheduler instance     Tears down and cleans up the scheduler. ", "void drm_sched_increase_karma(struct drm_sched_job *bad)": "drm_sched_increase_karma - Update sched_entity guilty flag     @bad: The job guilty of time out     Increment on every hang caused by the 'bad' job. If this exceeds the hang   limit of the scheduler then the respective sched entity is marked guilty and   jobs from it will not be scheduled further ", "void drm_sched_fence_free(struct drm_sched_fence *fence)": "to_drm_sched_fence(f);return (const char  )fence->sched->name;}static void drm_sched_fence_free_rcu(struct rcu_head  rcu){struct dma_fence  f = container_of(rcu, struct dma_fence, rcu);struct drm_sched_fence  fence = to_drm_sched_fence(f);if (!WARN_ON_ONCE(!fence))kmem_cache_free(sched_fence_slab, fence);}     drm_sched_fence_free - free up an uninitialized fence     @fence: fence to free     Free up the fence memory. Should only be used if drm_sched_fence_init()   has not been called yet. ", "int drm_sched_entity_init(struct drm_sched_entity *entity,  enum drm_sched_priority priority,  struct drm_gpu_scheduler **sched_list,  unsigned int num_sched_list,  atomic_t *guilty)": "drm_sched_entity_set_priority(). For changing the set of schedulers   @sched_list at runtime see drm_sched_entity_modify_sched().     An entity is cleaned up by callind drm_sched_entity_fini(). See also   drm_sched_entity_destroy().     Returns 0 on success or a negative error code on failure. ", "int drm_sched_entity_error(struct drm_sched_entity *entity)": "drm_sched_entity_error - return error of last scheduled job   @entity: scheduler entity to check     Opportunistically return the error of the last scheduled job. Result can   change any time when new jobs are pushed to the hw. ", "long drm_sched_entity_flush(struct drm_sched_entity *entity, long timeout)": "drm_sched_entity_flush - Flush a context entity     @entity: scheduler entity   @timeout: time to wait in for Q to become empty in jiffies.     Splitting drm_sched_entity_fini() into two functions, The first one does the   waiting, removes the entity from the runqueue and returns an error when the   process was killed.     Returns the remaining time in jiffies left from the input timeout ", "void drm_sched_entity_modify_sched(struct drm_sched_entity *entity,    struct drm_gpu_scheduler **sched_list,    unsigned int num_sched_list)": "drm_sched_entity_push_job(), or the driver needs to   guarantee through some other means that this is never called while new jobs   can be pushed to @entity. ", "bool drm_bridge_is_panel(const struct drm_bridge *bridge)": "drm_bridge_is_panel - Checks if a drm_bridge is a panel_bridge.     @bridge: The drm_bridge to be checked.     Returns true if the bridge is a panel bridge, or false otherwise. ", "struct drm_bridge *drm_panel_bridge_add(struct drm_panel *panel)": "devm_drm_panel_bridge_add() for an automatically managed version of this   function. ", "struct drm_bridge *drm_panel_bridge_add_typed(struct drm_panel *panel,      u32 connector_type)": "drm_panel_bridge_add_typed(panel, panel->connector_type);}EXPORT_SYMBOL(drm_panel_bridge_add);     drm_panel_bridge_add_typed - Creates a &drm_bridge and &drm_connector with   an explicit connector type.   @panel: The drm_panel being wrapped.  Must be non-NULL.   @connector_type: The connector type (DRM_MODE_CONNECTOR_ )     This is just like drm_panel_bridge_add(), but forces the connector type to   @connector_type instead of infering it from the panel.     This function is deprecated and should not be used in new drivers. Use   drm_panel_bridge_add() instead, and fix panel drivers as necessary if they   don't report a connector type. ", "if (connector->dev)drm_connector_cleanup(connector);}static void panel_bridge_atomic_pre_enable(struct drm_bridge *bridge,struct drm_bridge_state *old_bridge_state)": "drm_panel_bridge_set_orientation(connector, bridge);drm_connector_attach_encoder(&panel_bridge->connector,  bridge->encoder);if (bridge->dev->registered) {if (connector->funcs->reset)connector->funcs->reset(connector);drm_connector_register(connector);}return 0;}static void panel_bridge_detach(struct drm_bridge  bridge){struct panel_bridge  panel_bridge = drm_bridge_to_panel_bridge(bridge);struct drm_connector  connector = &panel_bridge->connector;    Cleanup the connector if we know it was initialized.     FIXME: This wouldn't be needed if the panel_bridge structure was   allocated with drmm_kzalloc(). This might be tricky since the   drm_device pointer can only be retrieved when the bridge is attached. ", "struct drm_bridge *devm_drm_panel_bridge_add_typed(struct device *dev,   struct drm_panel *panel,   u32 connector_type)": "devm_drm_panel_bridge_add_typed(dev, panel,       panel->connector_type);}EXPORT_SYMBOL(devm_drm_panel_bridge_add);     devm_drm_panel_bridge_add_typed - Creates a managed &drm_bridge and   &drm_connector with an explicit connector type.   @dev: device to tie the bridge lifetime to   @panel: The drm_panel being wrapped.  Must be non-NULL.   @connector_type: The connector type (DRM_MODE_CONNECTOR_ )     This is just like devm_drm_panel_bridge_add(), but forces the connector type   to @connector_type instead of infering it from the panel.     This function is deprecated and should not be used in new drivers. Use   devm_drm_panel_bridge_add() instead, and fix panel drivers as necessary if   they don't report a connector type. ", "struct drm_bridge *drmm_panel_bridge_add(struct drm_device *drm, struct drm_panel *panel)": "drmm_panel_bridge_add - Creates a DRM-managed &drm_bridge and                           &drm_connector that just calls the                           appropriate functions from &drm_panel.     @drm: DRM device to tie the bridge lifetime to   @panel: The drm_panel being wrapped.  Must be non-NULL.     This is the DRM-managed version of drm_panel_bridge_add() which   automatically calls drm_panel_bridge_remove() when @dev is cleaned   up. ", "struct drm_connector *drm_panel_bridge_connector(struct drm_bridge *bridge)": "drm_panel_bridge_connector - return the connector for the panel bridge   @bridge: The drm_bridge.     drm_panel_bridge creates the connector.   This function gives external access to the connector.     Returns: Pointer to drm_connector ", "struct drm_bridge *devm_drm_of_get_bridge(struct device *dev,  struct device_node *np,  u32 port, u32 endpoint)": "devm_drm_of_get_bridge - Return next bridge in the chain   @dev: device to tie the bridge lifetime to   @np: device tree node containing encoder output ports   @port: port in the device tree node   @endpoint: endpoint in the device tree node     Given a DT node's port and endpoint number, finds the connected node   and returns the associated bridge if any, or creates and returns a   drm panel bridge instance if a panel is connected.     Returns a pointer to the bridge if successful, or an error pointer   otherwise. ", "struct drm_bridge *drmm_of_get_bridge(struct drm_device *drm,      struct device_node *np,      u32 port, u32 endpoint)": "drmm_of_get_bridge - Return next bridge in the chain   @drm: device to tie the bridge lifetime to   @np: device tree node containing encoder output ports   @port: port in the device tree node   @endpoint: endpoint in the device tree node     Given a DT node's port and endpoint number, finds the connected node   and returns the associated bridge if any, or creates and returns a   drm panel bridge instance if a panel is connected.     Returns a drmm managed pointer to the bridge if successful, or an error   pointer otherwise. ", "int rockchip_drm_wait_vact_end(struct drm_crtc *crtc, unsigned int mstimeout)": "rockchip_drm_wait_vact_end   @crtc: CRTC to enable line flag   @mstimeout: millisecond for timeout     Wait for vact_end line flag irq or timeout.     Returns:   Zero on success, negative errno on failure. ", "int drm_connector_attach_content_protection_property(struct drm_connector *connector, bool hdcp_content_type)": "drm_hdcp_update_content_protection() to update   the content protection state of a connector.     Returns:   Zero on success, negative errno on failure. ", "void drm_dsc_dp_pps_header_init(struct dp_sdp_header *pps_header)": "drm_dsc_dp_pps_header_init() - Initializes the PPS Header   for DisplayPort as per the DP 1.4 spec.   @pps_header: Secondary data packet header for DSC Picture                Parameter Set as defined in &struct dp_sdp_header     DP 1.4 spec defines the secondary data packet for sending the   picture parameter infoframes from the source to the sink.   This function populates the SDP header defined in   &struct dp_sdp_header. ", "int drm_dsc_dp_rc_buffer_size(u8 rc_buffer_block_size, u8 rc_buffer_size)": "drm_dsc_dp_rc_buffer_size - get rc buffer size in bytes   @rc_buffer_block_size: block size code, according to DPCD offset 62h   @rc_buffer_size: number of blocks - 1, according to DPCD offset 63h     return:   buffer size in bytes, or 0 on invalid input ", "void drm_dsc_pps_payload_pack(struct drm_dsc_picture_parameter_set *pps_payload,const struct drm_dsc_config *dsc_cfg)": "drm_dsc_pps_payload_pack() - Populates the DSC PPS     @pps_payload:   Bitwise struct for DSC Picture Parameter Set. This is defined   by &struct drm_dsc_picture_parameter_set   @dsc_cfg:   DSC Configuration data filled by driver as defined by   &struct drm_dsc_config     DSC source device sends a picture parameter set (PPS) containing the   information required by the sink to decode the compressed frame. Driver   populates the DSC PPS struct using the DSC configuration parameters in   the order expected by the DSC Display Sink device. For the DSC, the sink   device expects the PPS payload in big endian format for fields   that span more than 1 byte. ", "void drm_dsc_set_const_params(struct drm_dsc_config *vdsc_cfg)": "drm_dsc_set_const_params() - Set DSC parameters considered typically   constant across operation modes     @vdsc_cfg:   DSC Configuration data partially filled by driver ", "void drm_dsc_set_rc_buf_thresh(struct drm_dsc_config *vdsc_cfg)": "drm_dsc_set_rc_buf_thresh() - Set thresholds for the RC model   in accordance with the DSC 1.2 specification.     @vdsc_cfg: DSC Configuration data partially filled by driver ", "int drm_dsc_setup_rc_params(struct drm_dsc_config *vdsc_cfg, enum drm_dsc_params_type type)": "drm_dsc_setup_rc_params() - Set parameters and limits for RC model in   accordance with the DSC 1.1 or 1.2 specification and DSC C Model   Required bits_per_pixel and bits_per_component to be set before calling this   function.     @vdsc_cfg: DSC Configuration data partially filled by driver   @type: operating mode and standard to follow     Return: 0 or -error code in case of an error ", "int drm_dsc_compute_rc_parameters(struct drm_dsc_config *vdsc_cfg)": "drm_dsc_compute_rc_parameters() - Write rate control   parameters to the dsc configuration defined in   &struct drm_dsc_config in accordance with the DSC 1.2   specification. Some configuration fields must be present   beforehand.     @vdsc_cfg:   DSC Configuration data partially filled by driver ", "u32 drm_dsc_get_bpp_int(const struct drm_dsc_config *vdsc_cfg)": "drm_dsc_get_bpp_int() - Get integer bits per pixel value for the given DRM DSC config   @vdsc_cfg: Pointer to DRM DSC config struct     Return: Integer BPP value ", "u8 drm_dsc_initial_scale_value(const struct drm_dsc_config *dsc)": "drm_dsc_initial_scale_value() - Calculate the initial scale value for the given DSC config   @dsc: Pointer to DRM DSC config struct     Return: Calculated initial scale value ", "u32 drm_dsc_flatness_det_thresh(const struct drm_dsc_config *dsc)": "drm_dsc_flatness_det_thresh() - Calculate the flatness_det_thresh for the given DSC config   @dsc: Pointer to DRM DSC config struct     Return: Calculated flatness det thresh value ", "val = DP_128B132B_TRAINING_AUX_RD_INTERVAL_MASK;}unit = (val & DP_128B132B_TRAINING_AUX_RD_INTERVAL_1MS_UNIT) ? 1 : 2;val &= DP_128B132B_TRAINING_AUX_RD_INTERVAL_MASK;return (val + 1) * unit * 1000;}EXPORT_SYMBOL(drm_dp_128b132b_read_aux_rd_interval": "drm_dp_128b132b_read_aux_rd_interval(struct drm_dp_aux  aux){int unit;u8 val;if (drm_dp_dpcd_readb(aux, DP_128B132B_TRAINING_AUX_RD_INTERVAL, &val) != 1) {drm_err(aux->drm_dev, \"%s: failed rd interval read\\n\",aux->name);  default to max ", "const char *drm_dp_phy_name(enum drm_dp_phy dp_phy)": "drm_dp_phy_name() - Get the name of the given DP PHY   @dp_phy: The DP PHY identifier     Given the @dp_phy, get a user friendly name of the DP PHY, either \"DPRX\" or   \"LTTPR <N>\", or \"<INVALID DP PHY>\" on errors. The returned string is always   non-NULL and valid.     Returns: Name of the DP PHY. ", "return link_rate / 27000;}}EXPORT_SYMBOL(drm_dp_link_rate_to_bw_code": "drm_dp_link_rate_to_bw_code(int link_rate){switch (link_rate) {case 1000000:return DP_LINK_BW_10;case 1350000:return DP_LINK_BW_13_5;case 2000000:return DP_LINK_BW_20;default:  Spec says link_bw = link_rate  0.27Gbps ", "return link_bw * 27000;}}EXPORT_SYMBOL(drm_dp_bw_code_to_link_rate": "drm_dp_bw_code_to_link_rate(u8 link_bw){switch (link_bw) {case DP_LINK_BW_10:return 1000000;case DP_LINK_BW_13_5:return 1350000;case DP_LINK_BW_20:return 2000000;default:  Spec says link_rate = link_bw   0.27Gbps ", "int drm_dp_dpcd_probe(struct drm_dp_aux *aux, unsigned int offset)": "drm_dp_dpcd_probe() - probe a given DPCD address with a 1-byte read access   @aux: DisplayPort AUX channel (SST)   @offset: address of the register to probe     Probe the provided DPCD address by reading 1 byte from it. The function can   be used to trigger some side-effect the read access has, like waking up the   sink, without the need for the read-out value.     Returns 0 if the read access suceeded, or a negative error code on failure. ", "return 400;}}return parse(aux, rd_interval & mask);}int drm_dp_read_clock_recovery_delay(struct drm_dp_aux *aux, const u8 dpcd[DP_RECEIVER_CAP_SIZE],     enum drm_dp_phy dp_phy, bool uhbr)": "drm_dp_dpcd_readb(aux, offset, &rd_interval) != 1) {drm_dbg_kms(aux->drm_dev, \"%s: failed rd interval read\\n\",    aux->name);  arbitrary default delay ", "ssize_t drm_dp_dpcd_write(struct drm_dp_aux *aux, unsigned int offset,  void *buffer, size_t size)": "drm_dp_dpcd_write() - write a series of bytes to the DPCD   @aux: DisplayPort AUX channel (SST or MST)   @offset: address of the (first) register to write   @buffer: buffer containing the values to write   @size: number of bytes in @buffer     Returns the number of bytes transferred on success, or a negative error   code on failure. -EIO is returned if the request was NAKed by the sink or   if the retry count was exceeded. If not all bytes were transferred, this   function returns -EPROTO. Errors from the underlying AUX channel transfer   function, with the exception of -EBUSY (which causes the transaction to   be retried), are propagated to the caller. ", "int drm_dp_dpcd_read_link_status(struct drm_dp_aux *aux, u8 status[DP_LINK_STATUS_SIZE])": "drm_dp_dpcd_read_link_status() - read DPCD link status (bytes 0x202-0x207)   @aux: DisplayPort AUX channel   @status: buffer to store the link status in (must be at least 6 bytes)     Returns the number of bytes transferred on success or a negative error   code on failure. ", "int drm_dp_dpcd_read_phy_link_status(struct drm_dp_aux *aux,     enum drm_dp_phy dp_phy,     u8 link_status[DP_LINK_STATUS_SIZE])": "drm_dp_dpcd_read_phy_link_status - get the link status information for a DP PHY   @aux: DisplayPort AUX channel   @dp_phy: the DP PHY to get the link status for   @link_status: buffer to return the status in     Fetch the AUX DPCD registers for the DPRX or an LTTPR PHY link status. The   layout of the returned @link_status matches the DPCD register layout of the   DPRX PHY link status.     Returns 0 if the information was read successfully or a negative error code   on failure. ", "bool drm_dp_downstream_is_type(const u8 dpcd[DP_RECEIVER_CAP_SIZE],       const u8 port_cap[4], u8 type)": "drm_dp_downstream_is_type() - is the downstream facing port of certain type?   @dpcd: DisplayPort configuration data   @port_cap: port capabilities   @type: port type to be checked. Can be:     %DP_DS_PORT_TYPE_DP, %DP_DS_PORT_TYPE_VGA, %DP_DS_PORT_TYPE_DVI,     %DP_DS_PORT_TYPE_HDMI, %DP_DS_PORT_TYPE_NON_EDID,    %DP_DS_PORT_TYPE_DP_DUALMODE or %DP_DS_PORT_TYPE_WIRELESS.     Caveat: Only works with DPCD 1.1+ port caps.     Returns: whether the downstream facing port matches the type. ", "bool drm_dp_downstream_is_tmds(const u8 dpcd[DP_RECEIVER_CAP_SIZE],       const u8 port_cap[4],       const struct edid *edid)": "drm_dp_downstream_is_tmds() - is the downstream facing port TMDS?   @dpcd: DisplayPort configuration data   @port_cap: port capabilities   @edid: EDID     Returns: whether the downstream facing port is TMDS (HDMIDVI). ", "bool drm_dp_send_real_edid_checksum(struct drm_dp_aux *aux,    u8 real_edid_checksum)": "drm_dp_send_real_edid_checksum() - send back real edid checksum value   @aux: DisplayPort AUX channel   @real_edid_checksum: real edid checksum for the last block     Returns:   True on success ", "int drm_dp_read_dpcd_caps(struct drm_dp_aux *aux,  u8 dpcd[DP_RECEIVER_CAP_SIZE])": "drm_dp_read_dpcd_caps() - read DPCD caps and extended DPCD caps if   available   @aux: DisplayPort AUX channel   @dpcd: Buffer to store the resulting DPCD in     Attempts to read the base DPCD caps for @aux. Additionally, this function   checks for and reads the extended DPRX caps (%DP_DP13_DPCD_REV) if   present.     Returns: %0 if the DPCD was read successfully, negative error code   otherwise. ", "int drm_dp_read_downstream_info(struct drm_dp_aux *aux,const u8 dpcd[DP_RECEIVER_CAP_SIZE],u8 downstream_ports[DP_MAX_DOWNSTREAM_PORTS])": "drm_dp_downstream_max_bpc()     Returns: 0 if either the downstream port info was read successfully or   there was no downstream info to read, or a negative error code otherwise. ", "int drm_dp_downstream_max_dotclock(const u8 dpcd[DP_RECEIVER_CAP_SIZE],   const u8 port_cap[4])": "drm_dp_downstream_max_dotclock() - extract downstream facing port max dot clock   @dpcd: DisplayPort configuration data   @port_cap: port capabilities     Returns: Downstream facing port max dot clock in kHz on success,   or 0 if max clock not defined ", "int drm_dp_downstream_max_tmds_clock(const u8 dpcd[DP_RECEIVER_CAP_SIZE],     const u8 port_cap[4],     const struct edid *edid)": "drm_dp_downstream_max_tmds_clock() - extract downstream facing port max TMDS clock   @dpcd: DisplayPort configuration data   @port_cap: port capabilities   @edid: EDID     Returns: HDMIDVI downstream facing port max TMDS clock in kHz on success,   or 0 if max TMDS clock not defined ", "int drm_dp_downstream_min_tmds_clock(const u8 dpcd[DP_RECEIVER_CAP_SIZE],     const u8 port_cap[4],     const struct edid *edid)": "drm_dp_downstream_min_tmds_clock() - extract downstream facing port min TMDS clock   @dpcd: DisplayPort configuration data   @port_cap: port capabilities   @edid: EDID     Returns: HDMIDVI downstream facing port min TMDS clock in kHz on success,   or 0 if max TMDS clock not defined ", "bool drm_dp_downstream_420_passthrough(const u8 dpcd[DP_RECEIVER_CAP_SIZE],       const u8 port_cap[4])": "drm_dp_downstream_420_passthrough() - determine downstream facing port                                         YCbCr 4:2:0 pass-through capability   @dpcd: DisplayPort configuration data   @port_cap: downstream facing port capabilities     Returns: whether the downstream facing port can pass through YCbCr 4:2:0 ", "bool drm_dp_downstream_444_to_420_conversion(const u8 dpcd[DP_RECEIVER_CAP_SIZE],     const u8 port_cap[4])": "drm_dp_downstream_444_to_420_conversion() - determine downstream facing port                                               YCbCr 4:4:4->4:2:0 conversion capability   @dpcd: DisplayPort configuration data   @port_cap: downstream facing port capabilities     Returns: whether the downstream facing port can convert YCbCr 4:4:4 to 4:2:0 ", "bool drm_dp_downstream_rgb_to_ycbcr_conversion(const u8 dpcd[DP_RECEIVER_CAP_SIZE],       const u8 port_cap[4],       u8 color_spc)": "drm_dp_downstream_rgb_to_ycbcr_conversion() - determine downstream facing port                                                 RGB->YCbCr conversion capability   @dpcd: DisplayPort configuration data   @port_cap: downstream facing port capabilities   @color_spc: Colorspace for which conversion cap is sought     Returns: whether the downstream facing port can convert RGB->YCbCr for a given   colorspace. ", "struct drm_display_mode *drm_dp_downstream_mode(struct drm_device *dev,       const u8 dpcd[DP_RECEIVER_CAP_SIZE],       const u8 port_cap[4])": "drm_dp_downstream_mode() - return a mode for downstream facing port   @dev: DRM device   @dpcd: DisplayPort configuration data   @port_cap: port capabilities     Provides a suitable mode for downstream facing ports without EDID.     Returns: A new drm_display_mode on success or NULL on failure ", "int drm_dp_downstream_id(struct drm_dp_aux *aux, char id[6])": "drm_dp_downstream_id() - identify branch device   @aux: DisplayPort AUX channel   @id: DisplayPort branch device id     Returns branch device id on success or NULL on failure ", "void drm_dp_downstream_debug(struct seq_file *m,     const u8 dpcd[DP_RECEIVER_CAP_SIZE],     const u8 port_cap[4],     const struct edid *edid,     struct drm_dp_aux *aux)": "drm_dp_downstream_debug() - debug DP branch devices   @m: pointer for debugfs file   @dpcd: DisplayPort configuration data   @port_cap: port capabilities   @edid: EDID   @aux: DisplayPort AUX channel   ", "enum drm_mode_subconnectordrm_dp_subconnector_type(const u8 dpcd[DP_RECEIVER_CAP_SIZE], const u8 port_cap[4])": "drm_dp_subconnector_type() - get DP branch device type   @dpcd: DisplayPort configuration data   @port_cap: port capabilities ", "void drm_dp_set_subconnector_property(struct drm_connector *connector,      enum drm_connector_status status,      const u8 *dpcd,      const u8 port_cap[4])": "drm_dp_set_subconnector_property - set subconnector for DP connector   @connector: connector to set property on   @status: connector status   @dpcd: DisplayPort configuration data   @port_cap: port capabilities     Called by a driver on every detect event. ", "bool drm_dp_read_sink_count_cap(struct drm_connector *connector,const u8 dpcd[DP_RECEIVER_CAP_SIZE],const struct drm_dp_desc *desc)": "drm_dp_read_sink_count_cap() - Check whether a given connector has a valid sink   count   @connector: The DRM connector to check   @dpcd: A cached copy of the connector's DPCD RX capabilities   @desc: A cached copy of the connector's DP descriptor     See also: drm_dp_read_sink_count()     Returns: %True if the (e)DP connector has a valid sink count that should   be probed, %false otherwise. ", "void drm_dp_remote_aux_init(struct drm_dp_aux *aux)": "drm_dp_remote_aux_init() - minimally initialise a remote aux channel   @aux: DisplayPort AUX channel     Used for remote aux channel in general. Merely initialize the crc work   struct. ", "void drm_dp_aux_init(struct drm_dp_aux *aux)": "drm_dp_aux_register() once the connector   has been registered to allow userspace access to the auxiliary DP channel.   Likewise, for such drivers you should also assign &drm_dp_aux.drm_dev as   early as possible so that the &drm_device that corresponds to the AUX adapter   may be mentioned in debugging output from the DRM DP helpers.     For devices which use a separate platform device for their AUX adapters, this   may be called as early as required by the driver.   ", "int drm_dp_aux_register(struct drm_dp_aux *aux)": "drm_dp_aux_unregister() in &drm_connector_funcs.early_unregister.   Functions which don't follow this will likely Oops when   %CONFIG_DRM_DP_AUX_CHARDEV is enabled.     For devices where the AUX channel is a device that exists independently of   the &drm_device that uses it, such as SoCs and bridge devices, it is   recommended to call drm_dp_aux_register() after a &drm_device has been   assigned to &drm_dp_aux.drm_dev, and likewise to call   drm_dp_aux_unregister() once the &drm_device should no longer be associated   with the AUX channel (e.g. on bridge detach).     Drivers which need to use the aux channel before either of the two points   mentioned above need to call drm_dp_aux_init() in order to use the AUX   channel before registration.     Returns 0 on success or a negative error code on failure. ", "int drm_dp_psr_setup_time(const u8 psr_cap[EDP_PSR_RECEIVER_CAP_SIZE])": "drm_dp_psr_setup_time() - PSR setup in time usec   @psr_cap: PSR capabilities from DPCD     Returns:   PSR setup time for the panel in microseconds,  negative   error code on failure. ", "int drm_dp_start_crc(struct drm_dp_aux *aux, struct drm_crtc *crtc)": "drm_dp_start_crc() - start capture of frame CRCs   @aux: DisplayPort AUX channel   @crtc: CRTC displaying the frames whose CRCs are to be captured     Returns 0 on success or a negative error code on failure. ", "int drm_dp_stop_crc(struct drm_dp_aux *aux)": "drm_dp_stop_crc() - stop capture of frame CRCs   @aux: DisplayPort AUX channel     Returns 0 on success or a negative error code on failure. ", "int drm_dp_read_desc(struct drm_dp_aux *aux, struct drm_dp_desc *desc,     bool is_branch)": "drm_dp_read_desc - read sinkbranch descriptor from DPCD   @aux: DisplayPort AUX channel   @desc: Device descriptor to fill from DPCD   @is_branch: true for branch devices, false for sink devices     Read DPCD 0x400 (sink) or 0x500 (branch) into @desc. Also debug log the   identification.     Returns 0 on success or a negative error code on failure. ", "u8 drm_dp_dsc_sink_max_slice_count(const u8 dsc_dpcd[DP_DSC_RECEIVER_CAP_SIZE],   bool is_edp)": "drm_dp_dsc_sink_max_slice_count() - Get the max slice count   supported by the DSC sink.   @dsc_dpcd: DSC capabilities from DPCD   @is_edp: true if its eDP, false for DP     Read the slice capabilities DPCD register from DSC sink to get   the maximum slice count supported. This is used to populate   the DSC parameters in the &struct drm_dsc_config by the driver.   Driver creates an infoframe using these parameters to populate   &struct drm_dsc_pps_infoframe. These are sent to the sink using DSC   infoframe using the helper function drm_dsc_pps_infoframe_pack()     Returns:   Maximum slice count supported by DSC sink or 0 its invalid ", "u8 drm_dp_dsc_sink_line_buf_depth(const u8 dsc_dpcd[DP_DSC_RECEIVER_CAP_SIZE])": "drm_dp_dsc_sink_line_buf_depth() - Get the line buffer depth in bits   @dsc_dpcd: DSC capabilities from DPCD     Read the DSC DPCD register to parse the line buffer depth in bits which is   number of bits of precision within the decoder line buffer supported by   the DSC sink. This is used to populate the DSC parameters in the   &struct drm_dsc_config by the driver.   Driver creates an infoframe using these parameters to populate   &struct drm_dsc_pps_infoframe. These are sent to the sink using DSC   infoframe using the helper function drm_dsc_pps_infoframe_pack()     Returns:   Line buffer depth supported by DSC panel or 0 its invalid ", "int drm_dp_dsc_sink_supported_input_bpcs(const u8 dsc_dpcd[DP_DSC_RECEIVER_CAP_SIZE], u8 dsc_bpc[3])": "drm_dp_dsc_sink_supported_input_bpcs() - Get all the input bits per component   values supported by the DSC sink.   @dsc_dpcd: DSC capabilities from DPCD   @dsc_bpc: An array to be filled by this helper with supported             input bpcs.     Read the DSC DPCD from the sink device to parse the supported bits per   component values. This is used to populate the DSC parameters   in the &struct drm_dsc_config by the driver.   Driver creates an infoframe using these parameters to populate   &struct drm_dsc_pps_infoframe. These are sent to the sink using DSC   infoframe using the helper function drm_dsc_pps_infoframe_pack()     Returns:   Number of input BPC values parsed from the DPCD ", "int drm_dp_read_lttpr_common_caps(struct drm_dp_aux *aux,  const u8 dpcd[DP_RECEIVER_CAP_SIZE],  u8 caps[DP_LTTPR_COMMON_CAP_SIZE])": "drm_dp_read_lttpr_common_caps - read the LTTPR common capabilities   @aux: DisplayPort AUX channel   @dpcd: DisplayPort configuration data   @caps: buffer to return the capability info in     Read capabilities common to all LTTPRs.     Returns 0 on success or a negative error code on failure. ", "int drm_dp_read_lttpr_phy_caps(struct drm_dp_aux *aux,       const u8 dpcd[DP_RECEIVER_CAP_SIZE],       enum drm_dp_phy dp_phy,       u8 caps[DP_LTTPR_PHY_CAP_SIZE])": "drm_dp_read_lttpr_phy_caps - read the capabilities for a given LTTPR PHY   @aux: DisplayPort AUX channel   @dpcd: DisplayPort configuration data   @dp_phy: LTTPR PHY to read the capabilities for   @caps: buffer to return the capability info in     Read the capabilities for the given LTTPR PHY.     Returns 0 on success or a negative error code on failure. ", "int drm_dp_lttpr_count(const u8 caps[DP_LTTPR_COMMON_CAP_SIZE])": "drm_dp_lttpr_count - get the number of detected LTTPRs   @caps: LTTPR common capabilities     Get the number of detected LTTPRs from the LTTPR common capabilities info.     Returns:     -ERANGE if more than supported number (8) of LTTPRs are detected     -EINVAL if the DP_PHY_REPEATER_CNT register contains an invalid value     otherwise the number of detected LTTPRs ", "int drm_dp_lttpr_max_link_rate(const u8 caps[DP_LTTPR_COMMON_CAP_SIZE])": "drm_dp_lttpr_max_link_rate - get the maximum link rate supported by all LTTPRs   @caps: LTTPR common capabilities     Returns the maximum link rate supported by all detected LTTPRs. ", "int drm_dp_lttpr_max_lane_count(const u8 caps[DP_LTTPR_COMMON_CAP_SIZE])": "drm_dp_lttpr_max_lane_count - get the maximum lane count supported by all LTTPRs   @caps: LTTPR common capabilities     Returns the maximum lane count supported by all detected LTTPRs. ", "booldrm_dp_lttpr_voltage_swing_level_3_supported(const u8 caps[DP_LTTPR_PHY_CAP_SIZE])": "drm_dp_lttpr_voltage_swing_level_3_supported - check for LTTPR vswing3 support   @caps: LTTPR PHY capabilities     Returns true if the @caps for an LTTPR TX PHY indicate support for   voltage swing level 3. ", "booldrm_dp_lttpr_pre_emphasis_level_3_supported(const u8 caps[DP_LTTPR_PHY_CAP_SIZE])": "drm_dp_lttpr_pre_emphasis_level_3_supported - check for LTTPR preemph3 support   @caps: LTTPR PHY capabilities     Returns true if the @caps for an LTTPR TX PHY indicate support for   pre-emphasis level 3. ", "int drm_dp_get_phy_test_pattern(struct drm_dp_aux *aux,struct drm_dp_phy_test_params *data)": "drm_dp_get_phy_test_pattern() - get the requested pattern from the sink.   @aux: DisplayPort AUX channel   @data: DP phy compliance test parameters.     Returns 0 on success or a negative error code on failure. ", "int drm_dp_set_phy_test_pattern(struct drm_dp_aux *aux,struct drm_dp_phy_test_params *data, u8 dp_rev)": "drm_dp_set_phy_test_pattern() - set the pattern to the sink.   @aux: DisplayPort AUX channel   @data: DP phy compliance test parameters.   @dp_rev: DP revision to use for compliance testing     Returns 0 on success or a negative error code on failure. ", "int drm_dp_get_pcon_max_frl_bw(const u8 dpcd[DP_RECEIVER_CAP_SIZE],       const u8 port_cap[4])": "drm_dp_get_pcon_max_frl_bw() - maximum frl supported by PCON   @dpcd: DisplayPort configuration data   @port_cap: port capabilities     Returns maximum frl bandwidth supported by PCON in GBPS,   returns 0 if not supported. ", "int drm_dp_pcon_frl_prepare(struct drm_dp_aux *aux, bool enable_frl_ready_hpd)": "drm_dp_pcon_frl_prepare() - Prepare PCON for FRL.   @aux: DisplayPort AUX channel   @enable_frl_ready_hpd: Configure DP_PCON_ENABLE_HPD_READY.     Returns 0 if success, else returns negative error code. ", "bool drm_dp_pcon_is_frl_ready(struct drm_dp_aux *aux)": "drm_dp_pcon_is_frl_ready() - Is PCON ready for FRL   @aux: DisplayPort AUX channel     Returns true if success, else returns false. ", "int drm_dp_pcon_frl_configure_1(struct drm_dp_aux *aux, int max_frl_gbps,u8 frl_mode)": "drm_dp_pcon_frl_configure_1() - Set HDMI LINK Configuration-Step1   @aux: DisplayPort AUX channel   @max_frl_gbps: maximum frl bw to be configured between PCON and HDMI sink   @frl_mode: FRL Training mode, it can be either Concurrent or Sequential.   In Concurrent Mode, the FRL link bring up can be done along with   DP Link training. In Sequential mode, the FRL link bring up is done prior to   the DP Link training.     Returns 0 if success, else returns negative error code. ", "int drm_dp_pcon_frl_configure_2(struct drm_dp_aux *aux, int max_frl_mask,u8 frl_type)": "drm_dp_pcon_frl_configure_2() - Set HDMI Link configuration Step-2   @aux: DisplayPort AUX channel   @max_frl_mask : Max FRL BW to be tried by the PCON with HDMI Sink   @frl_type : FRL training type, can be Extended, or Normal.   In Normal FRL training, the PCON tries each frl bw from the max_frl_mask   starting from min, and stops when link training is successful. In Extended   FRL training, all frl bw selected in the mask are trained by the PCON.     Returns 0 if success, else returns negative error code. ", "int drm_dp_pcon_reset_frl_config(struct drm_dp_aux *aux)": "drm_dp_pcon_reset_frl_config() - Re-Set HDMI Link configuration.   @aux: DisplayPort AUX channel     Returns 0 if success, else returns negative error code. ", "int drm_dp_pcon_frl_enable(struct drm_dp_aux *aux)": "drm_dp_pcon_frl_enable() - Enable HDMI link through FRL   @aux: DisplayPort AUX channel     Returns 0 if success, else returns negative error code. ", "bool drm_dp_pcon_hdmi_link_active(struct drm_dp_aux *aux)": "drm_dp_pcon_hdmi_link_active() - check if the PCON HDMI LINK status is active.   @aux: DisplayPort AUX channel     Returns true if link is active else returns false. ", "int drm_dp_pcon_hdmi_link_mode(struct drm_dp_aux *aux, u8 *frl_trained_mask)": "drm_dp_pcon_hdmi_link_mode() - get the PCON HDMI LINK MODE   @aux: DisplayPort AUX channel   @frl_trained_mask: pointer to store bitmask of the trained bw configuration.   Valid only if the MODE returned is FRL. For Normal Link training mode   only 1 of the bits will be set, but in case of Extended mode, more than   one bits can be set.     Returns the link mode : TMDS or FRL on success, else returns negative error   code. ", "void drm_dp_pcon_hdmi_frl_link_error_count(struct drm_dp_aux *aux,   struct drm_connector *connector)": "drm_dp_pcon_hdmi_frl_link_error_count() - print the error count per lane   during link failure between PCON and HDMI sink   @aux: DisplayPort AUX channel   @connector: DRM connector   code.  ", "bool drm_dp_pcon_enc_is_dsc_1_2(const u8 pcon_dsc_dpcd[DP_PCON_DSC_ENCODER_CAP_SIZE])": "drm_dp_pcon_enc_is_dsc_1_2 - Does PCON Encoder supports DSC 1.2   @pcon_dsc_dpcd: DSC capabilities of the PCON DSC Encoder     Returns true is PCON encoder is DSC 1.2 else returns false. ", "int drm_dp_pcon_dsc_max_slices(const u8 pcon_dsc_dpcd[DP_PCON_DSC_ENCODER_CAP_SIZE])": "drm_dp_pcon_dsc_max_slices - Get max slices supported by PCON DSC Encoder   @pcon_dsc_dpcd: DSC capabilities of the PCON DSC Encoder     Returns maximum no. of slices supported by the PCON DSC Encoder. ", "int drm_dp_pcon_dsc_max_slice_width(const u8 pcon_dsc_dpcd[DP_PCON_DSC_ENCODER_CAP_SIZE])": "drm_dp_pcon_dsc_max_slice_width() - Get max slice width for Pcon DSC encoder   @pcon_dsc_dpcd: DSC capabilities of the PCON DSC Encoder     Returns maximum width of the slices in pixel width i.e. no. of pixels x 320. ", "int drm_dp_pcon_dsc_bpp_incr(const u8 pcon_dsc_dpcd[DP_PCON_DSC_ENCODER_CAP_SIZE])": "drm_dp_pcon_dsc_bpp_incr() - Get bits per pixel increment for PCON DSC encoder   @pcon_dsc_dpcd: DSC capabilities of the PCON DSC Encoder     Returns the bpp precision supported by the PCON encoder. ", "int drm_dp_pcon_pps_default(struct drm_dp_aux *aux)": "drm_dp_pcon_pps_default() - Let PCON fill the default pps parameters   for DSC1.2 between PCON & HDMI2.1 sink   @aux: DisplayPort AUX channel     Returns 0 on success, else returns negative error code. ", "int drm_dp_pcon_pps_override_buf(struct drm_dp_aux *aux, u8 pps_buf[128])": "drm_dp_pcon_pps_override_buf() - Configure PPS encoder override buffer for   HDMI sink   @aux: DisplayPort AUX channel   @pps_buf: 128 bytes to be written into PPS buffer for HDMI sink by PCON.     Returns 0 on success, else returns negative error code. ", "int drm_dp_pcon_pps_override_param(struct drm_dp_aux *aux, u8 pps_param[6])": "drm_dp_pcon_pps_override_param() - Write PPS parameters to DSC encoder   override registers   @aux: DisplayPort AUX channel   @pps_param: 3 Parameters (2 Bytes each) : Slice Width, Slice Height,   bits_per_pixel.     Returns 0 on success, else returns negative error code. ", "int drm_dp_pcon_convert_rgb_to_ycbcr(struct drm_dp_aux *aux, u8 color_spc)": "drm_dp_pcon_convert_rgb_to_ycbcr() - Configure the PCon to convert RGB to Ycbcr   @aux: displayPort AUX channel   @color_spc: Color-spaces for which conversion is to be enabled, 0 for disable.     Returns 0 on success, else returns negative error code. ", "int drm_edp_backlight_set_level(struct drm_dp_aux *aux, const struct drm_edp_backlight_info *bl,u16 level)": "drm_edp_backlight_init()   @level: The brightness level to set     Sets the brightness level of an eDP panel's backlight. Note that the panel's backlight must   already have been enabled by the driver by calling drm_edp_backlight_enable().     Returns: %0 on success, negative error code on failure ", "int drm_edp_backlight_disable(struct drm_dp_aux *aux, const struct drm_edp_backlight_info *bl)": "drm_edp_backlight_disable() - Disable an eDP backlight using DPCD, if supported   @aux: The DP AUX channel to use   @bl: Backlight capability info from drm_edp_backlight_init()     This function handles disabling DPCD backlight controls on a panel over AUX.     Note that certain panels do not support being enabled or disabled via DPCD, but instead require   that the driver handle enablingdisabling the panel through implementation-specific means using   the EDP_BL_PWR GPIO. For such panels, &drm_edp_backlight_info.aux_enable will be set to %false,   this function becomes a no-op, and the driver is expected to handle powering the panel off using   the EDP_BL_PWR GPIO.     Returns: %0 on success or no-op, negative error code on failure. ", "int drm_panel_dp_aux_backlight(struct drm_panel *panel, struct drm_dp_aux *aux)": "drm_panel_dp_aux_backlight - create and use DP AUX backlight   @panel: DRM panel   @aux: The DP AUX channel to use     Use this function to create and handle backlight if your panel   supports backlight control over DP AUX channel using DPCD   registers as per VESA's standard backlight control interface.     When the panel is enabled backlight will be enabled after a   successful call to &drm_panel_funcs.enable()     When the panel is disabled backlight will be disabled before the   call to &drm_panel_funcs.disable().     A typical implementation for a panel driver supporting backlight   control over DP AUX will call this function at probe time.   Backlight will then be handled transparently without requiring   any intervention from the driver.     drm_panel_dp_aux_backlight() must be called after the call to drm_panel_init().     Return: 0 on success or a negative error code on failure. ", "voiddrm_dp_mst_get_port_malloc(struct drm_dp_mst_port *port)": "drm_dp_mst_get_port_malloc() - Increment the malloc refcount of an MST port   @port: The &struct drm_dp_mst_port to increment the malloc refcount of     Increments &drm_dp_mst_port.malloc_kref. When &drm_dp_mst_port.malloc_kref   reaches 0, the memory allocation for @port will be released and @port may   no longer be used.     Because @port could potentially be freed at any time by the DP MST helpers   if &drm_dp_mst_port.malloc_kref reaches 0, including during a call to this   function, drivers that which to make use of &struct drm_dp_mst_port should   ensure that they grab at least one main malloc reference to their MST ports   in &drm_dp_mst_topology_cbs.add_connector. This callback is called before   there is any chance for &drm_dp_mst_port.malloc_kref to reach 0.     See also: drm_dp_mst_put_port_malloc() ", "/** * drm_dp_mst_get_mstb_malloc() - Increment the malloc refcount of a branch * device * @mstb: The &struct drm_dp_mst_branch to increment the malloc refcount of * * Increments &drm_dp_mst_branch.malloc_kref. When * &drm_dp_mst_branch.malloc_kref reaches 0, the memory allocation for @mstb * will be released and @mstb may no longer be used. * * See also: drm_dp_mst_put_mstb_malloc() ": "drm_dp_mst_put_port_malloc(mstb->port_parent);kfree(mstb);}     DOC: Branch device and port refcounting     Topology refcount overview   ~~~~~~~~~~~~~~~~~~~~~~~~~~     The refcounting schemes for &struct drm_dp_mst_branch and &struct   drm_dp_mst_port are somewhat unusual. Both ports and branch devices have   two different kinds of refcounts: topology refcounts, and malloc refcounts.     Topology refcounts are not exposed to drivers, and are handled internally   by the DP MST helpers. The helpers use them in order to prevent the   in-memory topology state from being changed in the middle of critical   operations like changing the internal state of payload allocations. This   means each branch and port will be considered to be connected to the rest   of the topology until its topology refcount reaches zero. Additionally,   for ports this means that their associated &struct drm_connector will stay   registered with userspace until the port's refcount reaches 0.     Malloc refcount overview   ~~~~~~~~~~~~~~~~~~~~~~~~     Malloc references are used to keep a &struct drm_dp_mst_port or &struct   drm_dp_mst_branch allocated even after all of its topology references have   been dropped, so that the driver or MST helpers can safely access each   branch's last known state before it was disconnected from the topology.   When the malloc refcount of a port or branch reaches 0, the memory   allocation containing the &struct drm_dp_mst_branch or &struct   drm_dp_mst_port respectively will be freed.     For &struct drm_dp_mst_branch, malloc refcounts are not currently exposed   to drivers. As of writing this documentation, there are no drivers that   have a usecase for accessing &struct drm_dp_mst_branch outside of the MST   helpers. Exposing this API to drivers in a race-free manner would take more   tweaking of the refcounting scheme, however patches are welcome provided   there is a legitimate driver usecase for this.     Refcount relationships in a topology   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     Let's take a look at why the relationship between topology and malloc   refcounts is designed the way it is.     .. kernel-figure:: dp-msttopology-figure-1.dot        An example of topology and malloc refs in a DP MST topology with two      active payloads. Topology refcount increments are indicated by solid      lines, and malloc refcount increments are indicated by dashed lines.      Each starts from the branch which incremented the refcount, and ends at      the branch to which the refcount belongs to, i.e. the arrow points the      same way as the C pointers used to reference a structure.     As you can see in the above figure, every branch increments the topology   refcount of its children, and increments the malloc refcount of its   parent. Additionally, every payload increments the malloc refcount of its   assigned port by 1.     So, what would happen if MSTB #3 from the above figure was unplugged from   the system, but the driver hadn't yet removed payload #2 from port #3? The   topology would start to look like the figure below.     .. kernel-figure:: dp-msttopology-figure-2.dot        Ports and branch devices which have been released from memory are      colored grey, and references which have been removed are colored red.     Whenever a port or branch device's topology refcount reaches zero, it will   decrement the topology refcounts of all its children, the malloc refcount   of its parent, and finally its own malloc refcount. For MSTB #4 and port   #4, this means they both have been disconnected from the topology and freed   from memory. But, because payload #2 is still holding a reference to port   #3, port #3 is removed from the topology but its &struct drm_dp_mst_port   is still accessible from memory. This also means port #3 has not yet   decremented the malloc refcount of MSTB #3, so its &struct   drm_dp_mst_branch will also stay allocated in memory until port #3's   malloc refcount reaches 0.     This relationship is necessary because in order to release payload #2, we   need to be able to figure out the last relative of port #3 that's still   connected to the topology. In this case, we would travel up the topology as   shown below.     .. kernel-figure:: dp-msttopology-figure-3.dot     And finally, remove payload #2 by communicating with port #2 through   sideband transactions. ", "int drm_dp_mst_connector_late_register(struct drm_connector *connector,       struct drm_dp_mst_port *port)": "drm_dp_mst_connector_late_register() - Late MST connector registration   @connector: The MST connector   @port: The MST port for this connector     Helper to register the remote aux device for this MST port. Drivers should   call this from their mst connector's late_register hook to enable MST aux   devices.     Return: 0 on success, negative error code on failure. ", "void drm_dp_mst_connector_early_unregister(struct drm_connector *connector,   struct drm_dp_mst_port *port)": "drm_dp_mst_connector_early_unregister() - Early MST connector unregistration   @connector: The MST connector   @port: The MST port for this connector     Helper to unregister the remote aux device for this MST port, registered by   drm_dp_mst_connector_late_register(). Drivers should call this from their mst   connector's early_unregister hook. ", "txmsg->dst = mgr->mst_primary;build_query_stream_enc_status(txmsg, payload->vcpi, nonce);drm_dp_queue_down_tx(mgr, txmsg);ret = drm_dp_mst_wait_tx_reply(mgr->mst_primary, txmsg);if (ret < 0) ": "drm_dp_send_query_stream_enc_status(struct drm_dp_mst_topology_mgr  mgr,struct drm_dp_mst_port  port,struct drm_dp_query_stream_enc_status_ack_reply  status){struct drm_dp_mst_topology_state  state;struct drm_dp_mst_atomic_payload  payload;struct drm_dp_sideband_msg_tx  txmsg;u8 nonce[7];int ret;txmsg = kzalloc(sizeof( txmsg), GFP_KERNEL);if (!txmsg)return -ENOMEM;port = drm_dp_mst_topology_get_port_validated(mgr, port);if (!port) {ret = -EINVAL;goto out_get_port;}get_random_bytes(nonce, sizeof(nonce));drm_modeset_lock(&mgr->base.lock, NULL);state = to_drm_dp_mst_topology_state(mgr->base.state);payload = drm_atomic_get_mst_payload_state(state, port);    \"Source device targets the QUERY_STREAM_ENCRYPTION_STATUS message    transaction at the MST Branch device directly connected to the    Source\" ", "int drm_dp_add_payload_part1(struct drm_dp_mst_topology_mgr *mgr,     struct drm_dp_mst_topology_state *mst_state,     struct drm_dp_mst_atomic_payload *payload)": "drm_dp_add_payload_part1() - Execute payload update part 1   @mgr: Manager to use.   @mst_state: The MST atomic state   @payload: The payload to write     Determines the starting time slot for the given payload, and programs the VCPI for this payload   into hardware. After calling this, the driver should generate ACT and payload packets.     Returns: 0 on success, error code on failure. In the event that this fails,   @payload.vc_start_slot will also be set to -1. ", "void drm_dp_remove_payload(struct drm_dp_mst_topology_mgr *mgr,   struct drm_dp_mst_topology_state *mst_state,   const struct drm_dp_mst_atomic_payload *old_payload,   struct drm_dp_mst_atomic_payload *new_payload)": "drm_dp_remove_payload() - Remove an MST payload   @mgr: Manager to use.   @mst_state: The MST atomic state   @old_payload: The payload with its old state   @new_payload: The payload to write     Removes a payload from an MST topology if it was successfully assigned a start slot. Also updates   the starting time slots of all other payloads which would have been shifted towards the start of   the VC table as a result. After calling this, the driver should generate ACT and payload packets. ", "int drm_dp_add_payload_part2(struct drm_dp_mst_topology_mgr *mgr,     struct drm_atomic_state *state,     struct drm_dp_mst_atomic_payload *payload)": "drm_dp_add_payload_part2() - Execute payload update part 2   @mgr: Manager to use.   @state: The global atomic state   @payload: The payload to update     If @payload was successfully assigned a starting time slot by drm_dp_add_payload_part1(), this   function will send the sideband messages to finish allocating this payload.     Returns: 0 on success, negative error code on failure. ", "int drm_dp_get_vc_payload_bw(const struct drm_dp_mst_topology_mgr *mgr,     int link_rate, int link_lane_count)": "drm_dp_get_vc_payload_bw - get the VC payload BW for an MST link   @mgr: The &drm_dp_mst_topology_mgr to use   @link_rate: link rate in 10kbitss units   @link_lane_count: lane count     Calculate the total bandwidth of a MultiStream Transport link. The returned   value is in units of PBNs(timeslots1 MTP). This value can be used to   convert the number of PBNs required for a given stream to the number of   timeslots this stream requires in each MTP. ", "bool drm_dp_read_mst_cap(struct drm_dp_aux *aux, const u8 dpcd[DP_RECEIVER_CAP_SIZE])": "drm_dp_read_mst_cap() - check whether or not a sink supports MST   @aux: The DP AUX channel to use   @dpcd: A cached copy of the DPCD capabilities for this sink     Returns: %True if the sink supports MST, %false otherwise ", "if (clear_payload_id_table) ": "drm_dp_mst_topology_mgr_set_mst(). Namely, the CableMatters USB-C   2x DP hub. Sending a CLEAR_PAYLOAD_ID_TABLE message seems to make   things work again. ", "void drm_dp_mst_topology_mgr_suspend(struct drm_dp_mst_topology_mgr *mgr)": "drm_dp_mst_topology_mgr_suspend() - suspend the MST manager   @mgr: manager to suspend     This function tells the MST device that we can't handle UP messages   anymore. This should stop it from sending any since we are suspended. ", "int drm_dp_mst_topology_mgr_resume(struct drm_dp_mst_topology_mgr *mgr,   bool sync)": "drm_dp_mst_topology_mgr_resume() - resume the MST manager   @mgr: manager to resume   @sync: whether or not to perform topology reprobing synchronously     This will fetch DPCD and see if the device is still there,   if it is, it will rewrite the MSTM control bits, and return.     If the device fails this returns -1, and the driver should do   a full MST reprobe, in case we were undocked.     During system resume (where it is assumed that the driver will be calling   drm_atomic_helper_resume()) this function should be called beforehand with   @sync set to true. In contexts like runtime resume where the driver is not   expected to be calling drm_atomic_helper_resume(), this function should be   called with @sync set to false in order to avoid deadlocking.     Returns: -1 if the MST topology was removed while we were suspended, 0   otherwise. ", "int drm_dp_mst_hpd_irq_handle_event(struct drm_dp_mst_topology_mgr *mgr, const u8 *esi,    u8 *ack, bool *handled)": "drm_dp_mst_hpd_irq_send_new_request() if the 'handled' is set   after calling this function, to try to kick off a new request in   the queue if the previous message transaction is completed.     See also:   drm_dp_mst_hpd_irq_send_new_request() ", "intdrm_dp_mst_detect_port(struct drm_connector *connector,       struct drm_modeset_acquire_ctx *ctx,       struct drm_dp_mst_topology_mgr *mgr,       struct drm_dp_mst_port *port)": "drm_dp_mst_detect_port() - get connection status for an MST port   @connector: DRM connector for this port   @ctx: The acquisition context to use for grabbing locks   @mgr: manager for this port   @port: pointer to a port     This returns the current connection state for a port. ", "const struct drm_edid *drm_dp_mst_edid_read(struct drm_connector *connector,    struct drm_dp_mst_topology_mgr *mgr,    struct drm_dp_mst_port *port)": "drm_dp_mst_edid_read() - get EDID for an MST port   @connector: toplevel connector to get EDID for   @mgr: manager for this port   @port: unverified pointer to a port.     This returns an EDID for the port connected to a connector,   It validates the pointer still exists so the caller doesn't require a   reference. ", "struct edid *drm_dp_mst_get_edid(struct drm_connector *connector, struct drm_dp_mst_topology_mgr *mgr, struct drm_dp_mst_port *port)": "drm_dp_mst_get_edid() - get EDID for an MST port   @connector: toplevel connector to get EDID for   @mgr: manager for this port   @port: unverified pointer to a port.     This function is deprecated; please use drm_dp_mst_edid_read() instead.     This returns an EDID for the port connected to a connector,   It validates the pointer still exists so the caller doesn't require a   reference. ", "int drm_dp_atomic_find_time_slots(struct drm_atomic_state *state,  struct drm_dp_mst_topology_mgr *mgr,  struct drm_dp_mst_port *port, int pbn)": "drm_dp_mst_atomic_check().     Additionally, it is OK to call this function multiple times on the same   @port as needed. It is not OK however, to call this function and   drm_dp_atomic_release_time_slots() in the same atomic check phase.     See also:   drm_dp_atomic_release_time_slots()   drm_dp_mst_atomic_check()     Returns:   Total slots in the atomic state assigned for this port, or a negative error   code if the port no longer exists ", "int drm_dp_mst_atomic_setup_commit(struct drm_atomic_state *state)": "drm_dp_mst_atomic_setup_commit() - setup_commit hook for MST helpers   @state: global atomic state     This function saves all of the &drm_crtc_commit structs in an atomic state that touch any CRTCs   currently assigned to an MST topology. Drivers must call this hook from their   &drm_mode_config_helper_funcs.atomic_commit_setup hook.     Returns:   0 if all CRTC commits were retrieved successfully, negative error code otherwise ", "void drm_dp_mst_atomic_wait_for_dependencies(struct drm_atomic_state *state)": "drm_dp_mst_atomic_wait_for_dependencies() - Wait for all pending commits on MST topologies,   prepare new MST state for commit   @state: global atomic state     Goes through any MST topologies in this atomic state, and waits for any pending commits which   touched CRTCs that wereare on an MST topology to be programmed to hardware and flipped to before   returning. This is to prevent multiple non-blocking commits affecting an MST topology from racing   with eachother by forcing them to be executed sequentially in situations where the only resources   the modeset objects in these commits share are an MST topology.     This function also prepares the new MST state for commit by performing some state preparation   which can't be done until this point, such as reading back the final VC start slots (which are   determined at commit-time) from the previous state.     All MST drivers must call this function after calling drm_atomic_helper_wait_for_dependencies(),   or whatever their equivalent of that is. ", "int drm_dp_mst_root_conn_atomic_check(struct drm_connector_state *new_conn_state,      struct drm_dp_mst_topology_mgr *mgr)": "drm_dp_mst_root_conn_atomic_check() - Serialize CRTC commits on MST-capable connectors operating   in SST mode   @new_conn_state: The new connector state of the &drm_connector   @mgr: The MST topology manager for the &drm_connector     Since MST uses fake &drm_encoder structs, the generic atomic modesetting code isn't able to   serialize non-blocking commits happening on the real DP connector of an MST topology switching   intoaway from MST mode - as the CRTC on the real DP connector and the CRTCs on the connector's   MST topology will never share the same &drm_encoder.     This function takes care of this serialization issue, by checking a root MST connector's atomic   state to determine if it is about to have a modeset - and then pulling in the MST topology state   if so, along with adding any relevant CRTCs to &drm_dp_mst_topology_state.pending_crtc_mask.     Drivers implementing MST must call this function from the   &drm_connector_helper_funcs.atomic_check hook of any physical DP &drm_connector capable of   driving MST sinks.     Returns:   0 on success, negative error code otherwise ", "void drm_dp_mst_update_slots(struct drm_dp_mst_topology_state *mst_state, uint8_t link_encoding_cap)": "drm_dp_mst_update_slots() - updates the slot info depending on the DP ecoding format   @mst_state: mst_state to update   @link_encoding_cap: the ecoding format on the link ", "int drm_dp_check_act_status(struct drm_dp_mst_topology_mgr *mgr)": "drm_dp_check_act_status() - Polls for ACT handled status.   @mgr: manager to use     Tries waiting for the MST hub to finish updating it's payload table by   polling for the ACT handled bit for up to 3 seconds (yes-some hubs really   take that long).     Returns:   0 if the ACT was handled in time, negative error code on failure. ", "int drm_dp_calc_pbn_mode(int clock, int bpp, bool dsc)": "drm_dp_calc_pbn_mode() - Calculate the PBN for a mode.   @clock: dot clock for the mode   @bpp: bpp for the mode.   @dsc: DSC mode. If true, bpp has units of 116 of a bit per pixel     This uses the formula in the spec to calculate the PBN value for a mode. ", "void drm_dp_mst_dump_topology(struct seq_file *m,      struct drm_dp_mst_topology_mgr *mgr)": "drm_dp_mst_dump_topology(): dump topology to seq file.   @m: seq_file to dump output to   @mgr: manager to dump current topology for.     helper to dump MST topology to a seq file for debugfs. ", "int drm_dp_mst_add_affected_dsc_crtcs(struct drm_atomic_state *state, struct drm_dp_mst_topology_mgr *mgr)": "drm_dp_mst_atomic_enable_dsc() ", "payload = drm_atomic_get_mst_payload_state(topology_state, port);if (payload) ": "drm_atomic_get_mst_topology_state(state, mgr);if (IS_ERR(topology_state))return PTR_ERR(topology_state);conn_state = drm_atomic_get_new_connector_state(state, port->connector);topology_state->pending_crtc_mask |= drm_crtc_mask(conn_state->crtc);  Find the current allocation for this port, if any ", "struct drm_dp_mst_topology_state *drm_atomic_get_old_mst_topology_state(struct drm_atomic_state *state,      struct drm_dp_mst_topology_mgr *mgr)": "drm_atomic_get_old_mst_topology_state: get old MST topology state in atomic state, if any   @state: global atomic state   @mgr: MST topology manager, also the private object in this case     This function wraps drm_atomic_get_old_private_obj_state() passing in the MST atomic   state vtable so that the private object state returned is that of a MST   topology object.     Returns:     The old MST topology state, or NULL if there's no topology state for this MST mgr   in the global atomic state ", "struct drm_dp_mst_topology_state *drm_atomic_get_new_mst_topology_state(struct drm_atomic_state *state,      struct drm_dp_mst_topology_mgr *mgr)": "drm_atomic_get_new_mst_topology_state: get new MST topology state in atomic state, if any   @state: global atomic state   @mgr: MST topology manager, also the private object in this case     This function wraps drm_atomic_get_new_private_obj_state() passing in the MST atomic   state vtable so that the private object state returned is that of a MST   topology object.     Returns:     The new MST topology state, or NULL if there's no topology state for this MST mgr   in the global atomic state ", "int drm_dp_mst_topology_mgr_init(struct drm_dp_mst_topology_mgr *mgr, struct drm_device *dev, struct drm_dp_aux *aux, int max_dpcd_transaction_bytes, int max_payloads, int conn_base_id)": "drm_dp_mst_topology_mgr_init - initialise a topology manager   @mgr: manager struct to initialise   @dev: device providing this structure - for i2c addition.   @aux: DP helper aux channel to talk to this device   @max_dpcd_transaction_bytes: hw specific DPCD transaction limit   @max_payloads: maximum number of payloads this GPU can source   @conn_base_id: the connector object ID the MST device is connected to.     Return 0 for success, or negative error code on failure ", "void drm_dp_mst_topology_mgr_destroy(struct drm_dp_mst_topology_mgr *mgr)": "drm_dp_mst_topology_mgr_destroy() - destroy topology manager.   @mgr: manager to destroy ", "int drm_dp_mst_atomic_enable_dsc(struct drm_atomic_state *state, struct drm_dp_mst_port *port, int pbn, bool enable)": "drm_dp_mst_dsc_aux_for_port(pos->port))continue;crtc_state = drm_atomic_get_crtc_state(mst_state->base.state, crtc);if (IS_ERR(crtc_state))return PTR_ERR(crtc_state);drm_dbg_atomic(mgr->dev, \"[MST MGR:%p] Setting mode_changed flag on CRTC %p\\n\",       mgr, crtc);crtc_state->mode_changed = true;}return 0;}EXPORT_SYMBOL(drm_dp_mst_add_affected_dsc_crtcs);     drm_dp_mst_atomic_enable_dsc - Set DSC Enable Flag to OnOff   @state: Pointer to the new drm_atomic_state   @port: Pointer to the affected MST Port   @pbn: Newly recalculated bw required for link with DSC enabled   @enable: Boolean flag to enable or disable DSC on the port     This function enables DSC on the given Port   by recalculating its vcpi from pbn provided   and sets dsc_enable flag to keep track of which   ports have DSC enabled   ", "int drm_hdmi_infoframe_set_hdr_metadata(struct hdmi_drm_infoframe *frame,const struct drm_connector_state *conn_state)": "drm_hdmi_infoframe_set_hdr_metadata() - fill an HDMI DRM infoframe with                                           HDR metadata from userspace   @frame: HDMI DRM infoframe   @conn_state: Connector state containing HDR metadata     Return: 0 on success or a negative error code on failure. ", "void drm_hdmi_avi_infoframe_colorimetry(struct hdmi_avi_infoframe *frame,const struct drm_connector_state *conn_state)": "drm_hdmi_avi_infoframe_colorimetry() - fill the HDMI AVI infoframe                                         colorimetry information   @frame: HDMI AVI infoframe   @conn_state: connector state ", "void drm_hdmi_avi_infoframe_bars(struct hdmi_avi_infoframe *frame, const struct drm_connector_state *conn_state)": "drm_hdmi_avi_infoframe_bars() - fill the HDMI AVI infoframe                                   bar information   @frame: HDMI AVI infoframe   @conn_state: connector state ", "void drm_hdmi_avi_infoframe_content_type(struct hdmi_avi_infoframe *frame, const struct drm_connector_state *conn_state)": "drm_hdmi_avi_infoframe_content_type() - fill the HDMI AVI infoframe                                           content type information, based                                           on correspondent DRM property.   @frame: HDMI AVI infoframe   @conn_state: DRM display connector state   ", "ssize_t drm_dp_dual_mode_read(struct i2c_adapter *adapter,      u8 offset, void *buffer, size_t size)": "drm_dp_dual_mode_read - Read from the DP dual mode adaptor register(s)   @adapter: I2C adapter for the DDC bus   @offset: register offset   @buffer: buffer for return data   @size: sizo of the buffer     Reads @size bytes from the DP dual mode adaptor registers   starting at @offset.     Returns:   0 on success, negative error code on failure ", "ssize_t drm_dp_dual_mode_write(struct i2c_adapter *adapter,       u8 offset, const void *buffer, size_t size)": "drm_dp_dual_mode_write - Write to the DP dual mode adaptor register(s)   @adapter: I2C adapter for the DDC bus   @offset: register offset   @buffer: buffer for write data   @size: sizo of the buffer     Writes @size bytes to the DP dual mode adaptor registers   starting at @offset.     Returns:   0 on success, negative error code on failure ", "enum drm_dp_dual_mode_type drm_dp_dual_mode_detect(const struct drm_device *dev,   struct i2c_adapter *adapter)": "drm_dp_dual_mode_detect - Identify the DP dual mode adaptor   @dev: &drm_device to use   @adapter: I2C adapter for the DDC bus     Attempt to identify the type of the DP dual mode adaptor used.     Note that when the answer is @DRM_DP_DUAL_MODE_UNKNOWN it's not   certain whether we're dealing with a native HDMI port or   a type 1 DVI dual mode adaptor. The driver will have to use   some other hardwaredriver specific mechanism to make that   distinction.     Returns:   The type of the DP dual mode adaptor used ", "int drm_dp_dual_mode_max_tmds_clock(const struct drm_device *dev, enum drm_dp_dual_mode_type type,    struct i2c_adapter *adapter)": "drm_dp_dual_mode_max_tmds_clock - Max TMDS clock for DP dual mode adaptor   @dev: &drm_device to use   @type: DP dual mode adaptor type   @adapter: I2C adapter for the DDC bus     Determine the max TMDS clock the adaptor supports based on the   type of the dual mode adaptor and the DP_DUAL_MODE_MAX_TMDS_CLOCK   register (on type2 adaptors). As some type 1 adaptors have   problems with registers (see comments in drm_dp_dual_mode_detect())   we don't read the register on those, instead we simply assume   a 165 MHz limit based on the specification.     Returns:   Maximum supported TMDS clock rate for the DP dual mode adaptor in kHz. ", "int drm_dp_dual_mode_get_tmds_output(const struct drm_device *dev,     enum drm_dp_dual_mode_type type, struct i2c_adapter *adapter,     bool *enabled)": "drm_dp_dual_mode_get_tmds_output - Get the state of the TMDS output buffers in the DP dual mode adaptor   @dev: &drm_device to use   @type: DP dual mode adaptor type   @adapter: I2C adapter for the DDC bus   @enabled: current state of the TMDS output buffers     Get the state of the TMDS output buffers in the adaptor. For   type2 adaptors this is queried from the DP_DUAL_MODE_TMDS_OEN   register. As some type 1 adaptors have problems with registers   (see comments in drm_dp_dual_mode_detect()) we don't read the   register on those, instead we simply assume that the buffers   are always enabled.     Returns:   0 on success, negative error code on failure ", "int drm_dp_dual_mode_set_tmds_output(const struct drm_device *dev, enum drm_dp_dual_mode_type type,     struct i2c_adapter *adapter, bool enable)": "drm_dp_dual_mode_set_tmds_output - Enabledisable TMDS output buffers in the DP dual mode adaptor   @dev: &drm_device to use   @type: DP dual mode adaptor type   @adapter: I2C adapter for the DDC bus   @enable: enable (as opposed to disable) the TMDS output buffers     Set the state of the TMDS output buffers in the adaptor. For   type2 this is set via the DP_DUAL_MODE_TMDS_OEN register.   Type1 adaptors do not support any register writes.     Returns:   0 on success, negative error code on failure ", "const char *drm_dp_get_dual_mode_type_name(enum drm_dp_dual_mode_type type)": "drm_dp_get_dual_mode_type_name - Get the name of the DP dual mode adaptor type as a string   @type: DP dual mode adaptor type     Returns:   String representation of the DP dual mode adaptor type ", "int drm_lspcon_get_mode(const struct drm_device *dev, struct i2c_adapter *adapter,enum drm_lspcon_mode *mode)": "drm_lspcon_get_mode: Get LSPCON's current mode of operation by   reading offset (0x80, 0x41)   @dev: &drm_device to use   @adapter: I2C-over-aux adapter   @mode: current lspcon mode of operation output variable     Returns:   0 on success, sets the current_mode value to appropriate mode   -error on failure ", "int drm_lspcon_set_mode(const struct drm_device *dev, struct i2c_adapter *adapter,enum drm_lspcon_mode mode)": "drm_lspcon_set_mode: Change LSPCON's mode of operation by   writing offset (0x80, 0x40)   @dev: &drm_device to use   @adapter: I2C-over-aux adapter   @mode: required mode of operation     Returns:   0 on success, -error on failuretimeout ", "ssize_t drm_scdc_read(struct i2c_adapter *adapter, u8 offset, void *buffer,      size_t size)": "drm_scdc_read - read a block of data from SCDC   @adapter: I2C controller   @offset: start offset of block to read   @buffer: return location for the block to read   @size: size of the block to read     Reads a block of data from SCDC, starting at a given offset.     Returns:   0 on success, negative error code on failure. ", "ssize_t drm_scdc_write(struct i2c_adapter *adapter, u8 offset,       const void *buffer, size_t size)": "drm_scdc_write - write a block of data to SCDC   @adapter: I2C controller   @offset: start offset of block to write   @buffer: block of data to write   @size: size of the block to write     Writes a block of data to SCDC, starting at a given offset.     Returns:   0 on success, negative error code on failure. ", "bool drm_scdc_get_scrambling_status(struct drm_connector *connector)": "drm_scdc_get_scrambling_status - what is status of scrambling?   @connector: connector     Reads the scrambler status over SCDC, and checks the   scrambling status.     Returns:   True if the scrambling is enabled, false otherwise. ", "bool drm_scdc_set_scrambling(struct drm_connector *connector,     bool enable)": "drm_scdc_set_scrambling - enable scrambling   @connector: connector   @enable: bool to indicate if scrambling is to be enableddisabled     Writes the TMDS config register over SCDC channel, and:   enables scrambling when enable = 1   disables scrambling when enable = 0     Returns:   True if scrambling is setreset successfully, false otherwise. ", "bool drm_scdc_set_high_tmds_clock_ratio(struct drm_connector *connector,bool set)": "drm_scdc_set_high_tmds_clock_ratio - set TMDS clock ratio   @connector: connector   @set: ret or reset the high clock ratio      TMDS clock ratio calculations go like this:  TMDS character = 10 bit TMDS encoded value    TMDS character rate = The rate at which TMDS characters are  transmitted (Mcsc)    TMDS bit rate = 10x TMDS character rate    As per the spec:  TMDS clock rate for pixel clock < 340 MHz = 1x the character  rate = 110 pixel clock rate    TMDS clock rate for pixel clock > 340 MHz = 0.25x the character  rate = 140 pixel clock rate    Writes to the TMDS config register over SCDC channel, and:  sets TMDS clock ratio to 140 when set = 1    sets TMDS clock ratio to 110 when set = 0     Returns:   True if write is successful, false otherwise. ", "void drm_dp_cec_irq(struct drm_dp_aux *aux)": "drm_dp_cec_irq() - handle CEC interrupt, if any   @aux: DisplayPort AUX channel     Should be called when handling an IRQ_HPD request. If CEC-tunneling-over-AUX   is present, then it will check for a CEC_IRQ and handle it accordingly. ", "if (!aux->transfer)return;#ifndef CONFIG_MEDIA_CEC_RC/* * CEC_CAP_RC is part of CEC_CAP_DEFAULTS, but it is stripped by * cec_allocate_adapter() if CONFIG_MEDIA_CEC_RC is undefined. * * Do this here as well to ensure the tests against cec_caps are * correct. ": "drm_dp_cec_set_edid(struct drm_dp_aux  aux, const struct edid  edid){struct drm_connector  connector = aux->cec.connector;u32 cec_caps = CEC_CAP_DEFAULTS | CEC_CAP_NEEDS_HPD |       CEC_CAP_CONNECTOR_INFO;struct cec_connector_info conn_info;unsigned int num_las = 1;u8 cap;  No transfer function was set, so not a DP connector ", "if (!aux->transfer)return;cancel_delayed_work_sync(&aux->cec.unregister_work);mutex_lock(&aux->cec.lock);if (!aux->cec.adap)goto unlock;cec_phys_addr_invalidate(aux->cec.adap);/* * We're done if we want to keep the CEC device * (drm_dp_cec_unregister_delay is >= NEVER_UNREG_DELAY) or if the * DPCD still indicates the CEC capability (expected for an integrated * HDMI branch device). ": "drm_dp_cec_unset_edid(struct drm_dp_aux  aux){  No transfer function was set, so not a DP connector ", "cec_s_phys_addr_from_edid(aux->cec.adap, edid);}unlock:mutex_unlock(&aux->cec.lock);}EXPORT_SYMBOL(drm_dp_cec_set_edid);/* * The EDID disappeared (likely because of the HPD going down). ": "drm_dp_cec_register_connector() edid == NULL, so in   that case the phys addr is just invalidated. ", "void drm_dp_cec_unregister_connector(struct drm_dp_aux *aux)": "drm_dp_cec_unregister_connector() - unregister the CEC adapter, if any   @aux: DisplayPort AUX channel ", "if (!mbox->intr_type)bit = mbox_read_reg(mbox->parent, irqdisable) & ~bit;mbox_write_reg(mbox->parent, bit, irqdisable);}void omap_mbox_enable_irq(struct mbox_chan *chan, omap_mbox_irq_t irq)": "omap_mbox_disable_irq(struct omap_mbox  mbox, omap_mbox_irq_t irq){struct omap_mbox_fifo  fifo = (irq == IRQ_TX) ?&mbox->tx_fifo : &mbox->rx_fifo;u32 bit = fifo->intr_bit;u32 irqdisable = fifo->irqdisable;    Read and update the interrupt configuration register for pre-OMAP4.   OMAP4 and later SoCs have a dedicated interrupt disabling register. ", "int zorro_register_driver(struct zorro_driver *drv)": "zorro_register_driver - register a new Zorro driver        @drv: the driver structure to register              Adds the driver structure to the list of registered drivers        Returns zero or a negative error value.     ", "void zorro_unregister_driver(struct zorro_driver *drv)": "zorro_unregister_driver - unregister a zorro driver        @drv: the driver structure to unregister              Deletes the driver structure from the list of registered Zorro drivers,        gives it a chance to clean up by calling its remove() function for        each device it was responsible for, and marks those devices as        driverless.     ", "return driver_register(&drv->driver);}EXPORT_SYMBOL(zorro_register_driver);    /**     *  zorro_unregister_driver - unregister a zorro driver     *  @drv: the driver structure to unregister     *     *  Deletes the driver structure from the list of registered Zorro drivers,     *  gives it a chance to clean up by calling its remove() function for     *  each device it was responsible for, and marks those devices as     *  driverless.     ": "zorro_bus_type;  register with core ", "if (!ops->get_timing)ops->get_timing = sa1100_pcmcia_default_mecr_timing;/* Provide our SA11x0 specific timing routines. ": "sa11xx_drv_pcmcia_ops(struct pcmcia_low_level  ops){    set default MECR calculation if the board specific   code did not specify one... ", "for (i = 0; i < nr; i++) ": "sa11xx_drv_pcmcia_probe(struct device  dev, struct pcmcia_low_level  ops,    int first, int nr){struct skt_dev_info  sinfo;struct soc_pcmcia_socket  skt;int i, ret = 0;struct clk  clk;clk = devm_clk_get(dev, NULL);if (IS_ERR(clk))return PTR_ERR(clk);sa11xx_drv_pcmcia_ops(ops);sinfo = devm_kzalloc(dev, SKT_DEV_INFO_SIZE(nr), GFP_KERNEL);if (!sinfo)return -ENOMEM;sinfo->nskt = nr;  Initialize processor specific parameters ", "int pcmcia_register_driver(struct pcmcia_driver *driver)": "pcmcia_register_driver - register a PCMCIA driver with the bus core   @driver: the &driver being registered     Registers a PCMCIA driver with the PCMCIA bus core. ", "void pcmcia_unregister_driver(struct pcmcia_driver *driver)": "pcmcia_unregister_driver - unregister a PCMCIA driver with the bus core   @driver: the &driver being unregistered ", "struct pcmcia_device *pcmcia_dev_present(struct pcmcia_device *_p_dev)": "pcmcia_dev_present   returns NULL is probably really really small. ", "int pcmcia_register_socket(struct pcmcia_socket *socket)": "pcmcia_register_socket - add a new pcmcia socket device   @socket: the &socket to register ", "void pcmcia_unregister_socket(struct pcmcia_socket *socket)": "pcmcia_unregister_socket - remove a pcmcia socket device   @socket: the &socket to unregister ", "request_module_nowait(\"pcmcia\");return 0; err:down_write(&pcmcia_socket_list_rwsem);list_del(&socket->socket_list);up_write(&pcmcia_socket_list_rwsem);return ret;} /* pcmcia_register_socket ": "pcmcia_parse_events(socket, SS_DETECT);    Let's try to get the PCMCIA module for 16-bit PCMCIA support.   If it fails, it doesn't matter -- we still have 32-bit CardBus   support to offer, so this is not a failure mode. ", "void pcmcia_parse_uevents(struct pcmcia_socket *s, u_int events)": "pcmcia_parse_uevents() - tell pccardd to issue manual commands   @s:the PCMCIA socket we wan't to command   @events:events to pass to pccardd     userspace-issued insert, eject, suspend and resume commands must be   handled by pccardd to avoid any sysfs-related deadlocks. Valid events   are PCMCIA_UEVENT_EJECT (for eject), PCMCIA_UEVENT__INSERT (for insert),   PCMCIA_UEVENT_RESUME (for resume), PCMCIA_UEVENT_SUSPEND (for suspend)   and PCMCIA_UEVENT_REQUERY (for re-querying the PCMCIA card). ", "mutex_lock(&s->skt_mutex);if (c) ": "pccard_register_pcmcia(struct pcmcia_socket  s, struct pcmcia_callback  c){int ret = 0;  s->skt_mutex also protects s->callback ", "EXPORT_SYMBOL(pcmcia_reset_card": "pcmcia_reset_card(struct pcmcia_socket  skt){int ret;dev_dbg(&skt->dev, \"resetting socket\\n\");mutex_lock(&skt->skt_mutex);do {if (!(skt->state & SOCKET_PRESENT)) {dev_dbg(&skt->dev, \"can't reset, not present\\n\");ret = -ENODEV;break;}if (skt->state & SOCKET_SUSPEND) {dev_dbg(&skt->dev, \"can't reset, suspended\\n\");ret = -EBUSY;break;}if (skt->state & SOCKET_CARDBUS) {dev_dbg(&skt->dev, \"can't reset, is cardbus\\n\");ret = -EPERM;break;}if (skt->callback)skt->callback->suspend(skt);mutex_lock(&skt->ops_mutex);ret = socket_reset(skt);mutex_unlock(&skt->ops_mutex);if ((ret == 0) && (skt->callback))skt->callback->resume(skt);ret = 0;} while (0);mutex_unlock(&skt->skt_mutex);return ret;}   reset_card ", "socket->cis_mem.flags = 0;socket->cis_mem.speed = cis_speed;INIT_LIST_HEAD(&socket->cis_cache);init_completion(&socket->socket_released);init_completion(&socket->thread_done);mutex_init(&socket->skt_mutex);mutex_init(&socket->ops_mutex);spin_lock_init(&socket->thread_lock);if (socket->resource_ops->init) ": "pcmcia_socket_class;dev_set_name(&socket->dev, \"pcmcia_socket%u\", socket->sock);  base address = 0, map = 0 ", "ops->set_timing  = pxa2xx_pcmcia_set_timing;#ifdef CONFIG_CPU_FREQops->frequency_change = pxa2xx_pcmcia_frequency_change;#endif}EXPORT_SYMBOL(pxa2xx_drv_pcmcia_ops": "pxa2xx_drv_pcmcia_ops(struct pcmcia_low_level  ops){  Provide our PXA2xx specific timing routines. ", "static int pcmcia_do_loop_config(tuple_t *tuple, cisparse_t *parse, void *priv)": "pcmcia_loop_config()     pcmcia_do_loop_config() is the internal callback for the call from   pcmcia_loop_config() to pccard_loop_tuple(). Data is transferred   by a struct pcmcia_cfg_mem. ", "static int pcmcia_do_loop_tuple(tuple_t *tuple, cisparse_t *parse, void *priv)": "pcmcia_loop_tuple() to pccard_loop_tuple(). Data is transferred   by a struct pcmcia_cfg_mem. ", "static int pcmcia_do_get_tuple(struct pcmcia_device *p_dev, tuple_t *tuple,       void *priv)": "pcmcia_get_tuple()     pcmcia_do_get_tuple() is the internal callback for the call from   pcmcia_get_tuple() to pcmcia_loop_tuple(). As we're only interested in   the first tuple, return 0 unconditionally. Create a memory buffer large   enough to hold the content of the tuple, and fill it with the tuple data.   The caller is responsible to free the buffer. ", "static int pcmcia_do_get_mac(struct pcmcia_device *p_dev, tuple_t *tuple,     void *priv)": "pcmcia_get_mac_from_cis()     pcmcia_do_get_mac() is the internal callback for the call from   pcmcia_get_mac_from_cis() to pcmcia_loop_tuple(). We check whether the   tuple contains a proper LAN_NODE_ID of length 6, and copy the data   to struct net_device->dev_addr[i]. ", "soc_common_pcmcia_config_skt(skt, &dead_socket);iounmap(PCI_IOBASE + skt->res_io_io.start);release_resource(&skt->res_attr);release_resource(&skt->res_mem);release_resource(&skt->res_io);release_resource(&skt->res_skt);}EXPORT_SYMBOL(soc_pcmcia_remove_one": "soc_pcmcia_remove_one(struct soc_pcmcia_socket  skt){del_timer_sync(&skt->poll_timer);pcmcia_unregister_socket(&skt->socket);#ifdef CONFIG_CPU_FREQif (skt->ops->frequency_change)cpufreq_unregister_notifier(&skt->cpufreq_nb,    CPUFREQ_TRANSITION_NOTIFIER);#endifsoc_pcmcia_hw_shutdown(skt);  should not be required; violates some lowlevel drivers ", "skt->ops->set_timing(skt);ret = soc_pcmcia_hw_init(skt);if (ret)goto out_err_6;skt->socket.ops = &soc_common_pcmcia_operations;skt->socket.features = SS_CAP_STATIC_MAP|SS_CAP_PCCARD;skt->socket.resource_ops = &pccard_static_ops;skt->socket.irq_mask = 0;skt->socket.map_size = PAGE_SIZE;skt->socket.io_offset = (unsigned long)skt->res_io_io.start;skt->status = soc_common_pcmcia_skt_state(skt);#ifdef CONFIG_CPU_FREQif (skt->ops->frequency_change) ": "soc_pcmcia_add_one(struct soc_pcmcia_socket  skt){int ret;skt->cs_state = dead_socket;timer_setup(&skt->poll_timer, soc_common_pcmcia_poll_event, 0);skt->poll_timer.expires = jiffies + SOC_PCMCIA_POLL_PERIOD;ret = request_resource(&iomem_resource, &skt->res_skt);if (ret)goto out_err_1;ret = request_resource(&skt->res_skt, &skt->res_io);if (ret)goto out_err_2;ret = request_resource(&skt->res_skt, &skt->res_mem);if (ret)goto out_err_3;ret = request_resource(&skt->res_skt, &skt->res_attr);if (ret)goto out_err_4;skt->res_io_io = (struct resource) DEFINE_RES_IO_NAMED(skt->nr   0x1000 + 0x10000, 0x1000,     \"PCMCIA IO\");ret = pci_remap_iospace(&skt->res_io_io, skt->res_io.start);if (ret)goto out_err_5;    We initialize default socket timing here, because   we are not guaranteed to see a SetIOMap operation at   runtime. ", "static int pcmcia_access_config(struct pcmcia_device *p_dev,off_t where, u8 *val,int (*accessf) (struct pcmcia_socket *s,int attr, unsigned int addr,unsigned int len, void *ptr))": "pcmcia_write_config_byte(). ", "int pcmcia_map_mem_page(struct pcmcia_device *p_dev, struct resource *res,unsigned int offset)": "pcmcia_request_window()   @offset: card_offset to map     pcmcia_map_mem_page() modifies what can be read and written by accessing   an iomem range previously enabled by pcmcia_request_window(), by setting   the card_offset value to @offset. ", "int pcmcia_fixup_iowidth(struct pcmcia_device *p_dev)": "pcmcia_enable_device()   previously. ", "int pcmcia_fixup_vpp(struct pcmcia_device *p_dev, unsigned char new_vpp)": "pcmcia_disable_device(). ", "int pcmcia_release_configuration(struct pcmcia_device *p_dev)": "pcmcia_release_window() still need to be called, device drivers are   expected to call pcmcia_disable_device() instead. ", "int pcmcia_request_io(struct pcmcia_device *p_dev)": "pcmcia_request_io() - attempt to reserve port ranges for PCMCIA devices   @p_dev: the associated PCMCIA device     pcmcia_request_io() attempts to reserve the IO port ranges specified in   &struct pcmcia_device @p_dev->resource[0] and @p_dev->resource[1]. The   \"start\" value is the requested start of the IO port resource; \"end\"   reflects the number of ports requested. The number of IO lines requested   is specified in &struct pcmcia_device @p_dev->io_lines. ", "int __must_check pcmcia_request_irq(struct pcmcia_device *p_dev,    irq_handler_t handler)": "pcmcia_request_irq() - attempt to request a IRQ for a PCMCIA device   @p_dev: the associated PCMCIA device   @handler: IRQ handler to register     pcmcia_request_irq() is a wrapper around request_irq() which allows   the PCMCIA core to clean up the registration in pcmcia_disable_device().   Drivers are free to use request_irq() directly, but then they need to   call free_irq() themselfves, too. Also, only %IRQF_SHARED capable IRQ   handlers are allowed. ", "unsigned int cpufreq_quick_get(unsigned int cpu)": "cpufreq_quick_get - get the CPU frequency (in kHz) from policy->cur   @cpu: CPU number     This is the last known freq, without actually getting it from the driver.   Return value will be same as what is shown in scaling_cur_freq in sysfs. ", "unsigned int cpufreq_quick_get_max(unsigned int cpu)": "cpufreq_quick_get_max - get the max reported CPU frequency for this CPU   @cpu: CPU number     Just return the max possible frequency for a given CPU. ", "__weak unsigned int cpufreq_get_hw_max_freq(unsigned int cpu)": "cpufreq_get_hw_max_freq - get the max hardware frequency of the CPU   @cpu: CPU number     The default return value is the max_freq field of cpuinfo. ", "static BLOCKING_NOTIFIER_HEAD(cpufreq_policy_notifier_list);SRCU_NOTIFIER_HEAD_STATIC(cpufreq_transition_notifier_list);static int off __read_mostly;static int cpufreq_disabled(void)": "cpufreq_get(struct cpufreq_policy  policy);static int cpufreq_init_governor(struct cpufreq_policy  policy);static void cpufreq_exit_governor(struct cpufreq_policy  policy);static void cpufreq_governor_limits(struct cpufreq_policy  policy);static int cpufreq_set_policy(struct cpufreq_policy  policy,      struct cpufreq_governor  new_gov,      unsigned int new_pol);    Two notifier lists: the \"policy\" list is involved in the   validation process for a new CPU frequency policy; the   \"transition\" list for kernel code that needs to handle   changes to devices when the CPU clock speed changes.   The mutex locks both lists. ", "int cpufreq_register_notifier(struct notifier_block *nb, unsigned int list)": "cpufreq_register_notifier - Register a notifier with cpufreq.   @nb: notifier function to register.   @list: CPUFREQ_TRANSITION_NOTIFIER or CPUFREQ_POLICY_NOTIFIER.     Add a notifier to one of two lists: either a list of notifiers that run on   clock rate changes (once before and once after every transition), or a list   of notifiers that ron on cpufreq policy changes.     This function may sleep and it has the same return values as   blocking_notifier_chain_register(). ", "int cpufreq_unregister_notifier(struct notifier_block *nb, unsigned int list)": "cpufreq_unregister_notifier - Unregister a notifier from cpufreq.   @nb: notifier block to be unregistered.   @list: CPUFREQ_TRANSITION_NOTIFIER or CPUFREQ_POLICY_NOTIFIER.     Remove a notifier from one of the cpufreq notifier lists.     This function may sleep and it has the same return values as   blocking_notifier_chain_unregister(). ", "int cpufreq_get_policy(struct cpufreq_policy *policy, unsigned int cpu)": "cpufreq_get_policy - get the current cpufreq_policy   @policy: struct cpufreq_policy into which the current cpufreq_policy  is written   @cpu: CPU to find the policy for     Reads the current cpufreq policy. ", "static void cpufreq_out_of_sync(struct cpufreq_policy *policy,unsigned int new_freq)": "cpufreq_update_policy(), or scheduling handle_update(). ", "struct sync_file *sync_file_create(struct dma_fence *fence)": "sync_file_create() - creates a sync file   @fence:fence to add to the sync_fence     Creates a sync_file containg @fence. This function acquires and additional   reference of @fence for the newly-created &sync_file, if it succeeds. The   sync_file can be released with fput(sync_file->file). Returns the   sync_file or NULL in case of error. ", "struct dma_fence *sync_file_get_fence(int fd)": "sync_file_get_fence - get the fence related to the sync_file fd   @fd:sync_file fd to get the fence from     Ensures @fd references a valid sync_file and returns a fence that   represents all fence in the sync_file. On error NULL is returned. ", "void dma_resv_init(struct dma_resv *obj)": "dma_resv_init - initialize a reservation object   @obj: the reservation object ", "void dma_resv_fini(struct dma_resv *obj)": "dma_resv_fini - destroys a reservation object   @obj: the reservation object ", "int dma_resv_reserve_fences(struct dma_resv *obj, unsigned int num_fences)": "dma_resv_add_fence().  Must be called with @obj   locked through dma_resv_lock().     Note that the preallocated slots need to be re-reserved if @obj is unlocked   at any time before calling dma_resv_add_fence(). This is validated when   CONFIG_DEBUG_MUTEXES is enabled.     RETURNS   Zero for success, or -errno ", "void dma_resv_reset_max_fences(struct dma_resv *obj)": "dma_resv_reset_max_fences - reset fences for debugging   @obj: the dma_resv object to reset     Reset the number of pre-reserved fence slots to test that drivers do   correct slot allocation using dma_resv_reserve_fences(). See also   &dma_resv_list.max_fences. ", "void dma_resv_replace_fences(struct dma_resv *obj, uint64_t context,     struct dma_fence *replacement,     enum dma_resv_usage usage)": "dma_resv_replace_fences - replace fences in the dma_resv obj   @obj: the reservation object   @context: the context of the fences to replace   @replacement: the new fence to use instead   @usage: how the new fence is used, see enum dma_resv_usage     Replace fences with a specified context with a new fence. Only valid if the   operation represented by the original fence has no longer access to the   resources represented by the dma_resv object when the new fence completes.     And example for using this is replacing a preemption fence with a page table   update fence which makes the resource inaccessible. ", "struct dma_fence *dma_resv_iter_first_unlocked(struct dma_resv_iter *cursor)": "dma_resv_iter_next_unlocked().     Beware that the iterator can be restarted.  Code which accumulates statistics   or similar needs to check for this with dma_resv_iter_is_restarted(). For   this reason prefer the locked dma_resv_iter_first() whenver possible.     Returns the first fence from an unlocked dma_resv obj. ", "int dma_resv_copy_fences(struct dma_resv *dst, struct dma_resv *src)": "dma_resv_copy_fences - Copy all fences from src to dst.   @dst: the destination reservation object   @src: the source reservation object     Copy all fences from src to dst. dst-lock must be held. ", "struct dma_fence *dma_fence_get_stub(void)": "dma_fence_get_stub - return a signaled fence     Return a stub fence which is already signaled. The fence's   timestamp corresponds to the first time after boot this   function is called. ", "struct dma_fence *dma_fence_allocate_private_stub(ktime_t timestamp)": "dma_fence_signal_locked(&dma_fence_stub);}spin_unlock(&dma_fence_stub_lock);return dma_fence_get(&dma_fence_stub);}EXPORT_SYMBOL(dma_fence_get_stub);     dma_fence_allocate_private_stub - return a private, signaled fence   @timestamp: timestamp when the fence was signaled     Return a newly allocated and signaled stub fence. ", "/** * DOC: fence cross-driver contract * * Since &dma_fence provide a cross driver contract, all drivers must follow the * same rules: * * * Fences must complete in a reasonable time. Fences which represent kernels *   and shaders submitted by userspace, which could run forever, must be backed *   up by timeout and gpu hang recovery code. Minimally that code must prevent *   further command submission and force complete all in-flight fences, e.g. *   when the driver or hardware do not support gpu reset, or if the gpu reset *   failed for some reason. Ideally the driver supports gpu recovery which only *   affects the offending userspace context, and no other userspace *   submissions. * * * Drivers may have different ideas of what completion within a reasonable *   time means. Some hang recovery code uses a fixed timeout, others a mix *   between observing forward progress and increasingly strict timeouts. *   Drivers should not try to second guess timeout handling of fences from *   other drivers. * * * To ensure there's no deadlocks of dma_fence_wait() against other locks *   drivers should annotate all code required to reach dma_fence_signal(), *   which completes the fences, with dma_fence_begin_signalling() and *   dma_fence_end_signalling(). * * * Drivers are allowed to call dma_fence_wait() while holding dma_resv_lock(). *   This means any code required for fence completion cannot acquire a *   &dma_resv lock. Note that this also pulls in the entire established *   locking hierarchy around dma_resv_lock() and dma_resv_unlock(). * * * Drivers are allowed to call dma_fence_wait() from their &shrinker *   callbacks. This means any code required for fence completion cannot *   allocate memory with GFP_KERNEL. * * * Drivers are allowed to call dma_fence_wait() from their &mmu_notifier *   respectively &mmu_interval_notifier callbacks. This means any code required *   for fence completeion cannot allocate memory with GFP_NOFS or GFP_NOIO. *   Only GFP_ATOMIC is permissible, which might fail. * * Note that only GPU drivers have a reasonable excuse for both requiring * &mmu_interval_notifier and &shrinker callbacks at the same time as having to * track asynchronous compute work using &dma_fence. No driver outside of * drivers/gpu should ever call dma_fence_wait() in such contexts. ": "dma_fence_init() and completed using   dma_fence_signal(). Fences are associated with a context, allocated through   dma_fence_context_alloc(), and all fences on the same context are   fully ordered.     Since the purposes of fences is to facilitate cross-device and   cross-application synchronization, there's multiple ways to use one:     - Individual fences can be exposed as a &sync_file, accessed as a file     descriptor from userspace, created by calling sync_file_create(). This is     called explicit fencing, since userspace passes around explicit     synchronization points.     - Some subsystems also have their own explicit fencing primitives, like     &drm_syncobj. Compared to &sync_file, a &drm_syncobj allows the underlying     fence to be updated.     - Then there's also implicit fencing, where the synchronization points are     implicitly passed around as part of shared &dma_buf instances. Such     implicit fences are stored in &struct dma_resv through the     &dma_buf.resv pointer. ", "static const char *dma_fence_stub_get_name(struct dma_fence *fence)": "dma_fence_end_signalling().       Drivers are allowed to call dma_fence_wait() while holding dma_resv_lock().     This means any code required for fence completion cannot acquire a     &dma_resv lock. Note that this also pulls in the entire established     locking hierarchy around dma_resv_lock() and dma_resv_unlock().       Drivers are allowed to call dma_fence_wait() from their &shrinker     callbacks. This means any code required for fence completion cannot     allocate memory with GFP_KERNEL.       Drivers are allowed to call dma_fence_wait() from their &mmu_notifier     respectively &mmu_interval_notifier callbacks. This means any code required     for fence completeion cannot allocate memory with GFP_NOFS or GFP_NOIO.     Only GFP_ATOMIC is permissible, which might fail.     Note that only GPU drivers have a reasonable excuse for both requiring   &mmu_interval_notifier and &shrinker callbacks at the same time as having to   track asynchronous compute work using &dma_fence. No driver outside of   driversgpu should ever call dma_fence_wait() in such contexts. ", "int dma_fence_signal_timestamp_locked(struct dma_fence *fence,      ktime_t timestamp)": "dma_fence_add_callback(). Can be called multiple times, but since a fence   can only go from the unsignaled to the signaled state and not back, it will   only be effective the first time. Set the timestamp provided as the fence   signal timestamp.     Unlike dma_fence_signal_timestamp(), this function must be called with   &dma_fence.lock held.     Returns 0 on success and a negative error value when @fence has been   signalled already. ", "u64 dma_fence_context_alloc(unsigned num)": "dma_fence_signal_timestamp(fence, timestamp);return fence;}EXPORT_SYMBOL(dma_fence_allocate_private_stub);     dma_fence_context_alloc - allocate an array of fence contexts   @num: amount of contexts to allocate     This function will return the first index of the number of fence contexts   allocated.  The fence context is used for setting &dma_fence.context to a   unique number by passing the context to dma_fence_init(). ", "static atomic64_t dma_fence_context_counter = ATOMIC64_INIT(1);/** * DOC: DMA fences overview * * DMA fences, represented by &struct dma_fence, are the kernel internal * synchronization primitive for DMA operations like GPU rendering, video * encoding/decoding, or displaying buffers on a screen. * * A fence is initialized using dma_fence_init() and completed using * dma_fence_signal(). Fences are associated with a context, allocated through * dma_fence_context_alloc(), and all fences on the same context are * fully ordered. * * Since the purposes of fences is to facilitate cross-device and * cross-application synchronization, there's multiple ways to use one: * * - Individual fences can be exposed as a &sync_file, accessed as a file *   descriptor from userspace, created by calling sync_file_create(). This is *   called explicit fencing, since userspace passes around explicit *   synchronization points. * * - Some subsystems also have their own explicit fencing primitives, like *   &drm_syncobj. Compared to &sync_file, a &drm_syncobj allows the underlying *   fence to be updated. * * - Then there's also implicit fencing, where the synchronization points are *   implicitly passed around as part of shared &dma_buf instances. Such *   implicit fences are stored in &struct dma_resv through the *   &dma_buf.resv pointer. ": "dma_fence_signaled);static DEFINE_SPINLOCK(dma_fence_stub_lock);static struct dma_fence dma_fence_stub;    fence context counter: each execution context should have its own   fence context, this allows checking if fences belong to the same   context or not. One device can have multiple separate contexts,   and they're used if some engine can run independently of another. ", "signed longdma_fence_wait_timeout(struct dma_fence *fence, bool intr, signed long timeout)": "dma_fence_wait_any_timeout(). ", "void dma_fence_release(struct kref *kref)": "dma_fence_default_wait(fence, intr, timeout);trace_dma_fence_wait_end(fence);return ret;}EXPORT_SYMBOL(dma_fence_wait_timeout);     dma_fence_release - default relese function for fences   @kref: &dma_fence.recfount     This is the default release functions for &dma_fence. Drivers shouldn't call   this directly, but instead call dma_fence_put(). ", "void dma_fence_free(struct dma_fence *fence)": "dma_fence_free(fence);}EXPORT_SYMBOL(dma_fence_release);     dma_fence_free - default release function for &dma_fence.   @fence: fence to release     This is the default implementation for &dma_fence_ops.release. It calls   kfree_rcu() on @fence. ", "int dma_fence_get_status(struct dma_fence *fence)": "dma_fence_get_status - returns the status upon completion   @fence: the dma_fence to query     This wraps dma_fence_get_status_locked() to return the error status   condition on a signaled fence. See dma_fence_get_status_locked() for more   details.     Returns 0 if the fence has not yet been signaled, 1 if the fence has   been signaled without an error condition, or a negative error code   if the fence has been completed in err. ", "booldma_fence_remove_callback(struct dma_fence *fence, struct dma_fence_cb *cb)": "dma_fence_remove_callback - remove a callback from the signaling list   @fence: the fence to wait on   @cb: the callback to remove     Remove a previously queued callback from the fence. This function returns   true if the callback is successfully removed, or false if the fence has   already been signaled.      WARNING :   Cancelling a callback should only be done if you really know what you're   doing, since deadlocks and race conditions could occur all too easily. For   this reason, it should only ever be done on hardware lockup recovery,   with a reference held to the fence.     Behaviour is undefined if @cb has not been added to @fence using   dma_fence_add_callback() beforehand. ", "/** * dma_fence_set_deadline - set desired fence-wait deadline hint * @fence:    the fence that is to be waited on * @deadline: the time by which the waiter hopes for the fence to be *            signaled * * Give the fence signaler a hint about an upcoming deadline, such as * vblank, by which point the waiter would prefer the fence to be * signaled by.  This is intended to give feedback to the fence signaler * to aid in power management decisions, such as boosting GPU frequency * if a periodic vblank deadline is approaching but the fence is not * yet signaled.. ": "dma_fence_set_deadline.   The deadline hint provides a way for the waiting driver, or userspace, to   convey an appropriate sense of urgency to the signaling driver.     A deadline hint is given in absolute ktime (CLOCK_MONOTONIC for userspace   facing APIs).  The time could either be some point in the future (such as   the vblank based deadline for page-flipping, or the start of a compositor's   composition cycle), or the current time to indicate an immediate deadline   hint (Ie. forward progress cannot be made until this fence is signaled).     Multiple deadlines may be set on a given fence, even in parallel.  See the   documentation for &dma_fence_ops.set_deadline.     The deadline hint is just that, a hint.  The driver that created the fence   may react by increasing frequency, making different scheduling choices, etc.   Or doing nothing at all. ", "void dma_fence_describe(struct dma_fence *fence, struct seq_file *seq)": "dma_fence_describe - Dump fence describtion into seq_file   @fence: the 6fence to describe   @seq: the seq_file to put the textual description into     Dump a textual description of the fence and it's state into the seq_file. ", "struct dma_fence *dma_fence_chain_walk(struct dma_fence *fence)": "dma_fence_chain_walk - chain walking function   @fence: current chain node     Walk the chain to the next node. Returns the next fence or NULL if we are at   the end of the chain. Garbage collects chain nodes which are already   signaled. ", "int dma_fence_chain_find_seqno(struct dma_fence **pfence, uint64_t seqno)": "dma_fence_chain_find_seqno - find fence chain node by seqno   @pfence: pointer to the chain node where to start   @seqno: the sequence number to search for     Advance the fence pointer to the chain node which will signal this sequence   number. If no sequence number is provided then this is a no-op.     Returns EINVAL if the fence is not a chain node or the sequence number has   not yet advanced far enough. ", "void dma_fence_chain_init(struct dma_fence_chain *chain,  struct dma_fence *prev,  struct dma_fence *fence,  uint64_t seqno)": "dma_fence_chain_init - initialize a fence chain   @chain: the chain node to initialize   @prev: the previous fence   @fence: the current fence   @seqno: the sequence number to use for the fence chain     Initialize a new chain node and either start a new chain or add the node to   the existing chain of the previous fence. ", "struct dma_fence_array *dma_fence_array_create(int num_fences,       struct dma_fence **fences,       u64 context, unsigned seqno,       bool signal_on_any)": "dma_fence_array_create - Create a custom fence array   @num_fences:[in]number of fences to add in the array   @fences:[in]array containing the fences   @context:[in]fence context to use   @seqno:[in]sequence number to use   @signal_on_any:[in]signal on any fence in the array     Allocate a dma_fence_array object and initialize the base fence with   dma_fence_init().   In case of error it returns NULL.     The caller should allocate the fences array with num_fences size   and fill it with the fences it wants to add to the object. Ownership of this   array is taken and dma_fence_put() is used on each fence on release.     If @signal_on_any is true the fence array signals if any fence in the array   signals, otherwise it signals when all fences in the array signal. ", "bool dma_fence_match_context(struct dma_fence *fence, u64 context)": "dma_fence_match_context - Check if all fences are from the given context   @fence:[in]fence or fence array   @context:[in]fence context to check all fences against     Checks the provided fence or, for a fence array, all fences in the array   against the given context. Returns false if any fence is from a different   context. ", "/* compute in uV, round to mV ": "vid_from_reg(int val, u8 vrm){int vid;switch (vrm) {case 100:  VRD 10.0 ", "return 0;/* doesn't have VID ": "vid_which_vrm(void){struct cpuinfo_x86  c = &cpu_data(0);u8 vrm_ret;if (c->x86 < 6)  Any CPU with family lower than 6 ", "lsb = sch56xx_read_virtual_reg(addr, reg);if (lsb < 0)return lsb;msb = sch56xx_read_virtual_reg(addr, reg + 1);if (msb < 0)return msb;return lsb | (msb << 8);}EXPORT_SYMBOL(sch56xx_read_virtual_reg16": "sch56xx_read_virtual_reg16(u16 addr, u16 reg){int lsb, msb;  Read LSB first, this will cause the matching MSB to be latched ", "msb = sch56xx_read_virtual_reg(addr, msb_reg);if (msb < 0)return msb;lsn = sch56xx_read_virtual_reg(addr, lsn_reg);if (lsn < 0)return lsn;if (high_nibble)return (msb << 4) | (lsn >> 4);elsereturn (msb << 4) | (lsn & 0x0f);}EXPORT_SYMBOL(sch56xx_read_virtual_reg12": "sch56xx_read_virtual_reg12(u16 addr, u16 msb_reg, u16 lsn_reg,       int high_nibble){int msb, lsn;  Read MSB first, this will cause the matching LSN to be latched ", "mutex_lock(io_lock);control =sch56xx_read_virtual_reg(addr, SCH56XX_REG_WDOG_CONTROL);output_enable =sch56xx_read_virtual_reg(addr, SCH56XX_REG_WDOG_OUTPUT_ENABLE);mutex_unlock(io_lock);if (control < 0)return;if (output_enable < 0)return;if (check_enabled && !(output_enable & SCH56XX_WDOG_OUTPUT_ENABLE)) ": "sch56xx_watchdog_register(struct device  parent, u16 addr, u32 revision,       struct mutex  io_lock, int check_enabled){struct sch56xx_watchdog_data  data;int err, control, output_enable;  Cache the watchdog registers ", "int poweroff_delay_ms = CONFIG_THERMAL_EMERGENCY_POWEROFF_DELAY_MS;dev_emerg(&tz->device, \"%s: critical temperature reached, \"  \"shutting down\\n\", tz->type);hw_protection_shutdown(\"Temperature too high\", poweroff_delay_ms);}EXPORT_SYMBOL(thermal_zone_device_critical": "thermal_zone_device_critical(struct thermal_zone_device  tz){    poweroff_delay_ms must be a carefully profiled positive value.   Its a must for forced_emergency_poweroff_work to be scheduled. ", "int acpi_parse_trt(acpi_handle handle, int *trt_count, struct trt **trtp,bool create_dev)": "acpi_parse_trt - Thermal Relationship Table _TRT for passive cooling     @handle: ACPI handle of the device contains _TRT   @trt_count: the number of valid entries resulted from parsing _TRT   @trtp: pointer to pointer of array of _TRT entries in parsing result   @create_dev: whether to create platform devices for target and source   ", "int acpi_parse_art(acpi_handle handle, int *art_count, struct art **artp,bool create_dev)": "acpi_parse_art - Parse Active Relationship Table _ART     @handle: ACPI handle of the device contains _ART   @art_count: the number of valid entries resulted from parsing _ART   @artp: pointer to pointer of array of art entries in parsing result   @create_dev: whether to create platform devices for target and source   ", "static unsigned long nd_pfn_default_alignment(void)": "is_nd_pfn(dev));return nd_pfn;}EXPORT_SYMBOL(to_nd_pfn);static ssize_t mode_show(struct device  dev,struct device_attribute  attr, char  buf){struct nd_pfn  nd_pfn = to_nd_pfn_safe(dev);switch (nd_pfn->mode) {case PFN_MODE_RAM:return sprintf(buf, \"ram\\n\");case PFN_MODE_PMEM:return sprintf(buf, \"pmem\\n\");default:return sprintf(buf, \"none\\n\");}}static ssize_t mode_store(struct device  dev,struct device_attribute  attr, const char  buf, size_t len){struct nd_pfn  nd_pfn = to_nd_pfn_safe(dev);ssize_t rc = 0;device_lock(dev);nvdimm_bus_lock(dev);if (dev->driver)rc = -EBUSY;else {size_t n = len - 1;if (strncmp(buf, \"pmem\\n\", n) == 0|| strncmp(buf, \"pmem\", n) == 0) {nd_pfn->mode = PFN_MODE_PMEM;} else if (strncmp(buf, \"ram\\n\", n) == 0|| strncmp(buf, \"ram\", n) == 0)nd_pfn->mode = PFN_MODE_RAM;else if (strncmp(buf, \"none\\n\", n) == 0|| strncmp(buf, \"none\", n) == 0)nd_pfn->mode = PFN_MODE_NONE;elserc = -EINVAL;}dev_dbg(dev, \"result: %zd wrote: %s%s\", rc, buf,buf[len - 1] == '\\n' ? \"\" : \"\\n\");nvdimm_bus_unlock(dev);device_unlock(dev);return rc ? rc : len;}static DEVICE_ATTR_RW(mode);static ssize_t align_show(struct device  dev,struct device_attribute  attr, char  buf){struct nd_pfn  nd_pfn = to_nd_pfn_safe(dev);return sprintf(buf, \"%ld\\n\", nd_pfn->align);}static unsigned long  nd_pfn_supported_alignments(unsigned long  alignments){alignments[0] = PAGE_SIZE;if (has_transparent_hugepage()) {alignments[1] = HPAGE_PMD_SIZE;if (IS_ENABLED(CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD))alignments[2] = HPAGE_PUD_SIZE;}return alignments;}    Use pmd mapping if supported as default alignment ", "int nd_pfn_validate(struct nd_pfn *nd_pfn, const char *sig)": "nd_pfn_validate - read and validate info-block   @nd_pfn: fsdax namespace runtime state  properties   @sig: 'devdax' or 'fsdax' signature     Upon return the info-block buffer contents (->pfn_sb) are   indeterminate when validation fails, and a coherent info-block   otherwise. ", "nd_pfn->uuid = kmemdup(pfn_sb->uuid, 16, GFP_KERNEL);if (!nd_pfn->uuid)return -ENOMEM;nd_pfn->align = align;nd_pfn->mode = mode;} else ": "nd_pfn_probe() the uuid   is NULL (see: nd_pfn_devinit()) we init settings from   pfn_sb ", ";else if (nspm->lbasize == 4096)return 4096;elsedev_WARN(&ndns->dev, \"unsupported sector size: %ld\\n\",nspm->lbasize);}/* * There is no namespace label (is_namespace_io()), or the label * indicates the default sector size. ": "pmem_sector_size(struct nd_namespace_common  ndns){if (is_namespace_pmem(&ndns->dev)) {struct nd_namespace_pmem  nspm;nspm = to_nd_namespace_pmem(&ndns->dev);if (nspm->lbasize == 0 || nspm->lbasize == 512)  default ", "device_lock(&ndns->dev);device_unlock(&ndns->dev);if (ndns->dev.driver) ": "nvdimm_namespace_common_probe(struct device  dev){struct nd_btt  nd_btt = is_nd_btt(dev) ? to_nd_btt(dev) : NULL;struct nd_pfn  nd_pfn = is_nd_pfn(dev) ? to_nd_pfn(dev) : NULL;struct nd_dax  nd_dax = is_nd_dax(dev) ? to_nd_dax(dev) : NULL;struct nd_namespace_common  ndns = NULL;resource_size_t size;if (nd_btt || nd_pfn || nd_dax) {if (nd_btt)ndns = nd_btt->ndns;else if (nd_pfn)ndns = nd_pfn->ndns;else if (nd_dax)ndns = nd_dax->nd_pfn.ndns;if (!ndns)return ERR_PTR(-ENODEV);    Flush any in-progess probes  removals in the driver   for the raw personality of this namespace. ", "int nd_region_to_nstype(struct nd_region *nd_region)": "nd_region_to_nstype() - region to an integer namespace type   @nd_region: region-device to interrogate     This is the 'nstype' attribute of a region as well, an input to the   MODALIAS for namespace devices, and bit number for a nvdimm_bus to match   namespace devices with namespace drivers. ", "unsigned int nd_region_acquire_lane(struct nd_region *nd_region)": "nd_region_acquire_lane - allocate and lock a lane   @nd_region: region id and number of lanes possible     A lane correlates to a BLK-data-window andor a log slot in the BTT.   We optimize for the common case where there are 256 lanes, one   per-cpu.  For larger systems we need to lock to share lanes.  For now   this implementation assumes the cost of maintaining an allocator for   free lanes is on the order of the lock hold time, so it implements a   static lane = cpu % num_lanes mapping.     In the case of a BTT instance on top of a BLK namespace a lane may be   acquired recursively.  We lock on the first instance.     In the case of a BTT instance on top of PMEM, we only acquire a lane   for the BTT metadata updates. ", "if (is_nd_region(dev))set_dev_node(dev, to_nd_region(dev)->numa_node);dev->bus = &nvdimm_bus_type;device_set_pm_not_required(dev);if (dev->parent) ": "nd_device_register(struct device  dev, bool sync){if (!dev)return;    Ensure that region devices always have their NUMA node set as   early as possible. This way we are able to make certain that   any memory associated with the creation and the creation   itself of the region is associated with the correct node. ", "dev_set_drvdata(dev, nvdimm_bus->nd_desc);return 0;}static struct nd_device_driver nd_bus_driver = ": "nd_device_unregister(dev, ND_SYNC);return 0;}static void free_badrange_list(struct list_head  badrange_list){struct badrange_entry  bre,  next;list_for_each_entry_safe(bre, next, badrange_list, list) {list_del(&bre->list);kfree(bre);}list_del_init(badrange_list);}static void nd_bus_remove(struct device  dev){struct nvdimm_bus  nvdimm_bus = to_nvdimm_bus(dev);mutex_lock(&nvdimm_bus_list_mutex);list_del_init(&nvdimm_bus->list);mutex_unlock(&nvdimm_bus_list_mutex);wait_event(nvdimm_bus->wait,atomic_read(&nvdimm_bus->ioctl_active) == 0);nd_synchronize();device_for_each_child(&nvdimm_bus->dev, NULL, child_unregister);spin_lock(&nvdimm_bus->badrange.lock);free_badrange_list(&nvdimm_bus->badrange.list);spin_unlock(&nvdimm_bus->badrange.lock);nvdimm_bus_destroy_ndctl(nvdimm_bus);}static int nd_bus_probe(struct device  dev){struct nvdimm_bus  nvdimm_bus = to_nvdimm_bus(dev);int rc;rc = nvdimm_bus_create_ndctl(nvdimm_bus);if (rc)return rc;mutex_lock(&nvdimm_bus_list_mutex);list_add_tail(&nvdimm_bus->list, &nvdimm_bus_list);mutex_unlock(&nvdimm_bus_list_mutex);  enable bus provider attributes to look up their local context ", "int __nd_driver_register(struct nd_device_driver *nd_drv, struct module *owner,const char *mod_name)": "__nd_driver_register() - register a region or a namespace driver   @nd_drv: driver to register   @owner: automatically set by nd_driver_register() macro   @mod_name: automatically set by nd_driver_register() macro ", "if (disk_ro == nd_region->ro)return;dev_info(dev, \"%s read-%s, marking %s read-%s\\n\", dev_name(&nd_region->dev), nd_region->ro ? \"only\" : \"write\", disk->disk_name, nd_region->ro ? \"only\" : \"write\");set_disk_ro(disk, nd_region->ro);}EXPORT_SYMBOL(nvdimm_check_and_set_ro": "nvdimm_check_and_set_ro(struct gendisk  disk){struct device  dev = disk_to_dev(disk)->parent;struct nd_region  nd_region = to_nd_region(dev->parent);int disk_ro = get_disk_ro(disk);  catch the disk up with the region ro state ", "u64 nd_sb_checksum(struct nd_gen_sb *nd_gen_sb)": "nd_sb_checksum: compute checksum for a generic info block     Returns a fletcher64 checksum of everything in the given info block   except the last field (since that's where the checksum lives). ", "nd_btt_version(nd_btt, ndns, btt_sb);rawsize = size - nd_btt->initial_offset;if (rawsize < ARENA_MIN_SIZE) ": "nvdimm_namespace_attach_btt(struct nd_namespace_common  ndns){struct nd_btt  nd_btt = to_nd_btt(ndns->claim);struct nd_region  nd_region;struct btt_sb  btt_sb;struct btt  btt;size_t size, rawsize;int rc;if (!nd_btt->uuid || !nd_btt->ndns || !nd_btt->lbasize) {dev_dbg(&nd_btt->dev, \"incomplete btt configuration\\n\");return -ENODEV;}btt_sb = devm_kzalloc(&nd_btt->dev, sizeof( btt_sb), GFP_KERNEL);if (!btt_sb)return -ENOMEM;size = nvdimm_namespace_capacity(ndns);rc = devm_namespace_enable(&nd_btt->dev, ndns, size);if (rc)return rc;    If this returns < 0, that is ok as it just means there wasn't   an existing BTT, and we're creating a new one. We still need to   call this as we need the version dependent fields in nd_btt to be   set correctly based on the holder class ", "rc = -ENXIO;}device_unlock(dev);return rc;}static DEVICE_ATTR_RO(size);static ssize_t log_zero_flags_show(struct device *dev,struct device_attribute *attr, char *buf)": "is_nd_btt(dev));return nd_btt;}EXPORT_SYMBOL(to_nd_btt);static const unsigned long btt_lbasize_supported[] = { 512, 520, 528,4096, 4104, 4160, 4224, 0 };static ssize_t sector_size_show(struct device  dev,struct device_attribute  attr, char  buf){struct nd_btt  nd_btt = to_nd_btt(dev);return nd_size_select_show(nd_btt->lbasize, btt_lbasize_supported, buf);}static ssize_t sector_size_store(struct device  dev,struct device_attribute  attr, const char  buf, size_t len){struct nd_btt  nd_btt = to_nd_btt(dev);ssize_t rc;device_lock(dev);nvdimm_bus_lock(dev);rc = nd_size_select_store(dev, buf, &nd_btt->lbasize,btt_lbasize_supported);dev_dbg(dev, \"result: %zd wrote: %s%s\", rc, buf,buf[len - 1] == '\\n' ? \"\" : \"\\n\");nvdimm_bus_unlock(dev);device_unlock(dev);return rc ? rc : len;}static DEVICE_ATTR_RW(sector_size);static ssize_t uuid_show(struct device  dev,struct device_attribute  attr, char  buf){struct nd_btt  nd_btt = to_nd_btt(dev);if (nd_btt->uuid)return sprintf(buf, \"%pUb\\n\", nd_btt->uuid);return sprintf(buf, \"\\n\");}static ssize_t uuid_store(struct device  dev,struct device_attribute  attr, const char  buf, size_t len){struct nd_btt  nd_btt = to_nd_btt(dev);ssize_t rc;device_lock(dev);rc = nd_uuid_store(dev, &nd_btt->uuid, buf, len);dev_dbg(dev, \"result: %zd wrote: %s%s\", rc, buf,buf[len - 1] == '\\n' ? \"\" : \"\\n\");device_unlock(dev);return rc ? rc : len;}static DEVICE_ATTR_RW(uuid);static ssize_t namespace_show(struct device  dev,struct device_attribute  attr, char  buf){struct nd_btt  nd_btt = to_nd_btt(dev);ssize_t rc;nvdimm_bus_lock(dev);rc = sprintf(buf, \"%s\\n\", nd_btt->ndns? dev_name(&nd_btt->ndns->dev) : \"\");nvdimm_bus_unlock(dev);return rc;}static ssize_t namespace_store(struct device  dev,struct device_attribute  attr, const char  buf, size_t len){struct nd_btt  nd_btt = to_nd_btt(dev);ssize_t rc;device_lock(dev);nvdimm_bus_lock(dev);rc = nd_namespace_store(dev, &nd_btt->ndns, buf, len);dev_dbg(dev, \"result: %zd wrote: %s%s\", rc, buf,buf[len - 1] == '\\n' ? \"\" : \"\\n\");nvdimm_bus_unlock(dev);device_unlock(dev);return rc;}static DEVICE_ATTR_RW(namespace);static ssize_t size_show(struct device  dev,struct device_attribute  attr, char  buf){struct nd_btt  nd_btt = to_nd_btt(dev);ssize_t rc;device_lock(dev);if (dev->driver)rc = sprintf(buf, \"%llu\\n\", nd_btt->size);else {  no size to convey if the btt instance is disabled ", "bool nd_btt_arena_is_valid(struct nd_btt *nd_btt, struct btt_sb *super)": "nd_btt_arena_is_valid - check if the metadata layout is valid   @nd_btt:device with BTT geometry and backing device info   @super:pointer to the arena's info block being tested     Check consistency of the btt info block with itself by validating   the checksum, and with the parent namespace by verifying the   parent_uuid contained in the info block with the one supplied in.     Returns:   false for an invalid info block, true for a valid one ", "nd_btt->initial_offset = 0;nd_btt->version_major = 2;nd_btt->version_minor = 0;if (nvdimm_read_bytes(ndns, 0, btt_sb, sizeof(*btt_sb), 0))return -ENXIO;if (!nd_btt_arena_is_valid(nd_btt, btt_sb))return -ENODEV;if ((le16_to_cpu(btt_sb->version_major) != 2) ||(le16_to_cpu(btt_sb->version_minor) != 0))return -ENODEV;} else ": "nd_btt_version(struct nd_btt  nd_btt, struct nd_namespace_common  ndns,struct btt_sb  btt_sb){if (ndns->claim_class == NVDIMM_CCLASS_BTT2) {  Probesetup for BTT v2.0 ", "goto out;}if (!pdev)goto out;bus = pdev->bus;might_sleep_if(pdev->id.coreid != SSB_DEV_PCI);/* Enable interrupts for this device. ": "ssb_pcicore_dev_irqvecs_enable(struct ssb_pcicore  pc,   struct ssb_device  dev){struct ssb_device  pdev = pc->dev;struct ssb_bus  bus;int err = 0;u32 tmp;if (dev->bus->bustype != SSB_BUSTYPE_PCI) {  This SSB device is not on a PCI host-bus. So the IRQs are   not routed through the PCI core.   So we must not enable routing through the PCI core. ", "bus->mapped_device = NULL;#ifdef CONFIG_SSB_DRIVER_PCICOREbus->pcicore.setup_done = 0;#endiferr = ssb_bus_powerup(bus, 0);if (err)return err;err = ssb_pcmcia_hardware_setup(bus);if (err) ": "ssb_bus_resume(struct ssb_bus  bus){int err;  Reset HW state information in memory, so that HW is   completely reinitialized. ", "rate /= 2;}return rate;}EXPORT_SYMBOL(ssb_clockspeed": "ssb_clockspeed(struct ssb_bus  bus){u32 rate;u32 plltype;u32 clkctl_n, clkctl_m;if (bus->chipco.capabilities & SSB_CHIPCO_CAP_PMU)return ssb_pmu_get_controlclock(&bus->chipco);if (ssb_extif_available(&bus->extif))ssb_extif_get_clockcontrol(&bus->extif, &plltype,   &clkctl_n, &clkctl_m);else if (bus->chipco.dev)ssb_chipco_get_clockcontrol(&bus->chipco, &plltype,    &clkctl_n, &clkctl_m);elsereturn 0;if (bus->chip_id == 0x5365) {rate = 100000000;} else {rate = ssb_calc_clock_rate(plltype, clkctl_n, clkctl_m);if (plltype == SSB_PLLTYPE_3)   25Mhz, 2 dividers ", "if (ssb_read32(dev, SSB_TMSHIGH) & SSB_TMSHIGH_SERR)ssb_write32(dev, SSB_TMSHIGH, 0);val = ssb_read32(dev, SSB_IMSTATE);if (val & (SSB_IMSTATE_IBE | SSB_IMSTATE_TO)) ": "ssb_device_disable(dev, core_specific_flags);ssb_write32(dev, SSB_TMSLOW,    SSB_TMSLOW_RESET | SSB_TMSLOW_CLOCK |    SSB_TMSLOW_FGC | core_specific_flags);ssb_flush_tmslow(dev);  Clear SERR if set. This is a hw bug workaround. ", "int ssb_devices_freeze(struct ssb_bus *bus, struct ssb_freeze_context *ctx)": "ssb_bus_powerup(bus, 0);if (err)return err;err = ssb_pcmcia_hardware_setup(bus);if (err) {ssb_bus_may_powerdown(bus);return err;}ssb_chipco_resume(&bus->chipco);ssb_bus_may_powerdown(bus);return 0;}EXPORT_SYMBOL(ssb_bus_resume);int ssb_bus_suspend(struct ssb_bus  bus){ssb_chipco_suspend(&bus->chipco);ssb_pci_xtal(bus, SSB_GPIO_XTAL | SSB_GPIO_PLL, 0);return 0;}EXPORT_SYMBOL(ssb_bus_suspend);#ifdef CONFIG_SSB_SPROM   ssb_devices_freeze - Freeze all devices on the bus.     After freezing no device driver will be handling a device   on this bus anymore. ssb_devices_thaw() must be called after   a successful freeze to reactivate the devices.     @bus: The bus.   @ctx: Context structure. Pass this to ssb_devices_thaw(). ", "ssb_broadcast_value(dev, 0xFD8, 0);}EXPORT_SYMBOL(ssb_commit_settings": "ssb_commit_settings(struct ssb_bus  bus){struct ssb_device  dev;#ifdef CONFIG_SSB_DRIVER_PCICOREdev = bus->chipco.dev ? bus->chipco.dev : bus->pcicore.dev;#elsedev = bus->chipco.dev;#endifif (WARN_ON(!dev))return;  This forces an update of the cached registers. ", "base = (adm & SSB_ADM_BASE1);break;case SSB_ADM_TYPE2:WARN_ON(adm & SSB_ADM_NEG); /* unsupported ": "ssb_admatch_base(u32 adm){u32 base = 0;switch (adm & SSB_ADM_TYPE) {case SSB_ADM_TYPE0:base = (adm & SSB_ADM_BASE0);break;case SSB_ADM_TYPE1:WARN_ON(adm & SSB_ADM_NEG);   unsupported ", "size = ((adm & SSB_ADM_SZ1) >> SSB_ADM_SZ1_SHIFT);break;case SSB_ADM_TYPE2:WARN_ON(adm & SSB_ADM_NEG); /* unsupported ": "ssb_admatch_size(u32 adm){u32 size = 0;switch (adm & SSB_ADM_TYPE) {case SSB_ADM_TYPE0:size = ((adm & SSB_ADM_SZ0) >> SSB_ADM_SZ0_SHIFT);break;case SSB_ADM_TYPE1:WARN_ON(adm & SSB_ADM_NEG);   unsupported ", "struct key_entry *sparse_keymap_entry_from_scancode(struct input_dev *dev,    unsigned int code)": "sparse_keymap_entry_from_scancode - perform sparse keymap lookup   @dev: Input device using sparse keymap   @code: Scan code     This function is used to perform &struct key_entry lookup in an   input device using sparse keymap. ", "struct key_entry *sparse_keymap_entry_from_keycode(struct input_dev *dev,   unsigned int keycode)": "sparse_keymap_entry_from_keycode - perform sparse keymap lookup   @dev: Input device using sparse keymap   @keycode: Key code     This function is used to perform &struct key_entry lookup in an   input device using sparse keymap. ", "int sparse_keymap_setup(struct input_dev *dev,const struct key_entry *keymap,int (*setup)(struct input_dev *, struct key_entry *))": "sparse_keymap_setup - set up sparse keymap for an input device   @dev: Input device   @keymap: Keymap in form of array of &key_entry structures ending  with %KE_END type entry   @setup: Function that can be used to adjust keymap entries  depending on device's needs, may be %NULL     The function calculates size and allocates copy of the original   keymap after which sets up input device event bits appropriately.   The allocated copy of the keymap is automatically freed when it   is no longer needed. ", "void sparse_keymap_report_entry(struct input_dev *dev, const struct key_entry *ke,unsigned int value, bool autorelease)": "sparse_keymap_report_entry - report event corresponding to given key entry   @dev: Input device for which event should be reported   @ke: key entry describing event   @value: Value that should be reported (ignored by %KE_SW entries)   @autorelease: Signals whether release event should be emitted for %KE_KEY  entries right after reporting press event, ignored by all other  entries     This function is used to report input event described by given   &struct key_entry. ", "bool sparse_keymap_report_event(struct input_dev *dev, unsigned int code,unsigned int value, bool autorelease)": "sparse_keymap_report_event - report event corresponding to given scancode   @dev: Input device using sparse keymap   @code: Scan code   @value: Value that should be reported (ignored by %KE_SW entries)   @autorelease: Signals whether release event should be emitted for %KE_KEY  entries right after reporting press event, ignored by all other  entries     This function is used to perform lookup in an input device using sparse   keymap and report corresponding event. Returns %true if lookup was   successful and %false otherwise. ", "int matrix_keypad_build_keymap(const struct matrix_keymap_data *keymap_data,       const char *keymap_name,       unsigned int rows, unsigned int cols,       unsigned short *keymap,       struct input_dev *input_dev)": "matrix_keypad_build_keymap - convert platform keymap into matrix keymap   @keymap_data: keymap supplied by the platform code   @keymap_name: name of device tree property containing keymap (if device  tree support is enabled).   @rows: number of rows in target keymap array   @cols: number of cols in target keymap array   @keymap: expanded version of keymap that is suitable for use by   matrix keyboard driver   @input_dev: input devices for which we are setting up the keymap     This function converts platform keymap (encoded with KEY() macro) into   an array of keycodes that is suitable for using in a standard matrix   keyboard driver that uses row and col as indices.     If @keymap_data is not supplied and device tree support is enabled   it will attempt load the keymap from property specified by @keymap_name   argument (or \"linux,keymap\" if @keymap_name is %NULL).     If @keymap is %NULL the function will automatically allocate managed   block of memory to store the keymap. This memory will be associated with   the parent device and automatically freed when device unbinds from the   driver.     Callers are expected to set up input_dev->dev.parent before calling this   function. ", "void touchscreen_parse_properties(struct input_dev *input, bool multitouch,  struct touchscreen_properties *prop)": "touchscreen_parse_properties - parse common touchscreen properties   @input: input device that should be parsed   @multitouch: specifies whether parsed properties should be applied to  single-touch or multi-touch axes   @prop: pointer to a struct touchscreen_properties into which to store  axis swap and invert info for use with touchscreen_report_x_y();  or %NULL     This function parses common properties for touchscreens and sets up the   input device accordingly. The function keeps previously set up default   values if no value is specified. ", "void touchscreen_set_mt_pos(struct input_mt_pos *pos,    const struct touchscreen_properties *prop,    unsigned int x, unsigned int y)": "touchscreen_set_mt_pos - Set input_mt_pos coordinates   @pos: input_mt_pos to set coordinates of   @prop: pointer to a struct touchscreen_properties   @x: X coordinate to store in pos   @y: Y coordinate to store in pos     Adjust the passed in x and y values applying any axis inversion and   swapping requested in the passed in touchscreen_properties and store   the result in a struct input_mt_pos. ", "void touchscreen_report_pos(struct input_dev *input,    const struct touchscreen_properties *prop,    unsigned int x, unsigned int y,    bool multitouch)": "touchscreen_report_pos - Report touchscreen coordinates   @input: input_device to report coordinates for   @prop: pointer to a struct touchscreen_properties   @x: X coordinate to report   @y: Y coordinate to report   @multitouch: Report coordinates on single-touch or multi-touch axes     Adjust the passed in x and y values applying any axis inversion and   swapping requested in the passed in touchscreen_properties and then   report the resulting coordinates on the input_dev's x and y axis. ", "int input_mt_init_slots(struct input_dev *dev, unsigned int num_slots,unsigned int flags)": "input_mt_init_slots() - initialize MT input slots   @dev: input device supporting MT events and finger tracking   @num_slots: number of slots used by the device   @flags: mt tasks to handle in core     This function allocates all necessary memory for MT slot handling   in the input device, prepares the ABS_MT_SLOT and   ABS_MT_TRACKING_ID events for use and sets up appropriate buffers.   Depending on the flags set, it also performs pointer emulation and   frame synchronization.     May be called repeatedly. Returns -EINVAL if attempting to   reinitialize with a different number of slots. ", "void input_mt_destroy_slots(struct input_dev *dev)": "input_mt_destroy_slots() - frees the MT slots of the input device   @dev: input device with allocated MT slots     This function is only needed in error path as the input core will   automatically free the MT slots when the device is destroyed. ", "bool input_mt_report_slot_state(struct input_dev *dev,unsigned int tool_type, bool active)": "input_mt_report_slot_state() - report contact state   @dev: input device with allocated MT slots   @tool_type: the tool type to use in this slot   @active: true if contact is active, false otherwise     Reports a contact via ABS_MT_TRACKING_ID, and optionally   ABS_MT_TOOL_TYPE. If active is true and the slot is currently   inactive, or if the tool type is changed, a new tracking id is   assigned to the slot. The tool type is only reported if the   corresponding absbit field is set.     Returns true if contact is active. ", "void input_mt_report_finger_count(struct input_dev *dev, int count)": "input_mt_report_finger_count() - report contact count   @dev: input device with allocated MT slots   @count: the number of contacts     Reports the contact count via BTN_TOOL_FINGER, BTN_TOOL_DOUBLETAP,   BTN_TOOL_TRIPLETAP and BTN_TOOL_QUADTAP.     The input core ensures only the KEY events already setup for   this device will produce output. ", "void input_mt_report_pointer_emulation(struct input_dev *dev, bool use_count)": "input_mt_report_pointer_emulation() - common pointer emulation   @dev: input device with allocated MT slots   @use_count: report number of active contacts as finger count     Performs legacy pointer emulation via BTN_TOUCH, ABS_X, ABS_Y and   ABS_PRESSURE. Touchpad finger count is emulated if use_count is true.     The input core ensures only the KEY and ABS axes already setup for   this device will produce output. ", "void input_mt_drop_unused(struct input_dev *dev)": "input_mt_drop_unused(struct input_dev  dev, struct input_mt  mt){int i;lockdep_assert_held(&dev->event_lock);for (i = 0; i < mt->num_slots; i++) {if (input_mt_is_active(&mt->slots[i]) &&    !input_mt_is_used(mt, &mt->slots[i])) {input_handle_event(dev, EV_ABS, ABS_MT_SLOT, i);input_handle_event(dev, EV_ABS, ABS_MT_TRACKING_ID, -1);}}}     input_mt_drop_unused() - Inactivate slots not seen in this frame   @dev: input device with allocated MT slots     Lift all slots not seen since the last call to this function. ", "void input_mt_sync_frame(struct input_dev *dev)": "input_mt_sync_frame() - synchronize mt frame   @dev: input device with allocated MT slots     Close the frame and prepare the internal state for a new one.   Depending on the flags, marks unused slots as inactive and performs   pointer emulation. ", "int input_mt_assign_slots(struct input_dev *dev, int *slots,  const struct input_mt_pos *pos, int num_pos,  int dmax)": "input_mt_assign_slots() - perform a best-match assignment   @dev: input device with allocated MT slots   @slots: the slot assignment to be filled   @pos: the position array to match   @num_pos: number of positions   @dmax: maximum ABS_MT_POSITION displacement (zero for infinite)     Performs a best match against the current contacts and returns   the slot assignment list. New contacts are assigned to unused   slots.     The assignments are balanced so that all coordinate displacements are   below the euclidian distance dmax. If no such assignment can be found,   some contacts are assigned to unused slots.     Returns zero on success, or negative error in case of failure. ", "int input_mt_get_slot_by_key(struct input_dev *dev, int key)": "input_mt_get_slot_by_key() - return slot matching key   @dev: input device with allocated MT slots   @key: the key of the sought slot     Returns the slot of the given key, if it exists, otherwise   set the key on the first unused slot and return.     If no available slot can be found, -1 is returned.   Note that for this function to work properly, input_mt_sync_frame() has   to be called at each frame. ", "dev_err(dev->dev.parent ?: &dev->dev,\"%s: unable to allocate poller structure\\n\", __func__);return -ENOMEM;}INIT_DELAYED_WORK(&poller->work, input_dev_poller_work);poller->input = dev;poller->poll = poll_fn;dev->poller = poller;return 0;}EXPORT_SYMBOL(input_setup_polling": "input_setup_polling(struct input_dev  dev,void ( poll_fn)(struct input_dev  dev)){struct input_dev_poller  poller;poller = kzalloc(sizeof( poller), GFP_KERNEL);if (!poller) {    We want to show message even though kzalloc() may have   printed backtrace as knowing what instance of input   device we were dealing with is helpful. ", "dev->timestamp[INPUT_CLK_MONO] = ktime_set(0, 0);} else if (dev->num_vals >= dev->max_vals - 2) ": "input_event_dispose(struct input_dev  dev, int disposition,unsigned int type, unsigned int code, int value){if ((disposition & INPUT_PASS_TO_DEVICE) && dev->event)dev->event(dev, type, code, value);if (!dev->vals)return;if (disposition & INPUT_PASS_TO_HANDLERS) {struct input_value  v;if (disposition & INPUT_SLOT) {v = &dev->vals[dev->num_vals++];v->type = EV_ABS;v->code = ABS_MT_SLOT;v->value = dev->mt->slot;}v = &dev->vals[dev->num_vals++];v->type = type;v->code = code;v->value = value;}if (disposition & INPUT_FLUSH) {if (dev->num_vals >= 2)input_pass_values(dev, dev->vals, dev->num_vals);dev->num_vals = 0;    Reset the timestamp on flush so we won't end up   with a stale one. Note we only need to reset the   monolithic one as we use its presence when deciding   whether to generate a synthetic timestamp. ", "void input_event(struct input_dev *dev, unsigned int type, unsigned int code, int value)": "input_register_device(), but the event will not reach any of the   input handlers. Such early invocation of input_event() may be used   to 'seed' initial state of a switch or initial position of absolute   axis, etc. ", "void input_alloc_absinfo(struct input_dev *dev)": "input_alloc_absinfo - allocates array of input_absinfo structs   @dev: the input device emitting absolute events     If the absinfo struct the caller asked for is already allocated, this   functions will not do anything. ", "void input_copy_abs(struct input_dev *dst, unsigned int dst_axis,    const struct input_dev *src, unsigned int src_axis)": "input_copy_abs - Copy absinfo from one input_dev to another   @dst: Destination input device to copy the abs settings to   @dst_axis: ABS_  value selecting the destination axis   @src: Source input device to copy the abs settings from   @src_axis: ABS_  value selecting the source axis     Set absinfo for the selected destination axis by copying it from   the specified source input device's source axis.   This is useful to e.g. setup a penstylus input-device for combined   touchscreenpen hardware where the pen uses the same coordinates as   the touchscreen. ", "int input_grab_device(struct input_handle *handle)": "input_set_capability(dst, EV_ABS, dst_axis);if (!dst->absinfo)return;dst->absinfo[dst_axis] = src->absinfo[src_axis];}EXPORT_SYMBOL(input_copy_abs);     input_grab_device - grabs device for exclusive use   @handle: input handle that wants to own the device     When a device is grabbed by an input handle all events generated by   the device are delivered only to this handle. Also events injected   by other input handles are ignored while device is grabbed. ", "synchronize_rcu();list_for_each_entry(handle, &dev->h_list, d_node)if (handle->open && handle->handler->start)handle->handler->start(handle);}}/** * input_release_device - release previously grabbed device * @handle: input handle that owns the device * * Releases previously grabbed device so that other input handles can * start receiving input events. Upon release all handlers attached * to the device have their start() method called so they have a change * to synchronize device state with the rest of the system. ": "input_release_device(struct input_handle  handle){struct input_dev  dev = handle->dev;struct input_handle  grabber;grabber = rcu_dereference_protected(dev->grab,    lockdep_is_held(&dev->mutex));if (grabber == handle) {rcu_assign_pointer(dev->grab, NULL);  Make sure input_pass_values() notices that grab is gone ", "int input_open_device(struct input_handle *handle)": "input_open_device - open input device   @handle: handle through which device is being accessed     This function should be called by input handlers when they   want to start receive events from given input device. ", "void input_close_device(struct input_handle *handle)": "input_close_device - close input device   @handle: handle through which device is being accessed     This function should be called by input handlers when they   want to stop receive events from given input device. ", "int input_scancode_to_scalar(const struct input_keymap_entry *ke,     unsigned int *scancode)": "input_scancode_to_scalar() - converts scancode in &struct input_keymap_entry   @ke: keymap entry containing scancode to be converted.   @scancode: pointer to the location where converted scancode should  be stored.     This function is used to convert scancode stored in &struct keymap_entry   into scalar form understood by legacy keymap handling methods. These   methods expect scancodes to be represented as 'unsigned int'. ", "int input_get_keycode(struct input_dev *dev, struct input_keymap_entry *ke)": "input_get_keycode - retrieve keycode currently mapped to a given scancode   @dev: input device which keymap is being queried   @ke: keymap entry     This function should be called by anyone interested in retrieving current   keymap. Presently evdev handlers use it. ", "int input_set_keycode(struct input_dev *dev,      const struct input_keymap_entry *ke)": "input_set_keycode - attribute a keycode to a given scancode   @dev: input device which keymap is being updated   @ke: new keymap entry     This function should be called by anyone needing to update current   keymap. Presently keyboard and evdev handlers use it. ", "void input_reset_device(struct input_dev *dev)": "input_reset_device() - resetrestore the state of input device   @dev: input device whose state needs to be reset     This function tries to reset the state of an opened input device and   bring internal state and state if the hardware in sync with each other.   We mark all keys as released, restore LED state, repeat rate, etc. ", "struct input_dev *devm_input_allocate_device(struct device *dev)": "devm_input_allocate_device - allocate managed input device   @dev: device owning the input device being created     Returns prepared struct input_dev or %NULL.     Managed input devices do not need to be explicitly unregistered or   freed as it will be done automatically when owner device unbinds from   its driver (or binding fails). Once managed input device is allocated,   it is ready to be set up and registered in the same fashion as regular   input device. There are no special devm_input_device_[un]register()   variants, regular ones work with both managed and unmanaged devices,   should you need them. In most cases however, managed input device need   not be explicitly unregistered or freed.     NOTE: the owner device is set up as parent of input device and users   should not override it. ", "struct input_dev *input_allocate_device(void)": "input_unregister_device() should be used for already   registered devices. ", "void input_set_timestamp(struct input_dev *dev, ktime_t timestamp)": "input_set_timestamp - set timestamp for input events   @dev: input device to set timestamp for   @timestamp: the time at which the event has occurred     in CLOCK_MONOTONIC     This function is intended to provide to the input system a more   accurate time of when an event actually occurred. The driver should   call this function as soon as a timestamp is acquired ensuring   clock conversions in input_set_timestamp are done correctly.     The system entering suspend state between timestamp acquisition and   calling input_set_timestamp can result in inaccurate conversions. ", "ktime_t *input_get_timestamp(struct input_dev *dev)": "input_get_timestamp - get timestamp for input events   @dev: input device to get timestamp from     A valid timestamp is a timestamp of non-zero value. ", "void input_enable_softrepeat(struct input_dev *dev, int delay, int period)": "input_enable_softrepeat - enable software autorepeat   @dev: input device   @delay: repeat delay   @period: repeat period     Enable software autorepeat on the input device. ", "int input_register_handler(struct input_handler *handler)": "input_register_handler - register a new input handler   @handler: handler to be registered     This function registers a new input handler (interface) for input   devices in the system and attaches it to all input devices that   are compatible with the handler. ", "void input_unregister_handler(struct input_handler *handler)": "input_unregister_handler - unregisters an input handler   @handler: handler to be unregistered     This function disconnects a handler from its input devices and   removes it from lists of known handlers. ", "int input_handler_for_each_handle(struct input_handler *handler, void *data,  int (*fn)(struct input_handle *, void *))": "input_handler_for_each_handle - handle iterator   @handler: input handler to iterate   @data: data for the callback   @fn: function to be called for each handle     Iterate over @bus's list of devices, and call @fn for each, passing   it @data and stop when @fn returns a non-zero value. The function is   using RCU to traverse the list and therefore may be using in atomic   contexts. The @fn callback is invoked from RCU critical section and   thus must not sleep. ", "int input_get_new_minor(int legacy_base, unsigned int legacy_num,bool allow_dynamic)": "input_get_new_minor - allocates a new input minor number   @legacy_base: beginning or the legacy range to be searched   @legacy_num: size of legacy range   @allow_dynamic: whether we can also take ID from the dynamic range     This function allocates a new device minor for from input major namespace.   Caller can request legacy minor by specifying @legacy_base and @legacy_num   parameters and whether ID can be allocated from dynamic range if there are   no free IDs in legacy range. ", "void input_free_minor(unsigned int minor)": "input_free_minor - release previously allocated minor   @minor: minor to be released     This function releases previously allocated input minor so that it can be   reused later. ", "int ps2_sendbyte(struct ps2dev *ps2dev, u8 byte, unsigned int timeout)": "ps2_sendbyte - sends a byte to the device and wait for acknowledgement   @ps2dev: a PS2 device to send the data to   @byte: data to be sent to the device   @timeout: timeout for sending the data and receiving an acknowledge     The function doesn't handle retransmission, the caller is expected to handle   it when needed.     ps2_sendbyte() can only be called from a process context. ", "void ps2_begin_command(struct ps2dev *ps2dev)": "ps2_end_command() should be called. ", "void ps2_drain(struct ps2dev *ps2dev, size_t maxbytes, unsigned int timeout)": "ps2_drain - waits for device to transmit requested number of bytes   and discards them   @ps2dev: the PS2 device that should be drained   @maxbytes: maximum number of bytes to be drained   @timeout: time to drain the device ", "bool ps2_is_keyboard_id(u8 id_byte)": "ps2_is_keyboard_id - checks received ID byte against the list of     known keyboard IDs   @id_byte: data byte that should be checked ", "int __ps2_command(struct ps2dev *ps2dev, u8 *param, unsigned int command)": "ps2_command - send a command to PS2 device   @ps2dev: the PS2 device that should execute the command   @param: a buffer containing parameters to be sent along with the command,     or place where the results of the command execution will be deposited,     or both   @command: command word that encodes the command itself, as well as number of     additional parameter bytes that should be sent to the device and expected     length of the command response     Not serialized. Callers should use ps2_begin_command() and ps2_end_command()   to ensure proper serialization for complex commands. ", "int ps2_sliced_command(struct ps2dev *ps2dev, u8 command)": "ps2_sliced_command - sends an extended PS2 command to a mouse   @ps2dev: the PS2 device that should execute the command   @command: command byte     The command is sent using \"sliced\" syntax understood by advanced devices,   such as Logitech or Synaptics touchpads. The command is encoded as:   0xE6 0xE8 rr 0xE8 ss 0xE8 tt 0xE8 uu where (rr 64)+(ss 16)+(tt 4)+uu   is the command. ", "void ps2_init(struct ps2dev *ps2dev, struct serio *serio,      ps2_pre_receive_handler_t pre_receive_handler,      ps2_receive_handler_t receive_handler)": "ps2_init - initializes ps2dev structure   @ps2dev: structure to be initialized   @serio: serio port associated with the PS2 device   @pre_receive_handler: validation handler to check basic communication state   @receive_handler: main protocol handler     Prepares ps2dev structure for use in drivers for PS2 devices. ", "irqreturn_t ps2_interrupt(struct serio *serio, u8 data, unsigned int flags) ": "ps2_interrupt - common interrupt handler for PS2 devices   @serio: serio port for the device   @data: a data byte received from the device   @flags: flags such as %SERIO_PARITY or %SERIO_TIMEOUT indicating state of     the data transfer     ps2_interrupt() invokes pre-receive handler, optionally handles command   acknowledgement and response from the device, and finally passes the data   to the main protocol handler for future processing. ", "static int serio_bind_driver(struct serio *serio, struct serio_driver *drv)": "serio_reconnect_port(struct serio  serio);static void serio_disconnect_port(struct serio  serio);static void serio_reconnect_subtree(struct serio  serio);static void serio_attach_driver(struct serio_driver  drv);static int serio_connect_driver(struct serio  serio, struct serio_driver  drv){int retval;mutex_lock(&serio->drv_mutex);retval = drv->connect(serio, drv);mutex_unlock(&serio->drv_mutex);return retval;}static int serio_reconnect_driver(struct serio  serio){int retval = -1;mutex_lock(&serio->drv_mutex);if (serio->drv && serio->drv->reconnect)retval = serio->drv->reconnect(serio);mutex_unlock(&serio->drv_mutex);return retval;}static void serio_disconnect_driver(struct serio  serio){mutex_lock(&serio->drv_mutex);if (serio->drv)serio->drv->disconnect(serio);mutex_unlock(&serio->drv_mutex);}static int serio_match_port(const struct serio_device_id  ids, struct serio  serio){while (ids->type || ids->proto) {if ((ids->type == SERIO_ANY || ids->type == serio->id.type) &&    (ids->proto == SERIO_ANY || ids->proto == serio->id.proto) &&    (ids->extra == SERIO_ANY || ids->extra == serio->id.extra) &&    (ids->id == SERIO_ANY || ids->id == serio->id.id))return 1;ids++;}return 0;}    Basic serio -> driver core mappings ", "drv->manual_bind = true;error = driver_register(&drv->driver);if (error) ": "__serio_register_driver(struct serio_driver  drv, struct module  owner, const char  mod_name){bool manual_bind = drv->manual_bind;int error;drv->driver.bus = &serio_bus;drv->driver.owner = owner;drv->driver.mod_name = mod_name;    Temporarily disable automatic binding because probing   takes long time and we are better off doing it in kseriod ", "serio_remove_pending_events(drv);start_over:list_for_each_entry(serio, &serio_list, node) ": "serio_unregister_driver(struct serio_driver  drv){struct serio  serio;mutex_lock(&serio_mutex);drv->manual_bind = true;  so serio_find_driver ignores it ", "static void serio_init_port(struct serio *serio)": "serio_bus)) != NULL) {serio_disconnect_port(serio);error = serio_bind_driver(serio, to_serio_driver(drv));serio_remove_duplicate_events(serio, SERIO_RESCAN_PORT);} else {error = -EINVAL;}mutex_unlock(&serio_mutex);return error ? error : count;}static ssize_t serio_show_bind_mode(struct device  dev, struct device_attribute  attr, char  buf){struct serio  serio = to_serio_port(dev);return sprintf(buf, \"%s\\n\", serio->manual_bind ? \"manual\" : \"auto\");}static ssize_t serio_set_bind_mode(struct device  dev, struct device_attribute  attr, const char  buf, size_t count){struct serio  serio = to_serio_port(dev);int retval;retval = count;if (!strncmp(buf, \"manual\", count)) {serio->manual_bind = true;} else if (!strncmp(buf, \"auto\", count)) {serio->manual_bind = false;} else {retval = -EINVAL;}return retval;}static ssize_t firmware_id_show(struct device  dev, struct device_attribute  attr, char  buf){struct serio  serio = to_serio_port(dev);return sprintf(buf, \"%s\\n\", serio->firmware_id);}static DEVICE_ATTR_RO(type);static DEVICE_ATTR_RO(proto);static DEVICE_ATTR_RO(id);static DEVICE_ATTR_RO(extra);static struct attribute  serio_device_id_attrs[] = {&dev_attr_type.attr,&dev_attr_proto.attr,&dev_attr_id.attr,&dev_attr_extra.attr,NULL};static const struct attribute_group serio_id_attr_group = {.name= \"id\",.attrs= serio_device_id_attrs,};static DEVICE_ATTR_RO(modalias);static DEVICE_ATTR_WO(drvctl);static DEVICE_ATTR(description, S_IRUGO, serio_show_description, NULL);static DEVICE_ATTR(bind_mode, S_IWUSR | S_IRUGO, serio_show_bind_mode, serio_set_bind_mode);static DEVICE_ATTR_RO(firmware_id);static struct attribute  serio_device_attrs[] = {&dev_attr_modalias.attr,&dev_attr_description.attr,&dev_attr_drvctl.attr,&dev_attr_bind_mode.attr,&dev_attr_firmware_id.attr,NULL};static const struct attribute_group serio_device_attr_group = {.attrs= serio_device_attrs,};static const struct attribute_group  serio_device_attr_groups[] = {&serio_id_attr_group,&serio_device_attr_group,NULL};static void serio_release_port(struct device  dev){struct serio  serio = to_serio_port(dev);kfree(serio);module_put(THIS_MODULE);}    Prepare serio port for registration. ", "static DEFINE_MUTEX(i8042_mutex);struct i8042_port ": "i8042_unlock_chip() helpers) to ensure that   they do not disturb each other (unfortunately in many i8042   implementations write to one of the ports will immediately abort   command that is being processed by another port). ", "static DEFINE_SPINLOCK(i8042_lock);/* * Writers to AUX and KBD ports as well as users issuing i8042_command * directly should acquire i8042_mutex (by means of calling * i8042_lock_chip() and i8042_unlock_chip() helpers) to ensure that * they do not disturb each other (unfortunately in many i8042 * implementations write to one of the ports will immediately abort * command that is being processed by another port). ": "i8042_command and   the interrupt handler. ", "void rmi_unregister_transport_device(struct rmi_transport_dev *xport)": "rmi_unregister_transport_device - unregister a transport device connection   @xport: the transport driver to unregister   ", "if (irq == 0) ": "cma3000_init(struct device  dev, int irq,       const struct cma3000_bus_ops  bops){const struct cma3000_platform_data  pdata = dev_get_platdata(dev);struct cma3000_accl_data  data;struct input_dev  input_dev;int rev;int error;if (!pdata) {dev_err(dev, \"platform data not found\\n\");error = -EINVAL;goto err_out;}  if no IRQ return error ", "ad714x_hw_init(ad714x);mutex_init(&ad714x->mutex);/* a slider uses one input_dev instance ": "ad714x_probe(struct device  dev, u16 bus_type, int irq, ad714x_read_t read, ad714x_write_t write){int i;int error;struct input_dev  input;struct ad714x_platform_data  plat_data = dev_get_platdata(dev);struct ad714x_chip  ad714x;void  drv_mem;unsigned long irqflags;struct ad714x_button_drv  bt_drv;struct ad714x_slider_drv  sd_drv;struct ad714x_wheel_drv  wl_drv;struct ad714x_touchpad_drv  tp_drv;if (irq <= 0) {dev_err(dev, \"IRQ not configured!\\n\");error = -EINVAL;return ERR_PTR(error);}if (dev_get_platdata(dev) == NULL) {dev_err(dev, \"platform data for ad714x doesn't exist\\n\");error = -EINVAL;return ERR_PTR(error);}ad714x = devm_kzalloc(dev, sizeof( ad714x) + sizeof( ad714x->sw) +   sizeof( sd_drv)   plat_data->slider_num +   sizeof( wl_drv)   plat_data->wheel_num +   sizeof( tp_drv)   plat_data->touchpad_num +   sizeof( bt_drv)   plat_data->button_num,      GFP_KERNEL);if (!ad714x) {error = -ENOMEM;return ERR_PTR(error);}ad714x->hw = plat_data;drv_mem = ad714x + 1;ad714x->sw = drv_mem;drv_mem += sizeof( ad714x->sw);ad714x->sw->slider = sd_drv = drv_mem;drv_mem += sizeof( sd_drv)   ad714x->hw->slider_num;ad714x->sw->wheel = wl_drv = drv_mem;drv_mem += sizeof( wl_drv)   ad714x->hw->wheel_num;ad714x->sw->touchpad = tp_drv = drv_mem;drv_mem += sizeof( tp_drv)   ad714x->hw->touchpad_num;ad714x->sw->button = bt_drv = drv_mem;drv_mem += sizeof( bt_drv)   ad714x->hw->button_num;ad714x->read = read;ad714x->write = write;ad714x->irq = irq;ad714x->dev = dev;error = ad714x_hw_detect(ad714x);if (error)return ERR_PTR(error);  initialize and request swhw resources ", "drv->ignore = true;error = driver_register(&drv->driver);if (error) ": "__gameport_register_driver(struct gameport_driver  drv, struct module  owner,const char  mod_name){int error;drv->driver.bus = &gameport_bus;drv->driver.owner = owner;drv->driver.mod_name = mod_name;    Temporarily disable automatic binding because probing   takes long time and we are better off doing it in kgameportd ", "gameport_remove_pending_events(drv);start_over:list_for_each_entry(gameport, &gameport_list, node) ": "gameport_unregister_driver(struct gameport_driver  drv){struct gameport  gameport;mutex_lock(&gameport_mutex);drv->ignore = true;  so gameport_find_driver ignores it ", "static int gameport_bind_driver(struct gameport *gameport, struct gameport_driver *drv)": "gameport_close(gameport);t = 1000000   50;if (tx)t = tx;return t;}static int old_gameport_measure_speed(struct gameport  gameport){#if defined(__i386__)unsigned int i, t, t1, t2, t3, tx;unsigned long flags;if (gameport_open(gameport, NULL, GAMEPORT_MODE_RAW))return 0;tx = 1 << 30;for(i = 0; i < 50; i++) {local_irq_save(flags);GET_TIME(t1);for (t = 0; t < 50; t++) gameport_read(gameport);GET_TIME(t2);GET_TIME(t3);local_irq_restore(flags);udelay(i   10);if ((t = DELTA(t2,t1) - DELTA(t3,t2)) < tx) tx = t;}gameport_close(gameport);return 59659  (tx < 1 ? 1 : tx);#elif defined (__x86_64__)unsigned int i, t;unsigned long tx, t1, t2, flags;if (gameport_open(gameport, NULL, GAMEPORT_MODE_RAW))return 0;tx = 1 << 30;for(i = 0; i < 50; i++) {local_irq_save(flags);t1 = rdtsc();for (t = 0; t < 50; t++) gameport_read(gameport);t2 = rdtsc();local_irq_restore(flags);udelay(i   10);if (t2 - t1 < tx) tx = t2 - t1;}gameport_close(gameport);return (this_cpu_read(cpu_info.loops_per_jiffy)  (unsigned long)HZ  (1000  50))  (tx < 1 ? 1 : tx);#elseunsigned int j, t = 0;if (gameport_open(gameport, NULL, GAMEPORT_MODE_RAW))return 0;j = jiffies; while (j == jiffies);j = jiffies; while (j == jiffies) { t++; gameport_read(gameport); }gameport_close(gameport);return t   HZ  1000;#endif}void gameport_start_polling(struct gameport  gameport){spin_lock(&gameport->timer_lock);if (!gameport->poll_cnt++) {BUG_ON(!gameport->poll_handler);BUG_ON(!gameport->poll_interval);mod_timer(&gameport->poll_timer, jiffies + msecs_to_jiffies(gameport->poll_interval));}spin_unlock(&gameport->timer_lock);}EXPORT_SYMBOL(gameport_start_polling);void gameport_stop_polling(struct gameport  gameport){spin_lock(&gameport->timer_lock);if (!--gameport->poll_cnt)del_timer(&gameport->poll_timer);spin_unlock(&gameport->timer_lock);}EXPORT_SYMBOL(gameport_stop_polling);static void gameport_run_poll_handler(struct timer_list  t){struct gameport  gameport = from_timer(gameport, t, poll_timer);gameport->poll_handler(gameport);if (gameport->poll_cnt)mod_timer(&gameport->poll_timer, jiffies + msecs_to_jiffies(gameport->poll_interval));}    Basic gameport -> driver core mappings ", "int n = LO(cmd);int c;int empty;int head, tail;unsigned long flags;/* * Update head and tail of xmit buffer ": "iforce_send_packet(struct iforce  iforce, u16 cmd, unsigned char  data){  Copy data to buffer ", "input_report_abs(dev, ABS_X, (__s16) get_unaligned_le16(data));input_report_abs(dev, ABS_Y, (__s16) get_unaligned_le16(data + 2));input_report_abs(dev, ABS_THROTTLE, 255 - data[4]);if (len >= 8 && test_bit(ABS_RUDDER ,dev->absbit))input_report_abs(dev, ABS_RUDDER, (__s8)data[7]);iforce_report_hats_buttons(iforce, data);input_sync(dev);break;case 0x03:/* wheel position data ": "iforce_process_packet(struct iforce  iforce,   u8 packet_id, u8  data, size_t len){struct input_dev  dev = iforce->dev;int i, j;switch (packet_id) {case 0x01:  joystick position data ", "input_dev->id.bustype = bustype;input_dev->dev.parent = parent;input_set_drvdata(input_dev, iforce);input_dev->name = \"Unknown I-Force device\";input_dev->open = iforce_open;input_dev->close = iforce_close;/* * On-device memory allocation. ": "iforce_init_device(struct device  parent, u16 bustype,       struct iforce  iforce){struct input_dev  input_dev;struct ff_device  ff;u8 c[] = \"CEOV\";u8 buf[IFORCE_MAX_LENGTH];size_t len;int i, error;int ff_effects = 0;input_dev = input_allocate_device();if (!input_dev)return -ENOMEM;init_waitqueue_head(&iforce->wait);spin_lock_init(&iforce->xmit_lock);mutex_init(&iforce->mem_mutex);iforce->xmit.buf = iforce->xmit_data;iforce->dev = input_dev;    Input device fields. ", "br->levels = kmalloc_array(obj->package.count + ACPI_VIDEO_FIRST_LEVEL,   sizeof(*br->levels),   GFP_KERNEL);if (!br->levels) ": "acpi_video_get_levels(struct acpi_device  device,  struct acpi_video_device_brightness   dev_br,  int  pmax_level){union acpi_object  obj = NULL;int i, max_level = 0, count = 0, level_ac_battery = 0;union acpi_object  o;struct acpi_video_device_brightness  br = NULL;int result = 0;u32 value;if (ACPI_FAILURE(acpi_video_device_lcd_query_levels(device->handle, &obj))) {acpi_handle_debug(device->handle,  \"Could not query available LCD brightness level\\n\");result = -ENODEV;goto out;}if (obj->package.count < ACPI_VIDEO_FIRST_LEVEL) {result = -EINVAL;goto out;}br = kzalloc(sizeof( br), GFP_KERNEL);if (!br) {result = -ENOMEM;goto out;}    Note that we have to reserve 2 extra items (ACPI_VIDEO_FIRST_LEVEL),   in order to account for buggy BIOS which don't export the first two   special levels (see below) ", "acpi_video_run_bcl_for_osi(video);if (__acpi_video_get_backlight_type(false, &auto_detect) == acpi_backlight_video &&    !auto_detect)acpi_video_bus_register_backlight(video);acpi_video_bus_add_notify_handler(video);return 0;err_put_video:acpi_video_bus_put_devices(video);kfree(video->attached_array);err_free_video:kfree(video);device->driver_data = NULL;return error;}static void acpi_video_bus_remove(struct acpi_device *device)": "acpi_video_register_backlight() when an internal panel is detected.   Register the backlight now when not using auto-detection, so that   when the kernel cmdline or DMI-quirks are used the backlight will   get registered even if acpi_video_register_backlight() is not called. ", "mutex_lock(&init_mutex);if (!init_done) ": "__acpi_video_get_backlight_type(bool native, bool  auto_detect){static DEFINE_MUTEX(init_mutex);static bool nvidia_wmi_ec_present;static bool apple_gmux_present;static bool native_available;static bool init_done;static long video_caps;  Parse cmdline, dmi and acpi only once ", "if (acpi_has_method(handle, \"_DOD\") || acpi_has_method(handle, \"_DOS\"))video_caps |= ACPI_VIDEO_OUTPUT_SWITCHING;/* Is this device able to retrieve a video ROM ? ": "acpi_is_video_device(acpi_handle handle){long video_caps = 0;  Is this device able to support video switching ? ", "acpi_power_transition(adev, ACPI_STATE_D3_COLD);acpi_dev_put(adev);}}/** * acpi_scan_drop_device - Drop an ACPI device object. * @handle: Handle of an ACPI namespace node, not used. * @context: Address of the ACPI device object to drop. * * This is invoked by acpi_ns_delete_node() during the removal of the ACPI * namespace node the device object pointed to by @context is attached to. * * The unregistration is carried out asynchronously to avoid running * acpi_device_del() under the ACPICA's namespace mutex and the list is used to * ensure the correct ordering (the device objects must be unregistered in the * same order in which the corresponding namespace nodes are deleted). ": "acpi_device_hid(device))) {ida_free(&acpi_device_bus_id->instance_ida, device->pnp.instance_no);if (ida_is_empty(&acpi_device_bus_id->instance_ida)) {list_del(&acpi_device_bus_id->node);kfree_const(acpi_device_bus_id->bus_id);kfree(acpi_device_bus_id);}break;}list_del(&device->wakeup_list);mutex_unlock(&acpi_device_lock);acpi_power_add_remove_device(device, false);acpi_device_remove_files(device);if (device->remove)device->remove(device);device_del(&device->dev);}static BLOCKING_NOTIFIER_HEAD(acpi_reconfig_chain);static LIST_HEAD(acpi_device_del_list);static DEFINE_MUTEX(acpi_device_del_lock);static void acpi_device_del_work_fn(struct work_struct  work_not_used){for (;;) {struct acpi_device  adev;mutex_lock(&acpi_device_del_lock);if (list_empty(&acpi_device_del_list)) {mutex_unlock(&acpi_device_del_lock);break;}adev = list_first_entry(&acpi_device_del_list,struct acpi_device, del_list);list_del(&adev->del_list);mutex_unlock(&acpi_device_del_lock);blocking_notifier_call_chain(&acpi_reconfig_chain,     ACPI_RECONFIG_DEVICE_REMOVE, adev);acpi_device_del(adev);    Drop references to all power resources that might have been   used by the device. ", "if (adev->handle == INVALID_ACPI_HANDLE)goto err_out;if (adev->flags.is_dock_station) ": "acpi_bus_scan(adev->handle);if (error) {dev_warn(&adev->dev, \"Namespace scan failure\\n\");return error;}if (!adev->handler) {dev_warn(&adev->dev, \"Enumeration failure\\n\");error = -ENODEV;}} else {error = acpi_scan_device_not_present(adev);}return error;}static int acpi_scan_bus_check(struct acpi_device  adev, void  not_used){struct acpi_scan_handler  handler = adev->handler;int error;acpi_bus_get_status(adev);if (!(adev->status.present || adev->status.functional)) {acpi_scan_device_not_present(adev);return 0;}if (handler && handler->hotplug.scan_dependent)return handler->hotplug.scan_dependent(adev);error = acpi_bus_scan(adev->handle);if (error) {dev_warn(&adev->dev, \"Namespace scan failure\\n\");return error;}return acpi_dev_for_each_child(adev, acpi_scan_bus_check, NULL);}static int acpi_generic_hotplug_event(struct acpi_device  adev, u32 type){switch (type) {case ACPI_NOTIFY_BUS_CHECK:return acpi_scan_bus_check(adev, NULL);case ACPI_NOTIFY_DEVICE_CHECK:return acpi_scan_device_check(adev);case ACPI_NOTIFY_EJECT_REQUEST:case ACPI_OST_EC_OSPM_EJECT:if (adev->handler && !adev->handler->hotplug.enabled) {dev_info(&adev->dev, \"Eject disabled\\n\");return -EPERM;}acpi_evaluate_ost(adev->handle, ACPI_NOTIFY_EJECT_REQUEST,  ACPI_OST_SC_EJECT_IN_PROGRESS, NULL);return acpi_scan_hot_remove(adev);}return -EINVAL;}void acpi_device_hotplug(struct acpi_device  adev, u32 src){u32 ost_code = ACPI_OST_SC_NON_SPECIFIC_FAILURE;int error = -ENODEV;lock_device_hotplug();mutex_lock(&acpi_scan_lock);    The device object's ACPI handle cannot become invalid as long as we   are holding acpi_scan_lock, but it might have become invalid before   that lock was acquired. ", "int acpi_node_prop_get(const struct fwnode_handle *fwnode,       const char *propname, void **valptr)": "is_acpi_data_node(fwnode)) {const struct acpi_data_node  dn = to_acpi_data_node(fwnode);return &dn->data;}return NULL;}     acpi_node_prop_get - return an ACPI property with given name.   @fwnode: Firmware node to get the property from.   @propname: Name of the property.   @valptr: Location to store a pointer to the property value (if not %NULL). ", "for (i = 0; i < format_count; i++) ": "acpi_extract_package(union acpi_object  package,     struct acpi_buffer  format, struct acpi_buffer  buffer){u32 size_required = 0;u32 tail_offset = 0;char  format_string = NULL;u32 format_count = 0;u32 i = 0;u8  head = NULL;u8  tail = NULL;if (!package || (package->type != ACPI_TYPE_PACKAGE)    || (package->package.count < 1)) {pr_debug(\"Invalid package argument\\n\");return AE_BAD_PARAMETER;}if (!format || !format->pointer || (format->length < 1)) {pr_debug(\"Invalid format argument\\n\");return AE_BAD_PARAMETER;}if (!buffer) {pr_debug(\"Invalid buffer argument\\n\");return AE_BAD_PARAMETER;}format_count = (format->length  sizeof(char)) - 1;if (format_count > package->package.count) {pr_debug(\"Format specifies more objects [%d] than present [%d]\\n\", format_count, package->package.count);return AE_BAD_DATA;}format_string = format->pointer;    Calculate size_required. ", "status = acpi_evaluate_object(handle, pathname, arguments, &buffer);if (ACPI_FAILURE(status))goto end;package = buffer.pointer;if ((buffer.length == 0) || !package) ": "acpi_evaluate_reference(acpi_handle handle,acpi_string pathname,struct acpi_object_list  arguments,struct acpi_handle_list  list){acpi_status status = AE_OK;union acpi_object  package = NULL;union acpi_object  element = NULL;struct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };u32 i = 0;if (!list) {return AE_BAD_PARAMETER;}  Evaluate object. ", "acpi_statusacpi_evaluate_ost(acpi_handle handle, u32 source_event, u32 status_code,  struct acpi_buffer *status_buf)": "acpi_evaluate_ost: Evaluate _OST for hotplug operations   @handle: ACPI device handle   @source_event: source event code   @status_code: status code   @status_buf: optional detailed information (NULL if none)     Evaluate _OST for hotplug operations. All ACPI hotplug handlers   must call this function when evaluating _OST for hotplug operations.   When the platform does not support _OST, this function has no effect. ", "voidacpi_handle_printk(const char *level, acpi_handle handle, const char *fmt, ...)": "acpi_handle_printk: Print message with ACPI prefix and object path   @level: log level   @handle: ACPI device handle   @fmt: format string     This function is called through acpi_handle_<level> macros and prints   a message with ACPI prefix and object path.  This function acquires   the global namespace mutex to obtain an object path.  In interrupt   context, it shows the object path as <na>. ", "void__acpi_handle_debug(struct _ddebug *descriptor, acpi_handle handle,    const char *fmt, ...)": "__acpi_handle_debug: pr_debug with ACPI prefix and object path   @descriptor: Dynamic Debug descriptor   @handle: ACPI device handle   @fmt: format string     This function is called through acpi_handle_debug macro and debug   prints a message with ACPI prefix and object path. This function   acquires the global namespace mutex to obtain an object path.  In   interrupt context, it shows the object path as <na>. ", "bool acpi_has_method(acpi_handle handle, char *name)": "acpi_has_method: Check whether @handle has a method named @name   @handle: ACPI device handle   @name: name of object or method     Check whether @handle has a method named @name. ", "acpi_status acpi_evaluate_reg(acpi_handle handle, u8 space_id, u32 function)": "acpi_evaluate_reg: Evaluate _REG method to register OpRegion presence   @handle: ACPI device handle   @space_id: ACPI address space id to register OpRegion presence for   @function: Parameter to pass to _REG one of ACPI_REG_CONNECT or              ACPI_REG_DISCONNECT     Evaluate device's _REG method to register OpRegion presence. ", "union acpi_object *acpi_evaluate_dsm(acpi_handle handle, const guid_t *guid, u64 rev, u64 func,  union acpi_object *argv4)": "acpi_evaluate_dsm - evaluate device's _DSM method   @handle: ACPI device handle   @guid: GUID of requested functions, should be 16 bytes   @rev: revision number of requested function   @func: requested function number   @argv4: the function specific parameter     Evaluate device's _DSM method with specified GUID, revision id and   function number. Caller needs to free the returned object.     Though ACPI defines the fourth parameter for _DSM should be a package,   some old BIOSes do expect a buffer or an integer etc. ", "bool acpi_check_dsm(acpi_handle handle, const guid_t *guid, u64 rev, u64 funcs)": "acpi_check_dsm - check if _DSM method supports requested functions.   @handle: ACPI device handle   @guid: GUID of requested functions, should be 16 bytes at least   @rev: revision number of requested functions   @funcs: bitmap of requested functions     Evaluate device's _DSM method to check whether it supports requested   functions. Currently only support 64 functions at maximum, should be   enough for now. ", "bool acpi_dev_hid_uid_match(struct acpi_device *adev,    const char *hid2, const char *uid2)": "acpi_dev_hid_uid_match - Match device by supplied HID and UID   @adev: ACPI device to match.   @hid2: Hardware ID of the device.   @uid2: Unique ID of the device, pass NULL to not check _UID.     Matches HID and UID in @adev with given @hid2 and @uid2.   Returns true if matches. ", "int acpi_dev_uid_to_integer(struct acpi_device *adev, u64 *integer)": "acpi_dev_uid_to_integer - treat ACPI device _UID as integer   @adev: ACPI device to get _UID from   @integer: output buffer for integer     Considers _UID as integer and converts it to @integer.     Returns 0 on success, or negative error code otherwise. ", "bool acpi_dev_found(const char *hid)": "acpi_dev_found - Detect presence of a given ACPI device in the namespace.   @hid: Hardware ID of the device.     Return %true if the device was present at the moment of invocation.   Note that if the device is pluggable, it may since have disappeared.     For this function to work, acpi_bus_scan() must have been executed   which happens in the subsys_initcall() subsection. Hence, do not   call from a subsys_initcall() or earlier (use acpi_get_devices()   instead). Calling from module_init() is fine (which is synonymous   with device_initcall()). ", "bool acpi_dev_present(const char *hid, const char *uid, s64 hrv)": "acpi_dev_present - Detect that a given ACPI device is present   @hid: Hardware ID of the device.   @uid: Unique ID of the device, pass NULL to not check _UID   @hrv: Hardware Revision of the device, pass -1 to not check _HRV     Return %true if a matching device was present at the moment of invocation.   Note that if the device is pluggable, it may since have disappeared.     Note that unlike acpi_dev_found() this function checks the status   of the device. So for devices which are present in the DSDT, but   which are disabled (their _STA callback returns 0) this function   will return false.     For this function to work, acpi_bus_scan() must have been executed   which happens in the subsys_initcall() subsection. Hence, do not   call from a subsys_initcall() or earlier (use acpi_get_devices()   instead). Calling from module_init() is fine (which is synonymous   with device_initcall()). ", "struct acpi_device *acpi_dev_get_next_match_dev(struct acpi_device *adev, const char *hid, const char *uid, s64 hrv)": "acpi_dev_get_next_match_dev - Return the next match of ACPI device   @adev: Pointer to the previous ACPI device matching this @hid, @uid and @hrv   @hid: Hardware ID of the device.   @uid: Unique ID of the device, pass NULL to not check _UID   @hrv: Hardware Revision of the device, pass -1 to not check _HRV     Return the next match of ACPI device if another matching device was present   at the moment of invocation, or NULL otherwise.     The caller is responsible for invoking acpi_dev_put() on the returned device.   On the other hand the function invokes  acpi_dev_put() on the given @adev   assuming that its reference counter had been increased beforehand.     See additional information in acpi_dev_present() as well. ", "struct acpi_device *acpi_dev_get_first_match_dev(const char *hid, const char *uid, s64 hrv)": "acpi_dev_get_first_match_dev - Return the first match of ACPI device   @hid: Hardware ID of the device.   @uid: Unique ID of the device, pass NULL to not check _UID   @hrv: Hardware Revision of the device, pass -1 to not check _HRV     Return the first match of ACPI device if a matching device was present   at the moment of invocation, or NULL otherwise.     The caller is responsible for invoking acpi_dev_put() on the returned device.     See additional information in acpi_dev_present() as well. ", "int acpi_match_platform_list(const struct acpi_platform_list *plat)": "acpi_match_platform_list - Check if the system matches with a given list   @plat: pointer to acpi_platform_list table terminated by a NULL entry     Return the matched index if the system is found in the platform list.   Otherwise, return a negative error code. ", "int acpi_device_get_power(struct acpi_device *device, int *state)": "acpi_device_set_power() for @device   (if that power state depends on any power resources). ", "int acpi_pm_device_sleep_state(struct device *dev, int *d_min_p, int d_max_in)": "acpi_pm_device_sleep_state - Get preferred power state of ACPI device.   @dev: Device whose preferred target power state to return.   @d_min_p: Location to store the upper limit of the allowed states range.   @d_max_in: Deepest low-power state to take into consideration.   Return value: Preferred power state of the device on success, -ENODEV   if there's no 'struct acpi_device' for @dev, -EINVAL if @d_max_in is   incorrect, or -ENODATA on ACPI method failure.     The caller must ensure that @dev is valid before using this function. ", "input.count = 4;input.pointer = in_params;in_params[0].type = ACPI_TYPE_BUFFER;in_params[0].buffer.length = 16;in_params[0].buffer.pointer= (u8 *)&guid;in_params[1].type = ACPI_TYPE_INTEGER;in_params[1].integer.value = context->rev;in_params[2].type = ACPI_TYPE_INTEGER;in_params[2].integer.value= context->cap.length/sizeof(u32);in_params[3].type= ACPI_TYPE_BUFFER;in_params[3].buffer.length = context->cap.length;in_params[3].buffer.pointer = context->cap.pointer;status = acpi_evaluate_object(handle, \"_OSC\", &input, &output);if (ACPI_FAILURE(status))return status;if (!output.length)return AE_NULL_OBJECT;out_obj = output.pointer;if (out_obj->type != ACPI_TYPE_BUFFER|| out_obj->buffer.length != context->cap.length) ": "acpi_run_osc(acpi_handle handle, struct acpi_osc_context  context){acpi_status status;struct acpi_object_list input;union acpi_object in_params[4];union acpi_object  out_obj;guid_t guid;u32 errors;struct acpi_buffer output = {ACPI_ALLOCATE_BUFFER, NULL};if (!context)return AE_ERROR;if (guid_parse(context->uuid_str, &guid))return AE_ERROR;context->ret.length = ACPI_ALLOCATE_BUFFER;context->ret.pointer = NULL;  Setting up input parameters ", "int acpi_bus_register_driver(struct acpi_driver *driver)": "acpi_bus_register_driver - register a driver with the ACPI bus   @driver: driver being registered     Registers a driver with the ACPI bus.  Searches the namespace for all   devices that match the driver's criteria and binds.  Returns zero for   success or a negative error status for failure. ", "void acpi_bus_unregister_driver(struct acpi_driver *driver)": "acpi_bus_unregister_driver - unregisters a driver with the ACPI bus   @driver: driver to unregister     Unregisters a driver with the ACPI bus.  Searches the namespace for all   devices that match the driver's criteria and unbinds. ", "if (is_done != 0) ": "acpi_processor_notify_smm(struct module  calling_module){static int is_done;int result = 0;if (!acpi_processor_cpufreq_init)return -EBUSY;if (!try_module_get(calling_module))return -EINVAL;    is_done is set to negative if an error occurs and to 1 if no error   occurrs, but SMM has been notified already. This avoids repeated   notification which might lead to unexpected results. ", "for_each_possible_cpu(i) ": "acpi_processor_preregister_performance(struct acpi_processor_performance __percpu  performance){int count_target;int retval = 0;unsigned int i, j;cpumask_var_t covered_cpus;struct acpi_processor  pr;struct acpi_psd_package  pdomain;struct acpi_processor  match_pr;struct acpi_psd_package  match_pdomain;if (!zalloc_cpumask_var(&covered_cpus, GFP_KERNEL))return -ENOMEM;mutex_lock(&performance_mutex);    Check if another driver has already registered, and abort before   changing pr->performance if it has. Check input data as well. ", "memcpy(&addr, &gas->address, sizeof(addr));if (!addr || !gas->bit_width)return NULL;return acpi_os_map_iomem(addr, gas->bit_width / 8);}EXPORT_SYMBOL(acpi_os_map_generic_address": "acpi_os_map_generic_address(struct acpi_generic_address  gas){u64 addr;if (gas->space_id != ACPI_ADR_SPACE_SYSTEM_MEMORY)return NULL;  Handle possible alignment issues ", "memcpy(&addr, &gas->address, sizeof(addr));if (!addr || !gas->bit_width)return;mutex_lock(&acpi_ioremap_lock);map = acpi_map_lookup(addr, gas->bit_width / 8);if (!map) ": "acpi_os_unmap_generic_address(struct acpi_generic_address  gas){u64 addr;struct acpi_ioremap  map;if (gas->space_id != ACPI_ADR_SPACE_SYSTEM_MEMORY)return;  Handle possible alignment issues ", "acpi_status acpi_os_execute(acpi_execute_type type,    acpi_osd_exec_callback function, void *context)": "acpi_os_execute_deferred(struct work_struct  work){struct acpi_os_dpc  dpc = container_of(work, struct acpi_os_dpc, work);dpc->function(dpc->context);kfree(dpc);}#ifdef CONFIG_ACPI_DEBUGGERstatic struct acpi_debugger acpi_debugger;static bool acpi_debugger_initialized;int acpi_register_debugger(struct module  owner,   const struct acpi_debugger_ops  ops){int ret = 0;mutex_lock(&acpi_debugger.lock);if (acpi_debugger.ops) {ret = -EBUSY;goto err_lock;}acpi_debugger.owner = owner;acpi_debugger.ops = ops;err_lock:mutex_unlock(&acpi_debugger.lock);return ret;}EXPORT_SYMBOL(acpi_register_debugger);void acpi_unregister_debugger(const struct acpi_debugger_ops  ops){mutex_lock(&acpi_debugger.lock);if (ops == acpi_debugger.ops) {acpi_debugger.ops = NULL;acpi_debugger.owner = NULL;}mutex_unlock(&acpi_debugger.lock);}EXPORT_SYMBOL(acpi_unregister_debugger);int acpi_debugger_create_thread(acpi_osd_exec_callback function, void  context){int ret;int ( func)(acpi_osd_exec_callback, void  );struct module  owner;if (!acpi_debugger_initialized)return -ENODEV;mutex_lock(&acpi_debugger.lock);if (!acpi_debugger.ops) {ret = -ENODEV;goto err_lock;}if (!try_module_get(acpi_debugger.owner)) {ret = -ENODEV;goto err_lock;}func = acpi_debugger.ops->create_thread;owner = acpi_debugger.owner;mutex_unlock(&acpi_debugger.lock);ret = func(function, context);mutex_lock(&acpi_debugger.lock);module_put(owner);err_lock:mutex_unlock(&acpi_debugger.lock);return ret;}ssize_t acpi_debugger_write_log(const char  msg){ssize_t ret;ssize_t ( func)(const char  );struct module  owner;if (!acpi_debugger_initialized)return -ENODEV;mutex_lock(&acpi_debugger.lock);if (!acpi_debugger.ops) {ret = -ENODEV;goto err_lock;}if (!try_module_get(acpi_debugger.owner)) {ret = -ENODEV;goto err_lock;}func = acpi_debugger.ops->write_log;owner = acpi_debugger.owner;mutex_unlock(&acpi_debugger.lock);ret = func(msg);mutex_lock(&acpi_debugger.lock);module_put(owner);err_lock:mutex_unlock(&acpi_debugger.lock);return ret;}ssize_t acpi_debugger_read_cmd(char  buffer, size_t buffer_length){ssize_t ret;ssize_t ( func)(char  , size_t);struct module  owner;if (!acpi_debugger_initialized)return -ENODEV;mutex_lock(&acpi_debugger.lock);if (!acpi_debugger.ops) {ret = -ENODEV;goto err_lock;}if (!try_module_get(acpi_debugger.owner)) {ret = -ENODEV;goto err_lock;}func = acpi_debugger.ops->read_cmd;owner = acpi_debugger.owner;mutex_unlock(&acpi_debugger.lock);ret = func(buffer, buffer_length);mutex_lock(&acpi_debugger.lock);module_put(owner);err_lock:mutex_unlock(&acpi_debugger.lock);return ret;}int acpi_debugger_wait_command_ready(void){int ret;int ( func)(bool, char  , size_t);struct module  owner;if (!acpi_debugger_initialized)return -ENODEV;mutex_lock(&acpi_debugger.lock);if (!acpi_debugger.ops) {ret = -ENODEV;goto err_lock;}if (!try_module_get(acpi_debugger.owner)) {ret = -ENODEV;goto err_lock;}func = acpi_debugger.ops->wait_command_ready;owner = acpi_debugger.owner;mutex_unlock(&acpi_debugger.lock);ret = func(acpi_gbl_method_executing,   acpi_gbl_db_line_buf, ACPI_DB_LINE_BUFFER_SIZE);mutex_lock(&acpi_debugger.lock);module_put(owner);err_lock:mutex_unlock(&acpi_debugger.lock);return ret;}int acpi_debugger_notify_command_complete(void){int ret;int ( func)(void);struct module  owner;if (!acpi_debugger_initialized)return -ENODEV;mutex_lock(&acpi_debugger.lock);if (!acpi_debugger.ops) {ret = -ENODEV;goto err_lock;}if (!try_module_get(acpi_debugger.owner)) {ret = -ENODEV;goto err_lock;}func = acpi_debugger.ops->notify_command_complete;owner = acpi_debugger.owner;mutex_unlock(&acpi_debugger.lock);ret = func();mutex_lock(&acpi_debugger.lock);module_put(owner);err_lock:mutex_unlock(&acpi_debugger.lock);return ret;}int __init acpi_debugger_init(void){mutex_init(&acpi_debugger.lock);acpi_debugger_initialized = true;return 0;}#endif                                                                                    FUNCTION:    acpi_os_execute     PARAMETERS:  Type               - Type of the callback                Function           - Function to be executed                Context            - Function parameters     RETURN:      Status     DESCRIPTION: Depending on type, either queues function for deferred execution or                immediately executes function on a separate thread.                                                                                ", "if (acpi_sci_irq_valid())synchronize_hardirq(acpi_sci_irq);flush_workqueue(kacpid_wq);flush_workqueue(kacpi_notify_wq);}EXPORT_SYMBOL(acpi_os_wait_events_complete": "acpi_os_wait_events_complete(void){    Make sure the GPE handler or the fixed event handler is not used   on another CPU after removal. ", "chars = strlen(buffer) - 1;buffer[chars] = '\\0';}#elseint ret;ret = acpi_debugger_read_cmd(buffer, buffer_length);if (ret < 0)return AE_ERROR;if (bytes_read)*bytes_read = ret;#endifreturn AE_OK;}EXPORT_SYMBOL(acpi_os_get_line": "acpi_os_get_line(char  buffer, u32 buffer_length, u32  bytes_read){#ifdef ENABLE_DEBUGGERif (acpi_in_debugger) {u32 chars;kdb_read(buffer, buffer_length);  remove the CR kdb includes ", "size = nla_total_size(sizeof(struct acpi_genl_event)) +    nla_total_size(0);skb = genlmsg_new(size, GFP_ATOMIC);if (!skb)return -ENOMEM;/* add the genetlink message header ": "acpi_bus_generate_netlink_event(const char  device_class,      const char  bus_id,      u8 type, int data){struct sk_buff  skb;struct nlattr  attr;struct acpi_genl_event  event;void  msg_header;int size;  allocate memory ", "static inline bool acpi_ec_gpe_status_set(struct acpi_ec *ec)": "ec_write_cmd(struct acpi_ec  ec, u8 command){ec_dbg_raw(\"EC_SC(W) = 0x%2.2x\", command);outb(command, ec->command_addr);ec->timestamp = jiffies;}static inline void acpi_ec_write_data(struct acpi_ec  ec, u8 data){ec_dbg_raw(\"EC_DATA(W) = 0x%2.2x\", data);outb(data, ec->data_addr);ec->timestamp = jiffies;}#if defined(DEBUG) || defined(CONFIG_DYNAMIC_DEBUG)static const char  acpi_ec_cmd_string(u8 cmd){switch (cmd) {case 0x80:return \"RD_EC\";case 0x81:return \"WR_EC\";case 0x82:return \"BE_EC\";case 0x83:return \"BD_EC\";case 0x84:return \"QR_EC\";}return \"UNKNOWN\";}#else#define acpi_ec_cmd_string(cmd)\"UNDEF\"#endif  --------------------------------------------------------------------------                             GPE Registers   -------------------------------------------------------------------------- ", "if (t->irq_count == ec_storm_threshold)acpi_ec_mask_events(ec);}static void advance_transaction(struct acpi_ec *ec, bool interrupt)": "ec_transaction_polled(struct acpi_ec  ec){unsigned long flags;int ret = 0;spin_lock_irqsave(&ec->lock, flags);if (ec->curr && (ec->curr->flags & ACPI_EC_COMMAND_POLL))ret = 1;spin_unlock_irqrestore(&ec->lock, flags);return ret;}static int ec_transaction_completed(struct acpi_ec  ec){unsigned long flags;int ret = 0;spin_lock_irqsave(&ec->lock, flags);if (ec->curr && (ec->curr->flags & ACPI_EC_COMMAND_COMPLETE))ret = 1;spin_unlock_irqrestore(&ec->lock, flags);return ret;}static inline void ec_transaction_transition(struct acpi_ec  ec, unsigned long flag){ec->curr->flags |= flag;if (ec->curr->command != ACPI_EC_COMMAND_QUERY)return;switch (ec_event_clearing) {case ACPI_EC_EVT_TIMING_STATUS:if (flag == ACPI_EC_COMMAND_POLL)acpi_ec_close_event(ec);return;case ACPI_EC_EVT_TIMING_QUERY:if (flag == ACPI_EC_COMMAND_COMPLETE)acpi_ec_close_event(ec);return;case ACPI_EC_EVT_TIMING_EVENT:if (flag == ACPI_EC_COMMAND_COMPLETE)acpi_ec_complete_event(ec);}}static void acpi_ec_spurious_interrupt(struct acpi_ec  ec, struct transaction  t){if (t->irq_count < ec_storm_threshold)++t->irq_count;  Trigger if the threshold is 0 too. ", "#include <linux/kernel.h>#include <linux/init.h>#include <linux/slab.h>#include <linux/percpu.h>#include <linux/syscore_ops.h>#include <linux/rwsem.h>#include <linux/cpu.h>#include \"../leds.h\"#define MAX_NAME_LEN8struct led_trigger_cpu ": "ledtrig_cpu is exported for any user, who want to add CPU   activity indication in their code.     Copyright 2011 Linus Walleij <linus.walleij@linaro.org>   Copyright 2011 - 2012 Bryan Wu <bryan.wu@canonical.com> ", "if (i2c_of_match_device(drv->of_match_table, client))return 1;/* Then ACPI style match ": "i2c_transfer_trace_reg(void){static_branch_inc(&i2c_trace_msg_key);return 0;}void i2c_transfer_trace_unreg(void){static_branch_dec(&i2c_trace_msg_key);}const char  i2c_freq_mode_string(u32 bus_freq_hz){switch (bus_freq_hz) {case I2C_MAX_STANDARD_MODE_FREQ:return \"Standard Mode (100 kHz)\";case I2C_MAX_FAST_MODE_FREQ:return \"Fast Mode (400 kHz)\";case I2C_MAX_FAST_MODE_PLUS_FREQ:return \"Fast Mode Plus (1.0 MHz)\";case I2C_MAX_TURBO_MODE_FREQ:return \"Turbo Mode (1.4 MHz)\";case I2C_MAX_HIGH_SPEED_MODE_FREQ:return \"High Speed Mode (3.4 MHz)\";case I2C_MAX_ULTRA_FAST_MODE_FREQ:return \"Ultra Fast Mode (5.0 MHz)\";default:return \"Unknown Mode\";}}EXPORT_SYMBOL_GPL(i2c_freq_mode_string);const struct i2c_device_id  i2c_match_id(const struct i2c_device_id  id,const struct i2c_client  client){if (!(id && client))return NULL;while (id->name[0]) {if (strcmp(client->name, id->name) == 0)return id;id++;}return NULL;}EXPORT_SYMBOL_GPL(i2c_match_id);const void  i2c_get_match_data(const struct i2c_client  client){struct i2c_driver  driver = to_i2c_driver(client->dev.driver);const struct i2c_device_id  match;const void  data;data = device_get_match_data(&client->dev);if (!data) {match = i2c_match_id(driver->id_table, client);if (!match)return NULL;data = (const void  )match->driver_data;}return data;}EXPORT_SYMBOL(i2c_get_match_data);static int i2c_device_match(struct device  dev, struct device_driver  drv){struct i2c_client client = i2c_verify_client(dev);struct i2c_driver driver;  Attempt an OF style match ", "struct i2c_client *i2c_find_device_by_fwnode(struct fwnode_handle *fwnode)": "i2c_find_device_by_fwnode() - find an i2c_client for the fwnode   @fwnode: &struct fwnode_handle corresponding to the &struct i2c_client     Look up and return the &struct i2c_client corresponding to the @fwnode.   If no client can be found, or @fwnode is NULL, this returns NULL.     The user must call put_device(&client->dev) once done with the i2c client. ", "struct i2c_adapter *i2c_verify_adapter(struct device *dev)": "i2c_verify_adapter - return parameter as i2c_adapter or NULL   @dev: device, probably from some driver model iterator     When traversing the driver model tree, perhaps using driver model   iterators like @device_for_each_child(), you can't assume very much   about the nodes you find.  Use this function to avoid oopses caused   by wrongly treating some non-I2C device as an i2c_adapter. ", "int i2c_add_adapter(struct i2c_adapter *adapter)": "i2c_add_adapter - declare i2c adapter, use dynamic bus number   @adapter: the adapter to add   Context: can sleep     This routine is used to declare an I2C adapter when its bus number   doesn't matter or when its bus number is specified by an dt alias.   Examples of bases when the bus number doesn't matter: I2C adapters   dynamically added by USB links or PCI plugin cards.     When this returns zero, a new bus number was allocated and stored   in adap->nr, and the specified adapter became available for clients.   Otherwise, a negative errno value is returned. ", "void i2c_del_adapter(struct i2c_adapter *adap)": "i2c_del_adapter - unregister I2C adapter   @adap: the adapter being unregistered   Context: can sleep     This unregisters an I2C adapter which was previously registered   by @i2c_add_adapter or @i2c_add_numbered_adapter. ", "struct i2c_adapter *i2c_find_adapter_by_fwnode(struct fwnode_handle *fwnode)": "i2c_find_adapter_by_fwnode() - find an i2c_adapter for the fwnode   @fwnode: &struct fwnode_handle corresponding to the &struct i2c_adapter     Look up and return the &struct i2c_adapter corresponding to the @fwnode.   If no adapter can be found, or @fwnode is NULL, this returns NULL.     The user must call put_device(&adapter->dev) once done with the i2c adapter. ", "struct i2c_adapter *i2c_get_adapter_by_fwnode(struct fwnode_handle *fwnode)": "i2c_put_adapter(adapter) once done with the i2c adapter.   Note that this is different from i2c_find_adapter_by_node(). ", "if (WARN_ON(!is_registered))return -EAGAIN;/* add the driver to the list of i2c drivers in the driver core ": "i2c_register_driver(struct module  owner, struct i2c_driver  driver){int res;  Can't register until after driver model init ", "void i2c_del_driver(struct i2c_driver *driver)": "i2c_del_driver - unregister I2C driver   @driver: the driver being unregistered   Context: can sleep ", "int __i2c_transfer(struct i2c_adapter *adap, struct i2c_msg *msgs, int num)": "__i2c_transfer - unlocked flavor of i2c_transfer   @adap: Handle to I2C bus   @msgs: One or more messages to execute before STOP is issued to  terminate the operation; each message begins with a START.   @num: Number of messages to be executed.     Returns negative errno, else the number of messages executed.     Adapter lock must be held when calling this function. No debug logging   takes place. adap->algo->master_xfer existence isn't checked. ", "int i2c_transfer_buffer_flags(const struct i2c_client *client, char *buf,      int count, u16 flags)": "i2c_transfer_buffer_flags - issue a single I2C message transferring data         tofrom a buffer   @client: Handle to slave device   @buf: Where the data is stored   @count: How many bytes to transfer, must be less than 64k since msg.len is u16   @flags: The flags to be used for the message, e.g. I2C_M_RD for reads     Returns negative errno, or else the number of bytes transferred. ", "u8 i2c_smbus_pec(u8 crc, u8 *p, size_t count)": "i2c_smbus_pec - Incremental CRC8 over the given input data array   @crc: previous return crc8 value   @p: pointer to data buffer.   @count: number of bytes in data buffer.     Incremental CRC8 over count bytes in the array pointed to by p ", "s32 i2c_smbus_read_byte(const struct i2c_client *client)": "i2c_smbus_read_byte - SMBus \"receive byte\" protocol   @client: Handle to slave device     This executes the SMBus \"receive byte\" protocol, returning negative errno   else the byte received from the device. ", "s32 i2c_smbus_write_byte(const struct i2c_client *client, u8 value)": "i2c_smbus_xfer(client->adapter, client->addr, client->flags,I2C_SMBUS_READ, 0,I2C_SMBUS_BYTE, &data);return (status < 0) ? status : data.byte;}EXPORT_SYMBOL(i2c_smbus_read_byte);     i2c_smbus_write_byte - SMBus \"send byte\" protocol   @client: Handle to slave device   @value: Byte to be sent     This executes the SMBus \"send byte\" protocol, returning negative errno   else zero on success. ", "s32 i2c_smbus_read_byte_data(const struct i2c_client *client, u8 command)": "i2c_smbus_read_byte_data - SMBus \"read byte\" protocol   @client: Handle to slave device   @command: Byte interpreted by slave     This executes the SMBus \"read byte\" protocol, returning negative errno   else a data byte received from the device. ", "s32 i2c_smbus_write_byte_data(const struct i2c_client *client, u8 command,      u8 value)": "i2c_smbus_write_byte_data - SMBus \"write byte\" protocol   @client: Handle to slave device   @command: Byte interpreted by slave   @value: Byte being written     This executes the SMBus \"write byte\" protocol, returning negative errno   else zero on success. ", "s32 i2c_smbus_read_word_data(const struct i2c_client *client, u8 command)": "i2c_smbus_read_word_data - SMBus \"read word\" protocol   @client: Handle to slave device   @command: Byte interpreted by slave     This executes the SMBus \"read word\" protocol, returning negative errno   else a 16-bit unsigned \"word\" received from the device. ", "s32 i2c_smbus_write_word_data(const struct i2c_client *client, u8 command,      u16 value)": "i2c_smbus_write_word_data - SMBus \"write word\" protocol   @client: Handle to slave device   @command: Byte interpreted by slave   @value: 16-bit \"word\" being written     This executes the SMBus \"write word\" protocol, returning negative errno   else zero on success. ", "s32 i2c_smbus_read_block_data(const struct i2c_client *client, u8 command,      u8 *values)": "i2c_smbus_read_block_data - SMBus \"block read\" protocol   @client: Handle to slave device   @command: Byte interpreted by slave   @values: Byte array into which data will be read; big enough to hold  the data returned by the slave.  SMBus allows at most 32 bytes.     This executes the SMBus \"block read\" protocol, returning negative errno   else the number of data bytes in the slave's response.     Note that using this function requires that the client's adapter support   the I2C_FUNC_SMBUS_READ_BLOCK_DATA functionality.  Not all adapter drivers   support this; its emulation through I2C messaging relies on a specific   mechanism (I2C_M_RECV_LEN) which may not be implemented. ", "s32 i2c_smbus_write_block_data(const struct i2c_client *client, u8 command,       u8 length, const u8 *values)": "i2c_smbus_write_block_data - SMBus \"block write\" protocol   @client: Handle to slave device   @command: Byte interpreted by slave   @length: Size of data block; SMBus allows at most 32 bytes   @values: Byte array which will be written.     This executes the SMBus \"block write\" protocol, returning negative errno   else zero on success. ", "trace_smbus_write(adapter, addr, flags, read_write,  command, protocol, data);trace_smbus_read(adapter, addr, flags, read_write, command, protocol);flags &= I2C_M_TEN | I2C_CLIENT_PEC | I2C_CLIENT_SCCB;xfer_func = adapter->algo->smbus_xfer;if (i2c_in_atomic_xfer_mode()) ": "__i2c_smbus_xfer(adapter, addr, flags, read_write,       command, protocol, data);i2c_unlock_bus(adapter, I2C_LOCK_SEGMENT);return res;}EXPORT_SYMBOL(i2c_smbus_xfer);s32 __i2c_smbus_xfer(struct i2c_adapter  adapter, u16 addr,     unsigned short flags, char read_write,     u8 command, int protocol, union i2c_smbus_data  data){int ( xfer_func)(struct i2c_adapter  adap, u16 addr, unsigned short flags, char read_write, u8 command, int size, union i2c_smbus_data  data);unsigned long orig_jiffies;int try;s32 res;res = __i2c_check_suspended(adapter);if (res)return res;  If enabled, the following two tracepoints are conditional on   read_write and protocol. ", "s32 i2c_smbus_read_i2c_block_data_or_emulated(const struct i2c_client *client,      u8 command, u8 length, u8 *values)": "i2c_smbus_read_i2c_block_data_or_emulated - read block or emulate   @client: Handle to slave device   @command: Byte interpreted by slave   @length: Size of data block; SMBus allows at most I2C_SMBUS_BLOCK_MAX bytes   @values: Byte array into which data will be read; big enough to hold  the data returned by the slave.  SMBus allows at most  I2C_SMBUS_BLOCK_MAX bytes.     This executes the SMBus \"block read\" protocol if supported by the adapter.   If block read is not supported, it emulates it using either word or byte   read protocols depending on availability.     The addresses of the I2C slave device that are accessed with this function   must be mapped to a linear region, so that a block read will have the same   effect as a byte read. Before using this function you must double-check   if the I2C slave does support exchanging a block transfer with a byte   transfer. ", "if (PCI_FUNC(pdev->devfn) != 3)return -ENODEV;pci_read_config_byte(pdev, SMBGCFG, &temp);if ((temp & 128) == 0) ": "amd756_smbus = {.owner= THIS_MODULE,.class          = I2C_CLASS_HWMON | I2C_CLASS_SPD,.algo= &smbus_algorithm,};enum chiptype { AMD756, AMD766, AMD768, NFORCE, AMD8111 };static const char  chipname[] = {\"AMD756\", \"AMD766\", \"AMD768\",\"nVidia nForce\", \"AMD8111\",};static const struct pci_device_id amd756_ids[] = {{ PCI_DEVICE(PCI_VENDOR_ID_AMD, PCI_DEVICE_ID_AMD_VIPER_740B),  .driver_data = AMD756 },{ PCI_DEVICE(PCI_VENDOR_ID_AMD, PCI_DEVICE_ID_AMD_VIPER_7413),  .driver_data = AMD766 },{ PCI_DEVICE(PCI_VENDOR_ID_AMD, PCI_DEVICE_ID_AMD_OPUS_7443),  .driver_data = AMD768 },{ PCI_DEVICE(PCI_VENDOR_ID_AMD, PCI_DEVICE_ID_AMD_8111_SMBUS),  .driver_data = AMD8111 },{ PCI_DEVICE(PCI_VENDOR_ID_NVIDIA, PCI_DEVICE_ID_NVIDIA_NFORCE_SMBUS),  .driver_data = NFORCE },{ 0, }};MODULE_DEVICE_TABLE (pci, amd756_ids);static int amd756_probe(struct pci_dev  pdev, const struct pci_device_id  id){int nforce = (id->driver_data == NFORCE);int error;u8 temp;if (amd756_ioport) {dev_err(&pdev->dev, \"Only one device supported \"       \"(you have a strange motherboard, btw)\\n\");return -ENODEV;}if (nforce) {if (PCI_FUNC(pdev->devfn) != 1)return -ENODEV;pci_read_config_word(pdev, SMBBANFORCE, &amd756_ioport);amd756_ioport &= 0xfffc;} else {   amd ", "adap->algo = &i2c_bit_algo;adap->retries = 3;if (bit_adap->getscl == NULL)adap->quirks = &i2c_bit_quirk_no_clk_stretch;/* * We tried forcing SCL/SDA to an initial state here. But that caused a * regression, sadly. Check Bugzilla #200045 for details. ": "i2c_bit_add_bus(struct i2c_adapter  adap,     int ( add_adapter)(struct i2c_adapter  )){struct i2c_algo_bit_data  bit_adap = adap->algo_data;int ret;if (bit_test) {ret = test_bus(adap);if (bit_test >= 2 && ret < 0)return -ENODEV;}  register new adapter to i2c module... ", "adap->algo = &pcf_algo;if ((rval = pcf_init_8584(pcf_adap)))return rval;rval = i2c_add_adapter(adap);return rval;}EXPORT_SYMBOL(i2c_pcf_add_bus": "i2c_pcf_add_bus(struct i2c_adapter  adap){struct i2c_algo_pcf_data  pcf_adap = adap->algo_data;int rval;DEB2(dev_dbg(&adap->dev, \"hw routines registered.\\n\"));  register new adapter to i2c module... ", "EXPORT_SYMBOL(eisa_in8": "eisa_in8(unsigned short port){if (EISA_bus)return gsc_readb(eisa_permute(port));return 0xff;}unsigned short eisa_in16(unsigned short port){if (EISA_bus)return le16_to_cpu(gsc_readw(eisa_permute(port)));return 0xffff;}unsigned int eisa_in32(unsigned short port){if (EISA_bus)return le32_to_cpu(gsc_readl(eisa_permute(port)));return 0xffffffff;}void eisa_out8(unsigned char data, unsigned short port){if (EISA_bus)gsc_writeb(data, eisa_permute(port));}void eisa_out16(unsigned short data, unsigned short port){if (EISA_bus)gsc_writew(cpu_to_le16(data), eisa_permute(port));}void eisa_out32(unsigned int data, unsigned short port){if (EISA_bus)gsc_writel(cpu_to_le32(data), eisa_permute(port));}#ifndef CONFIG_PCI  We call these directly without PCI.  See asmio.h. ", "EXPORT_SYMBOL(eisa_in8);EXPORT_SYMBOL(eisa_in16": "eisa_in16(unsigned short port){if (EISA_bus)return le16_to_cpu(gsc_readw(eisa_permute(port)));return 0xffff;}unsigned int eisa_in32(unsigned short port){if (EISA_bus)return le32_to_cpu(gsc_readl(eisa_permute(port)));return 0xffffffff;}void eisa_out8(unsigned char data, unsigned short port){if (EISA_bus)gsc_writeb(data, eisa_permute(port));}void eisa_out16(unsigned short data, unsigned short port){if (EISA_bus)gsc_writew(cpu_to_le16(data), eisa_permute(port));}void eisa_out32(unsigned int data, unsigned short port){if (EISA_bus)gsc_writel(cpu_to_le32(data), eisa_permute(port));}#ifndef CONFIG_PCI  We call these directly without PCI.  See asmio.h. ", "EXPORT_SYMBOL(eisa_in8);EXPORT_SYMBOL(eisa_in16);EXPORT_SYMBOL(eisa_in32": "eisa_in32(unsigned short port){if (EISA_bus)return le32_to_cpu(gsc_readl(eisa_permute(port)));return 0xffffffff;}void eisa_out8(unsigned char data, unsigned short port){if (EISA_bus)gsc_writeb(data, eisa_permute(port));}void eisa_out16(unsigned short data, unsigned short port){if (EISA_bus)gsc_writew(cpu_to_le16(data), eisa_permute(port));}void eisa_out32(unsigned int data, unsigned short port){if (EISA_bus)gsc_writel(cpu_to_le32(data), eisa_permute(port));}#ifndef CONFIG_PCI  We call these directly without PCI.  See asmio.h. ", "EXPORT_SYMBOL(eisa_in8);EXPORT_SYMBOL(eisa_in16);EXPORT_SYMBOL(eisa_in32);EXPORT_SYMBOL(eisa_out8": "eisa_out8(unsigned char data, unsigned short port){if (EISA_bus)gsc_writeb(data, eisa_permute(port));}void eisa_out16(unsigned short data, unsigned short port){if (EISA_bus)gsc_writew(cpu_to_le16(data), eisa_permute(port));}void eisa_out32(unsigned int data, unsigned short port){if (EISA_bus)gsc_writel(cpu_to_le32(data), eisa_permute(port));}#ifndef CONFIG_PCI  We call these directly without PCI.  See asmio.h. ", "EXPORT_SYMBOL(eisa_in8);EXPORT_SYMBOL(eisa_in16);EXPORT_SYMBOL(eisa_in32);EXPORT_SYMBOL(eisa_out8);EXPORT_SYMBOL(eisa_out16": "eisa_out16(unsigned short data, unsigned short port){if (EISA_bus)gsc_writew(cpu_to_le16(data), eisa_permute(port));}void eisa_out32(unsigned int data, unsigned short port){if (EISA_bus)gsc_writel(cpu_to_le32(data), eisa_permute(port));}#ifndef CONFIG_PCI  We call these directly without PCI.  See asmio.h. ", "EXPORT_SYMBOL(eisa_in8);EXPORT_SYMBOL(eisa_in16);EXPORT_SYMBOL(eisa_in32);EXPORT_SYMBOL(eisa_out8);EXPORT_SYMBOL(eisa_out16);EXPORT_SYMBOL(eisa_out32": "eisa_out32(unsigned int data, unsigned short port){if (EISA_bus)gsc_writel(cpu_to_le32(data), eisa_permute(port));}#ifndef CONFIG_PCI  We call these directly without PCI.  See asmio.h. ", "for (cnt = 0; cnt < irt_num_entry; cnt++) ": "iosapic_serial_irq(struct parisc_device  dev){struct iosapic_info  isi;struct irt_entry  irte;struct vector_info  vi;int cnt;int intin;intin = (dev->mod_info >> 24) & 15;  lookup IRT entry for isislotpin set ", "irq = txn_claim_irq(irq);if (irq < 0) ": "gsc_claim_irq(struct gsc_irq  i, int irq){int c = irq;irq += CPU_IRQ_BASE;   virtualize the IRQ first ", "if (vlynq_linked(dev) && readl(&dev->remote->control) &   VLYNQ_CTRL_CLOCK_INT) ": "vlynq_enable_device(struct vlynq_device  dev){int result;struct plat_vlynq_ops  ops = dev->dev.platform_data;result = ops->on(dev);if (result)return result;switch (dev->divisor) {case vlynq_div_external:case vlynq_div_auto:  When the device is brought from reset it should have clock   generation negotiated by hardware.   Check which device is generating clocks and perform setup   accordingly ", "static int __vlynq_try_remote(struct vlynq_device *dev)": "vlynq_bus_type;return driver_register(&driver->driver);}EXPORT_SYMBOL(__vlynq_register_driver);void vlynq_unregister_driver(struct vlynq_driver  driver){driver_unregister(&driver->driver);}EXPORT_SYMBOL(vlynq_unregister_driver);    A VLYNQ remote device can clock the VLYNQ bus master   using a dedicated clock line. In that case, both the   remove device and the bus master should have the same   serial clock dividers configured. Iterate through the   8 possible dividers until we actually link with the   device. ", "if (vbg_gdev)return vbg_gdev;mutex_unlock(&vbg_gdev_mutex);return ERR_PTR(-ENODEV);}EXPORT_SYMBOL(vbg_get_gdev": "vbg_get_gdev(void){mutex_lock(&vbg_gdev_mutex);    Note on success we keep the mutex locked until vbg_put_gdev(),   this stops vbg_pci_remove from removing the device from underneath   vboxsf. vboxsf will only hold a reference for a short while. ", "if (vbg_gdev)return vbg_gdev;mutex_unlock(&vbg_gdev_mutex);return ERR_PTR(-ENODEV);}EXPORT_SYMBOL(vbg_get_gdev);void vbg_put_gdev(struct vbg_dev *gdev)": "vbg_put_gdev(),   this stops vbg_pci_remove from removing the device from underneath   vboxsf. vboxsf will only hold a reference for a short while. ", "ret = hgcm_call_preprocess(parms, parm_count, &bounce_bufs, &size);if (ret) ": "vbg_hgcm_call(struct vbg_dev  gdev, u32 requestor, u32 client_id,  u32 function, u32 timeout_ms,  struct vmmdev_hgcm_function_parameter  parms, u32 parm_count,  int  vbox_status){struct vmmdev_hgcm_call  call;void   bounce_bufs = NULL;bool leak_it;size_t size;int i, ret;size = sizeof(struct vmmdev_hgcm_call) +   parm_count   sizeof(struct vmmdev_hgcm_function_parameter);    Validate and buffer the parameters for the call. This also increases   call_size with the amount of extra space needed for page lists. ", "   int num_devs,/* number of subdevices      ": "mtd_concat_create(struct mtd_info  subdev[],  subdevices to concatenate ", "stats->failed++;continue;}stats->corrected += err;bitflips = max_t(u32, bitflips, err);}stats->bitflips = bitflips;}EXPORT_SYMBOL(mtk_ecc_get_stats": "mtk_ecc_get_stats(struct mtk_ecc  ecc, struct mtk_ecc_stats  stats,       int sectors){u32 offset, i, err;u32 bitflips = 0;stats->corrected = 0;stats->failed = 0;for (i = 0; i < sectors; i++) {offset = (i >> 2) << 2;err = readl(ecc->regs + ECC_DECENUM0 + offset);err = err >> ((i % 4)   ecc->caps->err_shift);err &= ecc->caps->err_mask;if (err == ecc->caps->err_mask) {  uncorrectable errors ", "if (!np)np = of_parse_phandle(of_node, \"ecc-engine\", 0);if (np) ": "of_mtk_ecc_get(struct device_node  of_node){struct mtk_ecc  ecc = NULL;struct device_node  np;np = of_parse_phandle(of_node, \"nand-ecc-engine\", 0);  for backward compatibility ", "if (ecc->caps->pg_irq_sel && config->mode == ECC_NFI_MODE)reg_val |= ECC_PG_IRQ_SEL;if (op == ECC_ENCODE)writew(reg_val, ecc->regs +       ecc->caps->ecc_regs[ECC_ENCIRQ_EN]);elsewritew(reg_val, ecc->regs +       ecc->caps->ecc_regs[ECC_DECIRQ_EN]);}writew(ECC_OP_ENABLE, ecc->regs + ECC_CTL_REG(op));return 0;}EXPORT_SYMBOL(mtk_ecc_enable": "mtk_ecc_enable(struct mtk_ecc  ecc, struct mtk_ecc_config  config){enum mtk_ecc_operation op = config->op;u16 reg_val;int ret;ret = mutex_lock_interruptible(&ecc->lock);if (ret) {dev_err(ecc->dev, \"interrupted when attempting to lock\\n\");return ret;}mtk_ecc_wait_idle(ecc, op);ret = mtk_ecc_config(ecc, config);if (ret) {mutex_unlock(&ecc->lock);return ret;}if (config->mode != ECC_NFI_MODE || op != ECC_ENCODE) {init_completion(&ecc->done);reg_val = ECC_IRQ_EN;    For ECC_NFI_MODE, if ecc->caps->pg_irq_sel is 1, then it   means this chip can only generate one ecc irq during page   read  write. If is 0, generate one ecc irq each ecc step. ", "if (readw(ecc->regs + ECC_CTL_REG(op)) != ECC_OP_ENABLE)op = ECC_DECODE;/* disable it ": "mtk_ecc_disable(struct mtk_ecc  ecc){enum mtk_ecc_operation op = ECC_ENCODE;  find out the running operation ", "len = (config->strength * ecc->caps->parity_bits + 7) >> 3;/* write the parity bytes generated by the ECC back to temp buffer ": "mtk_ecc_encode(struct mtk_ecc  ecc, struct mtk_ecc_config  config,   u8  data, u32 bytes){dma_addr_t addr;u32 len;int ret;addr = dma_map_single(ecc->dev, data, bytes, DMA_TO_DEVICE);ret = dma_mapping_error(ecc->dev, addr);if (ret) {dev_err(ecc->dev, \"dma mapping error\\n\");return -EINVAL;}config->op = ECC_ENCODE;config->addr = addr;ret = mtk_ecc_enable(ecc, config);if (ret) {dma_unmap_single(ecc->dev, addr, bytes, DMA_TO_DEVICE);return ret;}ret = mtk_ecc_wait_done(ecc, ECC_ENCODE);if (ret)goto timeout;mtk_ecc_wait_idle(ecc, ECC_ENCODE);  Program ECC bytes to OOB: per sector oob = FDM + ECC + SPARE ", "int nand_ecc_init_ctx(struct nand_device *nand)": "nand_ecc_cleanup_ctx(). ", "int nand_ecc_prepare_io_req(struct nand_device *nand,    struct nand_page_io_req *req)": "nand_ecc_prepare_io_req - Prepare an IO request   @nand: the NAND device   @req: the IO request ", "int nand_ecc_finish_io_req(struct nand_device *nand,   struct nand_page_io_req *req)": "nand_ecc_finish_io_req - Finish an IO request   @nand: the NAND device   @req: the IO request ", "bool nand_ecc_is_strong_enough(struct nand_device *nand)": "nand_ecc_is_strong_enough - Check if the chip configuration meets the                               datasheet requirements.     @nand: Device to check     If our configuration corrects A bits per B bytes and the minimum   required correction level is X bits per Y bytes, then we must ensure   both of the following are true:     (1) A  B >= X  Y   (2) A >= X     Requirement (1) ensures we can correct for the required bitflip density.   Requirement (2) ensures we can correct even when all bitflips are clumped   in the same sector. ", "list_for_each_entry(item, &on_host_hw_engines, node)if (item == engine)return 0;mutex_lock(&on_host_hw_engines_mutex);list_add_tail(&engine->node, &on_host_hw_engines);mutex_unlock(&on_host_hw_engines_mutex);return 0;}EXPORT_SYMBOL(nand_ecc_register_on_host_hw_engine": "nand_ecc_register_on_host_hw_engine(struct nand_ecc_engine  engine){struct nand_ecc_engine  item;if (!engine)return -EINVAL;  Prevent multiple registrations of one engine ", "np = of_parse_phandle(dev->of_node, \"nand-ecc-engine\", 0);if (np) ": "nand_ecc_get_on_host_hw_engine(struct nand_device  nand){struct nand_ecc_engine  engine = NULL;struct device  dev = &nand->mtd.dev;struct platform_device  pdev;struct device_node  np;if (list_empty(&on_host_hw_engines))return NULL;  Check for an explicit nand-ecc-engine property ", "int nand_ecc_sw_bch_calculate(struct nand_device *nand,      const unsigned char *buf, unsigned char *code)": "nand_ecc_sw_bch_calculate - Calculate the ECC corresponding to a data block   @nand: NAND device   @buf: Input buffer with raw data   @code: Output buffer with ECC ", "int nand_ecc_sw_bch_correct(struct nand_device *nand, unsigned char *buf,    unsigned char *read_ecc, unsigned char *calc_ecc)": "nand_ecc_sw_bch_correct - Detect, correct and report bit error(s)   @nand: NAND device   @buf: Raw data read from the chip   @read_ecc: ECC bytes from the chip   @calc_ecc: ECC calculated from the raw data     Detect and correct bit errors for a data block. ", "if (mtd->oobsize < 64) ": "nand_ecc_sw_bch_init_ctx(struct nand_device  nand){struct nand_ecc_props  conf = &nand->ecc.ctx.conf;struct mtd_info  mtd = nanddev_to_mtd(nand);struct nand_ecc_sw_bch_conf  engine_conf;unsigned int code_size = 0, nsteps;int ret;  Only large page NAND chips may use BCH ", "u32 cur;/* rp0..rp17 are the various accumulated parities (per byte) ": "ecc_sw_hamming_calculate(const unsigned char  buf, unsigned int step_size,     unsigned char  code, bool sm_order){const u32  bp = (uint32_t  )buf;const u32 eccsize_mult = (step_size == 256) ? 1 : 2;  current value in buffer ", "int nand_ecc_sw_hamming_calculate(struct nand_device *nand,  const unsigned char *buf, unsigned char *code)": "nand_ecc_sw_hamming_calculate - Calculate 3-byte ECC for 256512-byte block   @nand: NAND device   @buf: Input buffer with raw data   @code: Output buffer with ECC ", "static const char addressbits[256] = ": "nand_ecc_sw_hamming_correct for more details ", "if (conf->step_size != 256 && conf->step_size != 512)conf->step_size = 256;engine_conf = kzalloc(sizeof(*engine_conf), GFP_KERNEL);if (!engine_conf)return -ENOMEM;ret = nand_ecc_init_req_tweaking(&engine_conf->req_ctx, nand);if (ret)goto free_engine_conf;engine_conf->code_size = 3;engine_conf->calc_buf = kzalloc(mtd->oobsize, GFP_KERNEL);engine_conf->code_buf = kzalloc(mtd->oobsize, GFP_KERNEL);if (!engine_conf->calc_buf || !engine_conf->code_buf) ": "nand_ecc_sw_hamming_init_ctx(struct nand_device  nand){struct nand_ecc_props  conf = &nand->ecc.ctx.conf;struct nand_ecc_sw_hamming_conf  engine_conf;struct mtd_info  mtd = nanddev_to_mtd(nand);int ret;if (!mtd->ooblayout) {switch (mtd->oobsize) {case 8:case 16:mtd_set_ooblayout(mtd, nand_get_small_page_ooblayout());break;case 64:case 128:mtd_set_ooblayout(mtd,  nand_get_large_page_hamming_ooblayout());break;default:return -ENOTSUPP;}}conf->engine_type = NAND_ECC_ENGINE_TYPE_SOFT;conf->algo = NAND_ECC_ALGO_HAMMING;conf->step_size = nand->ecc.user_conf.step_size;conf->strength = 1;  Use the strongest configuration by default ", "static loff_t flexonenand_addr(struct onenand_chip *this, int block)": "onenand_addr - Return address of the block   @this:OneNAND device structure   @block:Block number on Flex-OneNAND     Return address of the block ", "int flexonenand_region(struct mtd_info *mtd, loff_t addr)": "flexonenand_region - [Flex-OneNAND] Return erase region of addr   @mtd:MTD device structure   @addr:address whose erase region needs to be identified ", "int nand_check_erased_ecc_chunk(void *data, int datalen,void *ecc, int ecclen,void *extraoob, int extraooblen,int bitflips_threshold)": "nand_check_erased_ecc_chunk - check if an ECC chunk contains (almost) only   0xff data   @data: data buffer to test   @datalen: data length   @ecc: ECC buffer   @ecclen: ECC length   @extraoob: extra OOB buffer   @extraooblen: extra OOB length   @bitflips_threshold: maximum number of bitflips     Check if a data buffer and its associated ECC and OOB data contains only   0xff pattern, which means the underlying region has been erased and is   ready to be programmed.   The bitflips_threshold specify the maximum number of bitflips before   considering the region as not erased.     Note:   1 ECC algorithms are working on pre-defined block sizes which are usually      different from the NAND page size. When fixing bitflips, ECC engines will      report the number of errors per chunk, and the NAND core infrastructure      expect you to return the maximum number of bitflips for the whole page.      This is why you should always use this function on a single chunk and      not on the whole page. After checking each chunk you should update your      max_bitflips value accordingly.   2 When checking for bitflips in erased pages you should not only check      the payload data but also their associated ECC data, because a user might      have programmed almost all bits to 1 but a few. In this case, we      shouldn't consider the chunk as erased, and checking ECC bytes prevent      this case.   3 The extraoob argument is optional, and should be used if some of your OOB      data are protected by the ECC engine.      It could also be used if you support subpages and want to attach some      extra OOB data to an ECC chunk.     Returns a positive number of bitflips less than or equal to   bitflips_threshold, or -ERROR_CODE for bitflips in excess of the   threshold. In case of success, the passed buffers are filled with 0xff. ", "int nand_read_page_raw_notsupp(struct nand_chip *chip, u8 *buf,       int oob_required, int page)": "nand_read_page_raw_notsupp - dummy read raw page function   @chip: nand chip info structure   @buf: buffer to store read data   @oob_required: caller requires OOB data read to chip->oob_poi   @page: page number to read     Returns -ENOTSUPP unconditionally. ", "int nand_monolithic_read_page_raw(struct nand_chip *chip, u8 *buf,  int oob_required, int page)": "nand_monolithic_read_page_raw - Monolithic page read in raw mode   @chip: NAND chip info structure   @buf: buffer to store read data   @oob_required: caller requires OOB data read to chip->oob_poi   @page: page number to read     This is a raw page read, ie. without any error detectioncorrection.   Monolithic means we are requesting all the relevant data (main plus   eventually OOB) to be loaded in the NAND cache and sent over the   bus (from the NAND chip to the NAND controller) in a single   operation. This is an alternative to nand_read_page_raw(), which   first reads the main data, and if the OOB data is requested too,   then reads more data on the bus. ", "int nand_read_oob_std(struct nand_chip *chip, int page)": "nand_read_oob_std - [REPLACEABLE] the most common OOB data read function   @chip: nand chip info structure   @page: page number to read ", "int nand_write_oob_std(struct nand_chip *chip, int page)": "nand_write_oob_std - [REPLACEABLE] the most common OOB data write function   @chip: nand chip info structure   @page: page number to write ", "int nand_write_page_raw_notsupp(struct nand_chip *chip, const u8 *buf,int oob_required, int page)": "nand_write_page_raw_notsupp - dummy raw page write function   @chip: nand chip info structure   @buf: data buffer   @oob_required: must write chip->oob_poi to OOB   @page: page number to write     Returns -ENOTSUPP unconditionally. ", "int nand_monolithic_write_page_raw(struct nand_chip *chip, const u8 *buf,   int oob_required, int page)": "nand_monolithic_write_page_raw - Monolithic page write in raw mode   @chip: NAND chip info structure   @buf: data buffer to write   @oob_required: must write chip->oob_poi to OOB   @page: page number to write     This is a raw page write, ie. without any error detectioncorrection.   Monolithic means we are requesting all the relevant data (main plus   eventually OOB) to be sent over the bus and effectively programmed   into the NAND chip arrays in a single operation. This is an   alternative to nand_write_page_raw(), which first sends the main   data, then eventually send the OOB data by latching more data   cycles on the NAND bus, and finally sends the program command to   synchronyze the NAND chip cache. ", "int rawnand_dt_parse_gpio_cs(struct device *dev, struct gpio_desc ***cs_array,     unsigned int *ncs_array)": "rawnand_dt_parse_gpio_cs - Parse the gpio-cs property of a controller   @dev: Device that will be parsed. Also used for managed allocations.   @cs_array: Array of GPIO desc pointers allocated on success   @ncs_array: Number of entries in @cs_array updated on success.   @return 0 on success, an error otherwise. ", "int nand_scan_with_ids(struct nand_chip *chip, unsigned int maxchips,       struct nand_flash_dev *ids)": "nand_scan_with_ids - [NAND Interface] Scan for the NAND device   @chip: NAND chip object   @maxchips: number of chips to scan for.   @ids: optional flash IDs table     This fills out all the uninitialized function pointers with the defaults.   The flash ID is read and the mtdchip structures are filled with the   appropriate values. ", "int elm_config(struct device *dev, enum bch_ecc bch_type,int ecc_steps, int ecc_step_size, int ecc_syndrome_size)": "elm_config - Configure ELM module   @dev:ELM device   @bch_type:Type of BCH ecc   @ecc_steps:ECC steps to assign to config   @ecc_step_size:ECC step size to assign to config   @ecc_syndrome_size:ECC syndrome size to assign to config ", "void elm_decode_bch_error_page(struct device *dev, u8 *ecc_calc,struct elm_errorvec *err_vec)": "elm_decode_bch_error_page - Locate error position   @dev:device pointer   @ecc_calc:calculated ECC bytes from GPMC   @err_vec:elm error vectors     Called with one or more error reported vectors & vectors with   error reported is updated in err_vec[].error_reported ", "int nand_get_set_features_notsupp(struct nand_chip *chip, int addr,  u8 *subfeature_param)": "nand_get_set_features_notsupp - setget features stub returning -ENOTSUPP   @chip: nand chip info structure   @addr: feature address.   @subfeature_param: the subfeature parameters, a four bytes array.     Should be used by NAND controller drivers that do not support the SETGET   FEATURES operations. ", "return DIV_ROUND_UP(strength * fls(step_size * 8), 16) * 2;}EXPORT_SYMBOL(denali_calc_ecc_bytes": "denali_calc_ecc_bytes(int step_size, int strength){  BCH code.  Denali requires ecc.bytes to be multiple of 2 ", "if (!denali->revision)denali->revision = swab16(ioread32(denali->reg + REVISION));denali->nbanks = 1 << FIELD_GET(FEATURES__N_BANKS, features);/* the encoding changed from rev 5.0 to 5.1 ": "denali_init(struct denali_controller  denali){u32 features = ioread32(denali->reg + FEATURES);int ret;nand_controller_init(&denali->controller);denali->controller.ops = &denali_controller_ops;init_completion(&denali->complete);spin_lock_init(&denali->irq_lock);INIT_LIST_HEAD(&denali->chips);denali->active_bank = DENALI_INVALID_BANK;    The REVISION register may not be reliable. Platforms are allowed to   override it. ", "int nand_create_bbt(struct nand_chip *this)": "nand_create_bbt - [NAND Interface] Select a default bad block table for the device   @this: NAND chip object     This function selects the default bad block table support for the device and   calls the nand_scan_bbt function. ", "struct platform_device *pdev = to_platform_device(userdev);const struct atmel_pmecc_caps *caps;const struct of_device_id *match;/* No PMECC engine available. ": "devm_atmel_pmecc_get(struct device  userdev){struct atmel_pmecc  pmecc;struct device_node  np;if (!userdev)return ERR_PTR(-EINVAL);if (!userdev->of_node)return NULL;np = of_parse_phandle(userdev->of_node, \"ecc-engine\", 0);if (np) {pmecc = atmel_pmecc_get_by_node(userdev, np);of_node_put(np);} else {    Support old DT bindings: in this case the PMECC iomem   resources are directly defined in the user pdev at position   1 and 2. Extract all relevant information from there. ", "mtd->_read = lpddr_read;mtd->type = MTD_NORFLASH;mtd->flags = MTD_CAP_NORFLASH;mtd->flags &= ~MTD_BIT_WRITEABLE;mtd->_erase = lpddr_erase;mtd->_write = lpddr_write_buffers;mtd->_writev = lpddr_writev;mtd->_lock = lpddr_lock;mtd->_unlock = lpddr_unlock;if (map_is_linear(map)) ": "lpddr_cmdset(struct map_info  map){struct lpddr_private  lpddr = map->fldrv_priv;struct flchip_shared  shared;struct flchip  chip;struct mtd_info  mtd;int numchips;int i, j;mtd = kzalloc(sizeof( mtd), GFP_KERNEL);if (!mtd)return NULL;mtd->priv = map;mtd->type = MTD_NORFLASH;  Fill in the default mtd operations ", "if (((type * interleave) > bankwidth) && ((cmd_ofs & 0xff) == 0xaa))addr |= (type >> 1)*interleave;return  addr;}EXPORT_SYMBOL(cfi_build_cmd_addr": "cfi_build_cmd_addr(uint32_t cmd_ofs,struct map_info  map, struct cfi_private  cfi){unsigned bankwidth = map_bankwidth(map);unsigned interleave = cfi_interleave(cfi);unsigned type = cfi->device_type;uint32_t addr;addr = (cmd_ofs   type)   interleave;  Modify the unlock address if we are in compatibility mode.   For 16bit devices on 8 bit busses   and 32bit devices on 16 bit busses   set the low bit of the alternating bit sequence of the address. ", "if (((type * interleave) > bankwidth) && ((cmd_ofs & 0xff) == 0xaa))addr |= (type >> 1)*interleave;return  addr;}EXPORT_SYMBOL(cfi_build_cmd": "cfi_build_cmd_addr(uint32_t cmd_ofs,struct map_info  map, struct cfi_private  cfi){unsigned bankwidth = map_bankwidth(map);unsigned interleave = cfi_interleave(cfi);unsigned type = cfi->device_type;uint32_t addr;addr = (cmd_ofs   type)   interleave;  Modify the unlock address if we are in compatibility mode.   For 16bit devices on 8 bit busses   and 32bit devices on 16 bit busses   set the low bit of the alternating bit sequence of the address. ", "if (map_bankwidth_is_large(map)) ": "cfi_merge_status(map_word val, struct map_info  map,   struct cfi_private  cfi){int wordwidth, words_per_bus, chip_mode, chips_per_word;unsigned long onestat, res = 0;int i;  We do it this way to give the compiler a fighting chance   of optimising away all the crap for 'bankwidth' larger than   an unsigned long, in the common case where that support is   disabled ", "cfi_qry_mode_on(base, map, cfi);/* Read in the Extended Query Table ": "cfi_read_pri(struct map_info  map, __u16 adr, __u16 size, const char  name){struct cfi_private  cfi = map->fldrv_priv;__u32 base = 0;  cfi->chips[0].start;int ofs_factor = cfi->interleave   cfi->device_type;int i;struct cfi_extquery  extp = NULL;if (!adr)goto out;printk(KERN_INFO \"%s Extended Query Table at 0x%4.4X\\n\", name, adr);extp = kmalloc(size, GFP_KERNEL);if (!extp)goto out;#ifdef CONFIG_MTD_XIPlocal_irq_disable();#endif  Switch it into Query Mode ", "i = 0;/* Skip all erase regions which are ended before the start of   the requested erase. Actually, to save on the calculations,   we skip to the first erase region which starts after the   start of the requested erase, and then go back one.": "cfi_varsize_frob(struct mtd_info  mtd, varsize_frob_t frob,     loff_t ofs, size_t len, void  thunk){struct map_info  map = mtd->priv;struct cfi_private  cfi = map->fldrv_priv;unsigned long adr;int chipnum, ret = 0;int i, first;struct mtd_erase_region_info  regions = mtd->eraseregions;  Check that both start and end of the requested erase are   aligned with the erasesize at the appropriate addresses. ", "struct mtd_info *do_map_probe(const char *name, struct map_info *map)": "unregister_mtd_chip_driver(struct mtd_chip_driver  drv){spin_lock(&chip_drvs_lock);list_del(&drv->list);spin_unlock(&chip_drvs_lock);}static struct mtd_chip_driver  get_mtd_chip_driver (const char  name){struct mtd_chip_driver  ret = NULL,  this;spin_lock(&chip_drvs_lock);list_for_each_entry(this, &chip_drvs_list, list) {if (!strcmp(this->name, name)) {ret = this;break;}}if (ret && !try_module_get(ret->module))ret = NULL;spin_unlock(&chip_drvs_lock);return ret;}  Hide all the horrid details, like some silly person taking   get_module_symbol() away from us, from the caller. ", "module_put(drv->module);return ret;}/* * Destroy an MTD device which was created for a map device. * Make sure the MTD device is already unregistered before calling this ": "do_map_probe(const char  name, struct map_info  map){struct mtd_chip_driver  drv;struct mtd_info  ret;drv = get_mtd_chip_driver(name);if (!drv && !request_module(\"%s\", name))drv = get_mtd_chip_driver(name);if (!drv)return NULL;ret = drv->probe(map);  We decrease the use count here. It may have been a   probe-only module, which is no longer required from this   point, having given us a handle on (and increased the use   count of) the actual driver code.", "cfi = genprobe_ident_chips(map, cp);if (!cfi)return NULL;map->fldrv_priv = cfi;/* OK we liked it. Now find a driver for the command set it talks ": "mtd_do_chip_probe(struct map_info  map, struct chip_probe  cp){struct mtd_info  mtd;struct cfi_private  cfi;  First probe the map to see if we have CFI stuff there. ", "int zcrypt_queue_register(struct zcrypt_queue *zq)": "zcrypt_queue_register() - Register a crypto queue device.   @zq: Pointer to a crypto queue device     Register a crypto queue device. Returns 0 if successful. ", "void zcrypt_queue_unregister(struct zcrypt_queue *zq)": "zcrypt_queue_unregister(): Unregister a crypto queue device.   @zq: Pointer to crypto queue device     Unregister a crypto queue device. ", "static DEFINE_SPINLOCK(ap_domain_lock);module_param_named(domain, ap_domain_index, int, 0440);MODULE_PARM_DESC(domain, \"domain index for ap devices\");EXPORT_SYMBOL(ap_domain_index": "ap_domain_index = -1;  Adjunct Processor Domain Index ", "int ap_test_config_usage_domain(unsigned int domain)": "ap_test_config_usage_domain(): Test, whether an AP usage domain   is configured.     Returns 0 if the usage domain is not configured     1 if the usage domain is configured or       if the configuration information is not available ", "int ap_test_config_ctrl_domain(unsigned int domain)": "ap_test_config_ctrl_domain(): Test, whether an AP control domain   is configured.   @domain AP control domain ID     Returns 1 if the control domain is configured     0 in all other cases ", "int ap_owned_by_def_drv(int card, int queue)": "ap_owned_by_def_drv: indicates whether an AP adapter is reserved for the  default host driver or not.   @card: the APID of the adapter card to check   @queue: the APQI of the queue to check     Note: the ap_perms_mutex must be locked by the caller of this function.     Return: an int specifying whether the AP adapter is reserved for the host (1)     or not (0). ", "int ap_apqn_in_matrix_owned_by_def_drv(unsigned long *apm,       unsigned long *aqm)": "ap_apqn_in_matrix_owned_by_def_drv: indicates whether every APQN contained in         a set is reserved for the host drivers         or not.   @apm: a bitmap specifying a set of APIDs comprising the APQNs to check   @aqm: a bitmap specifying a set of APQIs comprising the APQNs to check     Note: the ap_perms_mutex must be locked by the caller of this function.     Return: an int specifying whether each APQN is reserved for the host (1) or     not (0) ", "del_timer(&ap_config_timer);queue_work(system_long_wq, &ap_scan_work);flush_work(&ap_scan_work);}EXPORT_SYMBOL(ap_bus_force_rescan": "ap_bus_force_rescan(void){  processing a asynchronous bus rescan ", "if (bits & 0x07)return -EINVAL;size = BITS_TO_LONGS(bits) * sizeof(unsigned long);newmap = kmalloc(size, GFP_KERNEL);if (!newmap)return -ENOMEM;if (mutex_lock_interruptible(lock)) ": "ap_parse_mask_str(const char  str,      unsigned long  bitmap, int bits,      struct mutex  lock){unsigned long  newmap, size;int rc;  bits needs to be a multiple of 8 ", "static inline struct ap_queue_status__ap_send(ap_qid_t qid, unsigned long psmid, void *msg, size_t msglen,  int special)": "ap_send(): Send message to adjunct processor queue.   @qid: The AP queue number   @psmid: The program supplied message identifier   @msg: The message text   @msglen: The message length   @special: Special Bit     Returns AP queue status structure.   Condition code 1 on NQAP can't happen because the L bit is 1.   Condition code 2 on NQAP also means the send is incomplete,   because a segment boundary was reached. The NQAP is repeated. ", "int ap_queue_message(struct ap_queue *aq, struct ap_message *ap_msg)": "ap_queue_message(): Queue a request to an AP device.   @aq: The AP device to queue the message to   @ap_msg: The message that is to be added ", "void ap_cancel_message(struct ap_queue *aq, struct ap_message *ap_msg)": "ap_cancel_message(): Cancel a crypto request.   @aq: The AP device that has the message queued   @ap_msg: The message that is to be removed     Cancel a crypto request. This is done by removing the request   from the device pending or request queue. Note that the   request stays on the AP queue. When it finishes the message   reply will be discarded because the psmid can't be found. ", "static inline bool ap_q_supports_bind(struct ap_queue *aq)": "ap_flush_queue(struct ap_queue  aq);    some AP queue helper functions ", "} keyblock;} lv3;} __packed * prepparm;/* get already prepared memory for 2 cprbs with param block each ": "cca_genseckey(u16 cardnr, u16 domain,  u32 keybitsize, u8  seckey){int i, rc, keysize;int seckeysize;u8  mem,  ptr;struct CPRBX  preqcblk,  prepcblk;struct ica_xcRB xcrb;struct kgreqparm {u8  subfunc_code[2];u16 rule_array_len;struct lv1 {u16 len;char  key_form[8];char  key_length[8];char  key_type1[8];char  key_type2[8];} lv1;struct lv2 {u16 len;struct keyid {u16 len;u16 attr;u8  data[SECKEYBLOBSIZE];} keyid[6];} lv2;} __packed   preqparm;struct kgrepparm {u8  subfunc_code[2];u16 rule_array_len;struct lv3 {u16 len;u16 keyblocklen;struct {u16 toklen;u16 tokattr;u8  tok[];  ... some more data ... ", "} __packed * preqparm;struct lv2 ": "cca_clr2seckey(u16 cardnr, u16 domain, u32 keybitsize,   const u8  clrkey, u8  seckey){int rc, keysize, seckeysize;u8  mem,  ptr;struct CPRBX  preqcblk,  prepcblk;struct ica_xcRB xcrb;struct cmreqparm {u8  subfunc_code[2];u16 rule_array_len;char  rule_array[8];struct lv1 {u16 len;u8  clrkey[];} lv1;  followed by struct lv2 ", "} lv2;} __packed * preqparm;struct uskrepparm ": "cca_sec2protkey(u16 cardnr, u16 domain,    const u8  seckey, u8  protkey, u32  protkeylen,    u32  protkeytype){int rc;u8  mem,  ptr;struct CPRBX  preqcblk,  prepcblk;struct ica_xcRB xcrb;struct uskreqparm {u8  subfunc_code[2];u16 rule_array_len;struct lv1 {u16 len;u16 attr_len;u16 attr_flags;} lv1;struct lv2 {u16 len;u16 attr_len;u16 attr_flags;u8  token[];        cca secure key token ", "static const u8 aes_cipher_key_skeleton[] = ": "cca_clr2cipherkey(). ", "} kb;} __packed * preqparm;struct aurepparm ": "cca_cipher2protkey(u16 cardnr, u16 domain, const u8  ckey,       u8  protkey, u32  protkeylen, u32  protkeytype){int rc;u8  mem,  ptr;struct CPRBX  preqcblk,  prepcblk;struct ica_xcRB xcrb;struct aureqparm {u8  subfunc_code[2];u16 rule_array_len;u8  rule_array[8];struct {u16 len;u16 tk_blob_len;u16 tk_blob_tag;u8  tk_blob[66];} vud;struct {u16 len;u16 cca_key_token_len;u16 cca_key_token_flags;u8  cca_key_token[];   64 or more ", "u8  flags[2];u8  algo;u8  form;u8  pad1[3];u16 keylen;u8  key[];  /* the key (keylen bytes) ": "cca_ecc2protkey(u16 cardnr, u16 domain, const u8  key,    u8  protkey, u32  protkeylen, u32  protkeytype){int rc;u8  mem,  ptr;struct CPRBX  preqcblk,  prepcblk;struct ica_xcRB xcrb;struct aureqparm {u8  subfunc_code[2];u16 rule_array_len;u8  rule_array[8];struct {u16 len;u16 tk_blob_len;u16 tk_blob_tag;u8  tk_blob[66];} vud;struct {u16 len;u16 cca_key_token_len;u16 cca_key_token_flags;u8  cca_key_token[];} kb;} __packed   preqparm;struct aurepparm {u8  subfunc_code[2];u16 rule_array_len;struct {u16 len;u16 sublen;u16 tag;struct cpacfkeyblock {u8  version;    version of this struct ", "rc = alloc_and_prep_cprbmem(parmbsize, &mem, &preqcblk, &prepcblk);if (rc)return rc;/* fill request cprb struct ": "cca_query_crypto_facility(u16 cardnr, u16 domain,      const char  keyword,      u8  rarray, size_t  rarraylen,      u8  varray, size_t  varraylen){int rc;u16 len;u8  mem,  ptr;struct CPRBX  preqcblk,  prepcblk;struct ica_xcRB xcrb;struct fqreqparm {u8  subfunc_code[2];u16 rule_array_len;char  rule_array[8];struct lv1 {u16 len;u8  data[VARDATASIZE];} lv1;u16 dummylen;} __packed   preqparm;size_t parmbsize = sizeof(struct fqreqparm);struct fqrepparm {u8  subfunc_code[2];u8  lvdata[];} __packed   prepparm;  get already prepared memory for 2 cprbs with param block each ", "device_status = kvmalloc_array(MAX_ZDEV_ENTRIES_EXT,       sizeof(struct zcrypt_device_status_ext),       GFP_KERNEL);if (!device_status)return -ENOMEM;zcrypt_device_status_mask_ext(device_status);/* allocate 1k space for up to 256 apqns ": "ep11_findcard2(u32   apqns, u32  nr_apqns, u16 cardnr, u16 domain,   int minhwtype, int minapi, const u8  wkvp){struct zcrypt_device_status_ext  device_status;u32  _apqns = NULL, _nr_apqns = 0;int i, card, dom, rc = -ENOMEM;struct ep11_domain_info edi;struct ep11_card_info eci;  fetch status of all crypto cards ", ",     sizeof(*pmqi), (u8 *)pmqi);if (rc) ": "ep11_get_card_info(u16 card, struct ep11_card_info  info, int verify){int rc;struct ep11_module_query_info {u32 API_ord_nr;u32 firmware_id;u8  FW_major_vers;u8  FW_minor_vers;u8  CSP_major_vers;u8  CSP_minor_vers;u8  fwid[32];u8  xcp_config_hash[32];u8  CSP_config_hash[32];u8  serial[16];u8  module_date_time[16];u64 op_mode;u32 PKCS11_flags;u32 ext_flags;u32 domains;u32 sym_state_bytes;u32 digest_state_bytes;u32 pin_blob_bytes;u32 SPKI_bytes;u32 priv_key_blob_bytes;u32 sym_blob_bytes;u32 max_payload_bytes;u32 CP_profile_bytes;u32 max_CP_index;} __packed   pmqi = NULL;rc = card_cache_fetch(card, info);if (rc || verify) {pmqi = kmalloc(sizeof( pmqi), GFP_KERNEL);if (!pmqi)return -ENOMEM;rc = ep11_query_info(card, AUTOSEL_DOM,     0x01   module info query ", ",     sizeof(*p_dom_info), (u8 *)p_dom_info);if (rc)goto out;memset(info, 0, sizeof(*info));info->cur_wk_state = '0';info->new_wk_state = '0';if (p_dom_info->dom_flags & 0x10 /* left imprint mode ": "ep11_get_domain_info(u16 card, u16 domain, struct ep11_domain_info  info){int rc;struct ep11_domain_query_info {u32 dom_index;u8  cur_WK_VP[32];u8  new_WK_VP[32];u32 dom_flags;u64 op_mode;} __packed   p_dom_info;p_dom_info = kmalloc(sizeof( p_dom_info), GFP_KERNEL);if (!p_dom_info)return -ENOMEM;rc = ep11_query_info(card, domain, 0x03   domain info query ", "req = alloc_cprb(sizeof(struct keygen_req_pl));if (!req)goto out;req_pl = (struct keygen_req_pl *)(((u8 *)req) + sizeof(*req));api = (!keygenflags || keygenflags & 0x00200000) ? 4 : 1;prep_head(&req_pl->head, sizeof(*req_pl), api, 21); /* GenerateKey ": "ep11_genaeskey(u16 card, u16 domain, u32 keybitsize, u32 keygenflags,   u8  keybuf, size_t  keybufsize){struct keygen_req_pl {struct pl_head head;u8  var_tag;u8  var_len;u32 var;u8  keybytes_tag;u8  keybytes_len;u32 keybytes;u8  mech_tag;u8  mech_len;u32 mech;u8  attr_tag;u8  attr_len;u32 attr_header;u32 attr_bool_mask;u32 attr_bool_bits;u32 attr_val_len_type;u32 attr_val_len_value;u8  pin_tag;u8  pin_len;} __packed   req_pl;struct keygen_rep_pl {struct pl_head head;u8  rc_tag;u8  rc_len;u32 rc;u8  data_tag;u8  data_lenfmt;u16 data_len;u8  data[512];} __packed   rep_pl;struct ep11_cprb  req = NULL,  rep = NULL;struct ep11_target_dev target;struct ep11_urb  urb = NULL;struct ep11keyblob  kb;int api, rc = -ENOMEM;switch (keybitsize) {case 128:case 192:case 256:break;default:DEBUG_ERR(\"%s unknownunsupported keybitsize %d\\n\",__func__, keybitsize);rc = -EINVAL;goto out;}  request cprb and payload ", "keklen = MAXEP11AESKEYBLOBSIZE;kek = kmalloc(keklen, GFP_ATOMIC);if (!kek) ": "ep11_clr2keyblob(u16 card, u16 domain, u32 keybitsize, u32 keygenflags,     const u8  clrkey, u8  keybuf, size_t  keybufsize){int rc;struct ep11keyblob  kb;u8 encbuf[64],  kek = NULL;size_t clrkeylen, keklen, encbuflen = sizeof(encbuf);if (keybitsize == 128 || keybitsize == 192 || keybitsize == 256) {clrkeylen = keybitsize  8;} else {DEBUG_ERR(\"%s unknownunsupported keybitsize %d\\n\",__func__, keybitsize);return -EINVAL;}  allocate memory for the temp kek ", "hdr = (struct ep11kblob_header *)keyblob;if (hdr->type == TOKTYPE_NON_CCA &&    (hdr->version == TOKVER_EP11_AES_WITH_HEADER ||     hdr->version == TOKVER_EP11_ECC_WITH_HEADER) &&    is_ep11_keyblob(keyblob + sizeof(struct ep11kblob_header))) ": "ep11_kblob2protkey(u16 card, u16 dom, const u8  keyblob, size_t keybloblen,       u8  protkey, u32  protkeylen, u32  protkeytype){int rc = -EIO;u8  wkbuf = NULL;size_t wkbuflen, keylen;struct wk_info {u16 version;u8  res1[16];u32 pkeytype;u32 pkeybitsize;u64 pkeysize;u8  res2[8];u8  pkey[];} __packed   wki;const u8  key;struct ep11kblob_header  hdr;  key with or without header ? ", "MODULE_AUTHOR(\"IBM Corporation\");MODULE_DESCRIPTION(\"Cryptographic Coprocessor interface, \" \\   \"Copyright IBM Corp. 2001, 2012\");MODULE_LICENSE(\"GPL\");/* * zcrypt tracepoint functions ": "zcrypt_msgtype6.h\"#include \"zcrypt_msgtype50.h\"#include \"zcrypt_ccamisc.h\"#include \"zcrypt_ep11misc.h\"    Module description. ", "if (tdom < AP_DOMAINS &&    !ap_test_config_usage_domain(tdom) &&    ap_test_config_ctrl_domain(tdom))tdom = AUTOSEL_DOM;pref_zc = NULL;pref_zq = NULL;spin_lock(&zcrypt_list_lock);for_each_zcrypt_card(zc) ": "zcrypt_send_cprb(bool userspace, struct ap_perms  perms,      struct zcrypt_track  tr,      struct ica_xcRB  xcrb){struct zcrypt_card  zc,  pref_zc;struct zcrypt_queue  zq,  pref_zq;struct ap_message ap_msg;unsigned int wgt = 0, pref_wgt = 0;unsigned int func_code;unsigned short  domain, tdom;int cpen, qpen, qid = 0, rc = -ENODEV;struct module  mod;trace_s390_zcrypt_req(xcrb, TB_ZSECSENDCPRB);xcrb->status = 0;ap_init_message(&ap_msg);rc = prep_cca_ap_msg(userspace, xcrb, &ap_msg, &func_code, &domain);if (rc)goto out;tdom =  domain;if (perms != &ap_perms && tdom < AP_DOMAINS) {if (ap_msg.flags & AP_MSG_FLAG_ADMIN) {if (!test_bit_inv(tdom, perms->adm)) {rc = -ENODEV;goto out;}} else if ((ap_msg.flags & AP_MSG_FLAG_USAGE) == 0) {rc = -EOPNOTSUPP;goto out;}}    If a valid target domain is set and this domain is NOT a usage   domain but a control only domain, autoselect target domain. ", "targets = NULL;if (target_num != 0) ": "zcrypt_send_ep11_cprb(bool userspace, struct ap_perms  perms,   struct zcrypt_track  tr,   struct ep11_urb  xcrb){struct zcrypt_card  zc,  pref_zc;struct zcrypt_queue  zq,  pref_zq;struct ep11_target_dev  targets;unsigned short target_num;unsigned int wgt = 0, pref_wgt = 0;unsigned int func_code, domain;struct ap_message ap_msg;int cpen, qpen, qid = 0, rc = -ENODEV;struct module  mod;trace_s390_zcrypt_req(xcrb, TP_ZSENDEP11CPRB);ap_init_message(&ap_msg);target_num = (unsigned short)xcrb->targets_num;  empty list indicates autoselect (all available targets) ", "rc = ap_wait_init_apqn_bindings_complete(msecs_to_jiffies(60 * 1000));switch (rc) ": "zcrypt_wait_api_operational(void){static DEFINE_MUTEX(zcrypt_wait_api_lock);static int zcrypt_wait_api_state;int rc;rc = mutex_lock_interruptible(&zcrypt_wait_api_lock);if (rc)return rc;switch (zcrypt_wait_api_state) {case 0:  initial state, invoke wait for the ap bus complete ", "int zcrypt_card_register(struct zcrypt_card *zc)": "zcrypt_card_register() - Register a crypto card device.   @zc: Pointer to a crypto card device     Register a crypto card device. Returns 0 if successful. ", "void zcrypt_card_unregister(struct zcrypt_card *zc)": "zcrypt_card_unregister(): Unregister a crypto card device.   @zc: Pointer to crypto card device     Unregister a crypto card device. ", "if (device->discipline && device->discipline->dump_sense_dbf)device->discipline->dump_sense_dbf(device, irb, \"log\");}EXPORT_SYMBOL(dasd_log_sense_dbf": "dasd_log_sense_dbf(struct dasd_ccw_req  cqr, struct irb  irb){struct dasd_device  device;device = cqr->startdev;  dump sense data to s390 debugfeature", "struct dasd_ccw_req *dasd_default_erp_action(struct dasd_ccw_req *cqr)": "dasd_free_erp_request(struct dasd_ccw_req  cqr, struct dasd_device   device){unsigned long flags;spin_lock_irqsave(&device->mem_lock, flags);dasd_free_chunk(&device->erp_chunks, cqr);spin_unlock_irqrestore(&device->mem_lock, flags);atomic_dec(&device->ref_count);}    dasd_default_erp_action just retries the current cqr ", "while (cqr->refers != NULL) ": "dasd_default_erp_postaction(struct dasd_ccw_req  cqr){int success;unsigned long startclk, stopclk;struct dasd_device  startdev;BUG_ON(cqr->refers == NULL || cqr->function == NULL);success = cqr->status == DASD_CQR_DONE;startclk = cqr->startclk;stopclk = cqr->stopclk;startdev = cqr->startdev;  free all ERPs - but NOT the original cqr ", "BUG_ON(datasize > PAGE_SIZE ||       (cplength*sizeof(struct ccw1)) > PAGE_SIZE);size = (sizeof(struct dasd_ccw_req) + 7L) & -8L;if (cplength > 0)size += cplength * sizeof(struct ccw1);if (datasize > 0)size += datasize;spin_lock_irqsave(&device->mem_lock, flags);cqr = (struct dasd_ccw_req *)dasd_alloc_chunk(&device->erp_chunks, size);spin_unlock_irqrestore(&device->mem_lock, flags);if (cqr == NULL)return ERR_PTR(-ENOMEM);memset(cqr, 0, sizeof(struct dasd_ccw_req));INIT_LIST_HEAD(&cqr->devlist);INIT_LIST_HEAD(&cqr->blocklist);data = (char *) cqr + ((sizeof(struct dasd_ccw_req) + 7L) & -8L);cqr->cpaddr = NULL;if (cplength > 0) ": "dasd_alloc_erp_request(unsigned int magic, int cplength, int datasize,       struct dasd_device   device){unsigned long flags;struct dasd_ccw_req  cqr;char  data;int size;  Sanity checks ", "if (device->discipline && device->discipline->dump_sense)device->discipline->dump_sense(device, cqr, irb);}voiddasd_log_sense_dbf(struct dasd_ccw_req *cqr, struct irb *irb)": "dasd_log_sense(struct dasd_ccw_req  cqr, struct irb  irb){struct dasd_device  device;device = cqr->startdev;if (cqr->intrc == -ETIMEDOUT) {dev_err(&device->cdev->dev,\"A timeout error occurred for cqr %p\\n\", cqr);return;}if (cqr->intrc == -ENOLINK) {dev_err(&device->cdev->dev,\"A transport error occurred for cqr %p\\n\", cqr);return;}  dump sense data ", "static ssize_tdasd_ro_show(struct device *dev, struct device_attribute *attr, char *buf)": "dasd_set_feature(to_ccwdev(dev), DASD_FEATURE_FAILFAST, val);return rc ? : count;}static DEVICE_ATTR(failfast, 0644, dasd_ff_show, dasd_ff_store);    readonly controls the readonly status of a dasd ", "static void do_kick_device(struct work_struct *work)": "dasd_kick_device will schedule a call do do_kick_device to the kernel   event daemon. ", "static void do_reload_device(struct work_struct *work)": "dasd_reload_device will schedule a call do do_reload_device to the kernel   event daemon. ", "dasd_set_target_state(device, DASD_STATE_NEW);/* Now wait for the devices to come up. ": "dasd_enable_device(struct dasd_device  device){dasd_set_target_state(device, DASD_STATE_ONLINE);if (device->state <= DASD_STATE_KNOWN)  No discipline for device found. ", "rc = dasd_check_cqr(cqr);if (rc)return rc;retries = 0;device = (struct dasd_device *) cqr->startdev;while ((retries < 5) && (cqr->status == DASD_CQR_IN_IO)) ": "dasd_term_IO(struct dasd_ccw_req  cqr){struct dasd_device  device;int retries, rc;char errorstring[ERRORLENGTH];  Check the cqr ", "rc = dasd_check_cqr(cqr);if (rc) ": "dasd_start_IO(struct dasd_ccw_req  cqr){struct dasd_device  device;int rc;char errorstring[ERRORLENGTH];  Check the cqr ", "static int dasd_state_basic_to_ready(struct dasd_device *device)": "dasd_block_clear_timer(device->block);}rc = dasd_flush_device_queue(device);if (rc)return rc;dasd_device_clear_timer(device);dasd_profile_exit(&device->profile);dasd_hosts_exit(device);debugfs_remove(device->debugfs_dentry);DBF_DEV_EVENT(DBF_EMERG, device, \"%p debug area deleted\", device);if (device->debug_area != NULL) {debug_unregister(device->debug_area);device->debug_area = NULL;}device->state = DASD_STATE_KNOWN;return 0;}    Do the initial analysis. The do_analysis function may return   -EAGAIN in which case the device keeps the state DASD_STATE_BASIC   until the discipline decides to continue the startup sequence   by calling the function dasd_change_state. The eckd disciplines   uses this to start a ccw that detects the format. The completion   interrupt for this detection ccw uses the kernel event daemon to   trigger the call to dasd_change_state. All this is done in the   discipline code, see dasd_eckd.c.   After the analysis ccw is done (do_analysis returned 0) the block   device is setup.   In case the analysis returns an error, the device setup is stopped   (a fake disk was already added to allow formatting). ", "static int dasd_flush_block_queue(struct dasd_block *);static void dasd_device_tasklet(unsigned long);static void dasd_block_tasklet(unsigned long);static void do_kick_device(struct work_struct *);static void do_reload_device(struct work_struct *);static void do_requeue_requests(struct work_struct *);static void dasd_return_cqr_cb(struct dasd_ccw_req *, void *);static void dasd_device_timeout(struct timer_list *);static void dasd_block_timeout(struct timer_list *);static void __dasd_process_erp(struct dasd_device *, struct dasd_ccw_req *);static void dasd_profile_init(struct dasd_profile *, struct dentry *);static void dasd_profile_exit(struct dasd_profile *);static void dasd_hosts_init(struct dentry *, struct dasd_device *);static void dasd_hosts_exit(struct dasd_device *);static int dasd_handle_autoquiesce(struct dasd_device *, struct dasd_ccw_req *,   unsigned int);/* * SECTION: Operations on the device structure. ": "dasd_int_handler(struct ccw_device  , unsigned long, struct irb  );MODULE_AUTHOR(\"Holger Smolinski <Holger.Smolinski@de.ibm.com>\");MODULE_DESCRIPTION(\"Linux on S390 DASD device driver,\"   \" Copyright IBM Corp. 2000\");MODULE_LICENSE(\"GPL\");    SECTION: prototypes for static functions of dasd.c ", "if (!schedule_work(&device->kick_work))dasd_put_device(device);}EXPORT_SYMBOL(dasd_kick_device);/* * dasd_reload_device will schedule a call do do_reload_device to the kernel * event daemon. ": "dasd_schedule_device_bh(device);dasd_put_device(device);}void dasd_kick_device(struct dasd_device  device){dasd_get_device(device);  queue call to dasd_kick_device to the kernel event daemon. ", "dasd_schedule_device_bh(device);spin_unlock_irqrestore(get_ccwdev_lock(device->cdev), flags);}EXPORT_SYMBOL(dasd_add_request_head": "dasd_add_request_head(struct dasd_ccw_req  cqr){struct dasd_device  device;unsigned long flags;device = cqr->startdev;spin_lock_irqsave(get_ccwdev_lock(device->cdev), flags);cqr->status = DASD_CQR_QUEUED;list_add(&cqr->devlist, &device->ccw_queue);  let the bh start the request to keep them in order ", "dasd_schedule_device_bh(device);spin_unlock_irqrestore(get_ccwdev_lock(device->cdev), flags);}EXPORT_SYMBOL(dasd_add_request_tail": "dasd_add_request_tail(struct dasd_ccw_req  cqr){struct dasd_device  device;unsigned long flags;device = cqr->startdev;spin_lock_irqsave(get_ccwdev_lock(device->cdev), flags);cqr->status = DASD_CQR_QUEUED;list_add_tail(&cqr->devlist, &device->ccw_queue);  let the bh start the request to keep them in order ", "return 1;return ((cqr->status != DASD_CQR_DONE) &&(cqr->status != DASD_CQR_FAILED));} elsereturn (cqr->status == DASD_CQR_FILLED);}static int _dasd_sleep_on(struct dasd_ccw_req *maincqr, int interruptible)": "dasd_sleep_on_erp(struct dasd_ccw_req  cqr){struct dasd_device  device;dasd_erp_fn_t erp_fn;if (cqr->status == DASD_CQR_FILLED)return 0;device = cqr->startdev;if (test_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags)) {if (cqr->status == DASD_CQR_TERMINATED) {device->discipline->handle_terminated_request(cqr);return 1;}if (cqr->status == DASD_CQR_NEED_ERP) {erp_fn = device->discipline->erp_action(cqr);erp_fn(cqr);return 1;}if (cqr->status == DASD_CQR_FAILED)dasd_log_sense(cqr, &cqr->irb);if (cqr->refers) {__dasd_process_erp(device, cqr);return 1;}}return 0;}static int __dasd_sleep_on_loop_condition(struct dasd_ccw_req  cqr){if (test_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags)) {if (cqr->refers)   erp is not done yet ", "continue;if (test_bit(DASD_FLAG_LOCK_STOLEN, &device->flags) &&    !test_bit(DASD_CQR_ALLOW_SLOCK, &cqr->flags)) ": "dasd_sleep_on_queue(struct list_head  ccw_queue, int interruptible){struct dasd_device  device;struct dasd_ccw_req  cqr,  n;u8  sense = NULL;int rc;retry:list_for_each_entry_safe(cqr, n, ccw_queue, blocklist) {device = cqr->startdev;if (cqr->status != DASD_CQR_FILLED)  could be failed", "list_add(&cqr->devlist, device->ccw_queue.next);/* let the bh start the request to keep them in order ": "dasd_sleep_on_immediatly(struct dasd_ccw_req  cqr){struct dasd_device  device;int rc;device = cqr->startdev;if (test_bit(DASD_FLAG_LOCK_STOLEN, &device->flags) &&    !test_bit(DASD_CQR_ALLOW_SLOCK, &cqr->flags)) {cqr->status = DASD_CQR_FAILED;cqr->intrc = -EPERM;return -EIO;}spin_lock_irq(get_ccwdev_lock(device->cdev));rc = _dasd_term_running_cqr(device);if (rc) {spin_unlock_irq(get_ccwdev_lock(device->cdev));return rc;}cqr->callback = dasd_wakeup_cb;cqr->callback_data = DASD_SLEEPON_START_TAG;cqr->status = DASD_CQR_QUEUED;    add new request as second   first the terminated cqr needs to be finished ", "static int dasd_state_online_to_ready(struct dasd_device *device)": "dasd_schedule_block_bh(device->block);if ((device->features & DASD_FEATURE_USERAW)) {kobject_uevent(&disk_to_dev(device->block->gdp)->kobj,KOBJ_CHANGE);return 0;}disk_uevent(device->block->bdev->bd_disk, KOBJ_CHANGE);}return 0;}    Stop the requeueing of requests again. ", "dasd_path_set_tbvpm(device, ifccpm);dasd_schedule_device_bh(device);}if (oldopm && !dasd_path_get_opm(device) && !hpfpm && !ifccpm) ": "dasd_schedule_requeue(device);} else if (!dasd_path_get_opm(device) && ifccpm) {    device has no operational paths but at least one path is   disabled due to IFCC errors   trigger path verification on paths with IFCC errors ", "static int eer_pages = 5;module_param(eer_pages, int, S_IRUGO|S_IWUSR);struct eerbuffer ": "dasd_eer_write_buffer (one or more times per record to write the data)   The data can be written in several steps but you will have to compute   the total size up front for the invocation of dasd_eer_start_record.   If the ringbuffer is full, dasd_eer_start_record will remove the required   number of old records.     A record is typically read in two steps, first read the integer that   specifies the size of the following data, then read the data.   Both can be done by   - dasd_eer_read_buffer     For all mentioned functions you need to get the bufferlock first and keep   it until a complete record is written or read.     All information necessary to keep track of an internal buffer is kept in   a struct eerbuffer. The buffer specific to a file pointer is strored in   the private_data field of that file. To be able to write data to all   existing buffers, each buffer is also added to the bufferlist.   If the user does not want to read a complete record in one go, we have to   keep track of the rest of the record. residual stores the number of bytes   that are still to deliver. If the rest of the record is invalidated between   two reads then residual will be set to -1 so that the next read will fail.   All entries in the eerbuffer structure are protected with the bufferlock.   To avoid races between writing to a buffer on the one side and creating   and destroying buffers on the other side, the bufferlock must also be used   to protect the bufferlist. ", "static void __tape_do_irq (struct ccw_device *, unsigned long, struct irb *);static void tape_delayed_next_request(struct work_struct *);static void tape_long_busy_timeout(struct timer_list *t);/* * One list to contain all tape devices of all disciplines, so * we can assign the devices to minor numbers of the same major * The list is protected by the rwlock ": "TAPE_DBF_AREAtape_core_dbf#include \"tape.h\"#include \"tape_std.h\"#define LONG_BUSY_TIMEOUT 180   seconds ", "spin_unlock_irq(get_ccwdev_lock(device->cdev));break;case TS_UNUSED:/* * Need only to release the device. ": "tape_generic_remove(struct ccw_device  cdev){struct tape_device  device;device = dev_get_drvdata(&cdev->dev);if (!device) {return;}DBF_LH(3, \"(%08x): tape_generic_remove(%p)\\n\", device->cdev_id, cdev);spin_lock_irq(get_ccwdev_lock(device->cdev));switch (device->tape_state) {case TS_INIT:tape_state_set(device, TS_NOT_OPER);fallthrough;case TS_NOT_OPER:    Nothing to do. ", "request->device = NULL;tape_put_device(device);request->rc = -EIO;if (request->callback != NULL)request->callback(request, request->callback_data);}}/* * Driverfs tape remove function. * * This function is called whenever the common I/O layer detects the device * gone. This can happen at any time and we cannot refuse. ": "tape_generic_probe(struct ccw_device  cdev){struct tape_device  device;int ret;struct ccw_dev_id dev_id;device = tape_alloc_device();if (IS_ERR(device))return -ENODEV;ccw_device_set_options(cdev, CCWDEV_DO_PATHGROUP |     CCWDEV_DO_MULTIPATH);ret = sysfs_create_group(&cdev->dev.kobj, &tape_attr_group);if (ret) {tape_put_device(device);return ret;}dev_set_drvdata(&cdev->dev, device);cdev->handler = __tape_do_irq;device->cdev = cdev;ccw_device_get_id(cdev, &dev_id);device->cdev_id = devid_to_int(&dev_id);return ret;}static void__tape_discard_requests(struct tape_device  device){struct tape_request  request;struct list_head  l,  n;list_for_each_safe(l, n, &device->req_queue) {request = list_entry(l, struct tape_request, list);if (request->status == TAPE_REQUEST_IN_IO)request->status = TAPE_REQUEST_DONE;list_del(&request->list);  Decrease ref_count for removed request. ", "device->discipline = discipline;if (!try_module_get(discipline->owner)) ": "tape_generic_online(struct tape_device  device,   struct tape_discipline  discipline){int rc;DBF_LH(6, \"tape_enable_device(%p, %p)\\n\", device, discipline);if (device->tape_state != TS_INIT) {DBF_LH(3, \"Tapestate not INIT (%d)\\n\", device->tape_state);return -EINVAL;}timer_setup(&device->lb_timeout, tape_long_busy_timeout, 0);  Let the discipline have a go at the device. ", "static struct tape_device *tape_alloc_device(void)": "tape_generic_offline(struct ccw_device  cdev){struct tape_device  device;device = dev_get_drvdata(&cdev->dev);if (!device) {return -ENODEV;}DBF_LH(3, \"(%08x): tape_generic_offline(%p)\\n\",device->cdev_id, device);spin_lock_irq(get_ccwdev_lock(device->cdev));switch (device->tape_state) {case TS_INIT:case TS_NOT_OPER:spin_unlock_irq(get_ccwdev_lock(device->cdev));break;case TS_UNUSED:tape_state_set(device, TS_INIT);spin_unlock_irq(get_ccwdev_lock(device->cdev));tape_cleanup_device(device);break;default:DBF_EVENT(3, \"(%08x): Set offline failed \"\"- drive in use.\\n\",device->cdev_id);spin_unlock_irq(get_ccwdev_lock(device->cdev));return -EBUSY;}DBF_LH(3, \"(%08x): Drive set offline.\\n\", device->cdev_id);return 0;}    Allocate memory for a new device structure. ", "static int__tape_cancel_io(struct tape_device *device, struct tape_request *request)": "tape_med_state_set(struct tape_device  device, enum tape_medium_state newstate){enum tape_medium_state oldstate;oldstate = device->medium_state;if (oldstate == newstate)return;device->medium_state = newstate;switch(newstate){case MS_UNLOADED:device->tape_generic_status |= GMT_DR_OPEN(~0);if (oldstate == MS_LOADED)tape_med_state_work(device, MS_UNLOADED);break;case MS_LOADED:device->tape_generic_status &= ~GMT_DR_OPEN(~0);if (oldstate == MS_UNLOADED)tape_med_state_work(device, MS_LOADED);break;default:break;}wake_up(&device->state_change_wq);}    Stop running ccw. Has to be called with the device lock held. ", "static ssize_ttape_medium_state_show(struct device *dev, struct device_attribute *attr, char *buf)": "tape_op_verbose[TO_SIZE] ={[TO_BLOCK] = \"BLK\",[TO_BSB] = \"BSB\",[TO_BSF] = \"BSF\",[TO_DSE] = \"DSE\",[TO_FSB] = \"FSB\",[TO_FSF] = \"FSF\",[TO_LBL] = \"LBL\",[TO_NOP] = \"NOP\",[TO_RBA] = \"RBA\",[TO_RBI] = \"RBI\",[TO_RFO] = \"RFO\",[TO_REW] = \"REW\",[TO_RUN] = \"RUN\",[TO_WRI] = \"WRI\",[TO_WTM] = \"WTM\",[TO_MSEN] = \"MSN\",[TO_LOAD] = \"LOA\",[TO_READ_CONFIG] = \"RCF\",[TO_READ_ATTMSG] = \"RAT\",[TO_DIS] = \"DIS\",[TO_ASSIGN] = \"ASS\",[TO_UNASSIGN] = \"UAS\",  [TO_CRYPT_ON] = \"CON\",[TO_CRYPT_OFF] = \"COF\",[TO_KEKL_SET] = \"KLS\",[TO_KEKL_QUERY] = \"KLQ\",[TO_RDC] = \"RDC\",};static int devid_to_int(struct ccw_dev_id  dev_id){return dev_id->devno + (dev_id->ssid << 16);}    Some channel attached tape specific attributes.     FIXME: In the future the first_minor and blocksize attribute should be          replaced by a link to the cdev tree. ", "if (cplength > 0) ": "tape_alloc_request(int cplength, int datasize){struct tape_request  request;BUG_ON(datasize > PAGE_SIZE || (cplength sizeof(struct ccw1)) > PAGE_SIZE);DBF_LH(6, \"tape_alloc_request(%d, %d)\\n\", cplength, datasize);request = kzalloc(sizeof(struct tape_request), GFP_KERNEL);if (request == NULL) {DBF_EXCEPTION(1, \"cqra nomem\\n\");return ERR_PTR(-ENOMEM);}  allocate channel program ", "request->status = TAPE_REQUEST_QUEUED;schedule_delayed_work(&device->tape_dnr, 0);rc = 0;} else ": "tape_free_request (struct tape_request   request){DBF_LH(6, \"Free request %p\\n\", request);if (request->device)tape_put_device(request->device);kfree(request->cpdata);kfree(request->cpaddr);kfree(request);}static int__tape_start_io(struct tape_device  device, struct tape_request  request){int rc;rc = ccw_device_start(device->cdev,request->cpaddr,(unsigned long) request,0x00,request->options);if (rc == 0) {request->status = TAPE_REQUEST_IN_IO;} else if (rc == -EBUSY) {  The common IO subsystem is currently busy. Retry later. ", "static int__tape_start_request(struct tape_device *device, struct tape_request *request)": "tape_dump_sense_dbf(struct tape_device  device, struct tape_request  request,    struct irb  irb){unsigned int  sptr;const char  op;if (request != NULL)op = tape_op_verbose[request->op];elseop = \"---\";DBF_EVENT(3, \"DSTAT : %02x   CSTAT: %02x\\n\",  irb->scsw.cmd.dstat, irb->scsw.cmd.cstat);DBF_EVENT(3, \"DEVICE: %08x OP\\t: %s\\n\", device->cdev_id, op);sptr = (unsigned int  ) irb->ecw;DBF_EVENT(3, \"%08x %08x\\n\", sptr[0], sptr[1]);DBF_EVENT(3, \"%08x %08x\\n\", sptr[2], sptr[3]);DBF_EVENT(3, \"%08x %08x\\n\", sptr[4], sptr[5]);DBF_EVENT(3, \"%08x %08x\\n\", sptr[6], sptr[7]);}    IO helper function. Adds the request to the request queue   and starts it if the tape is idle. Has to be called with   the device lock held. ", "rc = __tape_start_request(device, request);spin_unlock_irq(get_ccwdev_lock(device->cdev));return rc;}/* * tape_do_io/__tape_wake_up * Add the request to the request queue, try to start it if the * tape is idle and wait uninterruptible for its completion. ": "tape_do_io_async(struct tape_device  device, struct tape_request  request){int rc;DBF_LH(6, \"tape_do_io_async(%p, %p)\\n\", device, request);spin_lock_irq(get_ccwdev_lock(device->cdev));  Add request to request queue and try to start it. ", "static void__tape_wake_up_interruptible(struct tape_request *request, void *data)": "tape_do_io_interruptible__tape_wake_up_interruptible   Add the request to the request queue, try to start it if the   tape is idle and wait uninterruptible for its completion. ", "if (request->callback == NULL)return 0;rc = 0;for (retries = 0; retries < 5; retries++) ": "tape_cancel_io(struct tape_device  device, struct tape_request  request){int retries;int rc;  Check if interrupt has already been processed ", "if (mt_op == MTBSR  || mt_op == MTFSR  || mt_op == MTFSF  ||    mt_op == MTBSF  || mt_op == MTFSFM || mt_op == MTBSFM) ": "tape_mtop(struct tape_device  device, int mt_op, int mt_count){tape_mtop_fn fn;int rc;DBF_EVENT(6, \"TAPE:mtio\\n\");DBF_EVENT(6, \"TAPE:ioop: %x\\n\", mt_op);DBF_EVENT(6, \"TAPE:arg: %x\\n\", mt_count);if (mt_op < 0 || mt_op >= TAPE_NR_MTOPS)return -EINVAL;fn = device->discipline->mtop_array[mt_op];if (fn == NULL)return -EINVAL;  We assume that the backends can handle count up to 500. ", "static voidtape_std_assign_timeout(struct timer_list *t)": "tape_std_assign ", "inttape_std_unassign (struct tape_device *device)": "tape_std_unassign ", "inttape_std_read_block_id(struct tape_device *device, __u64 *id)": "tape_std_display(struct tape_device  device, struct display_struct  disp){struct tape_request  request;int rc;request = tape_alloc_request(2, 17);if (IS_ERR(request)) {DBF_EVENT(3, \"TAPE: load display failed\\n\");return PTR_ERR(request);}request->op = TO_DIS; (unsigned char  ) request->cpdata = disp->cntrl;DBF_EVENT(5, \"TAPE: display cntrl=%04x\\n\", disp->cntrl);memcpy(((unsigned char  ) request->cpdata) + 1, disp->message1, 8);memcpy(((unsigned char  ) request->cpdata) + 9, disp->message2, 8);ASCEBC(((unsigned char ) request->cpdata) + 1, 16);tape_ccw_cc(request->cpaddr, LOAD_DISPLAY, 17, request->cpdata);tape_ccw_end(request->cpaddr + 1, NOP, 0, NULL);rc = tape_do_io_interruptible(device, request);tape_free_request(request);return rc;}    Read block id. ", "tape_ccw_cc(request->cpaddr, MODE_SET_DB, 1, device->modeset_byte);tape_ccw_cc(request->cpaddr + 1, READ_BLOCK_ID, 8, request->cpdata);tape_ccw_end(request->cpaddr + 2, NOP, 0, NULL);/* execute it ": "tape_std_read_block_id(struct tape_device  device, __u64  id){struct tape_request  request;int rc;request = tape_alloc_request(3, 8);if (IS_ERR(request))return PTR_ERR(request);request->op = TO_RBI;  setup ccws ", "inttape_std_mtsetblk(struct tape_device *device, int count)": "tape_std_mtload(struct tape_device  device, int count){return wait_event_interruptible(device->state_change_wq,(device->medium_state == MS_LOADED));}    MTSETBLK: Set block size. ", "device->char_data.block_size = 0;return 0;}if (device->char_data.idal_buf != NULL &&    device->char_data.idal_buf->size == count)/* We already have a idal buffer of that size. ": "tape_std_mtsetblk(struct tape_device  device, int count){struct idal_buffer  new;DBF_LH(6, \"tape_std_mtsetblk(%d)\\n\", count);if (count <= 0) {    Just set block_size to 0. tapechar_readtapechar_write   will realloc the idal buffer if a bigger one than the   current is needed. ", "inttape_std_mtfsf(struct tape_device *device, int mt_count)": "tape_std_mtreset(struct tape_device  device, int count){DBF_EVENT(6, \"TCHAR:devreset:\\n\");device->char_data.block_size = 0;return 0;}    MTFSF: Forward space over 'count' file marks. The tape is positioned   at the EOT (End of Tape) side of the file mark. ", "ccw = tape_ccw_cc(request->cpaddr, MODE_SET_DB, 1,  device->modeset_byte);ccw = tape_ccw_repeat(ccw, FORSPACEFILE, mt_count);ccw = tape_ccw_end(ccw, NOP, 0, NULL);/* execute it ": "tape_std_mtfsfm(struct tape_device  device, int mt_count){struct tape_request  request;struct ccw1  ccw;int rc;request = tape_alloc_request(mt_count + 2, 0);if (IS_ERR(request))return PTR_ERR(request);request->op = TO_FSF;  setup ccws ", "ccw = tape_ccw_cc(request->cpaddr, MODE_SET_DB, 1,  device->modeset_byte);ccw = tape_ccw_repeat(ccw, FORSPACEBLOCK, mt_count);ccw = tape_ccw_end(ccw, NOP, 0, NULL);/* execute it ": "tape_std_mtfsr(struct tape_device  device, int mt_count){struct tape_request  request;struct ccw1  ccw;int rc;request = tape_alloc_request(mt_count + 2, 0);if (IS_ERR(request))return PTR_ERR(request);request->op = TO_FSB;  setup ccws ", "ccw = tape_ccw_cc(request->cpaddr, MODE_SET_DB, 1,  device->modeset_byte);ccw = tape_ccw_repeat(ccw, BACKSPACEBLOCK, mt_count);ccw = tape_ccw_end(ccw, NOP, 0, NULL);/* execute it ": "tape_std_mtbsr(struct tape_device  device, int mt_count){struct tape_request  request;struct ccw1  ccw;int rc;request = tape_alloc_request(mt_count + 2, 0);if (IS_ERR(request))return PTR_ERR(request);request->op = TO_BSB;  setup ccws ", "ccw = tape_ccw_cc(request->cpaddr, MODE_SET_DB, 1,  device->modeset_byte);ccw = tape_ccw_repeat(ccw, WRITETAPEMARK, mt_count);ccw = tape_ccw_end(ccw, NOP, 0, NULL);/* execute it ": "tape_std_mtweof(struct tape_device  device, int mt_count){struct tape_request  request;struct ccw1  ccw;request = tape_alloc_request(mt_count + 2, 0);if (IS_ERR(request))return PTR_ERR(request);request->op = TO_WTM;  setup ccws ", "ccw = tape_ccw_cc(request->cpaddr, MODE_SET_DB, 1,  device->modeset_byte);ccw = tape_ccw_repeat(ccw, BACKSPACEFILE, mt_count);ccw = tape_ccw_end(ccw, NOP, 0, NULL);/* execute it ": "tape_std_mtbsfm(struct tape_device  device, int mt_count){struct tape_request  request;struct ccw1  ccw;request = tape_alloc_request(mt_count + 2, 0);if (IS_ERR(request))return PTR_ERR(request);request->op = TO_BSF;  setup ccws ", "tape_ccw_cc(request->cpaddr, MODE_SET_DB, 1,    device->modeset_byte);tape_ccw_cc(request->cpaddr + 1, REWIND, 0, NULL);tape_ccw_end(request->cpaddr + 2, NOP, 0, NULL);/* execute it ": "tape_std_mtrew(struct tape_device  device, int mt_count){struct tape_request  request;request = tape_alloc_request(3, 0);if (IS_ERR(request))return PTR_ERR(request);request->op = TO_REW;  setup ccws ", "tape_ccw_cc(request->cpaddr, MODE_SET_DB, 1, device->modeset_byte);tape_ccw_cc(request->cpaddr + 1, REWIND_UNLOAD, 0, NULL);tape_ccw_end(request->cpaddr + 2, NOP, 0, NULL);/* execute it ": "tape_std_mtoffl(struct tape_device  device, int mt_count){struct tape_request  request;request = tape_alloc_request(3, 0);if (IS_ERR(request))return PTR_ERR(request);request->op = TO_RUN;  setup ccws ", "tape_ccw_cc(request->cpaddr, MODE_SET_DB, 1, device->modeset_byte);tape_ccw_end(request->cpaddr + 1, NOP, 0, NULL);/* execute it ": "tape_std_mtnop(struct tape_device  device, int mt_count){struct tape_request  request;request = tape_alloc_request(2, 0);if (IS_ERR(request))return PTR_ERR(request);request->op = TO_NOP;  setup ccws ", "if ((rc = tape_mtop(device, MTREW, 1)) < 0)return rc;/* * The logical end of volume is given by two sewuential tapemarks. * Look for this by skipping to the next file (over one tapemark) * and then test for another one (fsr returns 1 if a tapemark was * encountered). ": "tape_std_mteom(struct tape_device  device, int mt_count){int rc;    Seek from the beginning of tape (rewind). ", "tape_ccw_cc(request->cpaddr, MODE_SET_DB, 1, device->modeset_byte);tape_ccw_cc(request->cpaddr + 1,FORSPACEFILE, 0, NULL);tape_ccw_cc(request->cpaddr + 2, NOP, 0, NULL);tape_ccw_end(request->cpaddr + 3, CCW_CMD_TIC, 0, request->cpaddr);/* execute it, MTRETEN rc gets ignored ": "tape_std_mtreten(struct tape_device  device, int mt_count){struct tape_request  request;request = tape_alloc_request(4, 0);if (IS_ERR(request))return PTR_ERR(request);request->op = TO_FSF;  setup ccws ", "tape_ccw_cc(request->cpaddr, MODE_SET_DB, 1, device->modeset_byte);tape_ccw_cc(request->cpaddr + 1, REWIND, 0, NULL);tape_ccw_cc(request->cpaddr + 2, ERASE_GAP, 0, NULL);tape_ccw_cc(request->cpaddr + 3, DATA_SEC_ERASE, 0, NULL);tape_ccw_cc(request->cpaddr + 4, REWIND, 0, NULL);tape_ccw_end(request->cpaddr + 5, NOP, 0, NULL);/* execute it ": "tape_std_mterase(struct tape_device  device, int mt_count){struct tape_request  request;request = tape_alloc_request(6, 0);if (IS_ERR(request))return PTR_ERR(request);request->op = TO_DSE;  setup ccws ", "inttape_std_mtcompression(struct tape_device *device, int mt_count)": "tape_std_mtunload(struct tape_device  device, int mt_count){return tape_mtop(device, MTOFFL, mt_count);}    MTCOMPRESSION: used to enable compression.   Sets the IDRC onoff. ", "if (mt_count == 0)*device->modeset_byte &= ~0x08;else*device->modeset_byte |= 0x08;tape_ccw_cc(request->cpaddr, MODE_SET_DB, 1, device->modeset_byte);tape_ccw_end(request->cpaddr + 1, NOP, 0, NULL);/* execute it ": "tape_std_mtcompression(struct tape_device  device, int mt_count){struct tape_request  request;if (mt_count < 0 || mt_count > 1) {DBF_EXCEPTION(6, \"xcom parm\\n\");return -EINVAL;}request = tape_alloc_request(2, 0);if (IS_ERR(request))return PTR_ERR(request);request->op = TO_NOP;  setup ccws ", "request->op = TO_RBA;tape_ccw_cc(request->cpaddr, MODE_SET_DB, 1, device->modeset_byte);tape_ccw_cc_idal(request->cpaddr + 1, READ_BACKWARD, device->char_data.idal_buf);tape_ccw_cc(request->cpaddr + 2, FORSPACEBLOCK, 0, NULL);tape_ccw_end(request->cpaddr + 3, NOP, 0, NULL);DBF_EVENT(6, \"xrop ccwg\");}/* * Write Block ": "tape_std_read_backward(struct tape_device  device, struct tape_request  request){    We have allocated 4 ccws in tape_std_read, so we can now   transform the request to a read backward, followed by a   forward space block. ", "voidtape_std_process_eov(struct tape_device *device)": "tape_std_write_block(struct tape_device  device, size_t count){struct tape_request  request;request = tape_alloc_request(2, 0);if (IS_ERR(request)) {DBF_EXCEPTION(6, \"xwbl fail\\n\");return request;}request->op = TO_WRI;tape_ccw_cc(request->cpaddr, MODE_SET_DB, 1, device->modeset_byte);tape_ccw_end_idal(request->cpaddr + 1, WRITE_CMD,  device->char_data.idal_buf);DBF_EVENT(6, \"xwbl ccwg\\n\");return request;}    This routine is called by frontend after an ENOSP on write ", "if (tape_mtop(device, MTBSR, 1) == 0 &&    tape_mtop(device, MTWEOF, 1) == 0) ": "tape_std_process_eov(struct tape_device  device){    End of volume: We have to backspace the last written record, then   we TRY to write a tapemark and then backspace over the written TM ", "rq = kzalloc(sizeof(*rq), GFP_KERNEL | GFP_DMA);if (!rq)return ERR_PTR(-ENOMEM);/* alloc output buffer. ": "raw3270_request_alloc(size_t size){struct raw3270_request  rq;  Allocate request structure ", "rq->rc = ccw_device_start(rp->cdev, &rq->ccw,  (unsigned long)rq, 0, 0);if (rq->rc) ": "raw3270_start(struct raw3270  rp, struct raw3270_view  view,   struct raw3270_request  rq){rq->view = view;raw3270_get_view(view);if (list_empty(&rp->req_queue) &&    !test_bit(RAW3270_FLAGS_BUSY, &rp->flags)) {  No other requests are on the queue. Start this one. ", "rp->state = RAW3270_STATE_INIT;} else if (MACHINE_IS_VM) ": "raw3270_reset_device_cb(struct raw3270_request  rq, void  data){struct raw3270  rp = rq->view->dev;if (rp->state != RAW3270_STATE_RESET)return;if (rq->rc) {  Reset command failed. ", "if (oldview) ": "raw3270_activate_view(struct raw3270  rp, struct raw3270_view  view){struct raw3270_view  oldview = NULL,  nv;int rc;if (rp->view == view)return 0;if (!raw3270_state_ready(rp))return -EBUSY;if (rp->view && rp->view->fn->deactivate) {oldview = rp->view;oldview->fn->deactivate(oldview);}rc = raw3270_assign_activate_view(rp, view);if (!rc)return 0;  Didn't work. Try to reactivate the old view. ", "list_del_init(&view->list);list_add_tail(&view->list, &rp->view_list);/* Try to activate another view. ": "raw3270_deactivate_view(struct raw3270_view  view){unsigned long flags;struct raw3270  rp;rp = view->dev;if (!rp)return;spin_lock_irqsave(get_ccwdev_lock(rp->cdev), flags);if (rp->view == view) {view->fn->deactivate(view);rp->view = NULL;  Move deactivated view to end of list. ", "list_for_each_entry(nv, &rp->view_list, list) ": "raw3270_del_view(struct raw3270_view  view){unsigned long flags;struct raw3270  rp;struct raw3270_view  nv;rp = view->dev;spin_lock_irqsave(get_ccwdev_lock(rp->cdev), flags);if (rp->view == view) {view->fn->deactivate(view);rp->view = NULL;}list_del_init(&view->list);if (!rp->view && raw3270_state_ready(rp)) {  Try to activate another view. ", "debug_info_t *TAPE_DBF_AREA = NULL;EXPORT_SYMBOL(TAPE_DBF_AREA": "TAPE_DBF_AREAtape_34xx_dbf#include \"tape.h\"#include \"tape_std.h\"    Pointer to debug area. ", "ssize_t hmcdrv_ftp_do(const struct hmcdrv_ftp_cmdspec *ftp)": "hmcdrv_ftp_do() - perform a HMC drive FTP, with data from kernel-space   @ftp: pointer to FTP command specification     Return: number of bytes readwritten or a negative error code ", "int hmcdrv_ftp_probe(void)": "hmcdrv_ftp_probe() - probe for the HMC drive FTP service     Return: 0 if service is available, else an (negative) error code ", "case -EBUSY:  /* but service seems to be available ": "hmcdrv_ftp_shutdown();switch (rc) {case -ENOENT:   no such filemedia or currently busy, ", "tty = tty_port_tty_get(kbd->port);/* FIXME this test is pretty racy ": "kbd_ioctl(struct kbd_data  kbd, unsigned int cmd, unsigned long arg){struct tty_struct  tty;void __user  argp;unsigned int ct;int perm;argp = (void __user  )arg;    To have permissions to do most of the vt ioctls, we either have   to be the owner of the tty, or have CAP_SYS_TTY_CONFIG. ", "voidkbd_ebcasc(struct kbd_data *kbd, unsigned char *ebcasc)": "kbd_ascebc(struct kbd_data  kbd, unsigned char  ascebc){unsigned short  keymap, keysym;int i, j, k;memset(ascebc, 0x40, 256);for (i = 0; i < ARRAY_SIZE(ebc_key_maps); i++) {keymap = kbd->key_maps[i];if (!keymap)continue;for (j = 0; j < NR_KEYS; j++) {k = ((i & 1) << 7) + j;keysym = keymap[j];if (KTYP(keysym) == (KT_LATIN | 0xf0) ||    KTYP(keysym) == (KT_LETTER | 0xf0))ascebc[KVAL(keysym)] = k;else if (KTYP(keysym) == (KT_DEAD | 0xf0))ascebc[ret_diacr[KVAL(keysym)]] = k;}}}#if 0    Generate ebcdic -> ascii translation table from kbd_data. ", "voidkbd_ascebc(struct kbd_data *kbd, unsigned char *ascebc)": "kbd_alloc(void) {struct kbd_data  kbd;int i;kbd = kzalloc(sizeof(struct kbd_data), GFP_KERNEL);if (!kbd)goto out;kbd->key_maps = kzalloc(sizeof(ebc_key_maps), GFP_KERNEL);if (!kbd->key_maps)goto out_kbd;for (i = 0; i < ARRAY_SIZE(ebc_key_maps); i++) {if (ebc_key_maps[i]) {kbd->key_maps[i] = kmemdup(ebc_key_maps[i],   sizeof(u_short)   NR_KEYS,   GFP_KERNEL);if (!kbd->key_maps[i])goto out_maps;}}kbd->func_table = kzalloc(sizeof(ebc_func_table), GFP_KERNEL);if (!kbd->func_table)goto out_maps;for (i = 0; i < ARRAY_SIZE(ebc_func_table); i++) {if (ebc_func_table[i]) {kbd->func_table[i] = kstrdup(ebc_func_table[i],     GFP_KERNEL);if (!kbd->func_table[i])goto out_func;}}kbd->fn_handler =kcalloc(NR_FN_HANDLER, sizeof(fn_handler_fn  ), GFP_KERNEL);if (!kbd->fn_handler)goto out_func;kbd->accent_table = kmemdup(ebc_accent_table,    sizeof(struct kbdiacruc)   MAX_DIACR,    GFP_KERNEL);if (!kbd->accent_table)goto out_fn_handler;kbd->accent_table_size = ebc_accent_table_size;return kbd;out_fn_handler:kfree(kbd->fn_handler);out_func:for (i = 0; i < ARRAY_SIZE(ebc_func_table); i++)kfree(kbd->func_table[i]);kfree(kbd->func_table);out_maps:for (i = 0; i < ARRAY_SIZE(ebc_key_maps); i++)kfree(kbd->key_maps[i]);kfree(kbd->key_maps);out_kbd:kfree(kbd);out:return NULL;}voidkbd_free(struct kbd_data  kbd){int i;kfree(kbd->accent_table);kfree(kbd->fn_handler);for (i = 0; i < ARRAY_SIZE(ebc_func_table); i++)kfree(kbd->func_table[i]);kfree(kbd->func_table);for (i = 0; i < ARRAY_SIZE(ebc_key_maps); i++)kfree(kbd->key_maps[i]);kfree(kbd->key_maps);kfree(kbd);}    Generate ascii -> ebcdic translation table from kbd_data. ", "if (kbd->sysrq) ": "kbd_keycode(struct kbd_data  kbd, unsigned int keycode){unsigned short keysym;unsigned char type, value;if (!kbd)return;if (keycode >= 384)keysym = kbd->key_maps[5][keycode - 384];else if (keycode >= 256)keysym = kbd->key_maps[4][keycode - 256];else if (keycode >= 128)keysym = kbd->key_maps[1][keycode - 128];elsekeysym = kbd->key_maps[0][keycode];type = KTYP(keysym);if (type >= 0xf0) {type -= 0xf0;if (type == KT_LETTER)type = KT_LATIN;value = KVAL(keysym);#ifdef CONFIG_MAGIC_SYSRQ         Handle the SysRq Hack ", "sclp_trace(2, \"RQAD\", __pa(req->sccb), _RET_IP_, false);req->status = SCLP_REQ_QUEUED;req->start_count = 0;list_add_tail(&req->list, &sclp_req_queue);rc = 0;if (req->queue_timeout) ": "sclp_add_request(struct sclp_req  req){unsigned long flags;int rc;spin_lock_irqsave(&sclp_lock, flags);if (!__sclp_can_add_request(req)) {spin_unlock_irqrestore(&sclp_lock, flags);return -EIO;}  RQAD: Request was added (a=sccb, b=caller) ", "sclp_trace(4, \"SYN1\", sclp_running_state, ++sync_count, false);/* We'll be disabling timer interrupts, so we need a custom timeout * mechanism ": "sclp_sync_wait(void){unsigned long long old_tick;unsigned long flags;unsigned long cr0, cr0_sync;static u64 sync_count;u64 timeout;int irq_context;  SYN1: Synchronous wait start (a=runstate, b=sync count) ", "static struct timer_list sclp_request_timer;/* Timer for queued requests. ": "sclp_register  reg){struct {u64 receive;u64 send;} d;d.receive = reg->receive_mask;d.send = reg->send_mask;sclp_trace(prio, id, a, b, false);sclp_trace_bin(prio, &d, sizeof(d), 0);}static int __init sclp_setup_console_pages(char  str){int pages, rc;rc = kstrtoint(str, 0, &pages);if (!rc && pages >= SCLP_CONSOLE_PAGES)sclp_console_pages = pages;return 1;}__setup(\"sclp_con_pages=\", sclp_setup_console_pages);static int __init sclp_setup_console_drop(char  str){return kstrtobool(str, &sclp_console_drop) == 0;}__setup(\"sclp_con_drop=\", sclp_setup_console_drop);  Timer for request retries. ", "sclp_trace_register(2, \"UREG\", 0, _RET_IP_, reg);spin_lock_irqsave(&sclp_lock, flags);list_del(&reg->list);spin_unlock_irqrestore(&sclp_lock, flags);sclp_init_mask(1);}EXPORT_SYMBOL(sclp_unregister": "sclp_unregister(struct sclp_register  reg){unsigned long flags;  UREG: Event listener unregistered (b=caller) ", "if (sclp_activation_state != sclp_activation_state_active) ": "sclp_deactivate(void){unsigned long flags;int rc;spin_lock_irqsave(&sclp_lock, flags);  Deactivate can only be called when active ", "if (sclp_activation_state != sclp_activation_state_inactive) ": "sclp_reactivate(void){unsigned long flags;int rc;spin_lock_irqsave(&sclp_lock, flags);  Reactivate can only be called when inactive ", "#include \"tape.h\"#include \"tape_std.h\"#include \"tape_3590.h\"static struct workqueue_struct *tape_3590_wq;/* * Pointer to debug area. ": "TAPE_DBF_AREAtape_3590_dbf#define BUFSIZE 512  size of buffers for dynamic generated messages ", "#undef DEBUG#undef DEBUGDATA#undef DEBUGCCW#define KMSG_COMPONENT \"ctcm\"#define pr_fmt(fmt) KMSG_COMPONENT \": \" fmt#include <linux/module.h>#include <linux/init.h>#include <linux/kernel.h>#include <linux/slab.h>#include <linux/errno.h>#include <linux/types.h>#include <linux/interrupt.h>#include <linux/timer.h>#include <linux/sched.h>#include <linux/signal.h>#include <linux/string.h>#include <linux/proc_fs.h>#include <linux/ip.h>#include <linux/if_arp.h>#include <linux/tcp.h>#include <linux/skbuff.h>#include <linux/ctype.h>#include <linux/netdevice.h>#include <net/dst.h>#include <linux/io.h>#include <linux/bitops.h>#include <linux/uaccess.h>#include <linux/wait.h>#include <linux/moduleparam.h>#include <asm/ccwdev.h>#include <asm/ccwgroup.h>#include <asm/idals.h>#include \"ctcm_main.h\"#include \"ctcm_mpc.h\"#include \"ctcm_fsms.h\"static const struct xid2 init_xid = ": "ctc_mpc_flow_control);", "voidfsm_modtimer(fsm_timer *this, int millisec, int event, void *arg)": "fsm_record_history(fsm_instance  fi, int state, int event){fi->history[fi->history_index].state = state;fi->history[fi->history_index++].event = event;fi->history_index %= FSM_HISTORY_SIZE;if (fi->history_size < FSM_HISTORY_SIZE)fi->history_size++;}#endifconst char  fsm_getstate_str(fsm_instance  fi){int st = atomic_read(&fi->state);if (st >= fi->f->nr_states)return \"Invalid\";return fi->f->state_names[st];}static voidfsm_expire_timer(struct timer_list  t){fsm_timer  this = from_timer(this, t, tl);#if FSM_TIMER_DEBUGprintk(KERN_DEBUG \"fsm(%s): Timer %p expired\\n\",       this->fi->name, this);#endiffsm_event(this->fi, this->expire_event, this->event_arg);}voidfsm_settimer(fsm_instance  fi, fsm_timer  this){this->fi = fi;#if FSM_TIMER_DEBUGprintk(KERN_DEBUG \"fsm(%s): Create timer %p\\n\", fi->name,       this);#endiftimer_setup(&this->tl, fsm_expire_timer, 0);}voidfsm_deltimer(fsm_timer  this){#if FSM_TIMER_DEBUGprintk(KERN_DEBUG \"fsm(%s): Delete timer %p\\n\", this->fi->name,this);#endifdel_timer(&this->tl);}intfsm_addtimer(fsm_timer  this, int millisec, int event, void  arg){#if FSM_TIMER_DEBUGprintk(KERN_DEBUG \"fsm(%s): Add timer %p %dms\\n\",       this->fi->name, this, millisec);#endiftimer_setup(&this->tl, fsm_expire_timer, 0);this->expire_event = event;this->event_arg = arg;this->tl.expires = jiffies + (millisec   HZ)  1000;add_timer(&this->tl);return 0;}  FIXME: this function is never used, why ", "int ccw_device_is_pathgroup(struct ccw_device *cdev)": "ccw_device_is_pathgroup() - determine if paths to this device are grouped   @cdev: ccw device     Return non-zero if there is a path group, zero otherwise. ", "int ccw_device_is_multipath(struct ccw_device *cdev)": "ccw_device_is_multipath() - determine if device is operating in multipath mode   @cdev: ccw device     Return non-zero if device is operating in multipath mode, zero otherwise. ", "void ccw_device_get_id(struct ccw_device *cdev, struct ccw_dev_id *dev_id)": "ccw_device_get_id() - obtain a ccw device id   @cdev: device to obtain the id for   @dev_id: where to fill in the values ", "int ccw_device_tm_start_timeout_key(struct ccw_device *cdev, struct tcw *tcw,    unsigned long intparm, u8 lpm, u8 key,    int expires)": "ccw_device_tm_start_timeout_key() - perform start function   @cdev: ccw device on which to perform the start function   @tcw: transport-command word to be started   @intparm: user defined parameter to be passed to the interrupt handler   @lpm: mask of paths to use   @key: storage key to use for storage access   @expires: time span in jiffies after which to abort request     Start the tcw on the given ccw device. Return zero on success, non-zero   otherwise. ", "int ccw_device_tm_start_key(struct ccw_device *cdev, struct tcw *tcw,    unsigned long intparm, u8 lpm, u8 key)": "ccw_device_tm_start_key() - perform start function   @cdev: ccw device on which to perform the start function   @tcw: transport-command word to be started   @intparm: user defined parameter to be passed to the interrupt handler   @lpm: mask of paths to use   @key: storage key to use for storage access     Start the tcw on the given ccw device. Return zero on success, non-zero   otherwise. ", "int ccw_device_get_mdc(struct ccw_device *cdev, u8 mask)": "ccw_device_get_mdc() - accumulate max data count   @cdev: ccw device for which the max data count is accumulated   @mask: mask of paths to use     Return the number of 64K-bytes blocks all paths at least support   for a transport command. Return value 0 indicates failure. ", "int ccw_device_tm_intrg(struct ccw_device *cdev)": "ccw_device_tm_intrg() - perform interrogate function   @cdev: ccw device on which to perform the interrogate function     Perform an interrogate function on the given ccw device. Return zero on   success, non-zero otherwise. ", "int ccw_device_set_options_mask(struct ccw_device *cdev, unsigned long flags)": "ccw_device_set_options_mask() - set some options and unset the rest   @cdev: device for which the options are to be set   @flags: options to be set     All flags specified in @flags are set, all flags not specified in @flags   are cleared.   Returns:     %0 on success, -%EINVAL on an invalid flag combination. ", "void ccw_device_clear_options(struct ccw_device *cdev, unsigned long flags)": "ccw_device_clear_options() - clear some options   @cdev: device for which the options are to be cleared   @flags: options to be cleared     All flags specified in @flags are cleared, the remainder is left untouched. ", "int ccw_device_start_timeout_key(struct ccw_device *cdev, struct ccw1 *cpa, unsigned long intparm, __u8 lpm, __u8 key, unsigned long flags, int expires)": "ccw_device_start_timeout_key() - start a s390 channel program with timeout and key   @cdev: target ccw device   @cpa: logical start address of channel program   @intparm: user specific interruption parameter; will be presented back to       @cdev's interrupt handler. Allows a device driver to associate       the interrupt with a particular IO request.   @lpm: defines the channel path to be used for a specific IO request. A   value of 0 will make cio use the opm.   @key: storage key to be used for the IO   @flags: additional flags; defines the action to be performed for IO     processing.   @expires: timeout value in jiffies     Start a S390 channel program. When the interrupt arrives, the   IRQ handler is called, either immediately, delayed (dev-end missing,   or sense required) or never (no IRQ handler registered).   This function notifies the device driver if the channel program has not   completed during the time specified by @expires. If a timeout occurs, the   channel program is terminated via xsch, hsch or csch, and the device's   interrupt handler will be called with an irb containing ERR_PTR(-%ETIMEDOUT).   The interruption handler will echo back the @intparm specified here, unless   another interruption parameter is specified by a subsequent invocation of   ccw_device_halt() or ccw_device_clear().   Returns:    %0, if the operation was successful;    -%EBUSY, if the device is busy, or status pending;    -%EACCES, if no path specified in @lpm is operational;    -%ENODEV, if the device is not operational.   Context:    Interrupts disabled, ccw device lock held ", "int ccw_device_resume(struct ccw_device *cdev)": "ccw_device_resume() - resume channel program execution   @cdev: target ccw device     ccw_device_resume() calls rsch on @cdev's subchannel.   Returns:    %0 on success,    -%ENODEV on device not operational,    -%EINVAL on invalid device state,    -%EBUSY on device busy or interrupt pending.   Context:    Interrupts disabled, ccw device lock held ", "int ccw_device_start_key(struct ccw_device *cdev, struct ccw1 *cpa, unsigned long intparm, __u8 lpm, __u8 key, unsigned long flags)": "ccw_device_start_key() - start a s390 channel program with key   @cdev: target ccw device   @cpa: logical start address of channel program   @intparm: user specific interruption parameter; will be presented back to       @cdev's interrupt handler. Allows a device driver to associate       the interrupt with a particular IO request.   @lpm: defines the channel path to be used for a specific IO request. A   value of 0 will make cio use the opm.   @key: storage key to be used for the IO   @flags: additional flags; defines the action to be performed for IO     processing.     Start a S390 channel program. When the interrupt arrives, the   IRQ handler is called, either immediately, delayed (dev-end missing,   or sense required) or never (no IRQ handler registered).   The interruption handler will echo back the @intparm specified here, unless   another interruption parameter is specified by a subsequent invocation of   ccw_device_halt() or ccw_device_clear().   Returns:    %0, if the operation was successful;    -%EBUSY, if the device is busy, or status pending;    -%EACCES, if no path specified in @lpm is operational;    -%ENODEV, if the device is not operational.   Context:    Interrupts disabled, ccw device lock held ", "struct ciw *ccw_device_get_ciw(struct ccw_device *cdev, __u32 ct)": "ccw_device_get_ciw() - Search for CIW command in extended sense data.   @cdev: ccw device to inspect   @ct: command type to look for     During SenseID, command information words (CIWs) describing special   commands available to the device may have been stored in the extended   sense data. This function searches for CIWs of a specified command   type in the extended sense data.   Returns:    %NULL if no extended sense data has been stored or if no CIW of the    specified command type could be found,    else a pointer to the CIW of the specified command type. ", "__u8 ccw_device_get_path_mask(struct ccw_device *cdev)": "ccw_device_get_path_mask() - get currently available paths   @cdev: ccw device to be queried   Returns:    %0 if no subchannel for the device is available,    else the mask of currently available paths for the ccw device's subchannel. ", "int qdio_start_irq(struct ccw_device *cdev)": "qdio_start_irq - enable interrupt processing for the device   @cdev: associated ccw_device for the qdio subchannel     Return codes     0 - success     1 - irqs not started since new data is available ", "int qdio_stop_irq(struct ccw_device *cdev)": "qdio_stop_irq - disable interrupt processing for the device   @cdev: associated ccw_device for the qdio subchannel     Return codes     0 - interrupts were already disabled     1 - interrupts successfully disabled ", "struct tcw *itcw_get_tcw(struct itcw *itcw)": "itcw_get_tcw - return pointer to tcw associated with the itcw   @itcw: address of the itcw     Return pointer to the tcw associated with the itcw. ", "struct itcw ": "itcw_finalize(itcw);   ", "void itcw_set_data(struct itcw *itcw, void *addr, int use_tidal)": "itcw_set_data - set data address and tida flag of the itcw   @itcw: address of the itcw   @addr: the data address   @use_tidal: zero of the data address specifies a contiguous block of data,   non-zero if it specifies a list if tidaws.     Set the inputoutput data address of the itcw (depending on the value of the   r-flag and w-flag). If @use_tidal is non-zero, the corresponding tida flag   is set as well. ", "int register_adapter_interrupt(struct airq_struct *airq)": "register_adapter_interrupt() - register adapter interrupt handler   @airq: pointer to adapter interrupt descriptor     Returns 0 on success, or -EINVAL. ", "void unregister_adapter_interrupt(struct airq_struct *airq)": "unregister_adapter_interrupt - unregister adapter interrupt handler   @airq: pointer to adapter interrupt descriptor ", "struct airq_iv *airq_iv_create(unsigned long bits, unsigned long flags,       unsigned long *vec)": "airq_iv_create - create an interrupt vector   @bits: number of bits in the interrupt vector   @flags: allocation flags   @vec: pointer to pinned guest memory if AIRQ_IV_GUESTVEC     Returns a pointer to an interrupt vector structure ", "void airq_iv_release(struct airq_iv *iv)": "airq_iv_release - release an interrupt vector   @iv: pointer to interrupt vector structure ", "unsigned long airq_iv_alloc(struct airq_iv *iv, unsigned long num)": "airq_iv_alloc - allocate irq bits from an interrupt vector   @iv: pointer to an interrupt vector structure   @num: number of consecutive irq bits to allocate     Returns the bit number of the first irq in the allocated block of irqs,   or -1UL if no bit is available or the AIRQ_IV_ALLOC flag has not been   specified ", "void airq_iv_free(struct airq_iv *iv, unsigned long bit, unsigned long num)": "airq_iv_free - free irq bits of an interrupt vector   @iv: pointer to interrupt vector structure   @bit: number of the first irq bit to free   @num: number of consecutive irq bits to free ", "unsigned long airq_iv_scan(struct airq_iv *iv, unsigned long start,   unsigned long end)": "airq_iv_scan - scan interrupt vector for non-zero bits   @iv: pointer to interrupt vector structure   @start: bit number to start the search   @end: bit number to end the search     Returns the bit number of the next non-zero interrupt bit, or   -1UL if the scan completed without finding any more any non-zero bits. ", "struct tcw *tcw_get_intrg(struct tcw *tcw)": "tcw_get_intrg - return pointer to associated interrogate tcw   @tcw: pointer to the original tcw     Return a pointer to the interrogate tcw associated with the specified tcw   or %NULL if there is no associated interrogate tcw. ", "void *tcw_get_data(struct tcw *tcw)": "tcw_get_data - return pointer to inputoutput data associated with tcw   @tcw: pointer to the tcw     Return the input or output data address specified in the tcw depending   on whether the r-bit or the w-bit is set. If neither bit is set, return   %NULL. ", "struct tccb *tcw_get_tccb(struct tcw *tcw)": "tcw_get_tccb - return pointer to tccb associated with tcw   @tcw: pointer to the tcw     Return pointer to the tccb associated with this tcw. ", "struct tsb *tcw_get_tsb(struct tcw *tcw)": "tcw_get_tsb - return pointer to tsb associated with tcw   @tcw: pointer to the tcw     Return pointer to the tsb associated with this tcw. ", "void tcw_init(struct tcw *tcw, int r, int w)": "tcw_init - initialize tcw data structure   @tcw: pointer to the tcw to be initialized   @r: initial value of the r-bit   @w: initial value of the w-bit     Initialize all fields of the specified tcw data structure with zero and   fill in the format, flags, r and w fields. ", "void tcw_finalize(struct tcw *tcw, int num_tidaws)": "tcw_finalize - finalize tcw length fields and tidaw list   @tcw: pointer to the tcw   @num_tidaws: the number of tidaws used to address inputoutput data or zero   if no tida is used     Calculate the input-output-count and tccbl field in the tcw, add a   tcat the tccb and terminate the data tidaw list if used.     Note: in case input- or output-tida is used, the tidaw-list must be stored   in contiguous storage (no ttic). The tcal field in the tccb must be   up-to-date. ", "void tcw_set_intrg(struct tcw *tcw, struct tcw *intrg_tcw)": "tcw_set_intrg - set the interrogate tcw address of a tcw   @tcw: the tcw address   @intrg_tcw: the address of the interrogate tcw     Set the address of the interrogate tcw in the specified tcw. ", "void tcw_set_data(struct tcw *tcw, void *data, int use_tidal)": "tcw_set_data - set data address and tida flag of a tcw   @tcw: the tcw address   @data: the data address   @use_tidal: zero of the data address specifies a contiguous block of data,   non-zero if it specifies a list if tidaws.     Set the inputoutput data address of a tcw (depending on the value of the   r-flag and w-flag). If @use_tidal is non-zero, the corresponding tida flag   is set as well. ", "void tcw_set_tccb(struct tcw *tcw, struct tccb *tccb)": "tcw_set_tccb - set tccb address of a tcw   @tcw: the tcw address   @tccb: the tccb address     Set the address of the tccb in the specified tcw. ", "void tcw_set_tsb(struct tcw *tcw, struct tsb *tsb)": "tcw_set_tsb - set tsb address of a tcw   @tcw: the tcw address   @tsb: the tsb address     Set the address of the tsb in the specified tcw. ", "void tccb_init(struct tccb *tccb, size_t size, u32 sac)": "tccb_init - initialize tccb   @tccb: the tccb address   @size: the maximum size of the tccb   @sac: the service-action-code to be user     Initialize the header of the specified tccb by resetting all values to zero   and filling in defaults for format, sac and initial tcal fields. ", "void tsb_init(struct tsb *tsb)": "tsb_init - initialize tsb   @tsb: the tsb address     Initialize the specified tsb by resetting all values to zero. ", "struct dcw *tccb_add_dcw(struct tccb *tccb, size_t tccb_size, u8 cmd, u8 flags, void *cd, u8 cd_count, u32 count)": "tccb_add_dcw - add a dcw to the tccb   @tccb: the tccb address   @tccb_size: the maximum tccb size   @cmd: the dcw command   @flags: flags for the dcw   @cd: pointer to control data for this dcw or NULL if none is required   @cd_count: number of control data bytes for this dcw   @count: number of data bytes for this dcw     Add a new dcw to the specified tccb by writing the dcw information specified   by @cmd, @flags, @cd, @cd_count and @count to the tca of the tccb. Return   a pointer to the newly added dcw on success or -%ENOSPC if the new dcw   would exceed the available space as defined by @tccb_size.     Note: the tcal field of the tccb header will be updates to reflect added   content. ", "struct tidaw *tcw_add_tidaw(struct tcw *tcw, int num_tidaws, u8 flags,    void *addr, u32 count)": "tcw_add_tidaw - add a tidaw to a tcw   @tcw: the tcw address   @num_tidaws: the current number of tidaws   @flags: flags for the new tidaw   @addr: address value for the new tidaw   @count: count value for the new tidaw     Add a new tidaw to the inputoutput data tidaw-list of the specified tcw   (depending on the value of the r-flag and w-flag) and return a pointer to   the new tidaw.     Note: the tidaw-list is assumed to be contiguous with no ttics. The caller   must ensure that there is enough space for the new tidaw. The last-tidaw   flag for the last tidaw in the list will be set by tcw_finalize. ", "put_device(&cdev->dev);return 0;error:cdev->private->state = DEV_STATE_OFFLINE;dev_fsm_event(cdev, DEV_EVENT_NOTOPER);spin_unlock_irq(cdev->ccwlock);/* Give up reference from ccw_device_set_online(). ": "ccw_device_set_online(). ", "int ccw_device_set_offline(struct ccw_device *cdev)": "ccw_device_set_offline() - disable a ccw device for IO   @cdev: target ccw device     This function calls the driver's set_offline() function for @cdev, if   given, and then disables @cdev.   Returns:     %0 on success and a negative error value on failure.   Context:    enabled, ccw device lock not held ", "int ccw_driver_register(struct ccw_driver *cdriver)": "ccw_driver_register() - register a ccw driver   @cdriver: driver to be registered     This function is mainly a wrapper around driver_register().   Returns:     %0 on success and a negative error value on failure. ", "void ccw_driver_unregister(struct ccw_driver *cdriver)": "ccw_driver_unregister() - deregister a ccw driver   @cdriver: driver to be deregistered     This function is mainly a wrapper around driver_unregister(). ", "struct ccw_device *get_ccwdev_by_busid(struct ccw_driver *cdrv,       const char *bus_id)": "get_ccwdev_by_busid() - obtain device from a bus id   @cdrv: driver the device is owned by   @bus_id: bus id of the device to be searched     This function searches all devices owned by @cdrv for a device with a bus   id matching @bus_id.   Returns:    If a match is found, its reference count of the found device is increased    and it is returned; else %NULL is returned. ", "int ccwgroup_set_online(struct ccwgroup_device *gdev)": "ccwgroup_set_online() - enable a ccwgroup device   @gdev: target ccwgroup device     This function attempts to put the ccwgroup device into the online state.   Returns:    %0 on success and a negative error value on failure. ", "int ccwgroup_set_offline(struct ccwgroup_device *gdev, bool call_gdrv)": "ccwgroup_set_offline() - disable a ccwgroup device   @gdev: target ccwgroup device   @call_gdrv: Call the registered gdrv set_offline function     This function attempts to put the ccwgroup device into the offline state.   Returns:    %0 on success and a negative error value on failure. ", "int ccwgroup_create_dev(struct device *parent, struct ccwgroup_driver *gdrv,int num_devices, const char *buf)": "ccwgroup_create_dev() - create and register a ccw group device   @parent: parent device for the new device   @gdrv: driver for the new group device   @num_devices: number of slave devices   @buf: buffer containing comma separated bus ids of slave devices     Create and register a new ccw group device as a child of @parent. Slave   devices are obtained from the list of bus ids given in @buf.   Returns:    %0 on success and an error code on failure.   Context:    non-atomic ", "int ccwgroup_driver_register(struct ccwgroup_driver *cdriver)": "ccwgroup_driver_register() - register a ccw group driver   @cdriver: driver to be registered     This function is mainly a wrapper around driver_register(). ", "void ccwgroup_driver_unregister(struct ccwgroup_driver *cdriver)": "ccwgroup_driver_unregister() - deregister a ccw group driver   @cdriver: driver to be deregistered     This function is mainly a wrapper around driver_unregister(). ", "int ccwgroup_probe_ccwdev(struct ccw_device *cdev)": "ccwgroup_probe_ccwdev() - probe function for slave devices   @cdev: ccw device to be probed     This is a dummy probe function for ccw devices that are slave devices in   a ccw group device.   Returns:    always %0 ", "void ccwgroup_remove_ccwdev(struct ccw_device *cdev)": "ccwgroup_remove_ccwdev() - remove function for slave devices   @cdev: ccw device to be removed     This is a remove function for ccw devices that are slave devices in a ccw   group device. It sets the ccw device offline and also deregisters the   embedding ccw group device. ", "int __parport_register_driver(struct parport_driver *drv, struct module *owner,      const char *mod_name)": "parport_get_port() to do so.  Calling  parport_register_device() on that port will do this for you.    The driver's detach() function may block.  The port that  detach() is given will be valid for the duration of the  callback, but if the driver wants to take a copy of the  pointer it must call parport_get_port() to do so.      Returns 0 on success. The non device model will always succeeds.  but the new device model can fail and will return the error code.  ", "struct parport *parport_get_port(struct parport *port)": "parport_put_port() call.  ", "struct parport *parport_register_port(unsigned long base, int irq, int dma,      struct parport_operations *ops)": "parport_remove_port().    If there is no memory to allocate a new parport structure,  this function will return %NULL.  ", "struct pardevice *parport_register_dev_model(struct parport *port, const char *name,   const struct pardev_cb *par_dev_cb, int id)": "parport_claim() is guaranteed to succeed when called from  inside the wake-up callback function.  If the driver wants to  claim the port it should do so; otherwise, it need not take  any action.  This function may not block, as it may be called  from interrupt context.  If the device driver does not want to  be explicitly invited to claim the port in this way, @wakeup can  be %NULL.    The interrupt handler, @irq_func, is called when an interrupt  arrives from the parallel port.  Note that if a device driver  wants to use interrupts it should use parport_enable_irq(),  and can also check the irq member of the parport structure  representing the port.    The parallel port (lowlevel) driver is the one that has called  request_irq() and whose interrupt handler is called first.  This handler does whatever needs to be done to the hardware to  acknowledge the interrupt (for PC-style ports there is nothing  special to be done).  It then tells the IEEE 1284 code about  the interrupt, which may involve reacting to an IEEE 1284  event depending on the current IEEE 1284 phase.  After this,  it calls @irq_func.  Needless to say, @irq_func will be called  from interrupt context, and may not block.    The %PARPORT_DEV_EXCL flag is for preventing port sharing, and  so should only be used when sharing the port with other device  drivers is impossible and would lead to incorrect behaviour.  Use it sparingly!  Normally, @flags will be zero.    This function returns a pointer to a structure that represents  the device on the port, or %NULL if there is not enough memory  to allocate space for that structure.  ", "void parport_unregister_device(struct pardevice *dev)": "parport_unregister_device - deregister a device on a parallel port  @dev: pointer to structure representing device    This undoes the effect of parport_register_device().  ", "struct parport *parport_find_number(int number)": "parport_find_number - find a parallel port by number  @number: parallel port number    This returns the parallel port with the specified number, or  %NULL if there is none.    There is an implicit parport_get_port() done already; to throw  away the reference to the port that parport_find_number()  gives you, use parport_put_port(). ", "struct parport *parport_find_base(unsigned long base)": "parport_find_base - find a parallel port by base address  @base: base IO address    This returns the parallel port with the specified base  address, or %NULL if there is none.    There is an implicit parport_get_port() done already; to throw  away the reference to the port that parport_find_base()  gives you, use parport_put_port(). ", "/* The cad_lock is still held for writing here ": "parport_claim_or_block(), or those with a wakeup function. ", "spin_lock_irq(&port->waitlist_lock);if (dev->waitprev || dev->waitnext || port->waithead == dev) ": "parport_release(dev);}spin_lock(&port->pardevice_lock);if (dev->next)dev->next->prev = dev->prev;if (dev->prev)dev->prev->next = dev->next;elseport->devices = dev->next;if (dev->flags & PARPORT_DEV_EXCL)port->flags &= ~PARPORT_FLAG_EXCL;spin_unlock(&port->pardevice_lock);    Make sure we haven't left any pointers around in the wait   list. ", "pdev = platform_device_register_simple(\"parport_pc\",       base, NULL, 0);if (IS_ERR(pdev))return NULL;dev = &pdev->dev;ret = dma_coerce_mask_and_coherent(dev, DMA_BIT_MASK(24));if (ret) ": "parport_pc_probe_port(unsigned long int base,       unsigned long int base_hi,       int irq, int dma,       struct device  dev,       int irqflags,       unsigned int mode_mask,       unsigned char ecr_writable){struct parport_pc_private  priv;struct parport_operations  ops;struct parport  p;int probedirq = PARPORT_IRQ_NONE;struct resource  base_res;struct resource ECR_res = NULL;struct resource EPP_res = NULL;struct platform_device  pdev = NULL;int ret;if (!dev) {  We need a physical device to attach to, but none was   provided. Create our own. ", "}EXPORT_SYMBOL(parport_pc_unregister_port": "parport_pc_unregister_port(struct parport  p){struct parport_pc_private  priv = p->private_data;struct parport_operations  ops = p->ops;parport_remove_port(p);spin_lock(&ports_lock);list_del_init(&priv->list);spin_unlock(&ports_lock);#if defined(CONFIG_PARPORT_PC_FIFO) && defined(HAS_DMA)if (p->dma != PARPORT_DMA_NONE)free_dma(p->dma);#endifif (p->irq != PARPORT_IRQ_NONE)free_irq(p->irq, p);release_region(p->base, 3);if (p->size > 3)release_region(p->base + 3, p->size - 3);if (p->modes & PARPORT_MODE_ECP)release_region(p->base_hi, 3);#if defined(CONFIG_PARPORT_PC_FIFO) && defined(HAS_DMA)if (priv->dma_buf)dma_free_coherent(p->physport->dev, PAGE_SIZE,    priv->dma_buf,    priv->dma_handle);#endifkfree(p->private_data);parport_del_port(p);kfree(ops);   hope no-one cached it ", "parport_frob_control (port,      PARPORT_CONTROL_AUTOFD      | PARPORT_CONTROL_STROBE      | PARPORT_CONTROL_INIT,      PARPORT_CONTROL_INIT);for (written = 0; written < len; written++, buf++) ": "parport_ieee1284_ecp_write_data (struct parport  port,const void  buffer, size_t len,int flags){#ifndef CONFIG_PARPORT_1284return 0;#elseconst unsigned char  buf = buffer;size_t written;int retry;port = port->physport;if (port->ieee1284.phase != IEEE1284_PH_FWD_IDLE)if (ecp_reverse_to_forward (port))return 0;port->ieee1284.phase = IEEE1284_PH_FWD_DATA;  HostAck high (data, not command) ", "unsigned char ctl;int rle = 0;ssize_t count = 0;port = port->physport;if (port->ieee1284.phase != IEEE1284_PH_REV_IDLE)if (ecp_forward_to_reverse (port))return 0;port->ieee1284.phase = IEEE1284_PH_REV_DATA;/* Set HostAck low to start accepting data. ": "parport_ieee1284_ecp_read_data (struct parport  port,       void  buffer, size_t len, int flags){#ifndef CONFIG_PARPORT_1284return 0;#elsestruct pardevice  dev = port->cad;unsigned char  buf = buffer;int rle_count = 0;   shut gcc up ", "parport_frob_control (port,      PARPORT_CONTROL_AUTOFD      | PARPORT_CONTROL_STROBE      | PARPORT_CONTROL_INIT,      PARPORT_CONTROL_AUTOFD      | PARPORT_CONTROL_INIT);for (written = 0; written < len; written++, buf++) ": "parport_ieee1284_ecp_write_addr (struct parport  port,const void  buffer, size_t len,int flags){#ifndef CONFIG_PARPORT_1284return 0;#elseconst unsigned char  buf = buffer;size_t written;int retry;port = port->physport;if (port->ieee1284.phase != IEEE1284_PH_FWD_IDLE)if (ecp_reverse_to_forward (port))return 0;port->ieee1284.phase = IEEE1284_PH_FWD_DATA;  HostAck low (command, not data) ", "for (i=0; i < len; i++) ": "parport_ieee1284_read_nibble (struct parport  port,      void  buffer, size_t len,     int flags){#ifndef CONFIG_PARPORT_1284return 0;#elseunsigned char  buf = buffer;int i;unsigned char byte = 0;len  = 2;   in nibbles ", "if (parport_read_status (port) & PARPORT_STATUS_ERROR) ": "parport_ieee1284_read_byte (struct parport  port,   void  buffer, size_t len,   int flags){#ifndef CONFIG_PARPORT_1284return 0;#elseunsigned char  buf = buffer;ssize_t count = 0;for (count = 0; count < len; count++) {unsigned char byte;  Data available? ", "parport_frob_control (port,      PARPORT_CONTROL_STROBE |      PARPORT_CONTROL_AUTOFD |      PARPORT_CONTROL_SELECT |      PARPORT_CONTROL_INIT,      PARPORT_CONTROL_STROBE |      PARPORT_CONTROL_INIT);port->ops->data_forward (port);for (; len > 0; len--, bp++) ": "parport_ieee1284_epp_write_addr (struct parport  port,const void  buffer, size_t len,int flags){unsigned char  bp = (unsigned char  ) buffer;size_t ret = 0;  set EPP idle state (just to make sure) with strobe low ", "parport_frob_control (port,      PARPORT_CONTROL_STROBE |      PARPORT_CONTROL_AUTOFD |      PARPORT_CONTROL_SELECT |      PARPORT_CONTROL_INIT,      PARPORT_CONTROL_INIT);port->ops->data_reverse (port);for (; len > 0; len--, bp++) ": "parport_ieee1284_epp_read_addr (struct parport  port,       void  buffer, size_t len,       int flags){unsigned char  bp = (unsigned char  ) buffer;unsigned ret = 0;  Set EPP idle state (just to make sure) with strobe high ", "int parport_negotiate (struct parport *port, int mode)": "parport_negotiate - negotiate an IEEE 1284 mode  @port: port to use  @mode: mode to negotiate to    Use this to negotiate to a particular IEEE 1284 transfer mode.  The @mode parameter should be one of the constants in  parport.h starting %IEEE1284_MODE_xxx.    The return value is 0 if the peripheral has accepted the  negotiation to the mode specified, -1 if the peripheral is not  IEEE 1284 compliant (or not present), or 1 if the peripheral  has rejected the negotiation. ", "/* Event 1: Set nSelectIn high, nAutoFd low ": "parport_write_data (port, m);udelay (400);   Shouldn't need to wait this long. ", "int parport_wait_peripheral(struct parport *port,    unsigned char mask,     unsigned char result)": "parport_set_timeout()) is  zero, the fast polling time is 35ms, and this function does  not call schedule().    If the timeout for this port is non-zero, after the fast  polling fails it uses parport_wait_event() to wait for up to  10ms, waking up if an interrupt occurs. ", "static void parport_ieee1284_wakeup (struct parport *port)": "parport_wait_peripheral wake up.   It will be useful to call this from an interrupt handler. ", "int parport_wait_event (struct parport *port, signed long timeout)": "parport_wait_event - wait for an event on a parallel port  @port: port to wait on  @timeout: time to wait (in jiffies)    This function waits for up to @timeout jiffies for an  interrupt to occur on a parallel port.  If the port timeout is  set to zero, it returns immediately.    If an interrupt occurs before the timeout period elapses, this  function returns zero immediately.  If it times out, it returns  one.  An error code less than zero indicates an error (most  likely a pending signal), and the calling code should finish  what it's doing as soon as it can. ", "pr_debug(\"%s: Data available\\n\", port->name);parport_ieee1284_ack_data_avail (port);}#endif /* IEEE1284 support ": "parport_ieee1284_interrupt (void  handle){struct parport  port = handle;parport_ieee1284_wakeup (port);#ifdef CONFIG_PARPORT_1284if (port->ieee1284.phase == IEEE1284_PH_REV_IDLE) {  An interrupt in this phase means that data   is now available. ", "void hdmi_avi_infoframe_init(struct hdmi_avi_infoframe *frame)": "hdmi_avi_infoframe_init() - initialize an HDMI AVI infoframe   @frame: HDMI AVI infoframe ", "int hdmi_avi_infoframe_check(struct hdmi_avi_infoframe *frame)": "hdmi_avi_infoframe_check_only(const struct hdmi_avi_infoframe  frame){if (frame->type != HDMI_INFOFRAME_TYPE_AVI ||    frame->version != 2 ||    frame->length != HDMI_AVI_INFOFRAME_SIZE)return -EINVAL;if (frame->picture_aspect > HDMI_PICTURE_ASPECT_16_9)return -EINVAL;return 0;}     hdmi_avi_infoframe_check() - check a HDMI AVI infoframe   @frame: HDMI AVI infoframe     Validates that the infoframe is consistent and updates derived fields   (eg. length) based on other fields.     Returns 0 on success or a negative error code on failure. ", "ssize_t hdmi_avi_infoframe_pack_only(const struct hdmi_avi_infoframe *frame,     void *buffer, size_t size)": "hdmi_avi_infoframe_pack_only() - write HDMI AVI infoframe to binary buffer   @frame: HDMI AVI infoframe   @buffer: destination buffer   @size: size of buffer     Packs the information contained in the @frame structure into a binary   representation that can be written into the corresponding controller   registers. Also computes the checksum as required by section 5.3.5 of   the HDMI 1.4 specification.     Returns the number of bytes packed into the binary buffer or a negative   error code on failure. ", "int hdmi_spd_infoframe_init(struct hdmi_spd_infoframe *frame,    const char *vendor, const char *product)": "hdmi_spd_infoframe_init() - initialize an HDMI SPD infoframe   @frame: HDMI SPD infoframe   @vendor: vendor string   @product: product string     Returns 0 on success or a negative error code on failure. ", "int hdmi_spd_infoframe_check(struct hdmi_spd_infoframe *frame)": "hdmi_spd_infoframe_check_only(const struct hdmi_spd_infoframe  frame){if (frame->type != HDMI_INFOFRAME_TYPE_SPD ||    frame->version != 1 ||    frame->length != HDMI_SPD_INFOFRAME_SIZE)return -EINVAL;return 0;}     hdmi_spd_infoframe_check() - check a HDMI SPD infoframe   @frame: HDMI SPD infoframe     Validates that the infoframe is consistent and updates derived fields   (eg. length) based on other fields.     Returns 0 on success or a negative error code on failure. ", "ssize_t hdmi_spd_infoframe_pack_only(const struct hdmi_spd_infoframe *frame,     void *buffer, size_t size)": "hdmi_spd_infoframe_pack_only() - write HDMI SPD infoframe to binary buffer   @frame: HDMI SPD infoframe   @buffer: destination buffer   @size: size of buffer     Packs the information contained in the @frame structure into a binary   representation that can be written into the corresponding controller   registers. Also computes the checksum as required by section 5.3.5 of   the HDMI 1.4 specification.     Returns the number of bytes packed into the binary buffer or a negative   error code on failure. ", "int hdmi_audio_infoframe_init(struct hdmi_audio_infoframe *frame)": "hdmi_audio_infoframe_init() - initialize an HDMI audio infoframe   @frame: HDMI audio infoframe     Returns 0 on success or a negative error code on failure. ", "int hdmi_audio_infoframe_check(const struct hdmi_audio_infoframe *frame)": "hdmi_audio_infoframe_check_only(const struct hdmi_audio_infoframe  frame){if (frame->type != HDMI_INFOFRAME_TYPE_AUDIO ||    frame->version != 1 ||    frame->length != HDMI_AUDIO_INFOFRAME_SIZE)return -EINVAL;return 0;}     hdmi_audio_infoframe_check() - check a HDMI audio infoframe   @frame: HDMI audio infoframe     Validates that the infoframe is consistent and updates derived fields   (eg. length) based on other fields.     Returns 0 on success or a negative error code on failure. ", "ssize_t hdmi_audio_infoframe_pack_only(const struct hdmi_audio_infoframe *frame,       void *buffer, size_t size)": "hdmi_audio_infoframe_pack_payload(const struct hdmi_audio_infoframe  frame,  u8  buffer){u8 channels;if (frame->channels >= 2)channels = frame->channels - 1;elsechannels = 0;buffer[0] = ((frame->coding_type & 0xf) << 4) | (channels & 0x7);buffer[1] = ((frame->sample_frequency & 0x7) << 2) | (frame->sample_size & 0x3);buffer[2] = frame->coding_type_ext & 0x1f;buffer[3] = frame->channel_allocation;buffer[4] = (frame->level_shift_value & 0xf) << 3;if (frame->downmix_inhibit)buffer[4] |= BIT(7);}     hdmi_audio_infoframe_pack_only() - write HDMI audio infoframe to binary buffer   @frame: HDMI audio infoframe   @buffer: destination buffer   @size: size of buffer     Packs the information contained in the @frame structure into a binary   representation that can be written into the corresponding controller   registers. Also computes the checksum as required by section 5.3.5 of   the HDMI 1.4 specification.     Returns the number of bytes packed into the binary buffer or a negative   error code on failure. ", "ssize_thdmi_audio_infoframe_pack_for_dp(const struct hdmi_audio_infoframe *frame, struct dp_sdp *sdp, u8 dp_version)": "hdmi_audio_infoframe_pack_for_dp - Pack a HDMI Audio infoframe for DisplayPort     @frame:      HDMI Audio infoframe   @sdp:        Secondary data packet for DisplayPort.   @dp_version: DisplayPort version to be encoded in the header     Packs a HDMI Audio Infoframe to be sent over DisplayPort. This function   fills the secondary data packet to be used for DisplayPort.     Return: Number of total written bytes or a negative errno on failure. ", "int hdmi_vendor_infoframe_init(struct hdmi_vendor_infoframe *frame)": "hdmi_vendor_infoframe_init() - initialize an HDMI vendor infoframe   @frame: HDMI vendor infoframe     Returns 0 on success or a negative error code on failure. ", "if (frame->vic != 0 && frame->s3d_struct != HDMI_3D_STRUCTURE_INVALID)return -EINVAL;if (frame->length != hdmi_vendor_infoframe_length(frame))return -EINVAL;return 0;}/** * hdmi_vendor_infoframe_check() - check a HDMI vendor infoframe * @frame: HDMI infoframe * * Validates that the infoframe is consistent and updates derived fields * (eg. length) based on other fields. * * Returns 0 on success or a negative error code on failure. ": "hdmi_vendor_infoframe_check_only(const struct hdmi_vendor_infoframe  frame){if (frame->type != HDMI_INFOFRAME_TYPE_VENDOR ||    frame->version != 1 ||    frame->oui != HDMI_IEEE_OUI)return -EINVAL;  only one of those can be supplied ", "ssize_t hdmi_vendor_infoframe_pack_only(const struct hdmi_vendor_infoframe *frame,void *buffer, size_t size)": "hdmi_vendor_infoframe_pack_only() - write a HDMI vendor infoframe to binary buffer   @frame: HDMI infoframe   @buffer: destination buffer   @size: size of buffer     Packs the information contained in the @frame structure into a binary   representation that can be written into the corresponding controller   registers. Also computes the checksum as required by section 5.3.5 of   the HDMI 1.4 specification.     Returns the number of bytes packed into the binary buffer or a negative   error code on failure. ", "int hdmi_drm_infoframe_init(struct hdmi_drm_infoframe *frame)": "hdmi_drm_infoframe_init() - initialize an HDMI Dynaminc Range and   mastering infoframe   @frame: HDMI DRM infoframe     Returns 0 on success or a negative error code on failure. ", "int hdmi_drm_infoframe_check(struct hdmi_drm_infoframe *frame)": "hdmi_drm_infoframe_check_only(const struct hdmi_drm_infoframe  frame){if (frame->type != HDMI_INFOFRAME_TYPE_DRM ||    frame->version != 1)return -EINVAL;if (frame->length != HDMI_DRM_INFOFRAME_SIZE)return -EINVAL;return 0;}     hdmi_drm_infoframe_check() - check a HDMI DRM infoframe   @frame: HDMI DRM infoframe     Validates that the infoframe is consistent.   Returns 0 on success or a negative error code on failure. ", "ssize_t hdmi_drm_infoframe_pack_only(const struct hdmi_drm_infoframe *frame,     void *buffer, size_t size)": "hdmi_drm_infoframe_pack_only() - write HDMI DRM infoframe to binary buffer   @frame: HDMI DRM infoframe   @buffer: destination buffer   @size: size of buffer     Packs the information contained in the @frame structure into a binary   representation that can be written into the corresponding controller   registers. Also computes the checksum as required by section 5.3.5 of   the HDMI 1.4 specification.     Returns the number of bytes packed into the binary buffer or a negative   error code on failure. ", "for (i = 0; i < size; i++)csum += ptr[i];return 256 - csum;}static void hdmi_infoframe_set_checksum(void *buffer, size_t size)": "hdmi_infoframe_checksum(const u8  ptr, size_t size){u8 csum = 0;size_t i;  compute checksum ", "ssize_thdmi_infoframe_pack_only(const union hdmi_infoframe *frame, void *buffer, size_t size)": "hdmi_infoframe_pack_only() - write a HDMI infoframe to binary buffer   @frame: HDMI infoframe   @buffer: destination buffer   @size: size of buffer     Packs the information contained in the @frame structure into a binary   representation that can be written into the corresponding controller   registers. Also computes the checksum as required by section 5.3.5 of   the HDMI 1.4 specification.     Returns the number of bytes packed into the binary buffer or a negative   error code on failure. ", "void hdmi_infoframe_log(const char *level,struct device *dev,const union hdmi_infoframe *frame)": "hdmi_infoframe_log_header(const char  level,      struct device  dev,      const struct hdmi_any_infoframe  frame){hdmi_log(\"HDMI infoframe: %s, version %u, length %u\\n\",hdmi_infoframe_type_get_name(frame->type),frame->version, frame->length);}static const char  hdmi_colorspace_get_name(enum hdmi_colorspace colorspace){switch (colorspace) {case HDMI_COLORSPACE_RGB:return \"RGB\";case HDMI_COLORSPACE_YUV422:return \"YCbCr 4:2:2\";case HDMI_COLORSPACE_YUV444:return \"YCbCr 4:4:4\";case HDMI_COLORSPACE_YUV420:return \"YCbCr 4:2:0\";case HDMI_COLORSPACE_RESERVED4:return \"Reserved (4)\";case HDMI_COLORSPACE_RESERVED5:return \"Reserved (5)\";case HDMI_COLORSPACE_RESERVED6:return \"Reserved (6)\";case HDMI_COLORSPACE_IDO_DEFINED:return \"IDO Defined\";}return \"Invalid\";}static const char  hdmi_scan_mode_get_name(enum hdmi_scan_mode scan_mode){switch (scan_mode) {case HDMI_SCAN_MODE_NONE:return \"No Data\";case HDMI_SCAN_MODE_OVERSCAN:return \"Overscan\";case HDMI_SCAN_MODE_UNDERSCAN:return \"Underscan\";case HDMI_SCAN_MODE_RESERVED:return \"Reserved\";}return \"Invalid\";}static const char  hdmi_colorimetry_get_name(enum hdmi_colorimetry colorimetry){switch (colorimetry) {case HDMI_COLORIMETRY_NONE:return \"No Data\";case HDMI_COLORIMETRY_ITU_601:return \"ITU601\";case HDMI_COLORIMETRY_ITU_709:return \"ITU709\";case HDMI_COLORIMETRY_EXTENDED:return \"Extended\";}return \"Invalid\";}static const char  hdmi_picture_aspect_get_name(enum hdmi_picture_aspect picture_aspect){switch (picture_aspect) {case HDMI_PICTURE_ASPECT_NONE:return \"No Data\";case HDMI_PICTURE_ASPECT_4_3:return \"4:3\";case HDMI_PICTURE_ASPECT_16_9:return \"16:9\";case HDMI_PICTURE_ASPECT_64_27:return \"64:27\";case HDMI_PICTURE_ASPECT_256_135:return \"256:135\";case HDMI_PICTURE_ASPECT_RESERVED:return \"Reserved\";}return \"Invalid\";}static const char  hdmi_active_aspect_get_name(enum hdmi_active_aspect active_aspect){if (active_aspect < 0 || active_aspect > 0xf)return \"Invalid\";switch (active_aspect) {case HDMI_ACTIVE_ASPECT_16_9_TOP:return \"16:9 Top\";case HDMI_ACTIVE_ASPECT_14_9_TOP:return \"14:9 Top\";case HDMI_ACTIVE_ASPECT_16_9_CENTER:return \"16:9 Center\";case HDMI_ACTIVE_ASPECT_PICTURE:return \"Same as Picture\";case HDMI_ACTIVE_ASPECT_4_3:return \"4:3\";case HDMI_ACTIVE_ASPECT_16_9:return \"16:9\";case HDMI_ACTIVE_ASPECT_14_9:return \"14:9\";case HDMI_ACTIVE_ASPECT_4_3_SP_14_9:return \"4:3 SP 14:9\";case HDMI_ACTIVE_ASPECT_16_9_SP_14_9:return \"16:9 SP 14:9\";case HDMI_ACTIVE_ASPECT_16_9_SP_4_3:return \"16:9 SP 4:3\";}return \"Reserved\";}static const char  hdmi_extended_colorimetry_get_name(enum hdmi_extended_colorimetry ext_col){switch (ext_col) {case HDMI_EXTENDED_COLORIMETRY_XV_YCC_601:return \"xvYCC 601\";case HDMI_EXTENDED_COLORIMETRY_XV_YCC_709:return \"xvYCC 709\";case HDMI_EXTENDED_COLORIMETRY_S_YCC_601:return \"sYCC 601\";case HDMI_EXTENDED_COLORIMETRY_OPYCC_601:return \"opYCC 601\";case HDMI_EXTENDED_COLORIMETRY_OPRGB:return \"opRGB\";case HDMI_EXTENDED_COLORIMETRY_BT2020_CONST_LUM:return \"BT.2020 Constant Luminance\";case HDMI_EXTENDED_COLORIMETRY_BT2020:return \"BT.2020\";case HDMI_EXTENDED_COLORIMETRY_RESERVED:return \"Reserved\";}return \"Invalid\";}static const char  hdmi_quantization_range_get_name(enum hdmi_quantization_range qrange){switch (qrange) {case HDMI_QUANTIZATION_RANGE_DEFAULT:return \"Default\";case HDMI_QUANTIZATION_RANGE_LIMITED:return \"Limited\";case HDMI_QUANTIZATION_RANGE_FULL:return \"Full\";case HDMI_QUANTIZATION_RANGE_RESERVED:return \"Reserved\";}return \"Invalid\";}static const char  hdmi_nups_get_name(enum hdmi_nups nups){switch (nups) {case HDMI_NUPS_UNKNOWN:return \"Unknown Non-uniform Scaling\";case HDMI_NUPS_HORIZONTAL:return \"Horizontally Scaled\";case HDMI_NUPS_VERTICAL:return \"Vertically Scaled\";case HDMI_NUPS_BOTH:return \"Horizontally and Vertically Scaled\";}return \"Invalid\";}static const char  hdmi_ycc_quantization_range_get_name(enum hdmi_ycc_quantization_range qrange){switch (qrange) {case HDMI_YCC_QUANTIZATION_RANGE_LIMITED:return \"Limited\";case HDMI_YCC_QUANTIZATION_RANGE_FULL:return \"Full\";}return \"Invalid\";}static const char  hdmi_content_type_get_name(enum hdmi_content_type content_type){switch (content_type) {case HDMI_CONTENT_TYPE_GRAPHICS:return \"Graphics\";case HDMI_CONTENT_TYPE_PHOTO:return \"Photo\";case HDMI_CONTENT_TYPE_CINEMA:return \"Cinema\";case HDMI_CONTENT_TYPE_GAME:return \"Game\";}return \"Invalid\";}static void hdmi_avi_infoframe_log(const char  level,   struct device  dev,   const struct hdmi_avi_infoframe  frame){hdmi_infoframe_log_header(level, dev,  (const struct hdmi_any_infoframe  )frame);hdmi_log(\"    colorspace: %s\\n\",hdmi_colorspace_get_name(frame->colorspace));hdmi_log(\"    scan mode: %s\\n\",hdmi_scan_mode_get_name(frame->scan_mode));hdmi_log(\"    colorimetry: %s\\n\",hdmi_colorimetry_get_name(frame->colorimetry));hdmi_log(\"    picture aspect: %s\\n\",hdmi_picture_aspect_get_name(frame->picture_aspect));hdmi_log(\"    active aspect: %s\\n\",hdmi_active_aspect_get_name(frame->active_aspect));hdmi_log(\"    itc: %s\\n\", frame->itc ? \"IT Content\" : \"No Data\");hdmi_log(\"    extended colorimetry: %s\\n\",hdmi_extended_colorimetry_get_name(frame->extended_colorimetry));hdmi_log(\"    quantization range: %s\\n\",hdmi_quantization_range_get_name(frame->quantization_range));hdmi_log(\"    nups: %s\\n\", hdmi_nups_get_name(frame->nups));hdmi_log(\"    video code: %u\\n\", frame->video_code);hdmi_log(\"    ycc quantization range: %s\\n\",hdmi_ycc_quantization_range_get_name(frame->ycc_quantization_range));hdmi_log(\"    hdmi content type: %s\\n\",hdmi_content_type_get_name(frame->content_type));hdmi_log(\"    pixel repeat: %u\\n\", frame->pixel_repeat);hdmi_log(\"    bar top %u, bottom %u, left %u, right %u\\n\",frame->top_bar, frame->bottom_bar,frame->left_bar, frame->right_bar);}static const char  hdmi_spd_sdi_get_name(enum hdmi_spd_sdi sdi){if (sdi < 0 || sdi > 0xff)return \"Invalid\";switch (sdi) {case HDMI_SPD_SDI_UNKNOWN:return \"Unknown\";case HDMI_SPD_SDI_DSTB:return \"Digital STB\";case HDMI_SPD_SDI_DVDP:return \"DVD Player\";case HDMI_SPD_SDI_DVHS:return \"D-VHS\";case HDMI_SPD_SDI_HDDVR:return \"HDD Videorecorder\";case HDMI_SPD_SDI_DVC:return \"DVC\";case HDMI_SPD_SDI_DSC:return \"DSC\";case HDMI_SPD_SDI_VCD:return \"Video CD\";case HDMI_SPD_SDI_GAME:return \"Game\";case HDMI_SPD_SDI_PC:return \"PC General\";case HDMI_SPD_SDI_BD:return \"Blu-Ray Disc (BD)\";case HDMI_SPD_SDI_SACD:return \"Super Audio CD\";case HDMI_SPD_SDI_HDDVD:return \"HD DVD\";case HDMI_SPD_SDI_PMP:return \"PMP\";}return \"Reserved\";}static void hdmi_spd_infoframe_log(const char  level,   struct device  dev,   const struct hdmi_spd_infoframe  frame){u8 buf[17];hdmi_infoframe_log_header(level, dev,  (const struct hdmi_any_infoframe  )frame);memset(buf, 0, sizeof(buf));strncpy(buf, frame->vendor, 8);hdmi_log(\"    vendor: %s\\n\", buf);strncpy(buf, frame->product, 16);hdmi_log(\"    product: %s\\n\", buf);hdmi_log(\"    source device information: %s (0x%x)\\n\",hdmi_spd_sdi_get_name(frame->sdi), frame->sdi);}static const char  hdmi_audio_coding_type_get_name(enum hdmi_audio_coding_type coding_type){switch (coding_type) {case HDMI_AUDIO_CODING_TYPE_STREAM:return \"Refer to Stream Header\";case HDMI_AUDIO_CODING_TYPE_PCM:return \"PCM\";case HDMI_AUDIO_CODING_TYPE_AC3:return \"AC-3\";case HDMI_AUDIO_CODING_TYPE_MPEG1:return \"MPEG1\";case HDMI_AUDIO_CODING_TYPE_MP3:return \"MP3\";case HDMI_AUDIO_CODING_TYPE_MPEG2:return \"MPEG2\";case HDMI_AUDIO_CODING_TYPE_AAC_LC:return \"AAC\";case HDMI_AUDIO_CODING_TYPE_DTS:return \"DTS\";case HDMI_AUDIO_CODING_TYPE_ATRAC:return \"ATRAC\";case HDMI_AUDIO_CODING_TYPE_DSD:return \"One Bit Audio\";case HDMI_AUDIO_CODING_TYPE_EAC3:return \"Dolby Digital +\";case HDMI_AUDIO_CODING_TYPE_DTS_HD:return \"DTS-HD\";case HDMI_AUDIO_CODING_TYPE_MLP:return \"MAT (MLP)\";case HDMI_AUDIO_CODING_TYPE_DST:return \"DST\";case HDMI_AUDIO_CODING_TYPE_WMA_PRO:return \"WMA PRO\";case HDMI_AUDIO_CODING_TYPE_CXT:return \"Refer to CXT\";}return \"Invalid\";}static const char  hdmi_audio_sample_size_get_name(enum hdmi_audio_sample_size sample_size){switch (sample_size) {case HDMI_AUDIO_SAMPLE_SIZE_STREAM:return \"Refer to Stream Header\";case HDMI_AUDIO_SAMPLE_SIZE_16:return \"16 bit\";case HDMI_AUDIO_SAMPLE_SIZE_20:return \"20 bit\";case HDMI_AUDIO_SAMPLE_SIZE_24:return \"24 bit\";}return \"Invalid\";}static const char  hdmi_audio_sample_frequency_get_name(enum hdmi_audio_sample_frequency freq){switch (freq) {case HDMI_AUDIO_SAMPLE_FREQUENCY_STREAM:return \"Refer to Stream Header\";case HDMI_AUDIO_SAMPLE_FREQUENCY_32000:return \"32 kHz\";case HDMI_AUDIO_SAMPLE_FREQUENCY_44100:return \"44.1 kHz (CD)\";case HDMI_AUDIO_SAMPLE_FREQUENCY_48000:return \"48 kHz\";case HDMI_AUDIO_SAMPLE_FREQUENCY_88200:return \"88.2 kHz\";case HDMI_AUDIO_SAMPLE_FREQUENCY_96000:return \"96 kHz\";case HDMI_AUDIO_SAMPLE_FREQUENCY_176400:return \"176.4 kHz\";case HDMI_AUDIO_SAMPLE_FREQUENCY_192000:return \"192 kHz\";}return \"Invalid\";}static const char  hdmi_audio_coding_type_ext_get_name(enum hdmi_audio_coding_type_ext ctx){if (ctx < 0 || ctx > 0x1f)return \"Invalid\";switch (ctx) {case HDMI_AUDIO_CODING_TYPE_EXT_CT:return \"Refer to CT\";case HDMI_AUDIO_CODING_TYPE_EXT_HE_AAC:return \"HE AAC\";case HDMI_AUDIO_CODING_TYPE_EXT_HE_AAC_V2:return \"HE AAC v2\";case HDMI_AUDIO_CODING_TYPE_EXT_MPEG_SURROUND:return \"MPEG SURROUND\";case HDMI_AUDIO_CODING_TYPE_EXT_MPEG4_HE_AAC:return \"MPEG-4 HE AAC\";case HDMI_AUDIO_CODING_TYPE_EXT_MPEG4_HE_AAC_V2:return \"MPEG-4 HE AAC v2\";case HDMI_AUDIO_CODING_TYPE_EXT_MPEG4_AAC_LC:return \"MPEG-4 AAC LC\";case HDMI_AUDIO_CODING_TYPE_EXT_DRA:return \"DRA\";case HDMI_AUDIO_CODING_TYPE_EXT_MPEG4_HE_AAC_SURROUND:return \"MPEG-4 HE AAC + MPEG Surround\";case HDMI_AUDIO_CODING_TYPE_EXT_MPEG4_AAC_LC_SURROUND:return \"MPEG-4 AAC LC + MPEG Surround\";}return \"Reserved\";}static void hdmi_audio_infoframe_log(const char  level,     struct device  dev,     const struct hdmi_audio_infoframe  frame){hdmi_infoframe_log_header(level, dev,  (const struct hdmi_any_infoframe  )frame);if (frame->channels)hdmi_log(\"    channels: %u\\n\", frame->channels - 1);elsehdmi_log(\"    channels: Refer to stream header\\n\");hdmi_log(\"    coding type: %s\\n\",hdmi_audio_coding_type_get_name(frame->coding_type));hdmi_log(\"    sample size: %s\\n\",hdmi_audio_sample_size_get_name(frame->sample_size));hdmi_log(\"    sample frequency: %s\\n\",hdmi_audio_sample_frequency_get_name(frame->sample_frequency));hdmi_log(\"    coding type ext: %s\\n\",hdmi_audio_coding_type_ext_get_name(frame->coding_type_ext));hdmi_log(\"    channel allocation: 0x%x\\n\",frame->channel_allocation);hdmi_log(\"    level shift value: %u dB\\n\",frame->level_shift_value);hdmi_log(\"    downmix inhibit: %s\\n\",frame->downmix_inhibit ? \"Yes\" : \"No\");}static void hdmi_drm_infoframe_log(const char  level,   struct device  dev,   const struct hdmi_drm_infoframe  frame){int i;hdmi_infoframe_log_header(level, dev,  (struct hdmi_any_infoframe  )frame);hdmi_log(\"length: %d\\n\", frame->length);hdmi_log(\"metadata type: %d\\n\", frame->metadata_type);hdmi_log(\"eotf: %d\\n\", frame->eotf);for (i = 0; i < 3; i++) {hdmi_log(\"x[%d]: %d\\n\", i, frame->display_primaries[i].x);hdmi_log(\"y[%d]: %d\\n\", i, frame->display_primaries[i].y);}hdmi_log(\"white point x: %d\\n\", frame->white_point.x);hdmi_log(\"white point y: %d\\n\", frame->white_point.y);hdmi_log(\"max_display_mastering_luminance: %d\\n\", frame->max_display_mastering_luminance);hdmi_log(\"min_display_mastering_luminance: %d\\n\", frame->min_display_mastering_luminance);hdmi_log(\"max_cll: %d\\n\", frame->max_cll);hdmi_log(\"max_fall: %d\\n\", frame->max_fall);}static const char  hdmi_3d_structure_get_name(enum hdmi_3d_structure s3d_struct){if (s3d_struct < 0 || s3d_struct > 0xf)return \"Invalid\";switch (s3d_struct) {case HDMI_3D_STRUCTURE_FRAME_PACKING:return \"Frame Packing\";case HDMI_3D_STRUCTURE_FIELD_ALTERNATIVE:return \"Field Alternative\";case HDMI_3D_STRUCTURE_LINE_ALTERNATIVE:return \"Line Alternative\";case HDMI_3D_STRUCTURE_SIDE_BY_SIDE_FULL:return \"Side-by-side (Full)\";case HDMI_3D_STRUCTURE_L_DEPTH:return \"L + Depth\";case HDMI_3D_STRUCTURE_L_DEPTH_GFX_GFX_DEPTH:return \"L + Depth + Graphics + Graphics-depth\";case HDMI_3D_STRUCTURE_TOP_AND_BOTTOM:return \"Top-and-Bottom\";case HDMI_3D_STRUCTURE_SIDE_BY_SIDE_HALF:return \"Side-by-side (Half)\";default:break;}return \"Reserved\";}static voidhdmi_vendor_any_infoframe_log(const char  level,      struct device  dev,      const union hdmi_vendor_any_infoframe  frame){const struct hdmi_vendor_infoframe  hvf = &frame->hdmi;hdmi_infoframe_log_header(level, dev,  (const struct hdmi_any_infoframe  )frame);if (frame->any.oui != HDMI_IEEE_OUI) {hdmi_log(\"    not a HDMI vendor infoframe\\n\");return;}if (hvf->vic == 0 && hvf->s3d_struct == HDMI_3D_STRUCTURE_INVALID) {hdmi_log(\"    empty frame\\n\");return;}if (hvf->vic)hdmi_log(\"    HDMI VIC: %u\\n\", hvf->vic);if (hvf->s3d_struct != HDMI_3D_STRUCTURE_INVALID) {hdmi_log(\"    3D structure: %s\\n\",hdmi_3d_structure_get_name(hvf->s3d_struct));if (hvf->s3d_struct >= HDMI_3D_STRUCTURE_SIDE_BY_SIDE_HALF)hdmi_log(\"    3D extension data: %d\\n\",hvf->s3d_ext_data);}}     hdmi_infoframe_log() - log info of HDMI infoframe   @level: logging level   @dev: device   @frame: HDMI infoframe ", "int hdmi_drm_infoframe_unpack_only(struct hdmi_drm_infoframe *frame,   const void *buffer, size_t size)": "hdmi_drm_infoframe_unpack_only() - unpack binary buffer of CTA-861-G DRM                                      infoframe DataBytes to a HDMI DRM                                      infoframe   @frame: HDMI DRM infoframe   @buffer: source buffer   @size: size of buffer     Unpacks CTA-861-G DRM infoframe DataBytes contained in the binary @buffer   into a structured @frame of the HDMI Dynamic Range and Mastering (DRM)   infoframe.     Returns 0 on success or a negative error code on failure. ", "int hdmi_infoframe_unpack(union hdmi_infoframe *frame,  const void *buffer, size_t size)": "hdmi_infoframe_unpack() - unpack binary buffer to a HDMI infoframe   @frame: HDMI infoframe   @buffer: source buffer   @size: size of buffer     Unpacks the information contained in binary buffer @buffer into a structured   @frame of a HDMI infoframe.   Also verifies the checksum as required by section 5.3.5 of the HDMI 1.4   specification.     Returns 0 on success or a negative error code on failure. ", "misc = vga_r(state->vgabase, VGA_MIS_R);iobase = (misc & 1) ? 0x3d0 : 0x3b0;vga_r(state->vgabase, iobase + 0xa);vga_w(state->vgabase, VGA_ATT_W, 0x00);attr10 = vga_rattr(state->vgabase, 0x10);vga_r(state->vgabase, iobase + 0xa);vga_w(state->vgabase, VGA_ATT_W, 0x20);if (attr10 & 1)return;/* save regs ": "save_vga_text(struct vgastate  state, void __iomem  fbbase){struct regstate  saved = (struct regstate  ) state->vidstate;int i;u8 misc, attr10, gr4, gr5, gr6, seq1, seq2, seq4;unsigned short iobase;  if in graphics mode, no need to save ", "gr1 = vga_rgfx(state->vgabase, VGA_GFX_SR_ENABLE);gr3 = vga_rgfx(state->vgabase, VGA_GFX_DATA_ROTATE);gr4 = vga_rgfx(state->vgabase, VGA_GFX_PLANE_READ);gr5 = vga_rgfx(state->vgabase, VGA_GFX_MODE);gr6 = vga_rgfx(state->vgabase, VGA_GFX_MISC);gr8 = vga_rgfx(state->vgabase, VGA_GFX_BIT_MASK);seq2 = vga_rseq(state->vgabase, VGA_SEQ_PLANE_WRITE);seq4 = vga_rseq(state->vgabase, VGA_SEQ_MEMORY_MODE);/* blank screen ": "restore_vga_text(struct vgastate  state, void __iomem  fbbase){struct regstate  saved = (struct regstate  ) state->vidstate;int i;u8 gr1, gr3, gr4, gr5, gr6, gr8;u8 seq1, seq2, seq4;  save regs ", "const char *video_get_options(const char *name)": "video_get_options - get kernel boot parameters   @name:name of the output as it would appear in the boot parameter  line (video=<name>:<options>)     Looks up the video= options for the given name. Names are connector   names with DRM, or driver names with fbdev. If no video option for   the name has been specified, the function returns the global video=   setting. A @name of NULL always returns the global video setting.     Returns:   The string of video options for the given name, or NULL if no video   option has been specified. ", "f->raw = (struct sti_rom_font *) (n + 3);kfree(old_font);}EXPORT_SYMBOL(sti_font_convert_bytemode": "sti_font_convert_bytemode(struct sti_struct  sti, struct sti_cooked_font  f){unsigned char  n,  p,  q;int size = f->raw->bytes_per_char   (f->raw->last_char + 1) + sizeof(struct sti_rom_font);struct sti_rom_font  old_font;if (sti->wordmode)return;old_font = f->raw_ptr;n = kcalloc(4, size, STI_LOWMEM);f->raw_ptr = n;if (!n)return;p = n + 3;q = (unsigned char  ) f->raw;while (size--) { p =  q++;p += 4;}  store new ptr to byte-mode font and delete old font ", "struct aperture_range ": "aperture_remove_conflicting_pci_devices() and   let the function detect the apertures automatically. Device drivers without   knowledge of the framebuffer's location can call   aperture_remove_all_conflicting_devices(), which removes all known devices.     Drivers that are susceptible to being removed by other drivers, such as   generic EFI or VESA drivers, have to register themselves as owners of their   framebuffer apertures. Ownership of the framebuffer memory is achieved   by calling devm_aperture_acquire_for_platform_device(). If successful, the   driver is the owner of the framebuffer range. The function fails if the   framebuffer is already owned by another driver. See below for an example.     .. code-block:: c    static int generic_probe(struct platform_device  pdev)  {  struct resource  mem;  resource_size_t base, size;    mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);  if (!mem)  return -ENODEV;  base = mem->start;  size = resource_size(mem);    ret = devm_aperture_acquire_for_platform_device(pdev, base, size);  if (ret)  return ret;     Initialize the hardware  ...    return 0;  }    static int generic_remove(struct platform_device  )  {   Hot-unplug the device  ...    return 0;  }    static const struct platform_driver generic_driver = {  .probe = generic_probe,  .remove = generic_remove,  ...  };     The similar to the previous example, the generic driver claims ownership   of the framebuffer memory from its probe function. This will fail if the   memory range, or parts of it, is already owned by another driver.     If successful, the generic driver is now subject to forced removal by   another driver. This only works for platform drivers that support hot   unplugging. When a driver calls aperture_remove_conflicting_devices()   et al for the registered framebuffer range, the aperture helpers call   platform_device_unregister() and the generic driver unloads itself. The   generic driver also has to provide a remove function to make this work.   Once hot unplugged from hardware, it may not access the device's   registers, framebuffer memory, ROM, etc afterwards. ", "int __aperture_remove_legacy_vga_devices(struct pci_dev *pdev)": "__aperture_remove_legacy_vga_devices - remove legacy VGA devices of a PCI devices   @pdev: PCI device     This function removes VGA devices provided by @pdev, such as a VGA   framebuffer or a console. This is useful if you have a VGA-compatible   PCI graphics device with framebuffers in non-BAR locations. Drivers   should acquire ownership of those memory areas and afterwards call   this helper to release remaining VGA devices.     If your hardware has its framebuffers accessible via PCI BARS, use   aperture_remove_conflicting_pci_devices() instead. The function will   release any VGA devices automatically.     WARNING: Apparently we must remove graphics drivers before calling            this helper. Otherwise the vga fbdev driver falls over if            we have vgacon configured.     Returns:   0 on success, or a negative errno code otherwise ", "static struct list_head backlight_dev_list;static struct mutex backlight_dev_list_mutex;static struct blocking_notifier_head backlight_notifier;static const char *const backlight_types[] = ": "devm_backlight_device_register(). The properties of the backlight   driver such as type and max_brightness must be specified.   When the core detect changes in for example brightness or power state   the update_status() operation is called. The backlight driver shall   implement this operation and use it to adjust backlight.     Several sysfs attributes are provided by the backlight core::     - brightness         RW, set the requested brightness level   - actual_brightness  RO, the brightness level used by the HW   - max_brightness     RO, the maximum  brightness level supported     See DocumentationABIstablesysfs-class-backlight for the full list.     The backlight can be adjusted using the sysfs interface, and   the backlight driver may also support adjusting backlight using   a hot-key or some other platform or firmware specific way.     The driver must implement the get_brightness() operation if   the HW do not support all the levels that can be specified in   brightness, thus providing user-space access to the actual level   via the actual_brightness attribute.     When the backlight changes this is reported to user-space using   an uevent connected to the actual_brightness attribute.   When brightness is set by platform specific means, for example   a hot-key to adjust backlight, the driver must notify the backlight   core that brightness has changed using backlight_force_update().     The backlight driver core receives notifications from fbdev and   if the event is FB_EVENT_BLANK and if the value of blank, from the   FBIOBLANK ioctrl, results in a change in the backlight state the   update_status() operation is called. ", "struct backlight_device *backlight_device_get_by_type(enum backlight_type type)": "backlight_device_get_by_type - find first backlight device of a type   @type: the type of backlight device     Look up the first backlight device of the specified type     RETURNS:     Pointer to backlight device if any was found. Otherwise NULL. ", "struct backlight_device *backlight_device_get_by_name(const char *name)": "backlight_device_get_by_name - Get backlight device by name   @name: Device name     This function looks up a backlight device by its name. It obtains a reference   on the backlight device and it is the caller's responsibility to drop the   reference by calling put_device().     Returns:   A pointer to the backlight device if found, otherwise NULL. ", "void backlight_device_unregister(struct backlight_device *bd)": "devm_backlight_device_unregister() ", "int backlight_register_notifier(struct notifier_block *nb)": "backlight_register_notifier - get notified of backlight (un)registration   @nb: notifier block with the notifier to call on backlight (un)registration     Register a notifier to get notified when backlight devices get registered   or unregistered.     RETURNS:     0 on success, otherwise a negative error code ", "int backlight_unregister_notifier(struct notifier_block *nb)": "backlight_unregister_notifier - unregister a backlight notifier   @nb: notifier block to unregister     Register a notifier to get notified when backlight devices get registered   or unregistered.     RETURNS:     0 on success, otherwise a negative error code ", "struct backlight_device *of_find_backlight_by_node(struct device_node *node)": "of_find_backlight_by_node() - find backlight device by device-tree node   @node: device-tree node of the backlight device     Returns a pointer to the backlight device corresponding to the given DT   node or NULL if no such backlight device exists or if the device hasn't   been probed yet.     This function obtains a reference on the backlight device and it is the   caller's responsibility to drop the reference by calling put_device() on   the backlight device's .dev field. ", "struct backlight_device *devm_of_find_backlight(struct device *dev)": "devm_of_find_backlight - find backlight for a device   @dev: the device     This function looks for a property named 'backlight' on the DT node   connected to @dev and looks up the backlight device. The lookup is   device managed so the reference to the backlight device is automatically   dropped on driver detach.     RETURNS:     A pointer to the backlight device if found.   Error pointer -EPROBE_DEFER if the DT property is set, but no backlight   device is found. NULL if there's no backlight property. ", "struct lcd_device *lcd_device_register(const char *name, struct device *parent,void *devdata, struct lcd_ops *ops)": "lcd_device_register - register a new object of lcd_device class.   @name: the name of the new object(must be the same as the name of the     respective framebuffer device).   @parent: pointer to the parent's struct device .   @devdata: an optional pointer to be stored in the device. The     methods may retrieve it by using lcd_get_data(ld).   @ops: the lcd operations structure.     Creates and registers a new lcd device. Returns either an ERR_PTR()   or a pointer to the newly allocated device. ", "void lcd_device_unregister(struct lcd_device *ld)": "lcd_device_unregister - unregisters a object of lcd_device class.   @ld: the lcd device object to be unregistered and freed.     Unregisters a previously registered via lcd_device_register object. ", "struct lcd_device *devm_lcd_device_register(struct device *dev,const char *name, struct device *parent,void *devdata, struct lcd_ops *ops)": "devm_lcd_device_register - resource managed lcd_device_register()   @dev: the device to register   @name: the name of the device   @parent: a pointer to the parent device   @devdata: an optional pointer to be stored for private driver use   @ops: the lcd operations structure     @return a struct lcd on success, or an ERR_PTR on error     Managed lcd_device_register(). The lcd_device returned from this function   are automatically freed on driver detach. See lcd_device_register()   for more information. ", "void devm_lcd_device_unregister(struct device *dev, struct lcd_device *ld)": "devm_lcd_device_unregister - resource managed lcd_device_unregister()   @dev: the device to unregister   @ld: the lcd device to unregister     Deallocated a lcd allocated with devm_lcd_device_register(). Normally   this function will not need to be called and the resource management   code will ensure that the resource is freed. ", "if (comadj == -1 && machine_is_collie())comadj = 128;if (on)locomolcd_on(comadj);elselocomolcd_off(comadj);local_irq_restore(flags);}EXPORT_SYMBOL(locomolcd_power": "locomolcd_power(int on){int comadj = sharpsl_param.comadj;unsigned long flags;local_irq_save(flags);if (!locomolcd_dev) {local_irq_restore(flags);return;}  read comadj ", "int mac_vmode_to_var(int vmode, int cmode, struct fb_var_screeninfo *var)": "mac_vmode_to_var - converts vmodecmode pair to var structure  @vmode: MacOS video mode  @cmode: MacOS color mode  @var: frame buffer video mode structure    Converts a MacOS vmodecmode pair to a frame buffer video  mode structure.    Returns negative errno on error, or zero for success.   ", "int mac_map_monitor_sense(int sense)": "mac_map_monitor_sense - Convert monitor sense to vmode  @sense: Macintosh monitor sense number    Converts a Macintosh monitor sense number to a MacOS  vmode number.    Returns MacOS vmode video mode number.   ", "int mac_find_mode(struct fb_var_screeninfo *var, struct fb_info *info,  const char *mode_option, unsigned int default_bpp)": "mac_find_mode - find a video mode  @var: frame buffer user defined part of display  @info: frame buffer info structure  @mode_option: video mode name (see mac_modedb[])  @default_bpp: default color depth in bits per pixel    Finds a suitable video mode.  Tries to set mode specified  by @mode_option.  If the name of the wanted mode begins with  'mac', the Mac video mode database will be used, otherwise it  will fall back to the standard video mode database.    Note: Function marked as __init and can only be used during  system boot.    Returns error code from fb_find_mode (see fb_find_mode  function).   ", "static inline void set_hsync_time(struct pxafb_info *fbi, unsigned int pcd)": "pxafb_get_hsync_time() is the  reciprocal    of the hsync period in seconds. ", "vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);/* Each page, see which map applies ": "sbusfb_mmap_helper(struct sbus_mmap_map  map,       unsigned long physbase,       unsigned long fbsize,       unsigned long iospace,       struct vm_area_struct  vma){unsigned int size, page, r, map_size;unsigned long map_offset = 0;unsigned long off;int i;                                        if (!(vma->vm_flags & (VM_SHARED | VM_MAYSHARE)))return -EINVAL;size = vma->vm_end - vma->vm_start;if (vma->vm_pgoff > (~0UL >> PAGE_SHIFT))return -EINVAL;off = vma->vm_pgoff << PAGE_SHIFT;  VM_IO | VM_DONTEXPAND | VM_DONTDUMP are set by remap_pfn_range() ", "case FBIOSCURPOS:case FBIOGCURPOS:case FBIOGCURMAX:return info->fbops->fb_ioctl(info, cmd, arg);case FBIOPUTCMAP32:case FBIOPUTCMAP_SPARC: ": "sbusfb_compat_ioctl(struct fb_info  info, unsigned int cmd, unsigned long arg){switch (cmd) {case FBIOGTYPE:case FBIOSATTR:case FBIOGATTR:case FBIOSVIDEO:case FBIOGVIDEO:case FBIOSCURSOR32:case FBIOGCURSOR32:  This is not implemented yet.   Later it should be converted... ", "if (cpu_is_omap15xx() || !lcd_dma.ext_ctrl)return;w = omap_readw(OMAP1610_DMA_LCD_CTRL);w |= 1 << 8;omap_writew(w, OMAP1610_DMA_LCD_CTRL);lcd_dma.active = 1;w = omap_readw(OMAP1610_DMA_LCD_CCR);w |= 1 << 7;omap_writew(w, OMAP1610_DMA_LCD_CCR);}EXPORT_SYMBOL(omap_enable_lcd_dma": "omap_enable_lcd_dma(void){u16 w;    Set the Enable bit only if an external controller is   connected. Otherwise the OMAP internal controller will   start the transfer when it gets enabled. ", "omap_writew(0x5440, OMAP1610_DMA_LCD_CCR);omap_writew(0x9102, OMAP1610_DMA_LCD_CSDP);omap_writew(0x0004, OMAP1610_DMA_LCD_LCH_CTRL);}set_b1_regs();if (!cpu_is_omap15xx()) ": "omap_setup_lcd_dma(void){BUG_ON(lcd_dma.active);if (!cpu_is_omap15xx()) {  Set some reasonable defaults ", "return;for (i = 0; i < OMAPFB_PLANE_NUM; i++)blocking_notifier_call_chain(&omapfb_client_list[i], event,    fbdev->fb_info[i]);}EXPORT_SYMBOL(omapfb_notify_clients": "omapfb_notify_clients(struct omapfb_device  fbdev, unsigned long event){int i;if (!notifier_inited)  no client registered yet ", "ivideo->sisfb_infoblock.sisfb_id         = SISFB_ID;ivideo->sisfb_infoblock.sisfb_version    = VER_MAJOR;ivideo->sisfb_infoblock.sisfb_revision   = VER_MINOR;ivideo->sisfb_infoblock.sisfb_patchlevel = VER_LEVEL;ivideo->sisfb_infoblock.chip_id = ivideo->chip_id;ivideo->sisfb_infoblock.sisfb_pci_vendor = ivideo->chip_vendor;ivideo->sisfb_infoblock.memory = ivideo->video_size / 1024;ivideo->sisfb_infoblock.heapstart = ivideo->heapstart / 1024;if(ivideo->modechanged) ": "sis_free((u32)sismemreq.offset);return -EFAULT;}break;   case FBIO_FREE:if(!capable(CAP_SYS_RAWIO))return -EPERM;if(get_user(gpu32, argp))return -EFAULT;sis_free(gpu32);break;   case FBIOGET_VBLANK:memset(&sisvbblank, 0, sizeof(struct fb_vblank));sisvbblank.count = 0;sisvbblank.flags = sisfb_setupvbblankflags(ivideo, &sisvbblank.vcount, &sisvbblank.hcount);if(copy_to_user((void __user  )arg, &sisvbblank, sizeof(sisvbblank)))return -EFAULT;break;   case SISFB_GET_INFO_SIZE:return put_user(sizeof(struct sisfb_info), argp);   case SISFB_GET_INFO_OLD:if(ivideo->warncount++ < 10)printk(KERN_INFO\"sisfb: Deprecated ioctl call received - update your application!\\n\");fallthrough;   case SISFB_GET_INFO:    For communication with X driver ", "mpitch = (mpitch >> 1) | 0x8000; /* disable linearization ": "matrox_cfbX_init(struct matrox_fb_info  minfo){u_int32_t maccess;u_int32_t mpitch;u_int32_t mopmode;int accel;DBG(__func__)mpitch = minfo->fbcon.var.xres_virtual;minfo->fbops.fb_copyarea = cfb_copyarea;minfo->fbops.fb_fillrect = cfb_fillrect;minfo->fbops.fb_imageblit = cfb_imageblit;minfo->fbops.fb_cursor = NULL;accel = (minfo->fbcon.var.accel_flags & FB_ACCELF_TEXT) == FB_ACCELF_TEXT;switch (minfo->fbcon.var.bits_per_pixel) {case 4:maccess = 0x00000000;  accelerate as 8bpp video ", "hw->DACreg[POS1064_XOUTPUTCONN] = 0x00;/* disable outputs ": "DAC1064_global_init(struct matrox_fb_info  minfo){struct matrox_hw_state  hw = &minfo->hw;hw->DACreg[POS1064_XMISCCTRL] &= M1064_XMISCCTRL_DAC_WIDTHMASK;hw->DACreg[POS1064_XMISCCTRL] |= M1064_XMISCCTRL_LUT_EN;hw->DACreg[POS1064_XPIXCLKCTRL] = M1064_XPIXCLKCTRL_PLL_UP | M1064_XPIXCLKCTRL_EN | M1064_XPIXCLKCTRL_SRC_PLL;#ifdef CONFIG_FB_MATROX_Gif (minfo->devflags.g450dac) {hw->DACreg[POS1064_XPWRCTRL] = 0x1F;  powerup everything ", "case 8:hw->DACreg[POS1064_XMULCTRL] = M1064_XMULCTRL_DEPTH_8BPP | M1064_XMULCTRL_GRAPHICS_PALETIZED;break;case 16:if (minfo->fbcon.var.green.length == 5)hw->DACreg[POS1064_XMULCTRL] = M1064_XMULCTRL_DEPTH_15BPP_1BPP | M1064_XMULCTRL_GRAPHICS_PALETIZED;elsehw->DACreg[POS1064_XMULCTRL] = M1064_XMULCTRL_DEPTH_16BPP | M1064_XMULCTRL_GRAPHICS_PALETIZED;break;case 24:hw->DACreg[POS1064_XMULCTRL] = M1064_XMULCTRL_DEPTH_24BPP | M1064_XMULCTRL_GRAPHICS_PALETIZED;break;case 32:hw->DACreg[POS1064_XMULCTRL] = M1064_XMULCTRL_DEPTH_32BPP | M1064_XMULCTRL_GRAPHICS_PALETIZED;break;default:return 1;/* unsupported depth ": "DAC1064_global_restore(struct matrox_fb_info  minfo){struct matrox_hw_state  hw = &minfo->hw;outDAC1064(minfo, M1064_XPIXCLKCTRL, hw->DACreg[POS1064_XPIXCLKCTRL]);outDAC1064(minfo, M1064_XMISCCTRL, hw->DACreg[POS1064_XMISCCTRL]);if (minfo->devflags.accelerator == FB_ACCEL_MATROX_MGAG400) {outDAC1064(minfo, 0x20, 0x04);outDAC1064(minfo, 0x1F, minfo->devflags.dfp_type);if (minfo->devflags.g450dac) {outDAC1064(minfo, M1064_XSYNCCTRL, 0xCC);outDAC1064(minfo, M1064_XPWRCTRL, hw->DACreg[POS1064_XPWRCTRL]);outDAC1064(minfo, M1064_XPANMODE, hw->DACreg[POS1064_XPANMODE]);outDAC1064(minfo, M1064_XOUTPUTCONN, hw->DACreg[POS1064_XOUTPUTCONN]);}}}static int DAC1064_init_1(struct matrox_fb_info  minfo, struct my_timming  m){struct matrox_hw_state  hw = &minfo->hw;DBG(__func__)memcpy(hw->DACreg, MGA1064_DAC, sizeof(MGA1064_DAC_regs));switch (minfo->fbcon.var.bits_per_pixel) {  case 4: not supported by MGA1064 DAC ", "if (!b->vendor)return -ENODEV;if (dev > 0) ": "matroxfb_unregister_driver(struct matroxfb_driver  drv) {struct matrox_fb_info  minfo;list_del(&drv->node);list_for_each_entry(minfo, &matroxfb_list, next_fb) {int i;for (i = 0; i < minfo->drivers_count; ) {if (minfo->drivers[i] == drv) {if (drv && drv->remove)drv->remove(minfo, minfo->drivers_data[i]);minfo->drivers[i] = minfo->drivers[--minfo->drivers_count];minfo->drivers_data[i] = minfo->drivers_data[minfo->drivers_count];} elsei++;}}}static void matroxfb_register_device(struct matrox_fb_info  minfo) {struct matroxfb_driver  drv;int i = 0;list_add(&minfo->next_fb, &matroxfb_list);for (drv = matroxfb_driver_l(matroxfb_driver_list.next);     drv != matroxfb_driver_l(&matroxfb_driver_list);     drv = matroxfb_driver_l(drv->node.next)) {if (drv->probe) {void  p = drv->probe(minfo);if (p) {minfo->drivers_data[i] = p;minfo->drivers[i++] = drv;if (i == MATROXFB_MAX_FB_DRIVERS)break;}}}minfo->drivers_count = i;}static void matroxfb_unregister_device(struct matrox_fb_info  minfo) {int i;list_del(&minfo->next_fb);for (i = 0; i < minfo->drivers_count; i++) {struct matroxfb_driver  drv = minfo->drivers[i];if (drv && drv->remove)drv->remove(minfo, minfo->drivers_data[i]);}}static int matroxfb_probe(struct pci_dev  pdev, const struct pci_device_id  dummy) {struct board  b;u_int16_t svid;u_int16_t sid;struct matrox_fb_info  minfo;int err;u_int32_t cmd;DBG(__func__)err = aperture_remove_conflicting_pci_devices(pdev, \"matroxfb\");if (err)return err;svid = pdev->subsystem_vendor;sid = pdev->subsystem_device;for (b = dev_list; b->vendor; b++) {if ((b->vendor != pdev->vendor) || (b->device != pdev->device) || (b->rev < pdev->revision)) continue;if (b->svid)if ((b->svid != svid) || (b->sid != sid)) continue;break;}  not match... ", "static void matrox_pan_var(struct matrox_fb_info *minfo,   struct fb_var_screeninfo *var)": "matroxfb_wait_for_sync(struct matrox_fb_info  minfo, u_int32_t crtc){struct matrox_vsync  vs;unsigned int cnt;int ret;switch (crtc) {case 0:vs = &minfo->crtc1.vsync;break;case 1:if (minfo->devflags.accelerator != FB_ACCEL_MATROX_MGAG400) {return -ENODEV;}vs = &minfo->crtc2.vsync;break;default:return -ENODEV;}ret = matroxfb_enable_irq(minfo, 0);if (ret) {return ret;}cnt = vs->cnt;ret = wait_event_interruptible_timeout(vs->wait, cnt != vs->cnt, HZ10);if (ret < 0) {return ret;}if (ret == 0) {matroxfb_enable_irq(minfo, 1);return -ETIMEDOUT;}return 0;}  --------------------------------------------------------------------- ", "mga_outl(M_ICLEAR, bm);mga_outl(M_IEN, mga_inl(M_IEN) | bm);} else if (reenable) ": "matroxfb_enable_irq(struct matrox_fb_info  minfo, int reenable){u_int32_t bm;if (minfo->devflags.accelerator == FB_ACCEL_MATROX_MGAG400)bm = 0x220;elsebm = 0x020;if (!test_and_set_bit(0, &minfo->irq_flags)) {if (request_irq(minfo->pcidev->irq, matrox_irq,IRQF_SHARED, \"matroxfb\", minfo)) {clear_bit(0, &minfo->irq_flags);return -EINVAL;}  Clear any pending field interrupts ", "static unsigned int g450_nextpll(const struct matrox_fb_info *minfo, const struct matrox_pll_limits *pi, unsigned int *fvco, unsigned int mnp)": "g450_mnp2f(const struct matrox_fb_info  minfo, unsigned int mnp){return g450_vco2f(mnp, g450_mnp2vco(minfo, mnp));}static inline unsigned int pll_freq_delta(unsigned int f1, unsigned int f2) {if (f2 < f1) {    f2 = f1 - f2;} else {f2 = f2 - f1;}return f2;}#define NO_MORE_MNP0x01FFFFFF#define G450_MNP_FREQBITS(0xFFFFFF43)  do not mask high byte so we'll catch NO_MORE_MNP ", "if (fout >= minfo->max_pixel_clock_panellink)tmp = 0;else tmp =M1064_XDVICLKCTRL_DVIDATAPATHSEL |M1064_XDVICLKCTRL_C1DVICLKSEL |M1064_XDVICLKCTRL_C1DVICLKEN |M1064_XDVICLKCTRL_DVILOOPCTL |M1064_XDVICLKCTRL_P1LOOPBWDTCTL;                                /* Setting this breaks PC systems so don't do it ": "matroxfb_g450_setpll_cond(struct matrox_fb_info  minfo, unsigned int mnp,       unsigned int pll){if (g450_cmppll(minfo, mnp, pll)) {g450_setpll(minfo, mnp, pll);}}static inline unsigned int g450_findworkingpll(struct matrox_fb_info  minfo,       unsigned int pll,       unsigned int  mnparray,       unsigned int mnpcount){unsigned int found = 0;unsigned int idx;unsigned int mnpfound = mnparray[0];for (idx = 0; idx < mnpcount; idx++) {unsigned int sarray[3];unsigned int  sptr;{unsigned int mnp;sptr = sarray;mnp = mnparray[idx];if (mnp & 0x38) { sptr++ = mnp - 8;}if ((mnp & 0x38) != 0x38) { sptr++ = mnp + 8;} sptr = mnp;}while (sptr >= sarray) {unsigned int mnp =  sptr--;if (g450_testpll(minfo, mnp - 0x0300, pll) &&    g450_testpll(minfo, mnp + 0x0300, pll) &&    g450_testpll(minfo, mnp - 0x0200, pll) &&    g450_testpll(minfo, mnp + 0x0200, pll) &&    g450_testpll(minfo, mnp - 0x0100, pll) &&    g450_testpll(minfo, mnp + 0x0100, pll)) {if (g450_testpll(minfo, mnp, pll)) {return mnp;}} else if (!found && g450_testpll(minfo, mnp, pll)) {mnpfound = mnp;found = 1;}}}g450_setpll(minfo, mnpfound, pll);return mnpfound;}static void g450_addcache(struct matrox_pll_cache  ci, unsigned int mnp_key, unsigned int mnp_value) {if (++ci->valid > ARRAY_SIZE(ci->data)) {ci->valid = ARRAY_SIZE(ci->data);}memmove(ci->data + 1, ci->data, (ci->valid - 1)   sizeof( ci->data));ci->data[0].mnp_key = mnp_key & G450_MNP_FREQBITS;ci->data[0].mnp_value = mnp_value;}static int g450_checkcache(struct matrox_fb_info  minfo,   struct matrox_pll_cache  ci, unsigned int mnp_key){unsigned int i;mnp_key &= G450_MNP_FREQBITS;for (i = 0; i < ci->valid; i++) {if (ci->data[i].mnp_key == mnp_key) {unsigned int mnp;mnp = ci->data[i].mnp_value;if (i) {memmove(ci->data + 1, ci->data, i   sizeof( ci->data));ci->data[0].mnp_key = mnp_key;ci->data[0].mnp_value = mnp;}return mnp;}}return NO_MORE_MNP;}static int __g450_setclk(struct matrox_fb_info  minfo, unsigned int fout, unsigned int pll, unsigned int  mnparray, unsigned int  deltaarray){unsigned int mnpcount;const struct matrox_pll_limits  pi;struct matrox_pll_cache  ci;switch (pll) {case M_PIXEL_PLL_A:case M_PIXEL_PLL_B:case M_PIXEL_PLL_C:{u_int8_t tmp, xpwrctrl;unsigned long flags;matroxfb_DAC_lock_irqsave(flags);xpwrctrl = matroxfb_DAC_in(minfo, M1064_XPWRCTRL);matroxfb_DAC_out(minfo, M1064_XPWRCTRL, xpwrctrl & ~M1064_XPWRCTRL_PANELPDN);mga_outb(M_SEQ_INDEX, M_SEQ1);mga_outb(M_SEQ_DATA, mga_inb(M_SEQ_DATA) | M_SEQ1_SCROFF);tmp = matroxfb_DAC_in(minfo, M1064_XPIXCLKCTRL);tmp |= M1064_XPIXCLKCTRL_DIS;if (!(tmp & M1064_XPIXCLKCTRL_PLL_UP)) {tmp |= M1064_XPIXCLKCTRL_PLL_UP;}matroxfb_DAC_out(minfo, M1064_XPIXCLKCTRL, tmp);  DVI PLL preferred for frequencies up to   panel link max, standard PLL otherwise ", "mt->pixclock = 1000000000 / pixclock;if (mt->pixclock < 1) mt->pixclock = 1;mt->mnp = -1;mt->dblscan = var->vmode & FB_VMODE_DOUBLE;mt->interlaced = var->vmode & FB_VMODE_INTERLACED;mt->HDisplay = var->xres;mt->HSyncStart = mt->HDisplay + var->right_margin;mt->HSyncEnd = mt->HSyncStart + var->hsync_len;mt->HTotal = mt->HSyncEnd + var->left_margin;mt->VDisplay = var->yres;mt->VSyncStart = mt->VDisplay + var->lower_margin;mt->VSyncEnd = mt->VSyncStart + var->vsync_len;mt->VTotal = mt->VSyncEnd + var->upper_margin;mt->sync = var->sync;}int matroxfb_PLL_calcclock(const struct matrox_pll_features* pll, unsigned int freq, unsigned int fmax,unsigned int* in, unsigned int* feed, unsigned int* post) ": "matroxfb_var2my(struct fb_var_screeninfo  var, struct my_timming  mt) {unsigned int pixclock = var->pixclock;DBG(__func__)if (!pixclock) pixclock = 10000;  10ns = 100MHz ", "hw->SEQ[2] = 0x0F;/* bitplanes ": "matroxfb_vgaHWinit(struct matrox_fb_info  minfo, struct my_timming  m){unsigned int hd, hs, he, hbe, ht;unsigned int vd, vs, ve, vt, lc;unsigned int wd;unsigned int divider;int i;struct matrox_hw_state   const hw = &minfo->hw;DBG(__func__)hw->SEQ[0] = 0x00;hw->SEQ[1] = 0x01;  or 0x09 ", "bd->output.tvout = 0;if (readb(vbios + 0x1D) != 'I' ||    readb(vbios + 0x1E) != 'B' ||    readb(vbios + 0x1F) != 'M' ||    readb(vbios + 0x20) != ' ') ": "matroxfb_vgaHWrestore(struct matrox_fb_info  minfo){int i;struct matrox_hw_state   const hw = &minfo->hw;CRITFLAGSDBG(__func__)dprintk(KERN_INFO \"MiscOutReg: %02X\\n\", hw->MiscOutReg);dprintk(KERN_INFO \"SEQ regs:   \");for (i = 0; i < 5; i++)dprintk(\"%02X:\", hw->SEQ[i]);dprintk(\"\\n\");dprintk(KERN_INFO \"GDC regs:   \");for (i = 0; i < 9; i++)dprintk(\"%02X:\", hw->GCTL[i]);dprintk(\"\\n\");dprintk(KERN_INFO \"CRTC regs: \");for (i = 0; i < 25; i++)dprintk(\"%02X:\", hw->CRTC[i]);dprintk(\"\\n\");dprintk(KERN_INFO \"ATTR regs: \");for (i = 0; i < 21; i++)dprintk(\"%02X:\", hw->ATTR[i]);dprintk(\"\\n\");CRITBEGINmga_inb(M_ATTR_RESET);mga_outb(M_ATTR_INDEX, 0);mga_outb(M_MISC_REG, hw->MiscOutReg);for (i = 1; i < 5; i++)mga_setr(M_SEQ_INDEX, i, hw->SEQ[i]);mga_setr(M_CRTC_INDEX, 17, hw->CRTC[17] & 0x7F);for (i = 0; i < 25; i++)mga_setr(M_CRTC_INDEX, i, hw->CRTC[i]);for (i = 0; i < 9; i++)mga_setr(M_GRAPHICS_INDEX, i, hw->GCTL[i]);for (i = 0; i < 21; i++) {mga_inb(M_ATTR_RESET);mga_outb(M_ATTR_INDEX, i);mga_outb(M_ATTR_INDEX, hw->ATTR[i]);}mga_outb(M_PALETTE_MASK, 0xFF);mga_outb(M_DAC_REG, 0x00);for (i = 0; i < 768; i++)mga_outb(M_DAC_VAL, hw->DACpal[i]);mga_inb(M_ATTR_RESET);mga_outb(M_ATTR_INDEX, 0x20);CRITEND}static void get_pins(unsigned char __iomem  pins, struct matrox_bios  bd) {unsigned int b0 = readb(pins);if (b0 == 0x2E && readb(pins+1) == 0x41) {unsigned int pins_len = readb(pins+2);unsigned int i;unsigned char cksum;unsigned char  dst = bd->pins;if (pins_len < 3 || pins_len > 128) {return;} dst++ = 0x2E; dst++ = 0x41; dst++ = pins_len;cksum = 0x2E + 0x41 + pins_len;for (i = 3; i < pins_len; i++) {cksum +=  dst++ = readb(pins+i);}if (cksum) {return;}bd->pins_len = pins_len;} else if (b0 == 0x40 && readb(pins+1) == 0x00) {unsigned int i;unsigned char  dst = bd->pins; dst++ = 0x40; dst++ = 0;for (i = 2; i < 0x40; i++) { dst++ = readb(pins+i);}bd->pins_len = 0x40;}}static void get_bios_version(unsigned char __iomem   vbios, struct matrox_bios  bd) {unsigned int pcir_offset;pcir_offset = readb(vbios + 24) | (readb(vbios + 25) << 8);if (pcir_offset >= 26 && pcir_offset < 0xFFE0 &&    readb(vbios + pcir_offset    ) == 'P' &&    readb(vbios + pcir_offset + 1) == 'C' &&    readb(vbios + pcir_offset + 2) == 'I' &&    readb(vbios + pcir_offset + 3) == 'R') {unsigned char h;h = readb(vbios + pcir_offset + 0x12);bd->version.vMaj = (h >> 4) & 0xF;bd->version.vMin = h & 0xF;bd->version.vRev = readb(vbios + pcir_offset + 0x13);} else {unsigned char h;h = readb(vbios + 5);bd->version.vMaj = (h >> 4) & 0xF;bd->version.vMin = h & 0xF;bd->version.vRev = 0;}}static void get_bios_output(unsigned char __iomem  vbios, struct matrox_bios  bd) {unsigned char b;b = readb(vbios + 0x7FF1);if (b == 0xFF) {b = 0;}bd->output.state = b;}static void get_bios_tvout(unsigned char __iomem  vbios, struct matrox_bios  bd) {unsigned int i;  Check for 'IBM . (V....TVO' string - it means TVO BIOS ", "EXPORT_SYMBOL(matroxfb_vgaHWrestore);/* DAC1064, Ti3026 ": "matroxfb_read_pins(struct matrox_fb_info  minfo){u32 opt;u32 biosbase;u32 fbbase;struct pci_dev  pdev = minfo->pcidev;memset(&minfo->bios, 0, sizeof(minfo->bios));pci_read_config_dword(pdev, PCI_OPTION_REG, &opt);pci_write_config_dword(pdev, PCI_OPTION_REG, opt | PCI_OPTION_ENABLE_ROM);pci_read_config_dword(pdev, PCI_ROM_ADDRESS, &biosbase);pci_read_config_dword(pdev, minfo->devflags.fbResource, &fbbase);pci_write_config_dword(pdev, PCI_ROM_ADDRESS, (fbbase & PCI_ROM_ADDRESS_MASK) | PCI_ROM_ADDRESS_ENABLE);parse_bios(vaddr_va(minfo->video.vbase), &minfo->bios);pci_write_config_dword(pdev, PCI_ROM_ADDRESS, biosbase);pci_write_config_dword(pdev, PCI_OPTION_REG, opt);#ifdef CONFIG_X86if (!minfo->bios.bios_valid) {unsigned char __iomem  b;b = ioremap(0x000C0000, 65536);if (!b) {printk(KERN_INFO \"matroxfb: Unable to map legacy BIOS\\n\");} else {unsigned int ven = readb(b+0x64+0) | (readb(b+0x64+1) << 8);unsigned int dev = readb(b+0x64+2) | (readb(b+0x64+3) << 8);if (ven != pdev->vendor || dev != pdev->device) {printk(KERN_INFO \"matroxfb: Legacy BIOS is for %04X:%04X, while this device is %04X:%04X\\n\",ven, dev, pdev->vendor, pdev->device);} else {parse_bios(b, &minfo->bios);}iounmap(b);}}#endifmatroxfb_set_limits(minfo, &minfo->bios);printk(KERN_INFO \"PInS memtype = %u\\n\",       (minfo->values.reg.opt & 0x1C00) >> 10);}EXPORT_SYMBOL(matroxfb_DAC_in);EXPORT_SYMBOL(matroxfb_DAC_out);EXPORT_SYMBOL(matroxfb_var2my);EXPORT_SYMBOL(matroxfb_PLL_calcclock);EXPORT_SYMBOL(matroxfb_vgaHWinit);  DAC1064, Ti3026 ", "if ((dy == sy && dx > sx) || (dy > sy)) ": "cfb_copyarea(struct fb_info  p, const struct fb_copyarea  area){u32 dx = area->dx, dy = area->dy, sx = area->sx, sy = area->sy;u32 height = area->height, width = area->width;unsigned long const bits_per_line = p->fix.line_length 8u;unsigned long __iomem  base = NULL;int bits = BITS_PER_LONG, bytes = bits >> 3;unsigned dst_idx = 0, src_idx = 0, rev_copy = 0;u32 bswapmask = fb_compute_bswapmask(p);if (p->state != FBINFO_STATE_RUNNING)return;  if the beginning of the target area might overlap with the end ofthe source area, be have to copy the area reverse. ", "left = bits % bpp;if (p->fbops->fb_sync)p->fbops->fb_sync(p);if (!left) ": "cfb_fillrect(struct fb_info  p, const struct fb_fillrect  rect){unsigned long pat, pat2, fg;unsigned long width = rect->width, height = rect->height;int bits = BITS_PER_LONG, bytes = bits >> 3;u32 bpp = p->var.bits_per_pixel;unsigned long __iomem  dst;int dst_idx, left;if (p->state != FBINFO_STATE_RUNNING)return;if (p->fix.visual == FB_VISUAL_TRUECOLOR ||    p->fix.visual == FB_VISUAL_DIRECTCOLOR )fg = ((u32  ) (p->pseudo_palette))[rect->color];elsefg = rect->color;pat = pixel_to_pat(bpp, fg);dst = (unsigned long __iomem  )((unsigned long)p->screen_base & ~(bytes-1));dst_idx = ((unsigned long)p->screen_base & (bytes - 1)) 8;dst_idx += rect->dy p->fix.line_length 8+rect->dx bpp;  FIXME For now we support 1-32 bpp only ", "list_for_each(pos, head) ": "fb_find_best_display(const struct fb_monspecs  specs,        struct list_head  head){struct list_head  pos;struct fb_modelist  modelist;const struct fb_videomode  m,  m1 = NULL,  md = NULL,  best = NULL;int first = 0;if (!head->prev || !head->next || list_empty(head))goto finished;  get the first detailed mode and the very first mode ", "void fb_videomode_to_var(struct fb_var_screeninfo *var, const struct fb_videomode *mode)": "fb_videomode_to_var - convert fb_videomode to fb_var_screeninfo   @var: pointer to struct fb_var_screeninfo   @mode: pointer to struct fb_videomode ", "void fb_var_to_videomode(struct fb_videomode *mode, const struct fb_var_screeninfo *var)": "fb_var_to_videomode - convert fb_var_screeninfo to fb_videomode   @mode: pointer to struct fb_videomode   @var: pointer to struct fb_var_screeninfo ", "int fb_mode_is_equal(const struct fb_videomode *mode1,     const struct fb_videomode *mode2)": "fb_mode_is_equal - compare 2 videomodes   @mode1: first videomode   @mode2: second videomode     RETURNS:   1 if equal, 0 if not ", "int fb_add_videomode(const struct fb_videomode *mode, struct list_head *head)": "fb_add_videomode - adds videomode entry to modelist   @mode: videomode to add   @head: struct list_head of modelist     NOTES:   Will only add unmatched mode entries ", "const struct fb_videomode *fb_match_mode(const struct fb_var_screeninfo *var, struct list_head *head)": "fb_match_mode - find a videomode which exactly matches the timings in var   @var: pointer to struct fb_var_screeninfo   @head: pointer to struct list_head of modelist     RETURNS:   struct fb_videomode, NULL if none found ", "const struct fb_videomode *fb_find_best_mode(const struct fb_var_screeninfo *var,     struct list_head *head)": "fb_find_best_mode - find best matching videomode   @var: pointer to struct fb_var_screeninfo   @head: pointer to struct list_head of modelist     RETURNS:   struct fb_videomode, NULL if none found     IMPORTANT:   This function assumes that all modelist entries in   info->modelist are valid.     NOTES:   Finds best matching videomode which has an equal or greater dimension than   var->xres and var->yres.  If more than 1 videomode is found, will return   the videomode with the highest refresh rate ", "const struct fb_videomode *fb_find_nearest_mode(const struct fb_videomode *mode,        struct list_head *head)": "fb_find_nearest_mode - find closest videomode     @mode: pointer to struct fb_videomode   @head: pointer to modelist     Finds best matching videomode, smaller or greater in dimension.   If more than 1 videomode is found, will return the videomode with   the closest refresh rate. ", "void fb_videomode_to_modelist(const struct fb_videomode *modedb, int num,      struct list_head *head)": "fb_videomode_to_modelist - convert mode array to mode list   @modedb: array of struct fb_videomode   @num: number of entries in array   @head: struct list_head of modelist ", "int fb_find_mode(struct fb_var_screeninfo *var, struct fb_info *info, const char *mode_option, const struct fb_videomode *db, unsigned int dbsize, const struct fb_videomode *default_mode, unsigned int default_bpp)": "fb_find_mode - finds a valid video mode   @var: frame buffer user defined part of display   @info: frame buffer info structure   @mode_option: string video mode to find   @db: video mode database   @dbsize: size of @db   @default_mode: default video mode to fall back to   @default_bpp: default color depth in bits per pixel     Finds a suitable video mode, starting with the specified mode   in @mode_option with fallback to @default_mode.  If   @default_mode fails, all modes in the video mode database will   be tried.     Valid mode specifiers for @mode_option::         <xres>x<yres>[M][R][-<bpp>][@<refresh>][i][p][m]     or ::         <name>[-<bpp>][@<refresh>]     with <xres>, <yres>, <bpp> and <refresh> decimal numbers and   <name> a string.     If 'M' is present after yres (and before refreshbpp if present),   the function will compute the timings using VESA(tm) Coordinated   Video Timings (CVT).  If 'R' is present after 'M', will compute with   reduced blanking (for flatpanels).  If 'i' or 'p' are present, compute   interlaced or progressive mode.  If 'm' is present, add margins equal   to 1.8% of xres rounded down to 8 pixels, and 1.8% of yres. The char   'i', 'p' and 'm' must be after 'M' and 'R'. Example::         1024x768MR-8@60m - Reduced blank with margins at 60Hz.     NOTE: The passed struct @var is _not_ cleared!  This allows you   to supply values for e.g. the grayscale and accel_flags fields.     Returns zero for failure, 1 if using specified @mode_option,   2 if using specified @mode_option with an ignored refresh rate,   3 if default mode is used, 4 if fall back to any valid mode. ", "if (db != modedb &&    info->monspecs.vfmin && info->monspecs.vfmax &&    info->monspecs.hfmin && info->monspecs.hfmax &&    info->monspecs.dclkmax) ": "fb_find_mode_cvt(&cvt_mode, margins, rb);if (!ret && !fb_try_mode(var, info, &cvt_mode, bpp)) {DPRINTK(\"modedb CVT: CVT mode ok\\n\");return 1;}DPRINTK(\"CVT mode invalid, getting mode from database\\n\");}DPRINTK(\"Trying specified video mode%s %ix%i\\n\",refresh_specified ? \"\" : \" (ignoring refresh rate)\",xres, yres);if (!refresh_specified) {    If the caller has provided a custom mode database and   a valid monspecs structure, we look for the mode with   the highest refresh rate.  Otherwise we play it safe   it and try to find a mode with a refresh rate closest   to the standard 60 Hz. ", "int fb_alloc_cmap_gfp(struct fb_cmap *cmap, int len, int transp, gfp_t flags)": "fb_alloc_cmap_gfp - allocate a colormap  @cmap: frame buffer colormap structure  @len: length of @cmap  @transp: boolean, 1 if there is transparency, 0 otherwise  @flags: flags for kmalloc memory allocation    Allocates memory for a colormap @cmap.  @len is the  number of entries in the palette.    Returns negative errno on error, or zero on success.   ", "void fb_dealloc_cmap(struct fb_cmap *cmap)": "fb_default_cmap(len), cmap);if (ret)goto fail;return 0;fail:fb_dealloc_cmap(cmap);return ret;}int fb_alloc_cmap(struct fb_cmap  cmap, int len, int transp){return fb_alloc_cmap_gfp(cmap, len, transp, GFP_ATOMIC);}          fb_dealloc_cmap - deallocate a colormap        @cmap: frame buffer colormap structure          Deallocates a colormap that was previously allocated with        fb_alloc_cmap().   ", "int fb_set_cmap(struct fb_cmap *cmap, struct fb_info *info)": "fb_set_cmap - set the colormap  @cmap: frame buffer colormap structure  @info: frame buffer info structure    Sets the colormap @cmap for a screen of device @info.    Returns negative errno on error, or zero on success.   ", "void fb_invert_cmaps(void)": "fb_invert_cmaps - invert all defaults colormaps    Invert all default colormaps.   ", "int fb_register_client(struct notifier_block *nb)": "fb_register_client - register a client notifier  @nb: notifier block to callback on events    Return: 0 on success, negative error code on failure. ", "int fb_unregister_client(struct notifier_block *nb)": "fb_unregister_client - unregister a client notifier  @nb: notifier block to callback on events    Return: 0 on success, negative error code on failure. ", "int fb_get_options(const char *name, char **option)": "fb_get_options - get kernel boot parameters   @name:   framebuffer name as it would appear in            the boot parameter line            (video=<name>:<options>)   @option: the option will be stored here     The caller owns the string returned in @option and is   responsible for releasing the memory.     NOTE: Needed to maintain backwards compatibility ", "if (buf->flags & FB_PIXMAP_IO) ": "fb_get_buffer_offset(struct fb_info  info, struct fb_pixmap  buf, u32 size){u32 align = buf->buf_align - 1, offset;char  addr = buf->addr;  If IO mapped, we need to sync before access, no sharing of   the pixmap is done ", "depth = 4;}/* Return if no suitable logo was found ": "fb_prepare_logo(struct fb_info  info, int rotate){int depth = fb_get_color_depth(&info->var, &info->fix);unsigned int yres;int height;memset(&fb_logo, 0, sizeof(struct logo_data));if (info->flags & FBINFO_MISC_TILEBLITTING ||    info->fbops->owner || !fb_logo_count)return 0;if (info->fix.visual == FB_VISUAL_DIRECTCOLOR) {depth = info->var.blue.length;if (info->var.red.length < depth)depth = info->var.red.length;if (info->var.green.length < depth)depth = info->var.green.length;}if (info->fix.visual == FB_VISUAL_STATIC_PSEUDOCOLOR && depth > 4) {  assume console colormap ", "if (logo == NULL || info->state != FBINFO_STATE_RUNNING ||    info->fbops->owner)return 0;image.depth = 8;image.data = logo->data;if (fb_logo.needs_cmapreset)fb_set_logocmap(info, logo);if (fb_logo.needs_truepalette ||    fb_logo.needs_directpalette) ": "fb_show_logo_line(struct fb_info  info, int rotate,     const struct linux_logo  logo, int y,     unsigned int n){u32  palette = NULL,  saved_pseudo_palette = NULL;unsigned char  logo_new = NULL,  logo_rotate = NULL;struct fb_image image;  Return if the frame buffer is not mapped or suspended ", "ret = fb_mode_is_equal(&mode1, &mode2);if (!ret) ": "fb_set_var(struct fb_info  info, struct fb_var_screeninfo  var){int ret = 0;u32 activate;struct fb_var_screeninfo old_var;struct fb_videomode mode;struct fb_event event;u32 unused;if (var->activate & FB_ACTIVATE_INV_MODE) {struct fb_videomode mode1, mode2;fb_var_to_videomode(&mode1, var);fb_var_to_videomode(&mode2, &info->var);  make sure we don't delete the videomode of current var ", "printk(KERN_WARNING \"Unable to create device for framebuffer %d; errno = %ld\\n\", i, PTR_ERR(fb_info->dev));fb_info->dev = NULL;} elsefb_init_device(fb_info);if (fb_info->pixmap.addr == NULL) ": "register_framebuffer(struct fb_info  fb_info){int i;struct fb_videomode mode;if (fb_check_foreignness(fb_info))return -ENOSYS;if (num_registered_fb == FB_MAX)return -ENXIO;num_registered_fb++;for (i = 0 ; i < FB_MAX; i++)if (!registered_fb[i])break;fb_info->node = i;refcount_set(&fb_info->count, 1);mutex_init(&fb_info->lock);mutex_init(&fb_info->mm_lock);fb_info->dev = device_create(fb_class, fb_info->device,     MKDEV(FB_MAJOR, i), NULL, \"fb%d\", i);if (IS_ERR(fb_info->dev)) {  Not fatal ", "put_fb_info(fb_info);}/** *register_framebuffer - registers a frame buffer device *@fb_info: frame buffer info structure * *Registers a frame buffer device @fb_info. * *Returns negative errno on error, or zero for success. * ": "unregister_framebuffer(struct fb_info  fb_info){unlink_framebuffer(fb_info);if (fb_info->pixmap.addr &&    (fb_info->pixmap.flags & FB_PIXMAP_DEFAULT)) {kfree(fb_info->pixmap.addr);fb_info->pixmap.addr = NULL;}fb_destroy_modelist(&fb_info->modelist);registered_fb[fb_info->node] = NULL;num_registered_fb--;fb_cleanup_device(fb_info);#ifdef CONFIG_GUMSTIX_AM200EPD{struct fb_event event;event.info = fb_info;fb_notifier_call_chain(FB_EVENT_FB_UNREGISTERED, &event);}#endiffbcon_fb_unregistered(fb_info);  this may free fb info ", "void fb_set_suspend(struct fb_info *info, int state)": "fb_set_suspend - low level driver signals suspend  @info: framebuffer affected  @state: 0 = resuming, !=0 = suspending    This is meant to be used by low level drivers to   signal suspendresume to the core & clients.  It must be called with the console semaphore held ", "#include <linux/kernel.h>#include <linux/slab.h>#include <linux/fb.h>#include <linux/fbcon.h>#include <linux/console.h>#include <linux/module.h>#define FB_SYSFS_FLAG_ATTR 1/** * framebuffer_alloc - creates a new frame buffer info structure * * @size: size of driver private data, can be zero * @dev: pointer to the device for this fb, this can be NULL * * Creates a new frame buffer info structure. Also reserves @size bytes * for driver private data (info->par). info->par (if any) will be * aligned to sizeof(long). * * Returns the new structure, or NULL if an error occurred. * ": "framebuffer_release here.  The reson for that is that until all drivers   are converted to use it a sysfsification will open OOPSable races. ", "caps->x = 1 << (8 - 1);caps->y = 1 << (16 - 1);caps->len = 256;} else ": "svga_get_caps(struct fb_info  info, struct fb_blit_caps  caps,   struct fb_var_screeninfo  var){if (var->bits_per_pixel == 0) {  can only support 256 8x16 bitmap ", "void svga_wseq_multi(void __iomem *regbase, const struct vga_regset *regset, u32 value)": "svga_wcrt_multi(void __iomem  regbase, const struct vga_regset  regset, u32 value){u8 regval, bitval, bitnum;while (regset->regnum != VGA_REGSET_END_VAL) {regval = vga_rcrt(regbase, regset->regnum);bitnum = regset->lowbit;while (bitnum <= regset->highbit) {bitval = 1 << bitnum;regval = regval & ~bitval;if (value & 1) regval = regval | bitval;bitnum ++;value = value >> 1;}vga_wcrt(regbase, regset->regnum, regval);regset ++;}}  Write a sequencer register value spread across multiple registers ", "/* Set graphics controller registers to sane values ": "svga_wseq_multi(void __iomem  regbase, const struct vga_regset  regset, u32 value){u8 regval, bitval, bitnum;while (regset->regnum != VGA_REGSET_END_VAL) {regval = vga_rseq(regbase, regset->regnum);bitnum = regset->lowbit;while (bitnum <= regset->highbit) {bitval = 1 << bitnum;regval = regval & ~bitval;if (value & 1) regval = regval | bitval;bitnum ++;value = value >> 1;}vga_wseq(regbase, regset->regnum, regval);regset ++;}}static unsigned int svga_regset_size(const struct vga_regset  regset){u8 count = 0;while (regset->regnum != VGA_REGSET_END_VAL) {count += regset->highbit - regset->lowbit + 1;regset ++;}return 1 << count;}  ------------------------------------------------------------------------- ", "vga_wgfx(regbase, VGA_GFX_SR_VALUE, 0x00);vga_wgfx(regbase, VGA_GFX_SR_ENABLE, 0x00);vga_wgfx(regbase, VGA_GFX_COMPARE_VALUE, 0x00);vga_wgfx(regbase, VGA_GFX_DATA_ROTATE, 0x00);vga_wgfx(regbase, VGA_GFX_PLANE_READ, 0x00);vga_wgfx(regbase, VGA_GFX_MODE, 0x00);/*vga_wgfx(regbase, VGA_GFX_MODE, 0x20); ": "svga_set_default_gfx_regs(void __iomem  regbase){  All standard GFX registers (GR00 - GR08) ", "for (count = 0; count <= 0xF; count ++)svga_wattr(regbase, count, count);svga_wattr(regbase, VGA_ATC_MODE, 0x01);/*svga_wattr(regbase, VGA_ATC_MODE, 0x41); ": "svga_set_default_atc_regs(void __iomem  regbase){u8 count;vga_r(regbase, 0x3DA);vga_w(regbase, VGA_ATT_W, 0x00);  All standard ATC registers (AR00 - AR14) ", "vga_wseq(regbase, VGA_SEQ_CLOCK_MODE, VGA_SR01_CHAR_CLK_8DOTS);vga_wseq(regbase, VGA_SEQ_PLANE_WRITE, VGA_SR02_ALL_PLANES);vga_wseq(regbase, VGA_SEQ_CHARACTER_MAP, 0x00);/*vga_wseq(regbase, VGA_SEQ_MEMORY_MODE, VGA_SR04_EXT_MEM | VGA_SR04_SEQ_MODE | VGA_SR04_CHN_4M); ": "svga_set_default_seq_regs(void __iomem  regbase){  Standard sequencer registers (SR01 - SR04), SR00 is not set ", "svga_wcrt_mask(regbase, 0x03, 0x80, 0x80);/* Enable vertical retrace EVRA ": "svga_set_default_crt_regs(void __iomem  regbase){  Standard CRT registers CR03 CR08 CR09 CR14 CR17 ", "   /* Switch 8/9 pixel per char ": "svga_set_textmode_vga_regs(void __iomem  regbase){  svga_wseq_mask(regbase, 0x1, 0x00, 0x01); ", "void svga_tilecopy(struct fb_info *info, struct fb_tilearea *area)": "svga_settile(struct fb_info  info, struct fb_tilemap  map){const u8  font = map->data;u8 __iomem  fb = (u8 __iomem  )info->screen_base;int i, c;if ((map->width != 8) || (map->height != 16) ||    (map->depth != 1) || (map->length != 256)) {fb_err(info, \"unsupported font parameters: width %d, height %d, depth %d, length %d\\n\",       map->width, map->height, map->depth, map->length);return;}fb += 2;for (c = 0; c < map->length; c++) {for (i = 0; i < map->height; i++) {fb_writeb(font[i], fb + i   4);fb[i   4] = font[i];}fb += 128;font += map->height;}}  Copy area in text (tileblit) mode ", "int colstride = 1 << (info->fix.type_aux & FB_AUX_TEXT_SVGA_MASK);int rowstride = colstride * (info->var.xres_virtual / 8);u16 __iomem *fb = (u16 __iomem *) info->screen_base;u16 __iomem *src, *dst;if ((area->sy > area->dy) ||    ((area->sy == area->dy) && (area->sx > area->dx))) ": "svga_tilecopy(struct fb_info  info, struct fb_tilearea  area){int dx, dy;   colstride is halved in this function because u16 are used ", "void svga_tileblit(struct fb_info *info, struct fb_tileblit *blit)": "svga_tilefill(struct fb_info  info, struct fb_tilerect  rect){int dx, dy;int colstride = 2 << (info->fix.type_aux & FB_AUX_TEXT_SVGA_MASK);int rowstride = colstride   (info->var.xres_virtual  8);int attr = (0x0F & rect->bg) << 4 | (0x0F & rect->fg);u8 __iomem  fb = (u8 __iomem  )info->screen_base;fb += rect->sx   colstride + rect->sy   rowstride;for (dy = 0; dy < rect->height; dy++) {u8 __iomem  fb2 = fb;for (dx = 0; dx < rect->width; dx++) {fb_writeb(rect->index, fb2);fb_writeb(attr, fb2 + 1);fb2 += colstride;}fb += rowstride;}}  Write text in text (tileblit) mode ", "void svga_tilecursor(void __iomem *regbase, struct fb_info *info, struct fb_tilecursor *cursor)": "svga_tileblit(struct fb_info  info, struct fb_tileblit  blit){int dx, dy, i;int colstride = 2 << (info->fix.type_aux & FB_AUX_TEXT_SVGA_MASK);int rowstride = colstride   (info->var.xres_virtual  8);int attr = (0x0F & blit->bg) << 4 | (0x0F & blit->fg);u8 __iomem  fb = (u8 __iomem  )info->screen_base;fb += blit->sx   colstride + blit->sy   rowstride;i=0;for (dy=0; dy < blit->height; dy ++) {u8 __iomem  fb2 = fb;for (dx = 0; dx < blit->width; dx ++) {fb_writeb(blit->indices[i], fb2);fb_writeb(attr, fb2 + 1);fb2 += colstride;i ++;if (i == blit->length) return;}fb += rowstride;}}  Set cursor in text (tileblit) mode ", "if (cursor -> shape == FB_TILE_CURSOR_NONE)return;switch (cursor -> shape) ": "svga_tilecursor(void __iomem  regbase, struct fb_info  info, struct fb_tilecursor  cursor){u8 cs = 0x0d;u8 ce = 0x0e;u16 pos =  cursor->sx + (info->var.xoffset   8)+ (cursor->sy + (info->var.yoffset  16))     (info->var.xres_virtual  8);if (! cursor -> mode)return;svga_wcrt_mask(regbase, 0x0A, 0x20, 0x20);   disable cursor ", "void svga_get_caps(struct fb_info *info, struct fb_blit_caps *caps,   struct fb_var_screeninfo *var)": "svga_get_tilemax(struct fb_info  info){return 256;}  Get capabilities of accelerator based on the mode ", "if ((f_vco >> ar) != f_wanted)return -EINVAL;/* It is usually better to have greater VCO clock   because of better frequency stability.   So first try r_max, then r smaller. ": "svga_compute_pll(const struct svga_pll  pll, u32 f_wanted, u16  m, u16  n, u16  r, int node){u16 am, an, ar;u32 f_vco, f_current, delta_current, delta_best;pr_debug(\"fb%d: ideal frequency: %d kHz\\n\", node, (unsigned int) f_wanted);ar = pll->r_max;f_vco = f_wanted << ar;  overflow check ", "value = var->xres + var->left_margin + var->right_margin + var->hsync_len;if (((value / 8) - 5) >= svga_regset_size (tm->h_total_regs))return -EINVAL;/* Check horizontal display and blank start ": "svga_check_timings(const struct svga_timing_regs  tm, struct fb_var_screeninfo  var, int node){u32 value;var->xres         = (var->xres+7)&~7;var->left_margin  = (var->left_margin+7)&~7;var->right_margin = (var->right_margin+7)&~7;var->hsync_len    = (var->hsync_len+7)&~7;  Check horizontal total ", "regval = vga_r(regbase, VGA_MIS_R);if (var->sync & FB_SYNC_HOR_HIGH_ACT) ": "svga_set_timings(void __iomem  regbase, const struct svga_timing_regs  tm,      struct fb_var_screeninfo  var,      u32 hmul, u32 hdiv, u32 vmul, u32 vdiv, u32 hborder, int node){u8 regval;u32 value;value = var->xres + var->left_margin + var->right_margin + var->hsync_len;value = (value   hmul)  hdiv;pr_debug(\"fb%d: horizontal total      : %d\\n\", node, value);svga_wcrt_multi(regbase, tm->h_total_regs, (value  8) - 5);value = var->xres;value = (value   hmul)  hdiv;pr_debug(\"fb%d: horizontal display    : %d\\n\", node, value);svga_wcrt_multi(regbase, tm->h_display_regs, (value  8) - 1);value = var->xres;value = (value   hmul)  hdiv;pr_debug(\"fb%d: horizontal blank start: %d\\n\", node, value);svga_wcrt_multi(regbase, tm->h_blank_start_regs, (value  8) - 1 + hborder);value = var->xres + var->left_margin + var->right_margin + var->hsync_len;value = (value   hmul)  hdiv;pr_debug(\"fb%d: horizontal blank end  : %d\\n\", node, value);svga_wcrt_multi(regbase, tm->h_blank_end_regs, (value  8) - 1 - hborder);value = var->xres + var->right_margin;value = (value   hmul)  hdiv;pr_debug(\"fb%d: horizontal sync start : %d\\n\", node, value);svga_wcrt_multi(regbase, tm->h_sync_start_regs, (value  8));value = var->xres + var->right_margin + var->hsync_len;value = (value   hmul)  hdiv;pr_debug(\"fb%d: horizontal sync end   : %d\\n\", node, value);svga_wcrt_multi(regbase, tm->h_sync_end_regs, (value  8));value = var->yres + var->upper_margin + var->lower_margin + var->vsync_len;value = (value   vmul)  vdiv;pr_debug(\"fb%d: vertical total        : %d\\n\", node, value);svga_wcrt_multi(regbase, tm->v_total_regs, value - 2);value = var->yres;value = (value   vmul)  vdiv;pr_debug(\"fb%d: vertical display      : %d\\n\", node, value);svga_wcrt_multi(regbase, tm->v_display_regs, value - 1);value = var->yres;value = (value   vmul)  vdiv;pr_debug(\"fb%d: vertical blank start  : %d\\n\", node, value);svga_wcrt_multi(regbase, tm->v_blank_start_regs, value);value = var->yres + var->upper_margin + var->lower_margin + var->vsync_len;value = (value   vmul)  vdiv;pr_debug(\"fb%d: vertical blank end    : %d\\n\", node, value);svga_wcrt_multi(regbase, tm->v_blank_end_regs, value - 2);value = var->yres + var->lower_margin;value = (value   vmul)  vdiv;pr_debug(\"fb%d: vertical sync start   : %d\\n\", node, value);svga_wcrt_multi(regbase, tm->v_sync_start_regs, value);value = var->yres + var->lower_margin + var->vsync_len;value = (value   vmul)  vdiv;pr_debug(\"fb%d: vertical sync end     : %d\\n\", node, value);svga_wcrt_multi(regbase, tm->v_sync_end_regs, value);  Set horizontal and vertical sync pulse polarity in misc register ", "for (i = 0; i < specs->modedb_len; i++) ": "fb_edid_to_monspecs(unsigned char  edid, struct fb_monspecs  specs){unsigned char  block;int i, found = 0;if (edid == NULL)return;if (!(edid_checksum(edid)))return;if (!(edid_check_header(edid)))return;memset(specs, 0, sizeof(struct fb_monspecs));specs->version = edid[EDID_STRUCT_VERSION];specs->revision = edid[EDID_STRUCT_REVISION];DPRINTK(\"========================================\\n\");DPRINTK(\"Display Information (EDID)\\n\");DPRINTK(\"========================================\\n\");DPRINTK(\"   EDID Version %d.%d\\n\", (int) specs->version,       (int) specs->revision);parse_vendor_block(edid + ID_MANUFACTURER_NAME, specs);block = edid + DETAILED_TIMING_DESCRIPTIONS_START;for (i = 0; i < 4; i++, block += DETAILED_TIMING_DESCRIPTION_SIZE) {if (edid_is_serial_block(block)) {copy_string(block, specs->serial_no);DPRINTK(\"   Serial Number: %s\\n\", specs->serial_no);} else if (edid_is_ascii_block(block)) {copy_string(block, specs->ascii);DPRINTK(\"   ASCII Block: %s\\n\", specs->ascii);} else if (edid_is_monitor_block(block)) {copy_string(block, specs->monitor);DPRINTK(\"   Monitor Name: %s\\n\", specs->monitor);}}DPRINTK(\"   Display Characteristics:\\n\");get_monspecs(edid, specs);specs->modedb = fb_create_modedb(edid, &specs->modedb_len, specs);if (!specs->modedb)return;    Workaround for buggy EDIDs that sets that the first   detailed timing is preferred but has not detailed   timing specified ", "*mode = *dmt_modes[i].mode;mode->flag |= FB_MODE_IS_STANDARD;DPRINTK(\"        DMT id=%d\\n\", dmt_modes[i].dmt_id);} else ": "fb_get_mode(FB_VSYNCTIMINGS | FB_IGNOREMON,    refresh, var, NULL);mode->xres = xres;mode->yres = yres;mode->pixclock = var->pixclock;mode->refresh = refresh;mode->left_margin = var->left_margin;mode->right_margin = var->right_margin;mode->upper_margin = var->upper_margin;mode->lower_margin = var->lower_margin;mode->hsync_len = var->hsync_len;mode->vsync_len = var->vsync_len;mode->vmode = 0;mode->sync = 0;kfree(var);}}static int get_est_timing(unsigned char  block, struct fb_videomode  mode){int num = 0;unsigned char c;c = block[0];if (c&0x80) {calc_mode_timings(720, 400, 70, &mode[num]);mode[num++].flag = FB_MODE_IS_CALCULATED;DPRINTK(\"      720x400@70Hz\\n\");}if (c&0x40) {calc_mode_timings(720, 400, 88, &mode[num]);mode[num++].flag = FB_MODE_IS_CALCULATED;DPRINTK(\"      720x400@88Hz\\n\");}if (c&0x20) {mode[num++] = vesa_modes[3];DPRINTK(\"      640x480@60Hz\\n\");}if (c&0x10) {calc_mode_timings(640, 480, 67, &mode[num]);mode[num++].flag = FB_MODE_IS_CALCULATED;DPRINTK(\"      640x480@67Hz\\n\");}if (c&0x08) {mode[num++] = vesa_modes[4];DPRINTK(\"      640x480@72Hz\\n\");}if (c&0x04) {mode[num++] = vesa_modes[5];DPRINTK(\"      640x480@75Hz\\n\");}if (c&0x02) {mode[num++] = vesa_modes[7];DPRINTK(\"      800x600@56Hz\\n\");}if (c&0x01) {mode[num++] = vesa_modes[8];DPRINTK(\"      800x600@60Hz\\n\");}c = block[1];if (c&0x80) {mode[num++] = vesa_modes[9];DPRINTK(\"      800x600@72Hz\\n\");}if (c&0x40) {mode[num++] = vesa_modes[10];DPRINTK(\"      800x600@75Hz\\n\");}if (c&0x20) {calc_mode_timings(832, 624, 75, &mode[num]);mode[num++].flag = FB_MODE_IS_CALCULATED;DPRINTK(\"      832x624@75Hz\\n\");}if (c&0x10) {mode[num++] = vesa_modes[12];DPRINTK(\"      1024x768@87Hz Interlaced\\n\");}if (c&0x08) {mode[num++] = vesa_modes[13];DPRINTK(\"      1024x768@60Hz\\n\");}if (c&0x04) {mode[num++] = vesa_modes[14];DPRINTK(\"      1024x768@70Hz\\n\");}if (c&0x02) {mode[num++] = vesa_modes[15];DPRINTK(\"      1024x768@75Hz\\n\");}if (c&0x01) {mode[num++] = vesa_modes[21];DPRINTK(\"      1280x1024@75Hz\\n\");}c = block[2];if (c&0x80) {mode[num++] = vesa_modes[17];DPRINTK(\"      1152x870@75Hz\\n\");}DPRINTK(\"      Manufacturer's mask: %x\\n\",c&0x7F);return num;}static int get_std_timing(unsigned char  block, struct fb_videomode  mode,  int ver, int rev, const struct fb_monspecs  specs){int i;for (i = 0; i < DMT_SIZE; i++) {u32 std_2byte_code = block[0] << 8 | block[1];if (std_2byte_code == dmt_modes[i].std_2byte_code)break;}if (i < DMT_SIZE && dmt_modes[i].mode) {  DMT mode found ", "int fb_validate_mode(const struct fb_var_screeninfo *var, struct fb_info *info)": "fb_validate_mode - validates var against monitor capabilities   @var: pointer to fb_var_screeninfo   @info: pointer to fb_info     DESCRIPTION:   Validates video mode against monitor capabilities specified in   info->monspecs.     REQUIRES:   A valid info->monspecs. ", "void fb_destroy_modedb(struct fb_videomode *modedb)": "fb_destroy_modedb - destroys mode database   @modedb: mode database to destroy     DESCRIPTION:   Destroy mode database created by fb_create_modedb ", "return min_t(unsigned long, height, 2048);}EXPORT_SYMBOL(omap_vrfb_max_height": "omap_vrfb_max_height(u32 phys_size, u16 width, u8 bytespp){unsigned long image_width_roundup = get_image_width_roundup(width,bytespp);unsigned long height;unsigned long extra;if (image_width_roundup > OMAP_VRFB_LINE_LEN)return 0;extra = get_extra_physical_size(image_width_roundup, bytespp);if (phys_size < extra)return 0;height = (phys_size - extra)  (width   bytespp);  Virtual views provided by VRFB are limited to 2048x2048. ", "if (yuv_mode) ": "omap_vrfb_setup(struct vrfb  vrfb, unsigned long paddr,u16 width, u16 height,unsigned bytespp, bool yuv_mode){unsigned pixel_size_exp;u16 vrfb_width;u16 vrfb_height;u8 ctx = vrfb->context;u32 size;u32 control;DBG(\"omapfb_set_vrfb(%d, %lx, %dx%d, %d, %d)\\n\", ctx, paddr,width, height, bytespp, yuv_mode);  For YUV2 and UYVY modes VRFB needs to handle pixels a bit   differently. See TRM. ", "if (dssdev->dev->of_node) ": "omapdss_register_display(struct omap_dss_device  dssdev){struct omap_dss_driver  drv = dssdev->driver;int id;    Note: this presumes all the displays are either using DT or non-DT,   which normally should be the case. This also presumes that all   displays either have an DT alias, or none has. ", "for (i = 0; i < dss_feat_get_num_ovls(); ++i)dispc_ovl_set_burst_size(i, burst_size);if (dispc.feat->has_writeback)dispc_ovl_set_burst_size(OMAP_DSS_WB, burst_size);}static u32 dispc_ovl_get_burst_size(enum omap_plane plane)": "dispc_ovl_enable_zorder_planes(void){int i;if (!dss_has_feature(FEAT_ALPHA_FREE_ZORDER))return;for (i = 0; i < dss_feat_get_num_ovls(); i++)REG_FLD_MOD(DISPC_OVL_ATTRIBUTES(i), 1, 25, 25);}static void dispc_ovl_set_pre_mult_alpha(enum omap_plane plane,enum omap_overlay_caps caps, bool enable){if ((caps & OMAP_DSS_OVL_CAP_PRE_MULT_ALPHA) == 0)return;REG_FLD_MOD(DISPC_OVL_ATTRIBUTES(plane), enable ? 1 : 0, 28, 28);}static void dispc_ovl_setup_global_alpha(enum omap_plane plane,enum omap_overlay_caps caps, u8 global_alpha){static const unsigned shifts[] = { 0, 8, 16, 24, };int shift;if ((caps & OMAP_DSS_OVL_CAP_GLOBAL_ALPHA) == 0)return;shift = shifts[plane];REG_FLD_MOD(DISPC_GLOBAL_ALPHA, global_alpha, shift + 7, shift);}static void dispc_ovl_set_pix_inc(enum omap_plane plane, s32 inc){dispc_write_reg(DISPC_OVL_PIXEL_INC(plane), inc);}static void dispc_ovl_set_row_inc(enum omap_plane plane, s32 inc){dispc_write_reg(DISPC_OVL_ROW_INC(plane), inc);}static void dispc_ovl_set_color_mode(enum omap_plane plane,enum omap_color_mode color_mode){u32 m = 0;if (plane != OMAP_DSS_GFX) {switch (color_mode) {case OMAP_DSS_COLOR_NV12:m = 0x0; break;case OMAP_DSS_COLOR_RGBX16:m = 0x1; break;case OMAP_DSS_COLOR_RGBA16:m = 0x2; break;case OMAP_DSS_COLOR_RGB12U:m = 0x4; break;case OMAP_DSS_COLOR_ARGB16:m = 0x5; break;case OMAP_DSS_COLOR_RGB16:m = 0x6; break;case OMAP_DSS_COLOR_ARGB16_1555:m = 0x7; break;case OMAP_DSS_COLOR_RGB24U:m = 0x8; break;case OMAP_DSS_COLOR_RGB24P:m = 0x9; break;case OMAP_DSS_COLOR_YUV2:m = 0xa; break;case OMAP_DSS_COLOR_UYVY:m = 0xb; break;case OMAP_DSS_COLOR_ARGB32:m = 0xc; break;case OMAP_DSS_COLOR_RGBA32:m = 0xd; break;case OMAP_DSS_COLOR_RGBX32:m = 0xe; break;case OMAP_DSS_COLOR_XRGB16_1555:m = 0xf; break;default:BUG(); return;}} else {switch (color_mode) {case OMAP_DSS_COLOR_CLUT1:m = 0x0; break;case OMAP_DSS_COLOR_CLUT2:m = 0x1; break;case OMAP_DSS_COLOR_CLUT4:m = 0x2; break;case OMAP_DSS_COLOR_CLUT8:m = 0x3; break;case OMAP_DSS_COLOR_RGB12U:m = 0x4; break;case OMAP_DSS_COLOR_ARGB16:m = 0x5; break;case OMAP_DSS_COLOR_RGB16:m = 0x6; break;case OMAP_DSS_COLOR_ARGB16_1555:m = 0x7; break;case OMAP_DSS_COLOR_RGB24U:m = 0x8; break;case OMAP_DSS_COLOR_RGB24P:m = 0x9; break;case OMAP_DSS_COLOR_RGBX16:m = 0xa; break;case OMAP_DSS_COLOR_RGBA16:m = 0xb; break;case OMAP_DSS_COLOR_ARGB32:m = 0xc; break;case OMAP_DSS_COLOR_RGBA32:m = 0xd; break;case OMAP_DSS_COLOR_RGBX32:m = 0xe; break;case OMAP_DSS_COLOR_XRGB16_1555:m = 0xf; break;default:BUG(); return;}}REG_FLD_MOD(DISPC_OVL_ATTRIBUTES(plane), m, 4, 1);}static void dispc_ovl_configure_burst_type(enum omap_plane plane,enum omap_dss_rotation_type rotation_type){if (!dss_has_feature(FEAT_BURST_2D))return;if (rotation_type == OMAP_DSS_ROT_TILER)REG_FLD_MOD(DISPC_OVL_ATTRIBUTES(plane), 1, 29, 29);elseREG_FLD_MOD(DISPC_OVL_ATTRIBUTES(plane), 0, 29, 29);}void dispc_ovl_set_channel_out(enum omap_plane plane, enum omap_channel channel){int shift;u32 val;int chan = 0, chan2 = 0;switch (plane) {case OMAP_DSS_GFX:shift = 8;break;case OMAP_DSS_VIDEO1:case OMAP_DSS_VIDEO2:case OMAP_DSS_VIDEO3:shift = 16;break;default:BUG();return;}val = dispc_read_reg(DISPC_OVL_ATTRIBUTES(plane));if (dss_has_feature(FEAT_MGR_LCD2)) {switch (channel) {case OMAP_DSS_CHANNEL_LCD:chan = 0;chan2 = 0;break;case OMAP_DSS_CHANNEL_DIGIT:chan = 1;chan2 = 0;break;case OMAP_DSS_CHANNEL_LCD2:chan = 0;chan2 = 1;break;case OMAP_DSS_CHANNEL_LCD3:if (dss_has_feature(FEAT_MGR_LCD3)) {chan = 0;chan2 = 2;} else {BUG();return;}break;case OMAP_DSS_CHANNEL_WB:chan = 0;chan2 = 3;break;default:BUG();return;}val = FLD_MOD(val, chan, shift, shift);val = FLD_MOD(val, chan2, 31, 30);} else {val = FLD_MOD(val, channel, shift, shift);}dispc_write_reg(DISPC_OVL_ATTRIBUTES(plane), val);}EXPORT_SYMBOL(dispc_ovl_set_channel_out);static enum omap_channel dispc_ovl_get_channel_out(enum omap_plane plane){int shift;u32 val;switch (plane) {case OMAP_DSS_GFX:shift = 8;break;case OMAP_DSS_VIDEO1:case OMAP_DSS_VIDEO2:case OMAP_DSS_VIDEO3:shift = 16;break;default:BUG();return 0;}val = dispc_read_reg(DISPC_OVL_ATTRIBUTES(plane));if (FLD_GET(val, shift, shift) == 1)return OMAP_DSS_CHANNEL_DIGIT;if (!dss_has_feature(FEAT_MGR_LCD2))return OMAP_DSS_CHANNEL_LCD;switch (FLD_GET(val, 31, 30)) {case 0:default:return OMAP_DSS_CHANNEL_LCD;case 1:return OMAP_DSS_CHANNEL_LCD2;case 2:return OMAP_DSS_CHANNEL_LCD3;case 3:return OMAP_DSS_CHANNEL_WB;}}static void dispc_ovl_set_burst_size(enum omap_plane plane,enum omap_burst_size burst_size){static const unsigned shifts[] = { 6, 14, 14, 14, 14, };int shift;shift = shifts[plane];REG_FLD_MOD(DISPC_OVL_ATTRIBUTES(plane), burst_size, shift + 1, shift);}static void dispc_configure_burst_sizes(void){int i;const int burst_size = BURST_SIZE_X8;  Configure burst size always to maximum size ", "dispc.fifo_assignment[fifo] = fifo;}/* * The GFX fifo on OMAP4 is smaller than the other fifos. The small fifo * causes problems with certain use cases, like using the tiler in 2D * mode. The below hack swaps the fifos of GFX and WB planes, thus * giving GFX plane a larger fifo. WB but should work fine with a * smaller fifo. ": "dispc_mgr_enable_cpr(enum omap_channel channel, bool enable){if (channel == OMAP_DSS_CHANNEL_DIGIT)return;mgr_fld_write(channel, DISPC_MGR_FLD_CPR, enable);}static void dispc_mgr_set_cpr_coef(enum omap_channel channel,const struct omap_dss_cpr_coefs  coefs){u32 coef_r, coef_g, coef_b;if (!dss_mgr_is_lcd(channel))return;coef_r = FLD_VAL(coefs->rr, 31, 22) | FLD_VAL(coefs->rg, 20, 11) |FLD_VAL(coefs->rb, 9, 0);coef_g = FLD_VAL(coefs->gr, 31, 22) | FLD_VAL(coefs->gg, 20, 11) |FLD_VAL(coefs->gb, 9, 0);coef_b = FLD_VAL(coefs->br, 31, 22) | FLD_VAL(coefs->bg, 20, 11) |FLD_VAL(coefs->bb, 9, 0);dispc_write_reg(DISPC_CPR_COEF_R(channel), coef_r);dispc_write_reg(DISPC_CPR_COEF_G(channel), coef_g);dispc_write_reg(DISPC_CPR_COEF_B(channel), coef_b);}static void dispc_ovl_set_vid_color_conv(enum omap_plane plane, bool enable){u32 val;BUG_ON(plane == OMAP_DSS_GFX);val = dispc_read_reg(DISPC_OVL_ATTRIBUTES(plane));val = FLD_MOD(val, enable, 9, 9);dispc_write_reg(DISPC_OVL_ATTRIBUTES(plane), val);}static void dispc_ovl_enable_replication(enum omap_plane plane,enum omap_overlay_caps caps, bool enable){static const unsigned shifts[] = { 5, 10, 10, 10 };int shift;if ((caps & OMAP_DSS_OVL_CAP_REPLICATION) == 0)return;shift = shifts[plane];REG_FLD_MOD(DISPC_OVL_ATTRIBUTES(plane), enable, shift, shift);}static void dispc_mgr_set_size(enum omap_channel channel, u16 width,u16 height){u32 val;val = FLD_VAL(height - 1, dispc.feat->mgr_height_start, 16) |FLD_VAL(width - 1, dispc.feat->mgr_width_start, 0);dispc_write_reg(DISPC_SIZE_MGR(channel), val);}static void dispc_init_fifos(void){u32 size;int fifo;u8 start, end;u32 unit;int i;unit = dss_feat_get_buffer_size_unit();dss_feat_get_reg_field(FEAT_REG_FIFOSIZE, &start, &end);for (fifo = 0; fifo < dispc.feat->num_fifos; ++fifo) {size = REG_GET(DISPC_OVL_FIFO_SIZE_STATUS(fifo), start, end);size  = unit;dispc.fifo_size[fifo] = size;    By default fifos are mapped directly to overlays, fifo 0 to   ovl 0, fifo 1 to ovl 1, etc. ", "298, 409, 0, 298, -208, -100, 298, 0, 517, 0,};const struct color_conv_coef ctbl_bt601_5_wb = ": "dispc_mgr_is_enabled(channel));WARN_ON(dispc_mgr_go_busy(channel));DSSDBG(\"GO %s\\n\", mgr_desc[channel].name);mgr_fld_write(channel, DISPC_MGR_FLD_GO, 1);}EXPORT_SYMBOL(dispc_mgr_go);static void dispc_ovl_write_firh_reg(enum omap_plane plane, int reg, u32 value){dispc_write_reg(DISPC_OVL_FIR_COEF_H(plane, reg), value);}static void dispc_ovl_write_firhv_reg(enum omap_plane plane, int reg, u32 value){dispc_write_reg(DISPC_OVL_FIR_COEF_HV(plane, reg), value);}static void dispc_ovl_write_firv_reg(enum omap_plane plane, int reg, u32 value){dispc_write_reg(DISPC_OVL_FIR_COEF_V(plane, reg), value);}static void dispc_ovl_write_firh2_reg(enum omap_plane plane, int reg, u32 value){BUG_ON(plane == OMAP_DSS_GFX);dispc_write_reg(DISPC_OVL_FIR_COEF_H2(plane, reg), value);}static void dispc_ovl_write_firhv2_reg(enum omap_plane plane, int reg,u32 value){BUG_ON(plane == OMAP_DSS_GFX);dispc_write_reg(DISPC_OVL_FIR_COEF_HV2(plane, reg), value);}static void dispc_ovl_write_firv2_reg(enum omap_plane plane, int reg, u32 value){BUG_ON(plane == OMAP_DSS_GFX);dispc_write_reg(DISPC_OVL_FIR_COEF_V2(plane, reg), value);}static void dispc_ovl_set_scale_coef(enum omap_plane plane, int fir_hinc,int fir_vinc, int five_taps,enum omap_color_component color_comp){const struct dispc_coef  h_coef,  v_coef;int i;h_coef = dispc_ovl_get_scale_coef(fir_hinc, true);v_coef = dispc_ovl_get_scale_coef(fir_vinc, five_taps);for (i = 0; i < 8; i++) {u32 h, hv;h = FLD_VAL(h_coef[i].hc0_vc00, 7, 0)| FLD_VAL(h_coef[i].hc1_vc0, 15, 8)| FLD_VAL(h_coef[i].hc2_vc1, 23, 16)| FLD_VAL(h_coef[i].hc3_vc2, 31, 24);hv = FLD_VAL(h_coef[i].hc4_vc22, 7, 0)| FLD_VAL(v_coef[i].hc1_vc0, 15, 8)| FLD_VAL(v_coef[i].hc2_vc1, 23, 16)| FLD_VAL(v_coef[i].hc3_vc2, 31, 24);if (color_comp == DISPC_COLOR_COMPONENT_RGB_Y) {dispc_ovl_write_firh_reg(plane, i, h);dispc_ovl_write_firhv_reg(plane, i, hv);} else {dispc_ovl_write_firh2_reg(plane, i, h);dispc_ovl_write_firhv2_reg(plane, i, hv);}}if (five_taps) {for (i = 0; i < 8; i++) {u32 v;v = FLD_VAL(v_coef[i].hc0_vc00, 7, 0)| FLD_VAL(v_coef[i].hc4_vc22, 15, 8);if (color_comp == DISPC_COLOR_COMPONENT_RGB_Y)dispc_ovl_write_firv_reg(plane, i, v);elsedispc_ovl_write_firv2_reg(plane, i, v);}}}static void dispc_ovl_write_color_conv_coef(enum omap_plane plane,const struct color_conv_coef  ct){#define CVAL(x, y) (FLD_VAL(x, 26, 16) | FLD_VAL(y, 10, 0))dispc_write_reg(DISPC_OVL_CONV_COEF(plane, 0), CVAL(ct->rcr, ct->ry));dispc_write_reg(DISPC_OVL_CONV_COEF(plane, 1), CVAL(ct->gy,  ct->rcb));dispc_write_reg(DISPC_OVL_CONV_COEF(plane, 2), CVAL(ct->gcb, ct->gcr));dispc_write_reg(DISPC_OVL_CONV_COEF(plane, 3), CVAL(ct->bcr, ct->by));dispc_write_reg(DISPC_OVL_CONV_COEF(plane, 4), CVAL(0, ct->bcb));REG_FLD_MOD(DISPC_OVL_ATTRIBUTES(plane), ct->full_range, 11, 11);#undef CVAL}static void dispc_setup_color_conv_coef(void){int i;int num_ovl = dss_feat_get_num_ovls();const struct color_conv_coef ctbl_bt601_5_ovl = {  YUV -> RGB ", "RR(IRQENABLE);DSSDBG(\"context restored\\n\");}#undef SR#undef RRint dispc_runtime_get(void)": "dispc_clear_irqstatus(DISPC_IRQ_SYNC_LOST_DIGIT);    enable last so IRQs won't trigger before   the context is fully restored ", "dispc_clear_irqstatus((mask ^ old_mask) & mask);dispc_write_reg(DISPC_IRQENABLE, mask);}EXPORT_SYMBOL(dispc_write_irqenable": "dispc_write_irqenable(u32 mask){u32 old_mask = dispc_read_reg(DISPC_IRQENABLE);  clear the irqstatus for newly enabled irqs ", "smp_wmb();r = devm_request_irq(&dispc.pdev->dev, dispc.irq, dispc_irq_handler,     IRQF_SHARED, \"OMAP DISPC\", &dispc);if (r) ": "dispc_request_irq(irq_handler_t handler, void  dev_id){int r;if (dispc.user_handler != NULL)return -EBUSY;dispc.user_handler = handler;dispc.user_data = dev_id;  ensure the dispc_irq_handler sees the values above ", "for (i = 0; i < DISPC_MAX_NR_ISRS; i++) ": "omap_dispc_register_isr(omap_dispc_isr_t isr, void  arg, u32 mask){int i;int ret;unsigned long flags;struct omap_dispc_isr_data  isr_data;if (isr == NULL)return -EINVAL;spin_lock_irqsave(&dispc_compat.irq_lock, flags);  check for duplicate entry ", "isr_data->isr = NULL;isr_data->arg = NULL;isr_data->mask = 0;ret = 0;break;}if (ret == 0)_omap_dispc_set_irqs();spin_unlock_irqrestore(&dispc_compat.irq_lock, flags);return ret;}EXPORT_SYMBOL(omap_dispc_unregister_isr": "omap_dispc_unregister_isr(omap_dispc_isr_t isr, void  arg, u32 mask){int i;unsigned long flags;int ret = -EINVAL;struct omap_dispc_isr_data  isr_data;spin_lock_irqsave(&dispc_compat.irq_lock, flags);for (i = 0; i < DISPC_MAX_NR_ISRS; i++) {isr_data = &dispc_compat.registered_isr[i];if (isr_data->isr != isr || isr_data->arg != arg ||isr_data->mask != mask)continue;  found the correct isr ", "void clkdev_drop(struct clk_lookup *cl)": "clkdev_drop - remove a clock dynamically allocated ", "if (dev_id)*cl = __clk_register_clkdev(hw, con_id, \"%s\", dev_id);else*cl = __clk_register_clkdev(hw, con_id, NULL);return *cl ? 0 : -ENOMEM;}/** * clk_register_clkdev - register one clock lookup for a struct clk * @clk: struct clk to associate with all clk_lookups * @con_id: connection ID string on device * @dev_id: string describing device name * * con_id or dev_id may be NULL as a wildcard, just as in the rest of * clkdev. * * To make things easier for mass registration, we detect error clks * from a previous clk_register() call, and return the error code for * those.  This is to permit this function to be called immediately * after clk_register(). ": "clk_register_clkdev(struct clk_hw  hw,const char  con_id,const char  dev_id, ...){struct clk_lookup  cl;va_list ap;va_start(ap, dev_id);cl = vclkdev_create(hw, con_id, dev_id, ap);va_end(ap);return cl;}static int do_clk_register_clkdev(struct clk_hw  hw,struct clk_lookup   cl, const char  con_id, const char  dev_id){if (IS_ERR(hw))return PTR_ERR(hw);    Since dev_id can be NULL, and NULL is handled specially, we must   pass it as either a NULL format string, or with \"%s\". ", "int clk_hw_register_clkdev(struct clk_hw *hw, const char *con_id,const char *dev_id)": "clk_hw_register_clkdev - register one clock lookup for a struct clk_hw   @hw: struct clk_hw to associate with all clk_lookups   @con_id: connection ID string on device   @dev_id: format string describing device name     con_id or dev_id may be NULL as a wildcard, just as in the rest of   clkdev.     To make things easier for mass registration, we detect error clk_hws   from a previous clk_hw_register_ () call, and return the error code for   those.  This is to permit this function to be called immediately   after clk_hw_register_ (). ", "int devm_clk_hw_register_clkdev(struct device *dev, struct clk_hw *hw,const char *con_id, const char *dev_id)": "devm_clk_hw_register_clkdev - managed clk lookup registration for clk_hw   @dev: device this lookup is bound   @hw: struct clk_hw to associate with all clk_lookups   @con_id: connection ID string on device   @dev_id: format string describing device name     con_id or dev_id may be NULL as a wildcard, just as in the rest of   clkdev.     To make things easier for mass registration, we detect error clk_hws   from a previous clk_hw_register_ () call, and return the error code for   those.  This is to permit this function to be called immediately   after clk_hw_register_ (). ", "struct clk *clk_hw_get_clk(struct clk_hw *hw, const char *con_id)": "clk_hw_get_clk - get clk consumer given an clk_hw   @hw: clk_hw associated with the clk being consumed   @con_id: connection ID string on device     Returns: new clk consumer   This is the function to be used by providers which need   to get a consumer clk and act on the clock element   Calls to this function must be balanced with calls clk_put() ", "static struct clk_core *clk_core_get(struct clk_core *core, u8 p_index)": "of_clk_get_hw_from_clkspec(struct of_phandle_args  clkspec);#elsestatic inline int of_parse_clkspec(const struct device_node  np, int index,   const char  name,   struct of_phandle_args  out_args){return -ENOENT;}static inline struct clk_hw  of_clk_get_hw_from_clkspec(struct of_phandle_args  clkspec){return ERR_PTR(-ENOENT);}#endif     clk_core_get - Find the clk_core parent of a clk   @core: clk to find parent of   @p_index: parent index to search for     This is the preferred method for clk providers to find the parent of a   clk when that parent is external to the clk controller. The parent_names   array is indexed and treated as a local name matching a string in the device   node's 'clock-names' property or as the 'con_id' matching the device's   dev_name() in a clk_lookup. This allows clk providers to use their own   namespace instead of looking for a globally unique parent string.     For example the following DT snippet would allow a clock registered by the   clock-controller@c001 that has a clk_init_data::parent_data array   with 'xtal' in the 'name' member to find the clock provided by the   clock-controller@f00abcd without needing to get the globally unique name of   the xtal clk.          parent: clock-controller@f00abcd {                reg = <0xf00abcd 0xabcd>;                #clock-cells = <0>;        };          clock-controller@c001 {                reg = <0xc001 0xf00d>;                clocks = <&parent>;                clock-names = \"xtal\";                #clock-cells = <1>;        };     Returns: -ENOENT when the provider can't be found or the clk doesn't   exist in the provider or the name can't be found in the DT node or   in a clkdev lookup. NULL when the provider knows about the clk but it   isn't provided on this system.   A valid clk_core pointer when the clk can be found in the provider. ", "struct clk *of_clk_get_by_name(struct device_node *np, const char *name)": "of_clk_get_by_name() - Parse and lookup a clock referenced by a device node   @np: pointer to clock consumer node   @name: name of consumer's clock input, or NULL for the first clock reference     This function parses the clocks and clock-names properties,   and uses them to look up the struct clk from the registered list of clock   providers. ", "int tegra_dfll_runtime_resume(struct device *dev)": "tegra_dfll_runtime_resume - enable all clocks needed by the DFLL   @dev: DFLL device       Enable all clocks needed by the DFLL. Assumes that clk_prepare()   has already been called on all the clocks.     XXX Should also handle context restore when returning from off. ", "int tegra_dfll_runtime_suspend(struct device *dev)": "tegra_dfll_runtime_suspend - disable all clocks needed by the DFLL   @dev: DFLL device       Disable all clocks needed by the DFLL. Assumes that other code   will later call clk_unprepare(). ", "int tegra_dfll_suspend(struct device *dev)": "tegra_dfll_suspend - check DFLL is disabled   @dev: DFLL instance     DFLL clock should be disabled by the CPUFreq driver. So, make   sure it is disabled and disable all clocks needed by the DFLL. ", "int tegra_dfll_resume(struct device *dev)": "tegra_dfll_resume - reinitialize DFLL on resume   @dev: DFLL instance     DFLL is disabled and reset during suspend and resume.   So, reinitialize the DFLL IP block back for use.   DFLL clock is enabled later in closed loop mode by CPUFreq   driver before switching its clock source to DFLL output. ", "int tegra_dfll_register(struct platform_device *pdev,struct tegra_dfll_soc_data *soc)": "tegra_dfll_register - probe a Tegra DFLL device   @pdev: DFLL platform_device     @soc: Per-SoC integration and characterization data for this DFLL instance     Probe and initialize a DFLL device instance. Intended to be called   by a SoC-specific shim driver that passes in per-SoC integration   and configuration data via @soc. Returns 0 on success or -err on failure. ", "struct tegra_dfll_soc_data *tegra_dfll_unregister(struct platform_device *pdev)": "tegra_dfll_unregister - release all of the DFLL driver resources for a device   @pdev: DFLL platform_device       Unbind this driver from the DFLL hardware device represented by   @pdev. The DFLL must be disabled for this to succeed. Returns a   soc pointer upon success or -EBUSY if the DFLL is still active. ", "void tegra114_clock_tune_cpu_trimmers_high(void)": "tegra114_clock_tune_cpu_trimmers_high - use high-voltage propagation delays     When the CPU rail voltage is in the high-voltage range, use the   built-in hardwired clock propagation delays in the CPU clock   shaper.  No return value. ", "void tegra114_clock_tune_cpu_trimmers_low(void)": "tegra114_clock_tune_cpu_trimmers_init().  The intention is to   maintain the input clock duty cycle that the FCPU subsystem   expects.  No return value. ", "void tegra114_clock_assert_dfll_dvco_reset(void)": "tegra114_clock_assert_dfll_dvco_reset - assert the DFLL's DVCO reset     Assert the reset line of the DFLL's DVCO.  No return value. ", "void tegra114_clock_deassert_dfll_dvco_reset(void)": "tegra114_clock_deassert_dfll_dvco_reset - deassert the DFLL's DVCO reset     Deassert the reset line of the DFLL's DVCO, allowing the DVCO to   operate.  No return value. ", "err = -ENOMEM;ptp = kzalloc(sizeof(struct ptp_clock), GFP_KERNEL);if (ptp == NULL)goto no_memory;index = ida_alloc_max(&ptp_clocks_map, MINORMASK, GFP_KERNEL);if (index < 0) ": "ptp_clock_register(struct ptp_clock_info  info,     struct device  parent){struct ptp_clock  ptp;int err = 0, index, major = MAJOR(ptp_devt);size_t size;if (info->n_alarm > PTP_MAX_ALARMS)return ERR_PTR(-EINVAL);  Initialize a clock structure. ", "if (ptp->pps_source)pps_unregister_source(ptp->pps_source);posix_clock_unregister(&ptp->clock);return 0;}EXPORT_SYMBOL(ptp_clock_unregister": "ptp_clock_unregister(struct ptp_clock  ptp){if (ptp_vclock_in_use(ptp)) {device_for_each_child(&ptp->dev, NULL, unregister_vclock);}ptp->defunct = 1;wake_up_interruptible(&ptp->tsev_wq);if (ptp->kworker) {kthread_cancel_delayed_work_sync(&ptp->aux_work);kthread_destroy_worker(ptp->kworker);}  Release the clock's resources. ", "static int ptp_clock_getres(struct posix_clock *pc, struct timespec64 *tp)": "ptp_clock_event  src){struct ptp_extts_event  dst;unsigned long flags;s64 seconds;u32 remainder;seconds = div_u64_rem(src->timestamp, 1000000000, &remainder);spin_lock_irqsave(&queue->lock, flags);dst = &queue->buf[queue->tail];dst->index = src->index;dst->t.sec = seconds;dst->t.nsec = remainder;if (!queue_free(queue))queue->head = (queue->head + 1) % PTP_MAX_TIMESTAMPS;queue->tail = (queue->tail + 1) % PTP_MAX_TIMESTAMPS;spin_unlock_irqrestore(&queue->lock, flags);}  posix clock implementation ", "int pch_set_station_address(u8 *addr, struct pci_dev *pdev)": "pch_set_station_address() - This API sets the station address used by      IEEE 1588 hardware when looking at PTP      traffic on the  ethernet interface   @addr:dress which contain the column separated address to be used.   @pdev:PCI device. ", "void mcp_set_telecom_divisor(struct mcp *mcp, unsigned int div)": "mcp_set_telecom_divisor - set the telecom divisor  @mcp: MCP interface structure  @div: SIB clock divisor    Set the telecom divisor on the MCP interface.  The resulting  sample rate is SIBCLOCKdiv. ", "void mcp_set_audio_divisor(struct mcp *mcp, unsigned int div)": "mcp_set_audio_divisor - set the audio divisor  @mcp: MCP interface structure  @div: SIB clock divisor    Set the audio divisor on the MCP interface. ", "void mcp_reg_write(struct mcp *mcp, unsigned int reg, unsigned int val)": "mcp_reg_write - write a device register  @mcp: MCP interface structure  @reg: 4-bit register index  @val: 16-bit data value    Write a device register.  The MCP interface must be enabled  to prevent this function hanging. ", "unsigned int mcp_reg_read(struct mcp *mcp, unsigned int reg)": "mcp_reg_read - read a device register  @mcp: MCP interface structure  @reg: 4-bit register index    Read a device register and return its value.  The MCP interface  must be enabled to prevent this function hanging. ", "void mcp_enable(struct mcp *mcp)": "mcp_disable to disable the interface. ", "if (sdev->usid % ctx->num_usids == 0)return sdev;function_parent_usid = sdev->usid;/* * Walk through the list of PMICs until we find the sibling USID. * The goal is to find the first USID which is less than the * number of USIDs in the PMIC array, e.g. for a PMIC with 2 USIDs * where the function device is under USID 3, we want to find the * device for USID 2. ": "qcom_pmic_get_base_usid(struct device  dev){struct spmi_device  sdev;struct qcom_spmi_dev  ctx;struct device_node  spmi_bus;struct device_node  other_usid = NULL;int function_parent_usid, ret;u32 pmic_addr;sdev = to_spmi_device(dev);ctx = dev_get_drvdata(&sdev->dev);    Quick return if the function device is already in the base   USID. This will always be hit for PMICs with only 1 USID. ", "chip->osc_vote |= client;/* If reference group is off - turn on": "pm8606_osc_enable(struct pm860x_chip  chip, unsigned short client){int ret = -EIO;struct i2c_client  i2c = (chip->id == CHIP_PM8606) ?chip->client : chip->companion;dev_dbg(chip->dev, \"%s(B): client=0x%x\\n\", __func__, client);dev_dbg(chip->dev, \"%s(B): vote=0x%x status=%d\\n\",__func__, chip->osc_vote,chip->osc_status);mutex_lock(&chip->osc_lock);  Update voting status ", "chip->osc_vote &= ~(client);/* * If reference group is off and this is the last client to release * - turn off ": "pm8606_osc_disable(struct pm860x_chip  chip, unsigned short client){int ret = -EIO;struct i2c_client  i2c = (chip->id == CHIP_PM8606) ?chip->client : chip->companion;dev_dbg(chip->dev, \"%s(B): client=0x%x\\n\", __func__, client);dev_dbg(chip->dev, \"%s(B): vote=0x%x status=%d\\n\",__func__, chip->osc_vote,chip->osc_status);mutex_lock(&chip->osc_lock);  Update voting status ", "if (of_property_read_bool(axp20x->dev->of_node,  \"x-powers,self-working-mode\") &&    axp20x->irq > 0) ": "axp20x_match_device(struct axp20x_dev  axp20x){struct device  dev = axp20x->dev;const struct acpi_device_id  acpi_id;const struct of_device_id  of_id;if (dev->of_node) {of_id = of_match_device(dev->driver->of_match_table, dev);if (!of_id) {dev_err(dev, \"Unable to match OF ID\\n\");return -ENODEV;}axp20x->variant = (long)of_id->data;} else {acpi_id = acpi_match_device(dev->driver->acpi_match_table, dev);if (!acpi_id || !acpi_id->driver_data) {dev_err(dev, \"Unable to match ACPI ID and data\\n\");return -ENODEV;}axp20x->variant = (long)acpi_id->driver_data;}switch (axp20x->variant) {case AXP152_ID:axp20x->nr_cells = ARRAY_SIZE(axp152_cells);axp20x->cells = axp152_cells;axp20x->regmap_cfg = &axp152_regmap_config;axp20x->regmap_irq_chip = &axp152_regmap_irq_chip;break;case AXP192_ID:axp20x->nr_cells = ARRAY_SIZE(axp192_cells);axp20x->cells = axp192_cells;axp20x->regmap_cfg = &axp192_regmap_config;axp20x->regmap_irq_chip = &axp192_regmap_irq_chip;break;case AXP202_ID:case AXP209_ID:axp20x->nr_cells = ARRAY_SIZE(axp20x_cells);axp20x->cells = axp20x_cells;axp20x->regmap_cfg = &axp20x_regmap_config;axp20x->regmap_irq_chip = &axp20x_regmap_irq_chip;break;case AXP221_ID:axp20x->nr_cells = ARRAY_SIZE(axp221_cells);axp20x->cells = axp221_cells;axp20x->regmap_cfg = &axp22x_regmap_config;axp20x->regmap_irq_chip = &axp22x_regmap_irq_chip;break;case AXP223_ID:axp20x->nr_cells = ARRAY_SIZE(axp223_cells);axp20x->cells = axp223_cells;axp20x->regmap_cfg = &axp22x_regmap_config;axp20x->regmap_irq_chip = &axp22x_regmap_irq_chip;break;case AXP288_ID:axp20x->cells = axp288_cells;axp20x->nr_cells = ARRAY_SIZE(axp288_cells);axp20x->regmap_cfg = &axp288_regmap_config;axp20x->regmap_irq_chip = &axp288_regmap_irq_chip;axp20x->irq_flags = IRQF_TRIGGER_LOW;break;case AXP313A_ID:axp20x->nr_cells = ARRAY_SIZE(axp313a_cells);axp20x->cells = axp313a_cells;axp20x->regmap_cfg = &axp313a_regmap_config;axp20x->regmap_irq_chip = &axp313a_regmap_irq_chip;break;case AXP803_ID:axp20x->nr_cells = ARRAY_SIZE(axp803_cells);axp20x->cells = axp803_cells;axp20x->regmap_cfg = &axp288_regmap_config;axp20x->regmap_irq_chip = &axp803_regmap_irq_chip;break;case AXP806_ID:    Don't register the power key part if in slave mode or   if there is no interrupt line. ", "if (axp20x->variant == AXP806_ID) ": "axp20x_device_probe(struct axp20x_dev  axp20x){int ret;    The AXP806 supports either masterstandalone or slave mode.   Slave mode allows sharing the serial bus, even with multiple   AXP806 which all have the same hardware address.     This is done with extra \"serial interface address extension\",   or AXP806_BUS_ADDR_EXT, and \"register address extension\", or   AXP806_REG_ADDR_EXT, registers. The former is read-only, with   1 bit customizable at the factory, and 1 bit depending on the   state of an external pin. The latter is writable. The device   will only respond to operations to its other registers when   the these device addressing bits (in the upper 4 bits of the   registers) match.     By default we support an AXP806 chained to an AXP809 in slave   mode. Boards which use an AXP806 in master mode can set the   property \"x-powers,master-mode\" to override the default. ", "void prcmu_configure_auto_pm(struct prcmu_auto_pm_config *sleep,struct prcmu_auto_pm_config *idle)": "prcmu_configure_auto_pm - Configure autonomous power management.   @sleep: Configuration for ApSleep.   @idle:  Configuration for ApIdle. ", "    (u8)((timeout << 4) & 0xf0),    (u8)((timeout >> 4) & 0xff),    (u8)((timeout >> 12) & 0xff),    (u8)((timeout >> 20) & 0xff));}EXPORT_SYMBOL(db8500_prcmu_load_a9wdog": "db8500_prcmu_load_a9wdog(u8 id, u32 timeout){return prcmu_a9wdog(MB4H_A9WDOG_LOAD,    (id & A9WDOG_ID_MASK) |            Put the lowest 28 bits of timeout at       offset 4. Four first bits are used for id.     ", "irqflags = IRQF_TRIGGER_HIGH | IRQF_ONESHOT;if (pdata->irq_flags)irqflags = pdata->irq_flags;/* use a GPIO for edge triggered controllers ": "wm8994_irq_init(struct wm8994  wm8994){int ret;unsigned long irqflags;struct wm8994_pdata  pdata = &wm8994->pdata;if (!wm8994->irq) {dev_warn(wm8994->dev, \"No interrupt specified, no interrupts\\n\");wm8994->irq_base = 0;return 0;}  select user or default irq flags ", "local_irq_save(flags);if (mA >= 500)mA = 500;else if (mA >= 100)mA = 100;elsemA = 0;the_tps->vbus = mA;if ((the_tps->chgstatus & TPS_CHG_USB)&& test_and_set_bit(FLAG_VBUS_CHANGED, &the_tps->flags)) ": "tps65010_set_vbus_draw(unsigned mA){unsigned longflags;if (!the_tps)return -ENODEV;  assumes non-SMP ", "if (offset < 4) ": "tps65010_set_vib(value);}static inttps65010_output(struct gpio_chip  chip, unsigned offset, int value){  GPIOs may be input-only ", "int tps65010_set_low_pwr(unsigned mode)": "tps65010_set_low_pwr parameter:   mode: ON or OFF ", "int tps65010_config_vregs1(unsigned value)": "tps65010_config_vregs1 parameter:   value to be written to VREGS1 register   Note: The complete register is written, set all bits you need ", "/* FIXME: Assumes AC or USB power is present. Setting AUA bit is notrequired if power supply is through a battery ": "tps65013_set_low_pwr parameter:   mode: ON or OFF ", "static bool dln2_transfer_complete(struct dln2_dev *dln2, struct urb *urb,   u16 handle, u16 rx_slot)": "dln2_transfer   is woke up. It will be resubmitted there. ", "void ucb1x00_io_set_dir(struct ucb1x00 *ucb, unsigned int in, unsigned int out)": "ucb1x00_io_set_dir - set IO direction  @ucb: UCB1x00 structure describing chip  @in:  bitfield of IO pins to be set as inputs  @out: bitfield of IO pins to be set as outputs    Set the IO direction of the ten general purpose IO pins on  the UCB1x00 chip.  The @in bitfield has priority over the  @out bitfield, in that if you specify a pin as both input  and output, it will end up as an input.    ucb1x00_enable must have been called to enable the comms  before using this function.    This function takes a spinlock, disabling interrupts. ", "void ucb1x00_io_write(struct ucb1x00 *ucb, unsigned int set, unsigned int clear)": "ucb1x00_io_write - set or clear IO outputs  @ucb:   UCB1x00 structure describing chip  @set:   bitfield of IO pins to set to logic '1'  @clear: bitfield of IO pins to set to logic '0'    Set the IO output state of the specified IO pins.  The value  is retained if the pins are subsequently configured as inputs.  The @clear bitfield has priority over the @set bitfield -  outputs will be cleared.    ucb1x00_enable must have been called to enable the comms  before using this function.    This function takes a spinlock, disabling interrupts. ", "unsigned int ucb1x00_io_read(struct ucb1x00 *ucb)": "ucb1x00_io_read - read the current state of the IO pins  @ucb: UCB1x00 structure describing chip    Return a bitfield describing the logic state of the ten  general purpose IO pins.    ucb1x00_enable must have been called to enable the comms  before using this function.    This function does not take any mutexes or spinlocks. ", "void ucb1x00_adc_enable(struct ucb1x00 *ucb)": "ucb1x00_adc_disable. ", "unsigned int ucb1x00_adc_read(struct ucb1x00 *ucb, int adc_channel, int sync)": "ucb1x00_adc_read - read the specified ADC channel  @ucb: UCB1x00 structure describing chip  @adc_channel: ADC channel mask  @sync: wait for syncronisation pulse.    Start an ADC conversion and wait for the result.  Note that  synchronised ADC conversions (via the ADCSYNC pin) must wait  until the trigger is asserted and the conversion is finished.    This function currently spins waiting for the conversion to  complete (2 frames max without sync).    If called for a synchronised ADC conversion, it may sleep  with the ADC mutex held. ", "int mfd_add_devices(struct device *parent, int id,    const struct mfd_cell *cells, int n_devs,    struct resource *mem_base,    int irq_base, struct irq_domain *domain)": "mfd_add_devices - register child devices     @parent:Pointer to parent device.   @id:Can be PLATFORM_DEVID_AUTO to let the Platform API take care  of device numbering, or will be added to a device's cell_id.   @cells:Array of (struct mfd_cell)s describing child devices.   @n_devs:Number of child devices to register.   @mem_base:Parent register range resource for child devices.   @irq_base:Base of the range of virtual interrupt numbers allocated for  this MFD device. Unused if @domain is specified.   @domain:Interrupt domain to create mappings for hardware interrupts. ", "int devm_mfd_add_devices(struct device *dev, int id, const struct mfd_cell *cells, int n_devs, struct resource *mem_base, int irq_base, struct irq_domain *domain)": "devm_mfd_add_devices - Resource managed version of mfd_add_devices()     Returns 0 on success or an appropriate negative error number on failure.   All child-devices of the MFD will automatically be removed when it gets   unbinded.     @dev:Pointer to parent device.   @id:Can be PLATFORM_DEVID_AUTO to let the Platform API take care  of device numbering, or will be added to a device's cell_id.   @cells:Array of (struct mfd_cell)s describing child devices.   @n_devs:Number of child devices to register.   @mem_base:Parent register range resource for child devices.   @irq_base:Base of the range of virtual interrupt numbers allocated for  this MFD device. Unused if @domain is specified.   @domain:Interrupt domain to create mappings for hardware interrupts. ", "return ret;}EXPORT_SYMBOL(twl6030_interrupt_unmask": "twl6030_interrupt_unmask(u8 bit_mask, u8 offset){int ret;u8 unmask_value;ret = twl_i2c_read_u8(TWL_MODULE_PIH, &unmask_value,REG_INT_STS_A + offset);unmask_value &= (~(bit_mask));ret |= twl_i2c_write_u8(TWL_MODULE_PIH, unmask_value,REG_INT_STS_A + offset);   unmask INT_MSK_ABC ", "return ret;}EXPORT_SYMBOL(twl6030_interrupt_mask": "twl6030_interrupt_mask(u8 bit_mask, u8 offset){int ret;u8 mask_value;ret = twl_i2c_read_u8(TWL_MODULE_PIH, &mask_value,REG_INT_STS_A + offset);mask_value |= (bit_mask);ret |= twl_i2c_write_u8(TWL_MODULE_PIH, mask_value,REG_INT_STS_A + offset);   mask INT_MSK_ABC ", "twl6030_interrupt_unmask(TWL6030_MMCDETECT_INT_MASK,REG_INT_MSK_LINE_B);twl6030_interrupt_unmask(TWL6030_MMCDETECT_INT_MASK,REG_INT_MSK_STS_B);/* * Initially Configuring MMC_CTRL for receiving interrupts & * Card status on TWL6030 for MMC1 ": "twl6030_mmc_card_detect_config(void){int ret;u8 reg_val = 0;  Unmasking the Card detect Interrupt line for MMC1 from Phoenix ", "ldoctl = TWL6040_HSLDOENA | TWL6040_REFENA | TWL6040_OSCENA;ret = twl6040_reg_write(twl6040, TWL6040_REG_LDOCTL, ldoctl);if (ret)return ret;usleep_range(10000, 10500);/* enable negative charge pump ": "twl6040_power_up_manual(struct twl6040  twl6040){u8 ldoctl, ncpctl, lppllctl;int ret;  enable high-side LDO, reference system and internal oscillator ", "if (pll_id != twl6040->pll) ": "twl6040_set_pll(struct twl6040  twl6040, int pll_id,    unsigned int freq_in, unsigned int freq_out){u8 hppllctl, lppllctl;int ret = 0;mutex_lock(&twl6040->mutex);hppllctl = twl6040_reg_read(twl6040, TWL6040_REG_HPPLLCTL);lppllctl = twl6040_reg_read(twl6040, TWL6040_REG_LPPLLCTL);  Force full reconfiguration when switching between PLL ", "int twl_i2c_write(u8 mod_no, u8 *value, u8 reg, unsigned num_bytes)": "twl_i2c_write - Writes a n bit register in TWL4030TWL5030TWL60X0   @mod_no: module number   @value: an array of num_bytes+1 containing data to write   @reg: register address (just offset will do)   @num_bytes: number of bytes to transfer     Returns 0 on success or else a negative error code. ", "int twl_i2c_read(u8 mod_no, u8 *value, u8 reg, unsigned num_bytes)": "twl_i2c_read - Reads a n bit register in TWL4030TWL5030TWL60X0   @mod_no: module number   @value: an array of num_bytes containing data to be read   @reg: register address (just offset will do)   @num_bytes: number of bytes to transfer     Returns 0 on success or else a negative error code. ", "int twl_set_regcache_bypass(u8 mod_no, bool enable)": "twl_set_regcache_bypass - Configure the regcache bypass for the regmap associated   with the module   @mod_no: module number   @enable: Regcache bypass state     Returns 0 else failure. ", "val &= ~(MCT_CTRL3_S1_AUTO_EN | MCT_CTRL3_S2_AUTO_EN);ret = menelaus_write_reg(MENELAUS_MCT_CTRL3, val);out:mutex_unlock(&the_menelaus->lock);return ret;}EXPORT_SYMBOL(menelaus_set_mmc_slot": "menelaus_set_mmc_slot(int slot, int enable, int power, int cd_en){int ret, val;if (slot != 1 && slot != 2)return -EINVAL;if (power >= 3)return -EINVAL;mutex_lock(&the_menelaus->lock);ret = menelaus_read_reg(MENELAUS_MCT_CTRL2);if (ret < 0)goto out;val = ret;if (slot == 1) {if (cd_en)val |= MCT_CTRL2_S1CD_BUFEN | MCT_CTRL2_S1CD_DBEN;elseval &= ~(MCT_CTRL2_S1CD_BUFEN | MCT_CTRL2_S1CD_DBEN);} else {if (cd_en)val |= MCT_CTRL2_S2CD_BUFEN | MCT_CTRL2_S2CD_BEN;elseval &= ~(MCT_CTRL2_S2CD_BUFEN | MCT_CTRL2_S2CD_BEN);}ret = menelaus_write_reg(MENELAUS_MCT_CTRL2, val);if (ret < 0)goto out;ret = menelaus_read_reg(MENELAUS_MCT_CTRL3);if (ret < 0)goto out;val = ret;if (slot == 1) {if (enable)val |= MCT_CTRL3_SLOT1_EN;elseval &= ~MCT_CTRL3_SLOT1_EN;} else {int b;if (enable)val |= MCT_CTRL3_SLOT2_EN;elseval &= ~MCT_CTRL3_SLOT2_EN;b = menelaus_read_reg(MENELAUS_MCT_CTRL2);b &= ~(MCT_CTRL2_VS2_SEL_D0 | MCT_CTRL2_VS2_SEL_D1);b |= power;ret = menelaus_write_reg(MENELAUS_MCT_CTRL2, b);if (ret < 0)goto out;}  Disable autonomous shutdown ", "void devm_nvmem_cell_put(struct device *dev, struct nvmem_cell *cell)": "devm_nvmem_cell_put() - Release previously allocated nvmem cell   from devm_nvmem_cell_get.     @dev: Device that requests the nvmem cell.   @cell: Previously allocated nvmem cell by devm_nvmem_cell_get(). ", "struct dma_buf *virtio_dma_buf_export(const struct dma_buf_export_info *exp_info)": "virtio_dma_buf_export - Creates a new dma-buf for a virtio exported object   @exp_info: [in] see dma_buf_export(). ops MUST refer to a dma_buf_ops  struct embedded in a virtio_dma_buf_ops.     This wraps dma_buf_export() to allow virtio drivers to create a dma-buf   for an virtio exported object that can be queried by other virtio drivers   for the object's UUID. ", "int virtio_dma_buf_attach(struct dma_buf *dma_buf,  struct dma_buf_attachment *attach)": "virtio_dma_buf_attach ||    !virtio_ops->get_uuid) {return ERR_PTR(-EINVAL);}return dma_buf_export(exp_info);}EXPORT_SYMBOL(virtio_dma_buf_export);     virtio_dma_buf_attach - mandatory attach callback for virtio dma-bufs ", "bool is_virtio_dma_buf(struct dma_buf *dma_buf)": "is_virtio_dma_buf - returns true if the given dma-buf is a virtio dma-buf   @dma_buf: buffer to query ", "int virtio_dma_buf_get_uuid(struct dma_buf *dma_buf,    uuid_t *uuid)": "virtio_dma_buf_get_uuid - gets a virtio dma-buf's exported object's uuid   @dma_buf: [in] buffer to query   @uuid: [out] the uuid     Returns: 0 on success, negative on failure. ", "PRIV(dev)->type = (mri & SUNI_MRI_TYPE) >> SUNI_MRI_TYPE_SHIFT;PUT(mri | SUNI_MRI_RESET,MRI);PUT(mri,MRI);PUT((GET(MT) & SUNI_MT_DS27_53),MT); /* disable all tests ": "suni_init(struct atm_dev  dev){unsigned char mri;if (!(dev->phy_data = kmalloc(sizeof(struct suni_priv),GFP_KERNEL)))return -ENOMEM;PRIV(dev)->dev = dev;mri = GET(MRI);   reset SUNI ", "if (!qcom_icc_rpm_smd_available())return -EPROBE_DEFER;desc = of_device_get_match_data(dev);if (!desc)return -EINVAL;qnodes = desc->nodes;num_nodes = desc->num_nodes;if (desc->num_intf_clocks) ": "qnoc_probe(struct platform_device  pdev){struct device  dev = &pdev->dev;const struct qcom_icc_desc  desc;struct icc_onecell_data  data;struct icc_provider  provider;struct qcom_icc_node   const  qnodes;struct qcom_icc_provider  qp;struct icc_node  node;size_t num_nodes, i;const char   const  cds = NULL;int cd_num;int ret;  wait for the RPM proxy ", "struct task_struct init_task#ifdef CONFIG_ARCH_TASK_STRUCT_ON_STACK__init_task_data#endif__aligned(L1_CACHE_BYTES)= ": "init_task.h>#include <linuxexport.h>#include <linuxmqueue.h>#include <linuxsched.h>#include <linuxschedsysctl.h>#include <linuxschedrt.h>#include <linuxschedtask.h>#include <linuxinit.h>#include <linuxfs.h>#include <linuxmm.h>#include <linuxaudit.h>#include <linuxnuma.h>#include <linuxscs.h>#include <linuxuaccess.h>static struct signal_struct init_signals = {.nr_threads= 1,.thread_head= LIST_HEAD_INIT(init_task.thread_node),.wait_chldexit= __WAIT_QUEUE_HEAD_INITIALIZER(init_signals.wait_chldexit),.shared_pending= {.list = LIST_HEAD_INIT(init_signals.shared_pending.list),.signal =  {{0}}},.multiprocess= HLIST_HEAD_INIT,.rlim= INIT_RLIMITS,.cred_guard_mutex = __MUTEX_INITIALIZER(init_signals.cred_guard_mutex),.exec_update_lock = __RWSEM_INITIALIZER(init_signals.exec_update_lock),#ifdef CONFIG_POSIX_TIMERS.posix_timers = LIST_HEAD_INIT(init_signals.posix_timers),.cputimer= {.cputime_atomic= INIT_CPUTIME_ATOMIC,},#endifINIT_CPU_TIMERS(init_signals).pids = {[PIDTYPE_PID]= &init_struct_pid,[PIDTYPE_TGID]= &init_struct_pid,[PIDTYPE_PGID]= &init_struct_pid,[PIDTYPE_SID]= &init_struct_pid,},INIT_PREV_CPUTIME(init_signals)};static struct sighand_struct init_sighand = {.count= REFCOUNT_INIT(1),.action= { { { .sa_handler = SIG_DFL, } }, },.siglock= __SPIN_LOCK_UNLOCKED(init_sighand.siglock),.signalfd_wqh= __WAIT_QUEUE_HEAD_INITIALIZER(init_sighand.signalfd_wqh),};#ifdef CONFIG_SHADOW_CALL_STACKunsigned long init_shadow_call_stack[SCS_SIZE  sizeof(long)]__init_task_data = {[(SCS_SIZE  sizeof(long)) - 1] = SCS_END_MAGIC};#endif    Set up the first task table, touch at your own risk!. Base=0,   limit=0x1fffff (=2MB) ", "kuid_t make_kuid(struct user_namespace *ns, uid_t uid)": "make_kuid - Map a user-namespace uid pair into a kuid.  @ns:  User namespace that the uid is in  @uid: User identifier    Maps a user-namespace uid pair into a kernel internal kuid,  and returns that kuid.    When there is no mapping defined for the user-namespace uid  pair INVALID_UID is returned.  Callers are expected to test  for and handle INVALID_UID being returned.  INVALID_UID  may be tested for using uid_valid(). ", "uid_t from_kuid(struct user_namespace *targ, kuid_t kuid)": "from_kuid - Create a uid from a kuid user-namespace pair.  @targ: The user namespace we want a uid in.  @kuid: The kernel internal uid to start with.    Map @kuid into the user-namespace specified by @targ and  return the resulting uid.    There is always a mapping into the initial user_namespace.    If @kuid has no mapping in @targ (uid_t)-1 is returned. ", "uid_t from_kuid_munged(struct user_namespace *targ, kuid_t kuid)": "from_kuid_munged - Create a uid from a kuid user-namespace pair.  @targ: The user namespace we want a uid in.  @kuid: The kernel internal uid to start with.    Map @kuid into the user-namespace specified by @targ and  return the resulting uid.    There is always a mapping into the initial user_namespace.    Unlike from_kuid from_kuid_munged never fails and always  returns a valid uid.  This makes from_kuid_munged appropriate  for use in syscalls like stat and getuid where failing the  system call and failing to provide a valid uid are not an  options.    If @kuid has no mapping in @targ overflowuid is returned. ", "kgid_t make_kgid(struct user_namespace *ns, gid_t gid)": "make_kgid - Map a user-namespace gid pair into a kgid.  @ns:  User namespace that the gid is in  @gid: group identifier    Maps a user-namespace gid pair into a kernel internal kgid,  and returns that kgid.    When there is no mapping defined for the user-namespace gid  pair INVALID_GID is returned.  Callers are expected to test  for and handle INVALID_GID being returned.  INVALID_GID may be  tested for using gid_valid(). ", "gid_t from_kgid(struct user_namespace *targ, kgid_t kgid)": "from_kgid - Create a gid from a kgid user-namespace pair.  @targ: The user namespace we want a gid in.  @kgid: The kernel internal gid to start with.    Map @kgid into the user-namespace specified by @targ and  return the resulting gid.    There is always a mapping into the initial user_namespace.    If @kgid has no mapping in @targ (gid_t)-1 is returned. ", "gid_t from_kgid_munged(struct user_namespace *targ, kgid_t kgid)": "from_kgid_munged - Create a gid from a kgid user-namespace pair.  @targ: The user namespace we want a gid in.  @kgid: The kernel internal gid to start with.    Map @kgid into the user-namespace specified by @targ and  return the resulting gid.    There is always a mapping into the initial user_namespace.    Unlike from_kgid from_kgid_munged never fails and always  returns a valid gid.  This makes from_kgid_munged appropriate  for use in syscalls like stat and getgid where failing the  system call and failing to provide a valid gid are not options.    If @kgid has no mapping in @targ overflowgid is returned. ", "kprojid_t make_kprojid(struct user_namespace *ns, projid_t projid)": "make_kprojid - Map a user-namespace projid pair into a kprojid.  @ns:  User namespace that the projid is in  @projid: Project identifier    Maps a user-namespace uid pair into a kernel internal kuid,  and returns that kuid.    When there is no mapping defined for the user-namespace projid  pair INVALID_PROJID is returned.  Callers are expected to test  for and handle INVALID_PROJID being returned.  INVALID_PROJID  may be tested for using projid_valid(). ", "projid_t from_kprojid(struct user_namespace *targ, kprojid_t kprojid)": "from_kprojid - Create a projid from a kprojid user-namespace pair.  @targ: The user namespace we want a projid in.  @kprojid: The kernel internal project identifier to start with.    Map @kprojid into the user-namespace specified by @targ and  return the resulting projid.    There is always a mapping into the initial user_namespace.    If @kprojid has no mapping in @targ (projid_t)-1 is returned. ", "projid_t from_kprojid_munged(struct user_namespace *targ, kprojid_t kprojid)": "from_kprojid_munged - Create a projiid from a kprojid user-namespace pair.  @targ: The user namespace we want a projid in.  @kprojid: The kernel internal projid to start with.    Map @kprojid into the user-namespace specified by @targ and  return the resulting projid.    There is always a mapping into the initial user_namespace.    Unlike from_kprojid from_kprojid_munged never fails and always  returns a valid projid.  This makes from_kprojid_munged  appropriate for use in syscalls like stat and where  failing the system call and failing to provide a valid projid are  not an options.    If @kprojid has no mapping in @targ OVERFLOW_PROJID is returned. ", "bool thread_group_exited(struct pid *pid)": "thread_group_exited - check that a thread group has exited   @pid: tgid of thread group to be checked.     Test if the thread group represented by tgid has exited (all   threads are zombies, dead or completely gone).     Return: true if the thread group has exited. false otherwise. ", "__weak __function_aligned void abort(void)": "abort() cold and drops alignment specified by   -falign-functions=N.     See https:gcc.gnu.orgbugzillashow_bug.cgi?id=88345#c11 ", "int __cpuhp_setup_state_cpuslocked(enum cpuhp_state state,   const char *name, bool invoke,   int (*startup)(unsigned int cpu),   int (*teardown)(unsigned int cpu),   bool multi_instance)": "__cpuhp_setup_state_cpuslocked - Setup the callbacks for an hotplug machine state   @state:The state to setup   @name:Name of the step   @invoke:If true, the startup function is invoked for cpus where  cpu state >= @state   @startup:startup callback function   @teardown:teardown callback function   @multi_instance:State is set up for multiple instances which get  added afterwards.     The caller needs to hold cpus read locked while calling this function.   Return:     On success:        Positive state number if @state is CPUHP_AP_ONLINE_DYN;        0 for all other states     On failure: proper (negative) error code ", "static void cpuhp_rollback_install(int failedcpu, enum cpuhp_state state,   struct hlist_node *node)": "__cpuhp_setup_state on a recoverable failure.     Note: The teardown callbacks for rollback are not allowed to fail! ", "void __cpuhp_remove_state_cpuslocked(enum cpuhp_state state, bool invoke)": "__cpuhp_remove_state_cpuslocked - Remove the callbacks for an hotplug machine state   @state:The state to remove   @invoke:If true, the teardown function is invoked for cpus where  cpu state >= @state     The caller needs to hold cpus read locked while calling this function.   The teardown callback is currently not allowed to fail. Think   about module removal! ", "void __post_watch_notification(struct watch_list *wlist,       struct watch_notification *n,       const struct cred *cred,       u64 id)": "__post_watch_notification - Post an event notification   @wlist: The watch list to post the event to.   @n: The notification record to post.   @cred: The creds of the process that triggered the notification.   @id: The ID to match on the watch.     Post a notification of an event into a set of watch queues and let the users   know.     The size of the notification should be set in n->info & WATCH_INFO_LENGTH and   should be in units of sizeof( n). ", "void put_watch_queue(struct watch_queue *wqueue)": "put_watch_queue(struct kref  kref){struct watch_queue  wqueue =container_of(kref, struct watch_queue, usage);struct watch_filter  wfilter;int i;for (i = 0; i < wqueue->nr_pages; i++)__free_page(wqueue->notes[i]);kfree(wqueue->notes);bitmap_free(wqueue->notes_bitmap);wfilter = rcu_access_pointer(wqueue->filter);if (wfilter)kfree_rcu(wfilter, rcu);kfree_rcu(wqueue, rcu);}     put_watch_queue - Dispose of a ref on a watchqueue.   @wqueue: The watch queue to unref. ", "int add_watch_to_object(struct watch *watch, struct watch_list *wlist)": "add_watch_to_object - Add a watch on an object to a watch list   @watch: The watch to add   @wlist: The watch list to add to     @watch->queue must have been set to point to the queue to post notifications   to and the watch list of the object to be watched.  @watch->cred must also   have been set to the appropriate credentials and a ref taken on them.     The caller must pin the queue and the list both and must hold the list   locked against racing watch additionsremovals. ", "int remove_watch_from_object(struct watch_list *wlist, struct watch_queue *wq,     u64 id, bool all)": "remove_watch_from_object - Remove a watch or all watches from an object.   @wlist: The watch list to remove from   @wq: The watch queue of interest (ignored if @all is true)   @id: The ID of the watch to remove (ignored if @all is true)   @all: True to remove all objects     Remove a specific watch or all watches from an object.  A notification is   sent to the watcher to tell them that this happened. ", "struct watch_queue *get_watch_queue(int fd)": "get_watch_queue - Get a watch queue from its file descriptor.   @fd: The fd to query. ", "audit_log_format(ab, \" res=%d\", allow_changes);audit_log_end(ab);return rc;}static int audit_do_config_change(char *function_name, u32 *to_change, u32 new)": "audit_log_format(ab, \"op=set %s=%u old=%u \", function_name, new, old);audit_log_session_info(ab);rc = audit_log_task_context(ab);if (rc)allow_changes = 0;   Something weird, deny request ", "for (i = 0; i <= AUDIT_LAST_FEATURE; i++) ": "audit_log_task_info(ab);audit_log_format(ab, \" feature=%s old=%u new=%u old_lock=%u new_lock=%u res=%d\", audit_feature_names[which], !!old_feature, !!new_feature, !!old_lock, !!new_lock, res);audit_log_end(ab);}static int audit_set_feature(struct audit_features  uaf){int i;BUILD_BUG_ON(AUDIT_LAST_FEATURE + 1 > ARRAY_SIZE(audit_feature_names));  if there is ever a version 2 we should handle that here ", "static atomic_taudit_lost = ATOMIC_INIT(0);/* Monotonically increasing sum of time the kernel has spent * waiting while the backlog limit is exceeded. ": "audit_log_start [kmalloc of struct audit_buffer]   2) out of memory in audit_log_move [alloc_skb]   3) suppressed due to audit_rate_limit   4) suppressed due to audit_backlog_limit", "if (audit_enabled == AUDIT_LOCKED)allow_changes = 0;elseallow_changes = 1;if (audit_enabled != AUDIT_OFF) ": "audit_log_end(ab);return rc;}static int audit_do_config_change(char  function_name, u32  to_change, u32 new){int allow_changes, rc = 0;u32 old =  to_change;  check if we are locked ", "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt#include <linux/file.h>#include <linux/init.h>#include <linux/types.h>#include <linux/atomic.h>#include <linux/mm.h>#include <linux/export.h>#include <linux/slab.h>#include <linux/err.h>#include <linux/kthread.h>#include <linux/kernel.h>#include <linux/syscalls.h>#include <linux/spinlock.h>#include <linux/rcupdate.h>#include <linux/mutex.h>#include <linux/gfp.h>#include <linux/pid.h>#include <linux/audit.h>#include <net/sock.h>#include <net/netlink.h>#include <linux/skbuff.h>#ifdef CONFIG_SECURITY#include <linux/security.h>#endif#include <linux/freezer.h>#include <linux/pid_namespace.h>#include <net/netns/generic.h>#include \"audit.h\"/* No auditing will take place until audit_initialized == AUDIT_INITIALIZED. * (Initialization happens after skb_init is called.) ": "audit_log  is called,       then a syscall record will be generated automatically for the       current syscall).    5) Netlink interface to user-space.    6) Support low-overhead kernel-based filtering to minimize the       information that must be passed to user-space.     Audit userspace, documentation, tests, and bugissue trackers:   https:github.comlinux-audit ", "# if THREAD_SIZE >= PAGE_SIZE || defined(CONFIG_VMAP_STACK)#  ifdef CONFIG_VMAP_STACK/* * vmalloc() is a bit slow, and calling vfree() enough times will force a TLB * flush.  Try to minimize the number of calls by caching stacks. ": "free_task_struct(struct task_struct  tsk){kmem_cache_free(task_struct_cachep, tsk);}#endif#ifndef CONFIG_ARCH_THREAD_STACK_ALLOCATOR    Allocate pages if THREAD_SIZE is >= PAGE_SIZE, otherwise use a   kmemcache based allocator. ", "raw_cpu_write(watchdog_hardlockup_touched, true);}EXPORT_SYMBOL(arch_touch_nmi_watchdog": "arch_touch_nmi_watchdog(void){    Using __raw here because some code paths have   preemption enabled.  If preemption is enabled   then interrupts should be enabled too, in which   case we shouldn't have to worry about the watchdog   going off. ", "notrace void touch_softlockup_watchdog_sched(void)": "touch_softlockup_watchdog_sched - touch watchdog on scheduler stalls     Call when the scheduler may have stalled for legitimate reasons   preventing the watchdog task from executing - e.g. the scheduler   entering idle state.  This should only be used for scheduler events.   Use touch_softlockup_watchdog() for everything else. ", "bool has_capability(struct task_struct *t, int cap)": "capable(__task_cred(t), ns, cap, CAP_OPT_NONE);rcu_read_unlock();return (ret == 0);}     has_capability - Does a task have a capability in init_user_ns   @t: The task in question   @cap: The capability to be tested for     Return true if the specified task has the given superior capability   currently in effect to the initial user namespace, false if not.     Note that this does not set PF_SUPERPRIV on the task. ", "bool has_capability_noaudit(struct task_struct *t, int cap)": "has_capability_noaudit - Does a task have a capability (unaudited) in the   initial user ns   @t: The task in question   @cap: The capability to be tested for     Return true if the specified task has the given superior capability   currently in effect to init_user_ns, false if not.  Don't write an   audit message for the check.     Note that this does not set PF_SUPERPRIV on the task. ", "bool ns_capable(struct user_namespace *ns, int cap)": "ns_capable_common(struct user_namespace  ns,      int cap,      unsigned int opts){int capable;if (unlikely(!cap_valid(cap))) {pr_crit(\"capable() called with invalid cap=%u\\n\", cap);BUG();}capable = security_capable(current_cred(), ns, cap, opts);if (capable == 0) {current->flags |= PF_SUPERPRIV;return true;}return false;}     ns_capable - Determine if the current task has a superior capability in effect   @ns:  The usernamespace we want the capability in   @cap: The capability to be tested for     Return true if the current task has the given superior capability currently   available for use, false if not.     This sets PF_SUPERPRIV on the task if the capability is available on the   assumption that it's about to be used. ", "bool ns_capable_noaudit(struct user_namespace *ns, int cap)": "ns_capable_noaudit - Determine if the current task has a superior capability   (unaudited) in effect   @ns:  The usernamespace we want the capability in   @cap: The capability to be tested for     Return true if the current task has the given superior capability currently   available for use, false if not.     This sets PF_SUPERPRIV on the task if the capability is available on the   assumption that it's about to be used. ", "bool ns_capable_setid(struct user_namespace *ns, int cap)": "ns_capable_setid - Determine if the current task has a superior capability   in effect, while signalling that this check is being done from within a   setid or setgroups syscall.   @ns:  The usernamespace we want the capability in   @cap: The capability to be tested for     Return true if the current task has the given superior capability currently   available for use, false if not.     This sets PF_SUPERPRIV on the task if the capability is available on the   assumption that it's about to be used. ", "bool file_ns_capable(const struct file *file, struct user_namespace *ns,     int cap)": "file_ns_capable - Determine if the file's opener had a capability in effect   @file:  The file we want to check   @ns:  The usernamespace we want the capability in   @cap: The capability to be tested for     Return true if task that opened the file had a capability in effect   when the file was opened.     This does not set PF_SUPERPRIV because the caller may not   actually be privileged. ", "bool capable_wrt_inode_uidgid(struct mnt_idmap *idmap,      const struct inode *inode, int cap)": "capable_wrt_inode_uidgid - Check nsown_capable and uid and gid mapped   @idmap: idmap of the mount @inode was found from   @inode: The inode in question   @cap: The capability in question     Return true if the current task has the given capability targeted at   its own user namespace and that the given inode's uid and gid are   mapped into the current user namespace. ", "DEFINE_SPINLOCK(dma_spin_lock);/* *If our port doesn't define this it has no PC like DMA ": "free_dma()'.     In order to avoid problems, all processes should allocate resources in   the same sequence and release them in the reverse order.     So, when allocating DMAs and IRQs, first allocate the IRQ, then the DMA.   When releasing them, first release the DMA, then release the IRQ.   If you don't, you may cause allocation requests to fail unnecessarily.   This doesn't really matter now, but it will once we get real semaphores   in the kernel. ", "#ifdef MAX_DMA_CHANNELS/* Channel n is busy iff dma_chan_busy[n].lock != 0. * DMA0 used to be reserved for DRAM refresh, but apparently not any more... * DMA4 is reserved for cascading. ": "dma_spin_lock);   If our port doesn't define this it has no PC like DMA ", "void set_groups(struct cred *new, struct group_info *group_info)": "set_groups - Change a group subscription in a set of credentials   @new: The newly prepared set of credentials to alter   @group_info: The group list to install ", "int set_current_groups(struct group_info *group_info)": "set_current_groups - Change current's group subscription   @group_info: The group list to impose     Validate a group subscription and, if valid, impose it upon current's task   security record. ", "__weak void __iomem *ioremap_cache(resource_size_t offset, unsigned long size)": "memremap ", "BUILD_BUG_ON(CONFIG_STACKLEAK_TRACK_MIN_SIZE > STACKLEAK_SEARCH_DEPTH);/* 'lowest_stack' should be aligned on the register width boundary ": "stackleak_track_stack(void){unsigned long sp = current_stack_pointer;    Having CONFIG_STACKLEAK_TRACK_MIN_SIZE larger than   STACKLEAK_SEARCH_DEPTH makes the poison search in   stackleak_erase() unreliable. Let's prevent that. ", "int proc_dobool(struct ctl_table *table, int write, void *buffer,size_t *lenp, loff_t *ppos)": "proc_dobool - readwrite a bool   @table: the sysctl table   @write: %TRUE if this is a write to the sysctl file   @buffer: the user buffer   @lenp: the size of the user buffer   @ppos: file position     Readswrites one integer value fromto the user buffer,   treated as an ASCII string.     table->data must point to a bool variable and table->maxlen must   be sizeof(bool).     Returns 0 on success. ", "bail_early:*ppos += *lenp;return err;}static int do_proc_douintvec_r(unsigned int *tbl_data, void *buffer,       size_t *lenp, loff_t *ppos,       int (*conv)(unsigned long *lvalp,   unsigned int *valp,   int write, void *data),       void *data)": "proc_douintvec_conv(unsigned long  lvalp,  unsigned int  valp,  int write, void  data){if (write) {if ( lvalp > UINT_MAX)return -EINVAL;WRITE_ONCE( valp,  lvalp);} else {unsigned int val = READ_ONCE( valp); lvalp = (unsigned long)val;}return 0;}static const char proc_wspace_sep[] = { ' ', '\\t', '\\n' };static int __do_proc_dointvec(void  tbl_data, struct ctl_table  table,  int write, void  buffer,  size_t  lenp, loff_t  ppos,  int ( conv)(bool  negp, unsigned long  lvalp, int  valp,      int write, void  data),  void  data){int  i, vleft, first = 1, err = 0;size_t left;char  p;if (!tbl_data || !table->maxlen || ! lenp || ( ppos && !write)) { lenp = 0;return 0;}i = (int  ) tbl_data;vleft = table->maxlen  sizeof( i);left =  lenp;if (!conv)conv = do_proc_dointvec_conv;if (write) {if (proc_first_pos_non_zero_ignore(ppos, table))goto out;if (left > PAGE_SIZE - 1)left = PAGE_SIZE - 1;p = buffer;}for (; left && vleft--; i++, first=0) {unsigned long lval;bool neg;if (write) {proc_skip_spaces(&p, &left);if (!left)break;err = proc_get_long(&p, &left, &lval, &neg,     proc_wspace_sep,     sizeof(proc_wspace_sep), NULL);if (err)break;if (conv(&neg, &lval, i, 1, data)) {err = -EINVAL;break;}} else {if (conv(&neg, &lval, i, 0, data)) {err = -EINVAL;break;}if (!first)proc_put_char(&buffer, &left, '\\t');proc_put_long(&buffer, &left, lval, neg);}}if (!write && !first && left && !err)proc_put_char(&buffer, &left, '\\n');if (write && !err && left)proc_skip_spaces(&p, &left);if (write && first)return err ? : -EINVAL; lenp -= left;out: ppos +=  lenp;return err;}static int do_proc_dointvec(struct ctl_table  table, int write,  void  buffer, size_t  lenp, loff_t  ppos,  int ( conv)(bool  negp, unsigned long  lvalp, int  valp,      int write, void  data),  void  data){return __do_proc_dointvec(table->data, table, write,buffer, lenp, ppos, conv, data);}static int do_proc_douintvec_w(unsigned int  tbl_data,       struct ctl_table  table,       void  buffer,       size_t  lenp, loff_t  ppos,       int ( conv)(unsigned long  lvalp,   unsigned int  valp,   int write, void  data),       void  data){unsigned long lval;int err = 0;size_t left;bool neg;char  p = buffer;left =  lenp;if (proc_first_pos_non_zero_ignore(ppos, table))goto bail_early;if (left > PAGE_SIZE - 1)left = PAGE_SIZE - 1;proc_skip_spaces(&p, &left);if (!left) {err = -EINVAL;goto out_free;}err = proc_get_long(&p, &left, &lval, &neg,     proc_wspace_sep,     sizeof(proc_wspace_sep), NULL);if (err || neg) {err = -EINVAL;goto out_free;}if (conv(&lval, tbl_data, 1, data)) {err = -EINVAL;goto out_free;}if (!err && left)proc_skip_spaces(&p, &left);out_free:if (err)return -EINVAL;return 0;  This is in keeping with old __do_proc_dointvec() ", "int *ip = write ? &tmp : valp;ret = do_proc_dointvec_ms_jiffies_conv(negp, lvalp, ip, write, data);if (ret)return ret;if (write) ": "proc_dointvec_ms_jiffies_conv(bool  negp, unsigned long  lvalp,    int  valp,    int write, void  data){if (write) {unsigned long jif = msecs_to_jiffies( negp ? - lvalp :  lvalp);if (jif > INT_MAX)return 1;WRITE_ONCE( valp, (int)jif);} else {int val = READ_ONCE( valp);unsigned long lval;if (val < 0) { negp = true;lval = -(unsigned long)val;} else { negp = false;lval = (unsigned long)val;} lvalp = jiffies_to_msecs(lval);}return 0;}static int do_proc_dointvec_ms_jiffies_minmax_conv(bool  negp, unsigned long  lvalp,int  valp, int write, void  data){int tmp, ret;struct do_proc_dointvec_minmax_conv_param  param = data;    If writing, first do so via a temporary local int so we can   bounds-check it before touching  valp. ", "struct do_proc_dointvec_minmax_conv_param ": "proc_dointvec_minmax_conv_param - proc_dointvec_minmax() range checking structure   @min: pointer to minimum allowable value   @max: pointer to maximum allowable value     The do_proc_dointvec_minmax_conv_param structure provides the   minimum and maximum values for doing range checking for those sysctl   parameters that use the proc_dointvec_minmax() handler. ", "int proc_dostring(struct ctl_table *table, int write,  void *buffer, size_t *lenp, loff_t *ppos)": "proc_dostring - read a string sysctl   @table: the sysctl table   @write: %TRUE if this is a write to the sysctl file   @buffer: the user buffer   @lenp: the size of the user buffer   @ppos: file position     Readswrites a string fromto the user buffer. If the kernel   buffer provided is not large enough to hold the string, the   string is truncated. The copied string is %NULL-terminated.   If the string is being read by the user process, it is copied   and a newline '\\n' is added. It is truncated if the buffer is   not large enough.     Returns 0 on success. ", "#include <linux/module.h>#include <linux/mm.h>#include <linux/swap.h>#include <linux/slab.h>#include <linux/sysctl.h>#include <linux/bitmap.h>#include <linux/signal.h>#include <linux/panic.h>#include <linux/printk.h>#include <linux/proc_fs.h>#include <linux/security.h>#include <linux/ctype.h>#include <linux/kmemleak.h>#include <linux/filter.h>#include <linux/fs.h>#include <linux/init.h>#include <linux/kernel.h>#include <linux/kobject.h>#include <linux/net.h>#include <linux/sysrq.h>#include <linux/highuid.h>#include <linux/writeback.h>#include <linux/ratelimit.h>#include <linux/hugetlb.h>#include <linux/initrd.h>#include <linux/key.h>#include <linux/times.h>#include <linux/limits.h>#include <linux/dcache.h>#include <linux/syscalls.h>#include <linux/vmstat.h>#include <linux/nfs_fs.h>#include <linux/acpi.h>#include <linux/reboot.h>#include <linux/ftrace.h>#include <linux/perf_event.h>#include <linux/oom.h>#include <linux/kmod.h>#include <linux/capability.h>#include <linux/binfmts.h>#include <linux/sched/sysctl.h>#include <linux/mount.h>#include <linux/userfaultfd_k.h>#include <linux/pid.h>#include \"../lib/kstrtox.h\"#include <linux/uaccess.h>#include <asm/processor.h>#ifdef CONFIG_X86#include <asm/nmi.h>#include <asm/stacktrace.h>#include <asm/io.h>#endif#ifdef CONFIG_SPARC#include <asm/setup.h>#endif#ifdef CONFIG_RT_MUTEXES#include <linux/rtmutex.h>#endif/* shared constants to be used in various sysctls ": "proc_doulongvec_ms_jiffies_minmax, 090899, Carlos H. Bauer.   Added proc_doulongvec_minmax, 090899, Carlos H. Bauer.   Changed linked lists to use list.h instead of lists.h, 022400, Bill    Wendling.   The list_for_each() macro wasn't appropriate for the sysctl loop.    Removed it and replaced it with older style, 032300, Bill Wendling ", "int proc_do_large_bitmap(struct ctl_table *table, int write, void *buffer, size_t *lenp, loff_t *ppos)": "proc_do_large_bitmap - readwrite fromto a large bitmap   @table: the sysctl table   @write: %TRUE if this is a write to the sysctl file   @buffer: the user buffer   @lenp: the size of the user buffer   @ppos: file position     The bitmap is stored at table->data and the bitmap length (in bits)   in table->maxlen.     We use a range comma separated format (e.g. 1,3-4,10-10) so that   large bitmaps may be represented in a compact manner. Writing into   the file will clear the bitmap then update it with the given input.     Returns 0 on success. ", "if (slab_is_available()) ": "param_set_charp(const char  val, const struct kernel_param  kp){if (strlen(val) > 1024) {pr_err(\"%s: string parameter too long\\n\", kp->name);return -ENOSPC;}maybe_kfree_parameter( (char   )kp->arg);  This is a hack.  We can't kmalloc in early boot, and we   don't need to; this mangled commandline is preserved. ", "if (!val) val = \"1\";/* One of =[yYnN01] ": "param_set_bool(const char  val, const struct kernel_param  kp){  No equals means \"set\"... ", "return sprintf(buffer, \"%c\\n\", *(bool *)kp->arg ? 'Y' : 'N');}EXPORT_SYMBOL(param_get_bool": "param_get_bool(char  buffer, const struct kernel_param  kp){  Y and N chosen as being relatively non-coder friendly ", "struct kernel_param boolkp = *kp;bool v;int ret;boolkp.arg = &v;ret = param_set_bool(val, &boolkp);if (ret == 0)*(int *)kp->arg = v;return ret;}EXPORT_SYMBOL(param_set_bint": "param_set_bint(const char  val, const struct kernel_param  kp){  Match bool exactly, by re-using it. ", "char *parse_args(const char *doing, char *args, const struct kernel_param *params, unsigned num, s16 min_level, s16 max_level, void *arg, int (*unknown)(char *param, char *val,const char *doing, void *arg))": "kernel_param_unlock(params[i].mod);return err;}}if (handle_unknown) {pr_debug(\"doing %s: %s='%s'\\n\", doing, param, val);return handle_unknown(param, val, doing, arg);}pr_debug(\"Unknown argument '%s'\\n\", param);return -ENOENT;}  Args looks like \"foo=bar,bar2 baz=fuz wiz\". ", "bool kthread_should_stop(void)": "kthread_stop() on your kthread, it will be woken   and this will return true.  You should then return, and your return   value will be passed through to kthread_stop(). ", "void __noreturn kthread_complete_and_exit(struct completion *comp, long code)": "kthread_complete_and_exit - Exit the current kthread.   @comp: Completion to complete   @code: The integer value to return to kthread_stop().     If present, complete @comp and then return code to kthread_stop().     A kernel thread whose module may be removed after the completion of   @comp can use this function to exit safely.     Does not return. ", "if (unlikely(wait_for_completion_killable(&done))) ": "kthread_create_on_node(int ( threadfn)(void  data),    void  data, int node,    const char namefmt[],    va_list args){DECLARE_COMPLETION_ONSTACK(done);struct task_struct  task;struct kthread_create_info  create = kmalloc(sizeof( create),     GFP_KERNEL);if (!create)return ERR_PTR(-ENOMEM);create->threadfn = threadfn;create->data = data;create->node = node;create->done = &done;create->full_name = kvasprintf(GFP_KERNEL, namefmt, args);if (!create->full_name) {task = ERR_PTR(-ENOMEM);goto free_create;}spin_lock(&kthread_create_lock);list_add_tail(&create->list, &kthread_create_list);spin_unlock(&kthread_create_lock);wake_up_process(kthreadd_task);    Wait for completion in killable state, for I might be chosen by   the OOM killer while kthreadd is trying to allocate memory for   new kernel thread. ", "raw_spin_lock_irqsave(&p->pi_lock, flags);do_set_cpus_allowed(p, mask);p->flags |= PF_NO_SETAFFINITY;raw_spin_unlock_irqrestore(&p->pi_lock, flags);}static void __kthread_bind(struct task_struct *p, unsigned int cpu, unsigned int state)": "kthread_bind_mask(struct task_struct  p, const struct cpumask  mask, unsigned int state){unsigned long flags;if (!wait_task_inactive(p, state)) {WARN_ON(1);return;}  It's safe because the task is inactive. ", "struct task_struct *kthread_create_on_cpu(int (*threadfn)(void *data),  void *data, unsigned int cpu,  const char *namefmt)": "kthread_create_on_cpu - Create a cpu bound kthread   @threadfn: the function to run until signal_pending(current).   @data: data ptr for @threadfn.   @cpu: The cpu on which the thread should be bound,   @namefmt: printf-style name for the thread. Format is restricted       to \"name. %u\". Code fills in cpu number.     Description: This helper function creates and names a kernel thread ", "WARN_ON(worker->task && worker->task != current);worker->task = current;if (worker->flags & KTW_FREEZABLE)set_freezable();repeat:set_current_state(TASK_INTERRUPTIBLE);/* mb paired w/ kthread_stop ": "kthread_create_worker () functions. ", "struct kthread_worker *kthread_create_worker_on_cpu(int cpu, unsigned int flags,     const char namefmt[], ...)": "kthread_create_worker_on_cpu - create a kthread worker and bind it  to a given CPU and the associated NUMA node.   @cpu: CPU number   @flags: flags modifying the default behavior of the worker   @namefmt: printf-style name for the kthread worker (task).     Use a valid CPU number if you want to bind the kthread worker   to the given CPU and the associated NUMA node.     A good practice is to add the cpu number also into the worker name.   For example, use kthread_create_worker_on_cpu(cpu, \"helper%d\", cpu).     CPU hotplug:   The kthread worker API is simple and generic. It just provides a way   to create, use, and destroy workers.     It is up to the API user how to handle CPU hotplug. They have to decide   how to handle pending work items, prevent queuing new ones, and   restore the functionality when the CPU goes off and on. There are a   few catches:        - CPU affinity gets lost when it is scheduled on an offline CPU.        - The worker might not exist when the CPU was off when the user        created the workers.     Good practice is to implement two CPU hotplug callbacks and to   destroycreate the worker when the CPU goes downup.     Return:   The pointer to the allocated worker on success, ERR_PTR(-ENOMEM)   when the needed structures could not get allocated, and ERR_PTR(-EINTR)   when the caller was killed by a fatal signal. ", "void kthread_delayed_work_timer_fn(struct timer_list *t)": "kthread_delayed_work_timer_fn - callback that queues the associated kthread  delayed work when the timer expires.   @t: pointer to the expired timer     The format of the function is defined by struct timer_list.   It should have been called from irqsafe timer with irq already off. ", "void kthread_destroy_worker(struct kthread_worker *worker)": "kthread_destroy_worker - destroy a kthread worker   @worker: worker to be destroyed     Flush and destroy @worker.  The simple flush is enough because the kthread   worker API is used only in trivial scenarios.  There are no multi-step state   machines needed.     Note that this function is not responsible for handling delayed work, so   caller should be responsible for queuing or canceling all delayed work items   before invoke this function. ", "void kthread_associate_blkcg(struct cgroup_subsys_state *css)": "kthread_associate_blkcg - associate blkcg to current kthread   @css: the cgroup info     Current thread must be a kthread. The thread is running jobs on behalf of   other threads. In some cases, we expect the jobs attach cgroup info of   original threads instead of that of current thread. This function stores   original thread's cgroup info in current kthread context for later   retrieval. ", "return false;}/* * After recalculating TIF_SIGPENDING, we need to make sure the task wakes up. * This is superfluous when called on current, the wakeup is a harmless no-op. ": "recalc_sigpending_tsk(struct task_struct  t){if ((t->jobctl & (JOBCTL_PENDING_MASK | JOBCTL_TRAP_FREEZE)) ||    PENDING(&t->pending, &t->blocked) ||    PENDING(&t->signal->shared_pending, &t->blocked) ||    cgroup_task_frozen(t)) {set_tsk_thread_flag(t, TIF_SIGPENDING);return true;}    We must never clear the flag in another thread, or in current   when it's possible the current syscall is returning -ERESTART .   So we don't clear it here, and only callers who know they should do. ", "HANDLER_SIG_DFL, /* Always use SIG_DFL handler semantics ": "send_sig_info(int sig, struct kernel_siginfo  info, struct task_struct  p,enum pid_type type){unsigned long flags;int ret = -ESRCH;if (lock_task_sighand(p, &flags)) {ret = send_signal_locked(sig, info, p, type);unlock_task_sighand(p, &flags);}return ret;}enum sig_handler {HANDLER_CURRENT,   If reachable use the current handler ", "result = TRACE_SIGNAL_ALREADY_PENDING;if (legacy_queue(pending, sig))goto ret;result = TRACE_SIGNAL_DELIVERED;/* * Skip useless siginfo allocation for SIGKILL and kernel threads. ": "send_signal_locked(int sig, struct kernel_siginfo  info,struct task_struct  t, enum pid_type type, bool force){struct sigpending  pending;struct sigqueue  q;int override_rlimit;int ret = 0, result;lockdep_assert_held(&t->sighand->siglock);result = TRACE_SIGNAL_IGNORED;if (!prepare_signal(sig, t, force))goto ret;pending = (type != PIDTYPE_PID) ? &t->signal->shared_pending : &t->pending;    Short-circuit ignored signals and support queuing   exactly one non-rt signal, so that we can get more   detailed information about the cause of the signal. ", "if (action->sa.sa_handler == SIG_DFL &&    (!t->ptrace || (handler == HANDLER_EXIT)))t->signal->flags &= ~SIGNAL_UNKILLABLE;ret = send_signal_locked(sig, info, t, PIDTYPE_PID);spin_unlock_irqrestore(&t->sighand->siglock, flags);return ret;}int force_sig_info(struct kernel_siginfo *info)": "force_sig_info_to_task(struct kernel_siginfo  info, struct task_struct  t,enum sig_handler handler){unsigned long int flags;int ret, blocked, ignored;struct k_sigaction  action;int sig = info->si_signo;spin_lock_irqsave(&t->sighand->siglock, flags);action = &t->sighand->action[sig-1];ignored = action->sa.sa_handler == SIG_IGN;blocked = sigismember(&t->blocked, sig);if (blocked || ignored || (handler != HANDLER_CURRENT)) {action->sa.sa_handler = SIG_DFL;if (handler == HANDLER_EXIT)action->sa.sa_flags |= SA_IMMUTABLE;if (blocked) {sigdelset(&t->blocked, sig);recalc_sigpending_and_wake(t);}}    Don't clear SIGNAL_UNKILLABLE for traced tasks, users won't expect   debugging to leave init killable. But HANDLER_EXIT is always fatal. ", "int __kill_pgrp_info(int sig, struct kernel_siginfo *info, struct pid *pgrp)": "kill_pgrp_info() sends a signal to a process group: this is what the tty   control characters do (^C, ^Z etc)   - the caller must hold at least a readlock on tasklist_lock ", "}}static int kill_proc_info(int sig, struct kernel_siginfo *info, pid_t pid)": "kill_pid_info(int sig, struct kernel_siginfo  info, struct pid  pid){int error = -ESRCH;struct task_struct  p;for (;;) {rcu_read_lock();p = pid_task(pid, PIDTYPE_PID);if (p)error = group_send_sig_info(sig, info, p, PIDTYPE_TGID);rcu_read_unlock();if (likely(!p || error != -ESRCH))return error;    The task was unhashed in between, try again.  If it   is dead, pid_task() will return NULL, if we race with   de_thread() it will find the new leader. ", "int sigprocmask(int how, sigset_t *set, sigset_t *oldset)": "sigprocmask(), the kernel   interface happily blocks \"unblockable\" signals like SIGKILL   and friends. ", "static int __init csdlock_debug(char *str)": "smp_call_function_single_interrupt);arch_send_call_function_single_ipi(cpu);}}static __always_inline voidsend_call_function_ipi_mask(struct cpumask  mask){trace_ipi_send_cpumask(mask, _RET_IP_,       generic_smp_call_function_single_interrupt);arch_send_call_function_ipi_mask(mask);}static __always_inline voidcsd_do_func(smp_call_func_t func, void  info, struct __call_single_data  csd){trace_csd_function_entry(func, csd);func(info);trace_csd_function_exit(func, csd);}#ifdef CONFIG_CSD_LOCK_WAIT_DEBUGstatic DEFINE_STATIC_KEY_MAYBE(CONFIG_CSD_LOCK_WAIT_DEBUG_DEFAULT, csdlock_debug_enabled);    Parse the csdlock_debug= kernel boot parameter.     If you need to restore the old \"ext\" value that once provided   additional debugging information, reapply the following commits:     de7b09ef658d (\"lockingcsd_lock: Prepare more CSD lock debugging\")   a5aabace5fb8 (\"lockingcsd_lock: Add more data to CSD lock debugging\") ", "#define SCF_WAIT(1U << 0)#define SCF_RUN_LOCAL(1U << 1)static void smp_call_function_many_cond(const struct cpumask *mask,smp_call_func_t func, void *info,unsigned int scf_flags,smp_cond_func_t cond_func)": "smp_call_function_many_cond().     %SCF_WAIT:Wait until function execution is completed   %SCF_RUN_LOCAL:Run also locally if local cpu is set in cpumask ", "__flush_smp_call_function_queue(false);irq_work_run();return 0;}void __init call_function_init(void)": "smp_call_function_queue(bool warn_cpu_offline);int smpcfd_prepare_cpu(unsigned int cpu){struct call_function_data  cfd = &per_cpu(cfd_data, cpu);if (!zalloc_cpumask_var_node(&cfd->cpumask, GFP_KERNEL,     cpu_to_node(cpu)))return -ENOMEM;if (!zalloc_cpumask_var_node(&cfd->cpumask_ipi, GFP_KERNEL,     cpu_to_node(cpu))) {free_cpumask_var(cfd->cpumask);return -ENOMEM;}cfd->csd = alloc_percpu(call_single_data_t);if (!cfd->csd) {free_cpumask_var(cfd->cpumask);free_cpumask_var(cfd->cpumask_ipi);return -ENOMEM;}return 0;}int smpcfd_dead_cpu(unsigned int cpu){struct call_function_data  cfd = &per_cpu(cfd_data, cpu);free_cpumask_var(cfd->cpumask);free_cpumask_var(cfd->cpumask_ipi);free_percpu(cfd->csd);return 0;}int smpcfd_dying_cpu(unsigned int cpu){    The IPIs for the smp-call-function callbacks queued by other   CPUs might arrive late, either due to hardware latencies or   because this CPU disabled interrupts (inside stop-machine)   before the IPIs were sent. So flush out any pending callbacks   explicitly (without waiting for the IPIs to arrive), to   ensure that the outgoing CPU doesn't go offline with work   still pending. ", "pr_alert(\"csd: %s non-responsive CSD lock (#%d) on CPU#%d, waiting %llu ns for CPU#%02d %pS(%ps).\\n\", firsttime ? \"Detected\" : \"Continued\", *bug_id, raw_smp_processor_id(), ts2 - ts0, cpu, csd->func, csd->info);if (cpu_cur_csd && csd != cpu_cur_csd) ": "nr_cpu_ids, \"%s: cpu = %d\\n\", __func__, cpu))cpux = 0;elsecpux = cpu;cpu_cur_csd = smp_load_acquire(&per_cpu(cur_csd, cpux));   Before func and info. ", "int register_reboot_notifier(struct notifier_block *nb)": "register_reboot_notifier - Register function to be called at reboot time  @nb: Info about notifier function to be called    Registers a function with the list of functions  to be called at reboot time.    Currently always returns zero, as blocking_notifier_chain_register()  always returns zero. ", "int unregister_reboot_notifier(struct notifier_block *nb)": "unregister_reboot_notifier - Unregister previously registered reboot notifier  @nb: Hook to be unregistered    Unregisters a previously registered reboot  notifier function.    Returns zero on success, or %-ENOENT on failure. ", "int register_restart_handler(struct notifier_block *nb)": "register_restart_handler - Register function to be called to reset     the system  @nb: Info about handler function to be called  @nb->priority:Handler priority. Handlers should follow the  following guidelines for setting priorities.  0:Restart handler of last resort,  with limited restart capabilities  128:Default restart handler; use if no other  restart handler is expected to be available,  andor if restart functionality is  sufficient to restart the entire system  255:Highest priority restart handler, will  preempt all other restart handlers    Registers a function with code to be called to restart the  system.    Registered functions will be called from machine_restart as last  step of the restart sequence (if the architecture specific  machine_restart function calls do_kernel_restart - see below  for details).  Registered functions are expected to restart the system immediately.  If more than one function is registered, the restart handler priority  selects which function will be called first.    Restart handlers are expected to be registered from non-architecture  code, typically from drivers. A typical use case would be a system  where restart functionality is provided through a watchdog. Multiple  restart handlers may exist; for example, one restart handler might  restart the entire system, while another only restarts the CPU.  In such cases, the restart handler which only restarts part of the  hardware is expected to register with low priority to ensure that  it only runs if no other means to restart the system is available.    Currently always returns zero, as atomic_notifier_chain_register()  always returns zero. ", "int unregister_restart_handler(struct notifier_block *nb)": "unregister_restart_handler - Unregister previously registered       restart handler  @nb: Hook to be unregistered    Unregisters a previously registered restart handler function.    Returns zero on success, or %-ENOENT on failure. ", "if (!current->softirq_disable_cnt) ": "__local_bh_disable_ip(unsigned long ip, unsigned int cnt){unsigned long flags;int newcnt;WARN_ON_ONCE(in_hardirq());  First entry of a task into a BH disabled section? ", "if (curcnt != cnt)goto out;pending = local_softirq_pending();if (!pending)goto out;/* * If this was called from non preemptible context, wake up the * softirq daemon. ": "__local_bh_enable_ip(unsigned long ip, unsigned int cnt){bool preempt_on = preemptible();unsigned long flags;u32 pending;int curcnt;WARN_ON_ONCE(in_hardirq());lockdep_assert_irqs_enabled();local_irq_save(flags);curcnt = __this_cpu_read(softirq_ctrl.cnt);    If this is not reenabling soft interrupts, no point in trying to   run pending ones. ", "local_bh_disable();local_bh_enable();} else ": "tasklet_unlock_spin_wait(struct tasklet_struct  t){while (test_bit(TASKLET_STATE_RUN, &(t)->state)) {if (IS_ENABLED(CONFIG_PREEMPT_RT)) {    Prevent a live lock when current preempted soft   interrupt processing or prevents ksoftirqd from   running. If the tasklet runs on a different CPU   then this has no effect other than doing the BH   disableenable dance for nothing. ", "bool freezing_slow_path(struct task_struct *p)": "freezing_slow_path - slow path for testing whether a task needs to be frozen   @p: task to be tested     This function is called by freezing() if freezer_active isn't zero   and tests whether @p needs to enter and stay in frozen state.  Can be   called under any context.  The freezers are responsible for ensuring the   target tasks see the updated state. ", "bool set_freezable(void)": "set_freezable - make %current freezable     Mark %current freezable and enter refrigerator if necessary. ", "struct work_structunbound_release_work;struct rcu_headrcu;} __aligned(1 << WORK_STRUCT_FLAG_BITS);/* * Structure used to wait for workqueue flush. ": "system_wq.  See put_pwq()   and pwq_unbound_release_workfn() for details.  pool_workqueue   itself is also RCU protected so that the first pwq can be   determined without grabbing wq->mutex. ", "smp_mb();}static void clear_work_data(struct work_struct *work)": "queue_work_on() {   3    test_and_set_bit(PENDING)   4 }                             set_..._and_clear_pending() {   5                                 set_work_data() # clear bit   6                                 smp_mb()   7                               work->current_func() {   8      LOAD event_indicated     }     Without an explicit full barrier speculative LOAD on line 8 can   be executed before CPU#0 does STORE on line 1.  If that happens,   CPU#0 observes the PENDING bit is still set and new execution of   a @work is not queued in a hope, that CPU#1 will eventually   finish the queued @work.  Meanwhile CPU#1 does not see   event_indicated is set, because speculative LOAD was executed   before actual STORE. ", "__queue_work(dwork->cpu, dwork->wq, &dwork->work);}EXPORT_SYMBOL(delayed_work_timer_fn": "delayed_work_timer_fn(struct timer_list  t){struct delayed_work  dwork = from_timer(dwork, t, timer);  should have been called from irqsafe timer with irq already off ", "bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,   struct delayed_work *dwork, unsigned long delay)": "queue_delayed_work_on - queue work on specific CPU after delay   @cpu: CPU number to execute work on   @wq: workqueue to use   @dwork: work to queue   @delay: number of jiffies to wait before queueing     Return: %false if @work was already on a queue, %true otherwise.  If   @delay is zero and @dwork is idle, it will be scheduled for immediate   execution. ", "bool queue_rcu_work(struct workqueue_struct *wq, struct rcu_work *rwork)": "queue_rcu_work - queue work after a RCU grace period   @wq: workqueue to use   @rwork: work to queue     Return: %false if @rwork was already pending, %true otherwise.  Note   that a full RCU grace period is guaranteed only after a %true return.   While @rwork is guaranteed to be executed after a %false return, the   execution may happen before a full RCU grace period has passed. ", "void __flush_workqueue(struct workqueue_struct *wq)": "__flush_workqueue - ensure that any scheduled work has run to completion.   @wq: workqueue to flush     This function sleeps until all work items which were queued on entry   have finished execution, but it is not livelocked by new incoming ones. ", "bool flush_delayed_work(struct delayed_work *dwork)": "flush_delayed_work - wait for a dwork to finish executing the last queueing   @dwork: the delayed work to flush     Delayed timer is cancelled and the pending work is queued for   immediate execution.  Like flush_work(), this function only   considers the last queueing instance of @dwork.     Return:   %true if flush_work() waited for the work to finish execution,   %false if it was already idle. ", "bool flush_rcu_work(struct rcu_work *rwork)": "flush_rcu_work - wait for a rwork to finish executing the last queueing   @rwork: the rcu work to flush     Return:   %true if flush_rcu_work() waited for the work to finish execution,   %false if it was already idle. ", "static bool work_fixup_free(void *addr, enum debug_obj_state state)": "cancel_work_sync(work);debug_object_init(work, &work_debug_descr);return true;default:return false;}}    fixup_free is called when:   - an active object is freed ", "bool cancel_work_sync(struct work_struct *work)": "cancel_delayed_work_sync() instead.     The caller must ensure that the workqueue on which @work was last   queued can't be destroyed before this function returns.     Return:   %true if @work was pending, %false otherwise. ", "static void move_linked_works(struct work_struct *work, struct list_head *head,      struct work_struct **nextp)": "current_work == work &&    worker->current_func == work->func)return worker;return NULL;}     move_linked_works - move linked works to a list   @work: start of series of works to be scheduled   @head: target list to append @work to   @nextp: out parameter for nested worklist walking     Schedule linked works starting from @work to @head.  Work series to   be scheduled starts at @work and includes any consecutive work with   WORK_STRUCT_LINKED set in its predecessor.     If @nextp is not NULL, it's updated to point to the next work of   the last scheduled work.  This allows move_linked_works() to be   nested inside outer list_for_each_entry_safe().     CONTEXT:   raw_spin_lock_irq(pool->lock). ", "struct subprocess_info *call_usermodehelper_setup(const char *path, char **argv,char **envp, gfp_t gfp_mask,int (*init)(struct subprocess_info *info, struct cred *new),void (*cleanup)(struct subprocess_info *info),void *data)": "call_usermodehelper_setup - prepare to call a usermode helper   @path: path to usermode executable   @argv: arg vector for process   @envp: environment for process   @gfp_mask: gfp mask for memory allocation   @init: an init function   @cleanup: a cleanup function   @data: arbitrary context sensitive data     Returns either %NULL on allocation failure, or a subprocess_info   structure.  This should be passed to call_usermodehelper_exec to   exec the process and free the structure.     The init function is used to customize the helper process prior to   exec.  A non-zero return code causes the process to error out, exit,   and return the failure to the calling process     The cleanup function is just before the subprocess_info is about to   be freed.  This can be used for freeing the argv and envp.  The   Function must be runnable in either a process context or the   context in which call_usermodehelper_exec is called. ", "if (comp)complete(comp);elsecall_usermodehelper_freeinfo(sub_info);}/* * This is the task which runs the usermode application ": "call_usermodehelper_freeinfo(struct subprocess_info  info){if (info->cleanup)( info->cleanup)(info);kfree(info);}static void umh_complete(struct subprocess_info  sub_info){struct completion  comp = xchg(&sub_info->complete, NULL);    See call_usermodehelper_exec(). If xchg() returns NULL   we own sub_info, the UMH_KILLABLE caller has gone away   or the caller used UMH_NO_WAIT. ", "void __weak crash_smp_send_stop(void)": "nmi_panic_self_stop(struct pt_regs  regs){panic_smp_self_stop();}    Stop other CPUs in panic.  Architecture dependent code may override this   with more suitable version.  For example, if the architecture supports   crash dump, it should save registers of each stopped CPU and disable   per-CPU features such as virtualization extensions. ", "/* * This function is used through-out the kernel (including mm and fs) * to indicate a major problem. ": "panic.c      Copyright (C) 1991, 1992  Linus Torvalds ", "kgdb_panic(buf);/* * If we have crashed and we have a crash kernel loaded let it handle * everything else. * If we want to run this after calling panic_notifiers, pass * the \"crash_kexec_post_notifiers\" option to the kernel. * * Bypass the panic_cpu check and call __crash_kexec directly. ": "test_taint(TAINT_DIE) && oops_in_progress <= 1)dump_stack();#endif    If kgdb is enabled, give it a chance to run before we stop all   the other CPUs or else we won't be able to debug processes left   running on them. ", "void add_taint(unsigned flag, enum lockdep_ok lockdep_ok)": "add_taint: add a taint flag if not already set.   @flag: one of the TAINT_  constants.   @lockdep_ok: whether lock debugging is still OK.     If something bad has gone wrong, you'll want @lockdebug_ok = false, but for   some notewortht-but-not-corrupting cases, it can be set to true. ", "int padata_do_parallel(struct padata_shell *ps,       struct padata_priv *padata, int *cb_cpu)": "padata_do_serial.     Return: 0 on success or else negative error code. ", "int padata_set_cpumask(struct padata_instance *pinst, int cpumask_type,       cpumask_var_t cpumask)": "padata_set_cpumasks(struct padata_instance  pinst, cpumask_var_t pcpumask, cpumask_var_t cbcpumask){int valid;int err;valid = padata_validate_cpumask(pinst, pcpumask);if (!valid) {__padata_stop(pinst);goto out_replace;}valid = padata_validate_cpumask(pinst, cbcpumask);if (!valid)__padata_stop(pinst);out_replace:cpumask_copy(pinst->cpumask.pcpu, pcpumask);cpumask_copy(pinst->cpumask.cbcpu, cbcpumask);err = padata_setup_cpumasks(pinst) ?: padata_replace(pinst);if (valid)__padata_start(pinst);return err;}     padata_set_cpumask - Sets specified by @cpumask_type cpumask to the value                        equivalent to @cpumask.   @pinst: padata instance   @cpumask_type: PADATA_CPU_SERIAL or PADATA_CPU_PARALLEL corresponding                  to parallel and serial cpumasks respectively.   @cpumask: the cpumask to use     Return: 0 on success or negative error code ", "static int padata_replace_one(struct padata_shell *ps)": "padata_alloc_pd(struct padata_shell  ps){struct padata_instance  pinst = ps->pinst;struct parallel_data  pd;pd = kzalloc(sizeof(struct parallel_data), GFP_KERNEL);if (!pd)goto err;pd->reorder_list = alloc_percpu(struct padata_list);if (!pd->reorder_list)goto err_free_pd;pd->squeue = alloc_percpu(struct padata_serial_queue);if (!pd->squeue)goto err_free_reorder_list;pd->ps = ps;if (!alloc_cpumask_var(&pd->cpumask.pcpu, GFP_KERNEL))goto err_free_squeue;if (!alloc_cpumask_var(&pd->cpumask.cbcpu, GFP_KERNEL))goto err_free_pcpu;cpumask_and(pd->cpumask.pcpu, pinst->cpumask.pcpu, cpu_online_mask);cpumask_and(pd->cpumask.cbcpu, pinst->cpumask.cbcpu, cpu_online_mask);padata_init_reorder_list(pd);padata_init_squeues(pd);pd->seq_nr = -1;refcount_set(&pd->refcnt, 1);spin_lock_init(&pd->lock);pd->cpu = cpumask_first(pd->cpumask.pcpu);INIT_WORK(&pd->reorder_work, invoke_padata_reorder);return pd;err_free_pcpu:free_cpumask_var(pd->cpumask.pcpu);err_free_squeue:free_percpu(pd->squeue);err_free_reorder_list:free_percpu(pd->reorder_list);err_free_pd:kfree(pd);err:return NULL;}static void padata_free_pd(struct parallel_data  pd){free_cpumask_var(pd->cpumask.pcpu);free_cpumask_var(pd->cpumask.cbcpu);free_percpu(pd->reorder_list);free_percpu(pd->squeue);kfree(pd);}static void __padata_start(struct padata_instance  pinst){pinst->flags |= PADATA_INIT;}static void __padata_stop(struct padata_instance  pinst){if (!(pinst->flags & PADATA_INIT))return;pinst->flags &= ~PADATA_INIT;synchronize_rcu();}  Replace the internal control structure with a new one. ", "void*pw_data;};static DEFINE_SPINLOCK(padata_works_lock);static struct padata_work *padata_works;static LIST_HEAD(padata_free_works);struct padata_mt_job_state ": "padata_free_works linkage ", "struct padata_shell *padata_alloc_shell(struct padata_instance *pinst)": "padata_alloc_shell - Allocate and initialize padata shell.     @pinst: Parent padata_instance object.     Return: new shell on success, NULL on error ", "void padata_free_shell(struct padata_shell *ps)": "padata_free_shell - free a padata shell     @ps: padata shell to free ", "void __put_cred(struct cred *cred)": "__put_cred - Destroy a set of credentials   @cred: The record to release     Destroy a set of credentials on which no references remain. ", "const struct cred *get_task_cred(struct task_struct *task)": "get_task_cred - Get another task's objective credentials   @task: The task to query     Get the objective credentials of a task, pinning them so that they can't go   away.  Accessing a task's credentials directly is not permitted.     The caller must also make sure task doesn't get deleted, either by holding a   ref on task or by holding tasklist_lock to prevent it from being unlinked. ", "struct cred *prepare_creds(void)": "abort_creds(new);return NULL;}     prepare_creds - Prepare a new set of credentials for modification     Prepare a new set of task credentials for modification.  A task's creds   shouldn't generally be modified directly, therefore this function is used to   prepare a new copy, which the caller then modifies and then commits by   calling commit_creds().     Preparation involves making a copy of the objective creds for modification.     Returns a pointer to the new creds-to-be if successful, NULL otherwise.     Call commit_creds() or abort_creds() to clean up. ", "const struct cred *override_creds(const struct cred *new)": "override_creds - Override the current process's subjective credentials   @new: The credentials to be assigned     Install a set of temporary override subjective credentials on the current   process, returning the old set for later reversion. ", "void revert_creds(const struct cred *old)": "revert_creds - Revert a temporary subjective credentials override   @old: The credentials to be restored     Revert a temporary set of override subjective credentials to an old set,   discarding the override set. ", "int cred_fscmp(const struct cred *a, const struct cred *b)": "cred_fscmp - Compare two credentials with respect to filesystem access.   @a: The first credential   @b: The second credential     cred_cmp() will return zero if both credentials have the same   fsuid, fsgid, and supplementary groups.  That is, if they will both   provide the same access to files based on modeuidgid.   If the credentials are different, then either -1 or 1 will   be returned depending on whether @a comes before or after @b   respectively in an arbitrary, but stable, ordering of credentials.     Return: -1, 0, or 1 depending on comparison ", "struct cred *prepare_kernel_cred(struct task_struct *daemon)": "prepare_kernel_cred - Prepare a set of credentials for a kernel service   @daemon: A userspace daemon to be used as a reference     Prepare a set of credentials for a kernel service.  This can then be used to   override a task's own credentials so that work can be done on behalf of that   task that requires a different subjective context.     @daemon is used to provide a base cred, with the security data derived from   that; if this is \"&init_task\", they'll be set to 0, no groups, full   capabilities, and no keys.     The caller may change these controls afterwards if desired.     Returns the new credentials or NULL if out of memory. ", "int set_security_override(struct cred *new, u32 secid)": "set_security_override - Set the security ID in a set of credentials   @new: The credentials to alter   @secid: The LSM security ID to set     Set the LSM security ID in a set of credentials so that the subjective   security is overridden when an alternative set of credentials is used. ", "int set_security_override_from_ctx(struct cred *new, const char *secctx)": "set_security_override_from_ctx - Set the security ID in a set of credentials   @new: The credentials to alter   @secctx: The LSM security context to generate the security ID from.     Set the LSM security ID in a set of credentials so that the subjective   security is overridden when an alternative set of credentials is used.  The   security ID is specified in string form as a security context to be   interpreted by the LSM. ", "int set_create_files_as(struct cred *new, struct inode *inode)": "set_create_files_as - Set the LSM file create context in a set of credentials   @new: The credentials to alter   @inode: The inode to take the context from     Change the LSM file creation context in a set of credentials to be the same   as the object context of the specified inode, so that the new inodes have   the same MAC context as that inode. ", "size = resource_size(tmp);tmp->start = 0;tmp->end = size - 1;}}void release_child_resources(struct resource *r)": "release_resource(struct resource  old, bool release_child){struct resource  tmp,   p,  chd;p = &old->parent->child;for (;;) {tmp =  p;if (!tmp)break;if (tmp == old) {if (release_child || !(tmp->child)) { p = tmp->sibling;} else {for (chd = tmp->child;; chd = chd->sibling) {chd->parent = tmp->parent;if (!(chd->sibling))break;} p = tmp->child;chd->sibling = tmp->sibling;}old->parent = NULL;return 0;}p = &tmp->sibling;}return -EINVAL;}static void __release_child_resources(struct resource  r){struct resource  tmp,  p;resource_size_t size;p = r->child;r->child = NULL;while (p) {tmp = p;p = p->sibling;tmp->parent = NULL;tmp->sibling = NULL;__release_child_resources(tmp);printk(KERN_DEBUG \"release child resource %pR\\n\", tmp);  need to restore size, and keep flags ", "static int reallocate_resource(struct resource *root, struct resource *old,       resource_size_t newsize,       struct resource_constraint *constraint)": "allocate_resource - allocate a slot in the resource tree given range & alignment.  The resource will be relocated if the new size cannot be reallocated in the  current location.     @root: root resource descriptor   @old:  resource descriptor desired by caller   @newsize: new size of the resource descriptor   @constraint: the size and alignment constraints to be met. ", "int adjust_resource(struct resource *res, resource_size_t start,    resource_size_t size)": "adjust_resource(struct resource  res, resource_size_t start,resource_size_t size){struct resource  tmp,  parent = res->parent;resource_size_t end = start + size - 1;int result = -EBUSY;if (!parent)goto skip;if ((start < parent->start) || (end > parent->end))goto out;if (res->sibling && (res->sibling->start <= end))goto out;tmp = parent->child;if (tmp != res) {while (tmp->sibling != res)tmp = tmp->sibling;if (start <= tmp->end)goto out;}skip:for (tmp = res->child; tmp; tmp = tmp->sibling)if ((tmp->start < start) || (tmp->end > end))goto out;res->start = start;res->end = end;result = 0; out:return result;}     adjust_resource - modify a resource's start and size   @res: resource to modify   @start: new start value   @size: new size     Given an existing resource, change its start and size to match the   arguments.  Returns 0 on success, -EBUSY if it can't fit.   Existing children of the resource are assumed to be immutable. ", "if (conflict->desc == IORES_DESC_DEVICE_PRIVATE_MEMORY) ": "__request_region_locked(struct resource  res, struct resource  parent,   resource_size_t start, resource_size_t n,   const char  name, int flags){DECLARE_WAITQUEUE(wait, current);res->name = name;res->start = start;res->end = start + n - 1;for (;;) {struct resource  conflict;res->flags = resource_type(parent) | resource_ext_type(parent);res->flags |= IORESOURCE_BUSY | flags;res->desc = parent->desc;conflict = __request_resource(parent, res);if (!conflict)break;    mmhmm.c reserves physical addresses which then   become unavailable to other users.  Conflicts are   not expected.  Warn to aid debugging if encountered. ", "void __release_region(struct resource *parent, resource_size_t start,      resource_size_t n)": "__release_region - release a previously reserved resource region   @parent: parent resource descriptor   @start: resource start address   @n: resource region size     The described resource region must match a currently busy region. ", "int devm_request_resource(struct device *dev, struct resource *root,  struct resource *new)": "devm_release_resource()   rather than the regular release_resource().     When a conflict is detected between any existing resources and the newly   requested resource, an error message will be printed.     Returns 0 on success or a negative error code on failure. ", "pos = READ_ONCE(area[0]) + 1;if (likely(pos < t->kcov_size)) ": "__sanitizer_cov_trace_pc(void){struct task_struct  t;unsigned long  area;unsigned long ip = canonicalize_ip(_RET_IP_);unsigned long pos;t = current;if (!check_kcov_mode(KCOV_MODE_TRACE_PC, t))return;area = t->kcov_area;  The first 64-bit word is the number of subsequent PCs. ", "kcov_reset(kcov);spin_unlock_irqrestore(&kcov_remote_lock, flags);}static void kcov_disable(struct task_struct *t, struct kcov *kcov)": "kcov_remote_start(). ", "intsequence;};struct kcov_remote_area ": "kcov_remote_stop(), see the comment there. ", "static inline bool kcov_mode_enabled(unsigned int mode)": "kcov_common_handle()   and passed to the spawned threads via custom annotations. Those kernel   threads must in turn be annotated with kcov_remote_start(common_handle) and   kcov_remote_stop(). All of the threads that are spawned by the same process   obtain the same handle, hence the name \"common\".     See Documentationdev-toolskcov.rst for more details.     Internally, kcov_remote_start() looks up the kcov device associated with the   provided handle, allocates an area for coverage collection, and saves the   pointers to kcov and area into the current task_struct to allow coverage to   be collected via __sanitizer_cov_trace_pc().   In turns kcov_remote_stop() clears those pointers from task_struct to stop   collecting coverage and copies all collected coverage into the kcov area. ", "void __sched mutex_lock(struct mutex *lock)": "mutex_lock_slowpath(struct mutex  lock);     mutex_lock - acquire the mutex   @lock: the mutex to be acquired     Lock the mutex exclusively for this task. If the mutex is not   available right now, it will sleep until it can get it.     The mutex must later on be released by the same task that   acquired it. Recursive locking is not allowed. The task   may not exit without first unlocking the mutex. Also, kernel   memory where the mutex resides must not be freed with   the mutex still locked. The mutex must first be initialized   (or statically defined) before it can be locked. memset()-ing   the mutex to 0 is not allowed.     (The CONFIG_DEBUG_MUTEXES .config option turns on debugging   checks that will enforce the restrictions and will also do   deadlock debugging)     This function is similar to (but not equivalent to) down(). ", "static void__mutex_add_waiter(struct mutex *lock, struct mutex_waiter *waiter,   struct list_head *list)": "mutex_unlock_fast(struct mutex  lock){unsigned long curr = (unsigned long)current;return atomic_long_try_cmpxchg_release(&lock->owner, &curr, 0UL);}#endifstatic inline void __mutex_set_flag(struct mutex  lock, unsigned long flag){atomic_long_or(flag, &lock->owner);}static inline void __mutex_clear_flag(struct mutex  lock, unsigned long flag){atomic_long_andnot(flag, &lock->owner);}static inline bool __mutex_waiter_is_first(struct mutex  lock, struct mutex_waiter  waiter){return list_first_entry(&lock->wait_list, struct mutex_waiter, list) == waiter;}    Add @waiter to a given location in the lock wait_list and set the   FLAG_WAITERS flag if it's the first waiter. ", "void __sched ww_mutex_unlock(struct ww_mutex *lock)": "ww_mutex_unlock - release the ww mutex   @lock: the mutex to be released     Unlock a mutex that has been locked by this task previously with any of the   ww_mutex_lock  functions (with or without an acquire context). It is   forbidden to release the locks after releasing the acquire context.     This function must not be used in interrupt context. Unlocking   of a unlocked mutex is not allowed. ", "int ww_mutex_trylock(struct ww_mutex *ww, struct ww_acquire_ctx *ww_ctx)": "ww_mutex_trylock - tries to acquire the ww mutex with optional acquire context   @ww: mutex to lock   @ww_ctx: optional ww acquire context     Trylocks a mutex with the optional acquire context; no deadlock detection is   possible. Returns 1 if the mutex has been acquired successfully, 0 otherwise.     Unlike ww_mutex_lock, no deadlock handling is performed. However, if a @ctx is   specified, -EALREADY handling may happen in calls to ww_mutex_trylock.     A mutex acquired with this function must be released with ww_mutex_unlock. ", "unsigned long flags = __owner_flags(owner);unsigned long task = owner & ~MUTEX_FLAGS;if (task) ": "mutex_trylock_common(struct mutex  lock, bool handoff){unsigned long owner, curr = (unsigned long)current;owner = atomic_long_read(&lock->owner);for (;;) {   must loop, can race against a flag ", "if (ww_ctx->acquired > 0 && READ_ONCE(ww->ctx))return false;/* * If we aren't on the wait list yet, cancel the spin * if there are waiters. We want  to avoid stealing the * lock from a waiter with an earlier stamp, since the * other thread may already own a lock that we also * need. ": "ww_mutex_lock. ", "int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock)": "atomic_dec_and_mutex_lock - return holding mutex if we dec to 0   @cnt: the atomic which we are to dec   @lock: the mutex to return holding if we dec to 0     return true and hold lock if we dec to 0, return false otherwise ", "#include <linux/compiler.h>#include <linux/kernel.h>#include <linux/export.h>#include <linux/sched.h>#include <linux/sched/debug.h>#include <linux/semaphore.h>#include <linux/spinlock.h>#include <linux/ftrace.h>#include <trace/events/lock.h>static noinline void __down(struct semaphore *sem);static noinline int __down_interruptible(struct semaphore *sem);static noinline int __down_killable(struct semaphore *sem);static noinline int __down_timeout(struct semaphore *sem, long timeout);static noinline void __up(struct semaphore *sem);/** * down - acquire the semaphore * @sem: the semaphore to be acquired * * Acquires the semaphore.  If no more tasks are allowed to acquire the * semaphore, calling this function will put the task to sleep until the * semaphore is released. * * Use of this function is deprecated, please use down_interruptible() or * down_killable() instead. ": "up() can be called from interrupt context, so we   have to disable interrupts when taking the lock.  It turns out various   parts of the kernel expect to be able to use down() on a semaphore in   interrupt context when they know it will succeed, so we have to use   irqsave variants for down(), down_interruptible() and down_killable()   too.     The ->count variable represents how many more tasks can acquire this   semaphore.  If it's zero, there may be tasks waiting on the wait_list. ", "void __sched down(struct semaphore *sem)": "down_timeout(struct semaphore  sem, long timeout);static noinline void __up(struct semaphore  sem);     down - acquire the semaphore   @sem: the semaphore to be acquired     Acquires the semaphore.  If no more tasks are allowed to acquire the   semaphore, calling this function will put the task to sleep until the   semaphore is released.     Use of this function is deprecated, please use down_interruptible() or   down_killable() instead. ", "debug_check_no_locks_freed((void *)lock, sizeof(*lock));lockdep_init_map_wait(&lock->dep_map, name, key, 0, inner);#endiflock->raw_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;lock->magic = SPINLOCK_MAGIC;lock->owner = SPINLOCK_OWNER_INIT;lock->owner_cpu = -1;}EXPORT_SYMBOL(__raw_spin_lock_init": "__raw_spin_lock_init(raw_spinlock_t  lock, const char  name,  struct lock_class_key  key, short inner){#ifdef CONFIG_DEBUG_LOCK_ALLOC    Make sure we are not reinitializing a held lock: ", "debug_check_no_locks_freed((void *)lock, sizeof(*lock));lockdep_init_map_wait(&lock->dep_map, name, key, 0, LD_WAIT_CONFIG);#endiflock->raw_lock = (arch_rwlock_t) __ARCH_RW_LOCK_UNLOCKED;lock->magic = RWLOCK_MAGIC;lock->owner = SPINLOCK_OWNER_INIT;lock->owner_cpu = -1;}EXPORT_SYMBOL(__rwlock_init": "__rwlock_init(rwlock_t  lock, const char  name,   struct lock_class_key  key){#ifdef CONFIG_DEBUG_LOCK_ALLOC    Make sure we are not reinitializing a held lock: ", "extern char __lock_text_start[], __lock_text_end[];return addr >= (unsigned long)__lock_text_start&& addr < (unsigned long)__lock_text_end;}EXPORT_SYMBOL(in_lock_functions": "in_lock_functions(unsigned long addr){  Linker adds these: start and end of __lockfunc functions ", "void __sched rt_mutex_lock_nested(struct rt_mutex *lock, unsigned int subclass)": "mutex_lock_common(struct rt_mutex  lock,  unsigned int state,  struct lockdep_map  nest_lock,  unsigned int subclass){int ret;might_sleep();mutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, _RET_IP_);ret = __rt_mutex_lock(&lock->rtmutex, state);if (ret)mutex_release(&lock->dep_map, _RET_IP_);return ret;}void rt_mutex_base_init(struct rt_mutex_base  rtb){__rt_mutex_base_init(rtb);}EXPORT_SYMBOL(rt_mutex_base_init);#ifdef CONFIG_DEBUG_LOCK_ALLOC     rt_mutex_lock_nested - lock a rt_mutex     @lock: the rt_mutex to be locked   @subclass: the lockdep subclass ", "int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)": "mutex_lock_interruptible - lock a rt_mutex interruptible     @lock:the rt_mutex to be locked     Returns:    0on success   -EINTRwhen interrupted by a signal ", "int __sched rt_mutex_lock_killable(struct rt_mutex *lock)": "mutex_lock_killable - lock a rt_mutex killable     @lock:the rt_mutex to be locked     Returns:    0on success   -EINTRwhen interrupted by a signal ", "int __sched rt_mutex_trylock(struct rt_mutex *lock)": "mutex_trylock - try to lock a rt_mutex     @lock:the rt_mutex to be locked     This function can only be called in thread context. It's safe to call it   from atomic regions, but not from hard or soft interrupt context.     Returns:    1 on success    0 on contention ", "void __sched rt_mutex_unlock(struct rt_mutex *lock)": "mutex_unlock - unlock a rt_mutex     @lock: the rt_mutex to be unlocked ", "void __lockfunc queued_read_lock_slowpath(struct qrwlock *lock)": "queued_read_lock_slowpath - acquire read lock of a queued rwlock   @lock: Pointer to queued rwlock structure ", "void __lockfunc queued_write_lock_slowpath(struct qrwlock *lock)": "queued_write_lock_slowpath - acquire write lock of a queued rwlock   @lock : Pointer to queued rwlock structure ", "/** * queued_spin_lock_slowpath - acquire the queued spinlock * @lock: Pointer to queued spinlock structure * @val: Current value of the queued spinlock 32-bit word * * (queue tail, pending bit, lock value) * *              fast     :    slow                                  :    unlock *                       :                                          : * uncontended  (0,0,0) -:--> (0,0,1) ------------------------------:--> (*,*,0) *                       :       | ^--------.------.             /  : *                       :       v           \\      \\            |  : * pending               :    (0,1,1) +--> (0,1,0)   \\           |  : *                       :       | ^--'              |           |  : *                       :       v                   |           |  : * uncontended           :    (n,x,y) +--> (n,0,0) --'           |  : *   queue               :       | ^--'                          |  : *                       :       v                               |  : * contended             :    (*,x,y) +--> (*,0,0) ---> (*,0,1) -'  : *   queue               :         ^--'                             : ": "queued_spin_lock_slowpathnative_queued_spin_lock_slowpath#endif#endif   _GEN_PV_LOCK_SLOWPATH ", "if (ww_ctx->acquired == 0)ww_ctx->wounded = 0;if (__rt_mutex_trylock(&rtm->rtmutex)) ": "ww_mutex_trylock(struct ww_mutex  lock, struct ww_acquire_ctx  ww_ctx){struct rt_mutex  rtm = &lock->base;if (!ww_ctx)return rt_mutex_trylock(rtm);    Reset the wounded flag after a kill. No other process can   race and wound us here, since they can't have a valid owner   pointer if we don't have any locks held. ", "debug_check_no_locks_freed((void *)sem, sizeof(*sem));lockdep_init_map_wait(&sem->dep_map, name, key, 0, LD_WAIT_SLEEP);#endif#ifdef CONFIG_DEBUG_RWSEMSsem->magic = sem;#endifatomic_long_set(&sem->count, RWSEM_UNLOCKED_VALUE);raw_spin_lock_init(&sem->wait_lock);INIT_LIST_HEAD(&sem->wait_list);atomic_long_set(&sem->owner, 0L);#ifdef CONFIG_RWSEM_SPIN_ON_OWNERosq_lock_init(&sem->osq);#endif}EXPORT_SYMBOL(__init_rwsem": "__init_rwsem(struct rw_semaphore  sem, const char  name,  struct lock_class_key  key){#ifdef CONFIG_DEBUG_LOCK_ALLOC    Make sure we are not reinitializing a held semaphore: ", "#define RWSEM_WRITER_LOCKED(1UL << 0)#define RWSEM_FLAG_WAITERS(1UL << 1)#define RWSEM_FLAG_HANDOFF(1UL << 2)#define RWSEM_FLAG_READFAIL(1UL << (BITS_PER_LONG - 1))#define RWSEM_READER_SHIFT8#define RWSEM_READER_BIAS(1UL << RWSEM_READER_SHIFT)#define RWSEM_READER_MASK(~(RWSEM_READER_BIAS - 1))#define RWSEM_WRITER_MASKRWSEM_WRITER_LOCKED#define RWSEM_LOCK_MASK(RWSEM_WRITER_MASK|RWSEM_READER_MASK)#define RWSEM_READ_FAILED_MASK(RWSEM_WRITER_MASK|RWSEM_FLAG_WAITERS|\\ RWSEM_FLAG_HANDOFF|RWSEM_FLAG_READFAIL)/* * All writes to owner are protected by WRITE_ONCE() to make sure that * store tearing can't happen as optimistic spinners may read and use * the owner value concurrently without lock. Read from owner, however, * may not need READ_ONCE() as long as the pointer value is only used * for comparison and isn't being dereferenced. * * Both rwsem_": "down_read() fastpath   just in case we need to use up more of the reader bits for other purpose   in the future.     atomic_long_fetch_add() is used to obtain reader lock, whereas   atomic_long_cmpxchg() will be used to obtain writer lock.     There are three places where the lock handoff bit may be set or cleared.   1) rwsem_mark_wake() for readers-- set, clear   2) rwsem_try_write_lock() for writers-- set, clear   3) rwsem_del_waiter()-- clear     For all the above cases, wait_lock will be held. A writer must also   be the first one in the wait_list to be eligible for setting the handoff   bit. So concurrent settingclearing of handoff bit is not possible. ", "static inline int __down_write_common(struct rw_semaphore *sem, int state)": "down_read_trylock(struct rw_semaphore  sem){int ret = 0;long tmp;DEBUG_RWSEMS_WARN_ON(sem->magic != sem, sem);preempt_disable();tmp = atomic_long_read(&sem->count);while (!(tmp & RWSEM_READ_FAILED_MASK)) {if (atomic_long_try_cmpxchg_acquire(&sem->count, &tmp,    tmp + RWSEM_READER_BIAS)) {rwsem_set_reader_owned(sem);ret = 1;break;}}preempt_enable();return ret;}    lock for writing ", "if (rwsem_can_spin_on_owner(sem) && rwsem_optimistic_spin(sem)) ": "down_write_slowpath(struct rw_semaphore  sem, int state){struct rwsem_waiter waiter;DEFINE_WAKE_Q(wake_q);  do optimistic spinning and steal lock if possible ", "static inline void __up_read(struct rw_semaphore *sem)": "down_write_trylock(struct rw_semaphore  sem){int ret;preempt_disable();DEBUG_RWSEMS_WARN_ON(sem->magic != sem, sem);ret = rwsem_write_trylock(sem);preempt_enable();return ret;}    unlock after reading ", "static inline void rwsem_clear_reader_owned(struct rw_semaphore *sem)": "up_read_non_owner(). ", "static struct rw_semaphore *rwsem_wake(struct rw_semaphore *sem)": "up_write has decremented the active part of count if we come here ", "DEBUG_RWSEMS_WARN_ON(rwsem_owner(sem) != current, sem);preempt_disable();tmp = atomic_long_fetch_add_release(-RWSEM_WRITER_LOCKED+RWSEM_READER_BIAS, &sem->count);rwsem_set_reader_owned(sem);if (tmp & RWSEM_FLAG_WAITERS)rwsem_downgrade_wake(sem);preempt_enable();}#else /* !CONFIG_PREEMPT_RT ": "downgrade_write(struct rw_semaphore  sem){long tmp;    When downgrading from exclusive to shared ownership,   anything inside the write-locked region cannot leak   into the read side. In contrast, anything in the   read-locked region is ok to be re-ordered into the   write side. As such, rely on RELEASE semantics. ", "__rwsem_set_reader_owned(sem, NULL);}EXPORT_SYMBOL(down_read_non_owner": "down_read_non_owner(struct rw_semaphore  sem){might_sleep();__down_read(sem);    The owner value for a reader-owned lock is mostly for debugging   purpose only and is not critical to the correct functioning of   rwsem. So it is perfectly fine to set it in a preempt-enabled   context here. ", "int __wake_up(struct wait_queue_head *wq_head, unsigned int mode,      int nr_exclusive, void *key)": "__wake_up_common(struct wait_queue_head  wq_head, unsigned int mode,int nr_exclusive, int wake_flags, void  key,wait_queue_entry_t  bookmark){wait_queue_entry_t  curr,  next;int cnt = 0;lockdep_assert_held(&wq_head->lock);if (bookmark && (bookmark->flags & WQ_FLAG_BOOKMARK)) {curr = list_next_entry(bookmark, entry);list_del(&bookmark->entry);bookmark->flags = 0;} elsecurr = list_first_entry(&wq_head->head, wait_queue_entry_t, entry);if (&curr->entry == &wq_head->head)return nr_exclusive;list_for_each_entry_safe_from(curr, next, &wq_head->head, entry) {unsigned flags = curr->flags;int ret;if (flags & WQ_FLAG_BOOKMARK)continue;ret = curr->func(curr, mode, wake_flags, key);if (ret < 0)break;if (ret && (flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)break;if (bookmark && (++cnt > WAITQUEUE_WALK_BREAK_CNT) &&(&next->entry != &wq_head->head)) {bookmark->flags = WQ_FLAG_BOOKMARK;list_add_tail(&bookmark->entry, &next->entry);break;}}return nr_exclusive;}static int __wake_up_common_lock(struct wait_queue_head  wq_head, unsigned int mode,int nr_exclusive, int wake_flags, void  key){unsigned long flags;wait_queue_entry_t bookmark;int remaining = nr_exclusive;bookmark.flags = 0;bookmark.private = NULL;bookmark.func = NULL;INIT_LIST_HEAD(&bookmark.entry);do {spin_lock_irqsave(&wq_head->lock, flags);remaining = __wake_up_common(wq_head, mode, remaining,wake_flags, key, &bookmark);spin_unlock_irqrestore(&wq_head->lock, flags);} while (bookmark.flags & WQ_FLAG_BOOKMARK);return nr_exclusive - remaining;}     __wake_up - wake up threads blocked on a waitqueue.   @wq_head: the waitqueue   @mode: which threads   @nr_exclusive: how many wake-one or wake-many threads to wake up   @key: is directly passed to the wakeup function     If this function wakes up a task, it executes a full memory barrier   before accessing the task state.  Returns the number of exclusive   tasks that were awaken. ", "list_del_init(&wq_entry->entry);ret = -ERESTARTSYS;} else ": "autoremove_wake_function;INIT_LIST_HEAD(&wq_entry->entry);}EXPORT_SYMBOL(init_wait_entry);long prepare_to_wait_event(struct wait_queue_head  wq_head, struct wait_queue_entry  wq_entry, int state){unsigned long flags;long ret = 0;spin_lock_irqsave(&wq_head->lock, flags);if (signal_pending_state(state, current)) {    Exclusive waiter must not fail if it was selected by wakeup,   it should \"consume\" the condition we were waiting for.     The caller will recheck the condition and return success if   we were already woken up, we can not miss the event because   wakeup locksunlocks the same wq_head->lock.     But we need to ensure that set-condition + wakeup after that   can't see us, it should wake up another exclusive waiter if   we fail. ", "void finish_wait(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)": "finish_wait - clean up after waiting in a queue   @wq_head: waitqueue waited on   @wq_entry: wait descriptor     Sets current thread back to running state and removes   the wait descriptor from the given waitqueue if still   queued. ", "long wait_woken(struct wait_queue_entry *wq_entry, unsigned mode, long timeout)": "woken_wake_function()         p->state = mode;wq_entry->flags |= WQ_FLAG_WOKEN;       smp_mb();  Atry_to_wake_up():       if (!(wq_entry->flags & WQ_FLAG_WOKEN))   <full barrier>           schedule()   if (p->state & mode)       p->state = TASK_RUNNING;      p->state = TASK_RUNNING;       wq_entry->flags &= ~WQ_FLAG_WOKEN;~~~~~~~~~~~~~~~~~~       smp_mb();  Bcondition = true;   }smp_mb();  C   remove_wait_queue(&wq_head, &wait);wq_entry->flags |= WQ_FLAG_WOKEN; ", "autogroup_kref_put(ag);}EXPORT_SYMBOL(sched_autogroup_create_attach": "sched_autogroup_create_attach(struct task_struct  p){struct autogroup  ag = autogroup_create();autogroup_move_group(p, ag);  Drop extra reference added by autogroup_create(): ", "void wake_q_add(struct wake_q_head *head, struct task_struct *task)": "wake_up_process(); IOW the task   must be ready to be woken at this location. ", "if (DEBUG_LOCKS_WARN_ON((preempt_count() < 0)))return;#endif__preempt_count_add(val);#ifdef CONFIG_DEBUG_PREEMPT/* * Spinlock count overflowing soon? ": "preempt_count_add(int val){#ifdef CONFIG_DEBUG_PREEMPT    Underflow? ", "if (DEBUG_LOCKS_WARN_ON(val > preempt_count()))return;/* * Is the spinlock portion underflowing? ": "preempt_count_sub(int val){#ifdef CONFIG_DEBUG_PREEMPT    Underflow? ", "#include <linux/highmem.h>#include <linux/hrtimer_api.h>#include <linux/ktime_api.h>#include <linux/sched/signal.h>#include <linux/syscalls_api.h>#include <linux/debug_locks.h>#include <linux/prefetch.h>#include <linux/capability.h>#include <linux/pgtable_api.h>#include <linux/wait_bit.h>#include <linux/jiffies.h>#include <linux/spinlock_api.h>#include <linux/cpumask_api.h>#include <linux/lockdep_api.h>#include <linux/hardirq.h>#include <linux/softirq.h>#include <linux/refcount_api.h>#include <linux/topology.h>#include <linux/sched/clock.h>#include <linux/sched/cond_resched.h>#include <linux/sched/cputime.h>#include <linux/sched/debug.h>#include <linux/sched/hotplug.h>#include <linux/sched/init.h>#include <linux/sched/isolation.h>#include <linux/sched/loadavg.h>#include <linux/sched/mm.h>#include <linux/sched/nohz.h>#include <linux/sched/rseq_api.h>#include <linux/sched/rt.h>#include <linux/blkdev.h>#include <linux/context_tracking.h>#include <linux/cpuset.h>#include <linux/delayacct.h>#include <linux/init_task.h>#include <linux/interrupt.h>#include <linux/ioprio.h>#include <linux/kallsyms.h>#include <linux/kcov.h>#include <linux/kprobes.h>#include <linux/llist_api.h>#include <linux/mmu_context.h>#include <linux/mmzone.h>#include <linux/mutex_api.h>#include <linux/nmi.h>#include <linux/nospec.h>#include <linux/perf_event_api.h>#include <linux/profile.h>#include <linux/psi.h>#include <linux/rcuwait_api.h>#include <linux/sched/wake_q.h>#include <linux/scs.h>#include <linux/slab.h>#include <linux/syscalls.h>#include <linux/vtime.h>#include <linux/wait_api.h>#include <linux/workqueue_api.h>#ifdef CONFIG_PREEMPT_DYNAMIC# ifdef CONFIG_GENERIC_ENTRY#  include <linux/entry-common.h># endif#endif#include <uapi/linux/sched/types.h>#include <asm/irq_regs.h>#include <asm/switch_to.h>#include <asm/tlb.h>#define CREATE_TRACE_POINTS#include <linux/sched/rseq_api.h>#include <trace/events/sched.h>#include <trace/events/ipi.h>#undef CREATE_TRACE_POINTS#include \"sched.h\"#include \"stats.h\"#include \"autogroup.h\"#include \"autogroup.h\"#include \"pelt.h\"#include \"smp.h\"#include \"stats.h\"#include \"../workqueue_internal.h\"#include \"../../io_uring/io-wq.h\"#include \"../smpboot.h\"EXPORT_TRACEPOINT_SYMBOL_GPL(ipi_send_cpu);EXPORT_TRACEPOINT_SYMBOL_GPL(ipi_send_cpumask);/* * Export tracepoints that act as a bare tracehook (ie: have no trace event * associated with them) to allow external modules to probe them. ": "scheduler code and related syscalls      Copyright (C) 1991-2002  Linus Torvalds ", "preempt_disable_notrace();preempt_latency_start(1);__schedule(SM_PREEMPT);preempt_latency_stop(1);preempt_enable_no_resched_notrace();/* * Check again in case we missed a preemption opportunity * between schedule and now. ": "preempt_schedule_common(void){do {    Because the function tracer can trace preempt_count_sub()   and it also uses preempt_enabledisable_notrace(), if   NEED_RESCHED is set, the preempt_enable_notrace() called   by the function tracer will call this function again and   cause infinite recursion.     Preemption must be disabled here before the function   tracer can trace. Break up preempt_disable() into two   calls. One to disable preemption without fear of being   traced. The other to still record the preemption latency,   which can also be traced by the function tracer. ", "void raw_spin_rq_lock_nested(struct rq *rq, int subclass)": "set_user_nice():p->se.load, p-> prio    - __sched_setscheduler():p->sched_class, p->policy, p-> prio,  p->se.load, p->rt_priority,  p->dl.dl_{runtime, deadline, period, flags, bw, density}    - sched_setnuma():p->numa_preferred_nid    - sched_move_task():p->sched_task_group    - uclamp_update_active()p->uclamp      p->state <- TASK_ :       is changed locklessly using set_current_state(), __set_current_state() or     set_special_state(), see their respective comments, or by     try_to_wake_up(). This latter uses p->pi_lock to serialize against     concurrent self.     p->on_rq <- { 0, 1 = TASK_ON_RQ_QUEUED, 2 = TASK_ON_RQ_MIGRATING }:       is set by activate_task() and cleared by deactivate_task(), under     rq->lock. Non-zero indicates the task is runnable, the special     ON_RQ_MIGRATING state is used for migration without holding both     rq->locks. It indicates task_cpu() is not stable, see task_rq_lock().     p->on_cpu <- { 0, 1 }:       is set by prepare_task() and cleared by finish_task() such that it will be     set before p is scheduled-in and cleared after p is scheduled-out, both     under rq->lock. Non-zero indicates the task is running on its CPU.       [ The astute reader will observe that it is possible for two tasks on one       CPU to have ->on_cpu = 1 at the same time. ]     task_cpu(p): is changed by set_task_cpu(), the rules are:      - Don't call set_task_cpu() on a blocked task:        We don't care what CPU we're not running on, this simplifies hotplug,      the CPU assignment of blocked tasks isn't required to be valid.      - for try_to_wake_up(), called under p->pi_lock:        This allows try_to_wake_up() to only take one rq->lock, see its comment.      - for migration called under rq->lock:      [ see task_on_rq_migrating() in task_rq_lock() ]        o move_queued_task()      o detach_task()      - for migration called under double_rq_lock():        o __migrate_swap_task()      o push_rt_task()  pull_rt_task()      o push_dl_task()  pull_dl_task()      o dl_task_offline_migration()   ", "#ifndef CONFIG_PREEMPT_RCUrcu_all_qs();#endifreturn 0;}EXPORT_SYMBOL(__cond_resched": "__cond_resched(void){if (should_resched(0)) {preempt_schedule_common();return 1;}    In preemptible kernels, ->rcu_read_lock_nesting tells the tick   whether the current CPU is in an RCU read-side critical section,   so the tick can report quiescent states even for CPUs looping   in kernel context.  In contrast, in non-preemptible kernels,   RCU readers leave no in-memory hints, which means that CPU-bound   processes executing in kernel context might never report an   RCU quiescent state.  Therefore, the following code causes   cond_resched() to report a quiescent state, but only when RCU   is in urgent need of one. ", "int __cond_resched_lock(spinlock_t *lock)": "__cond_resched_lock() - if a reschedule is pending, drop the given lock,   call schedule, and on return reacquire the lock.     This works OK both with and without CONFIG_PREEMPTION. We do strange low-level   operations here to prevent schedule() from being called twice (once via   spin_unlock(), once by hand). ", "if (unlikely(queued)) ": "yield - it could be a while. ", "long __sched io_schedule_timeout(long timeout)": "io_schedule_prepare(void){int old_iowait = current->in_iowait;current->in_iowait = 1;blk_flush_plug(current->plug, true);return old_iowait;}void io_schedule_finish(int token){current->in_iowait = token;}    This task is about to go to sleep on IO. Increment rq->nr_iowait so   that process accounting knows that this is a task in IO wait state. ", "WARN_ONCE(state != TASK_RUNNING && current->task_state_change,\"do not call blocking ops when !TASK_RUNNING; \"\"state=%x set at [<%p>] %pS\\n\", state,(void *)current->task_state_change,(void *)current->task_state_change);__might_resched(file, line, 0);}EXPORT_SYMBOL(__might_sleep": "__might_sleep(const char  file, int line){unsigned int state = get_current_state();    Blocking primitives will set (and therefore destroy) current->state,   since we will exit with TASK_RUNNING make sure we enter with it,   otherwise we will destroy state. ", "static unsigned long prev_jiffy;unsigned long preempt_disable_ip;/* WARN_ON_ONCE() by default, no rate limit required: ": "__might_resched(file, line, 0);}EXPORT_SYMBOL(__might_sleep);static void print_preempt_disable_ip(int preempt_offset, unsigned long ip){if (!IS_ENABLED(CONFIG_DEBUG_PREEMPT))return;if (preempt_count() == preempt_offset)return;pr_err(\"Preemption disabled at:\");print_ip_sym(KERN_ERR, ip);}static inline bool resched_offsets_ok(unsigned int offsets){unsigned int nested = preempt_count();nested += rcu_preempt_depth() << MIGHT_RESCHED_RCU_SHIFT;return nested == offsets;}void __might_resched(const char  file, int line, unsigned int offsets){  Ratelimiting timestamp: ", "int __sched__wait_on_bit(struct wait_queue_head *wq_head, struct wait_bit_queue_entry *wbq_entry,      wait_bit_action_f *action, unsigned mode)": "__wait_on_bit_lock() are   permitted return codes. Nonzero return codes halt waiting and return. ", "void wake_up_bit(void *word, int bit)": "wake_up_bit(struct wait_queue_head  wq_head, void  word, int bit){struct wait_bit_key key = __WAIT_BIT_KEY_INITIALIZER(word, bit);if (waitqueue_active(wq_head))__wake_up(wq_head, TASK_NORMAL, 1, &key);}EXPORT_SYMBOL(__wake_up_bit);     wake_up_bit - wake up a waiter on a bit   @word: the word being waited on, a kernel virtual address   @bit: the bit of the word being waited on     There is a standard hashed waitqueue table for generic use. This   is the part of the hashtable's accessor API that wakes up waiters   on a bit. For instance, if one were to have waiters on a bitflag,   one would call wake_up_bit() after clearing the bit.     In order for this to function properly, as it uses waitqueue_active()   internally, some kind of memory barrier must be done prior to calling   this. Typically, this will be smp_mb__after_atomic(), but in some   cases where bitflags are manipulated non-atomically under a lock, one   may need to use a less regular barrier, such fsinode.c's smp_mb(),   because spin_unlock() does not guarantee a memory barrier. ", "/** * complete: - signals a single thread waiting on this completion * @x:  holds the state of this particular completion * * This will wake up a single thread waiting on this completion. Threads will be * awakened in the same order in which they were queued. * * See also complete_all(), wait_for_completion() and related routines. * * If this function wakes up a task, it executes a full memory barrier before * accessing the task state. ": "wait_for_completion default blocks whereas semaphore default non-block. The   interface also makes it easy to 'complete' multiple waiting threads,   something which isn't entirely natural for semaphores.     But more importantly, the primitive documents the usage. Semaphores would   typically be used for exclusion which gives rise to priority inversion.   Waiting for completion is a typically sync point, but not an exclusion point. ", "void complete(struct completion *x)": "complete_all(), wait_for_completion() and related routines.     If this function wakes up a task, it executes a full memory barrier before   accessing the task state. ", "void __sched wait_for_completion(struct completion *x)": "wait_for_completion_timeout()) with timeout   and interrupt capability. Also see complete(). ", "void __sched wait_for_completion_io(struct completion *x)": "wait_for_completion_io: - waits for completion of a task   @x:  holds the state of this particular completion     This waits to be signaled for completion of a specific task. It is NOT   interruptible and there is no timeout. The caller is accounted as waiting   for IO (which traditionally means blkio only). ", "unsigned long __schedwait_for_completion_io_timeout(struct completion *x, unsigned long timeout)": "wait_for_completion_io_timeout: - waits for completion of a task (wtimeout)   @x:  holds the state of this particular completion   @timeout:  timeout value in jiffies     This waits for either a completion of a specific task to be signaled or for a   specified timeout to expire. The timeout is in jiffies. It is not   interruptible. The caller is accounted as waiting for IO (which traditionally   means blkio only).     Return: 0 if timed out, and positive (at least 1, or number of jiffies left   till timeout) if completed. ", "int __sched wait_for_completion_interruptible(struct completion *x)": "wait_for_completion_interruptible: - waits for completion of a task (wintr)   @x:  holds the state of this particular completion     This waits for completion of a specific task to be signaled. It is   interruptible.     Return: -ERESTARTSYS if interrupted, 0 if completed. ", "long __schedwait_for_completion_interruptible_timeout(struct completion *x,  unsigned long timeout)": "wait_for_completion_interruptible_timeout: - waits for completion (w(to,intr))   @x:  holds the state of this particular completion   @timeout:  timeout value in jiffies     This waits for either a completion of a specific task to be signaled or for a   specified timeout to expire. It is interruptible. The timeout is in jiffies.     Return: -ERESTARTSYS if interrupted, 0 if timed out, positive (at least 1,   or number of jiffies left till timeout) if completed. ", "int __sched wait_for_completion_killable(struct completion *x)": "wait_for_completion_killable: - waits for completion of a task (killable)   @x:  holds the state of this particular completion     This waits to be signaled for completion of a specific task. It can be   interrupted by a kill signal.     Return: -ERESTARTSYS if interrupted, 0 if completed. ", "long __schedwait_for_completion_killable_timeout(struct completion *x,     unsigned long timeout)": "wait_for_completion_killable_timeout: - waits for completion of a task (w(to,killable))   @x:  holds the state of this particular completion   @timeout:  timeout value in jiffies     This waits for either a completion of a specific task to be   signaled or for a specified timeout to expire. It can be   interrupted by a kill signal. The timeout is in jiffies.     Return: -ERESTARTSYS if interrupted, 0 if timed out, positive (at least 1,   or number of jiffies left till timeout) if completed. ", "bool try_wait_for_completion(struct completion *x)": "try_wait_for_completion - try to decrement a completion without blocking  @x:completion structure    Return: 0 if a decrement cannot be done without blocking   1 if a decrement succeeded.    If a completion is being used as a counting completion,  attempt to decrement the counter without blocking. This  enables us to avoid waiting if the resource the completion  is protecting is not available. ", "void complete_all(struct completion *x)": "completion_done() can not be used   to know if there are still waiters after complete_all() has been called. ", "void swake_up_all_locked(struct swait_queue_head *q)": "swake_up_all() to allow usage from   hard interrupt context and interrupt disabled regions. ", "list_del_init(&wait->task_list);ret = -ERESTARTSYS;} else ": "prepare_to_swait_event(struct swait_queue_head  q, struct swait_queue  wait, int state){unsigned long flags;long ret = 0;raw_spin_lock_irqsave(&q->lock, flags);if (signal_pending_state(state, current)) {    See prepare_to_wait_event(). TL;DR, subsequent swake_up_one()   must not see us. ", "/* Variables and functions for calc_load ": "avenrun[n] = avenrun[0]   exp_n + nr_active   (1 - exp_n)     Due to a number of reasons the above turns in the mess below:      - for_each_possible_cpu() is prohibitively expensive on machines with      serious number of CPUs, therefore we need to take a distributed approach      to calculating nr_active.            \\Sum_i x_i(t) = \\Sum_i x_i(t) - x_i(t_0) | x_i(t_0) := 0                        = \\Sum_i { \\Sum_j=1 x_i(t_j) - x_i(t_j-1) }        So assuming nr_active := 0 when we start out -- true per definition, we      can simply take per-CPU deltas and fold those into a global accumulate      to obtain the same result. See calc_load_fold_active().        Furthermore, in order to avoid synchronizing all per-CPU delta folding      across the machine, we assume 10 ticks is sufficient time for every      CPU to have completed this task.        This places an upper-bound on the IRQ-off latency of the machine. Then      again, being late doesn't loose the delta, just wrecks the sample.      - cpu_rq()->nr_uninterruptible isn't accurately tracked per-CPU because      this would add another cross-CPU cacheline miss and atomic operation      to the wakeup path. Instead we increment on whatever CPU the task ran      when it went into uninterruptible state and decrement on whatever CPU      did the wakeup. This means that only the sum of nr_uninterruptible over      all CPUs yields the correct result.      This covers the NO_HZ=n code, for extra head-aches, see the comment below. ", "void pm_vt_switch_required(struct device *dev, bool required)": "pm_vt_switch_required - indicate VT switch at suspend requirements   @dev: device   @required: if true, caller needs VT switch at suspendresume time     The different console drivers may or may not require VT switches across   suspendresume, depending on how they handle restoring video state and   what may be running.     Drivers can indicate support for switchless suspendresume, which can   save time and flicker, by using this routine and passing 'false' as   the argument.  If any loaded driver needs VT switching, or the   no_console_suspend argument has been passed on the command line, VT   switches will occur. ", "void pm_vt_switch_unregister(struct device *dev)": "pm_vt_switch_unregister - stop tracking a device's VT switching needs   @dev: device     Remove @dev from the vt switch list. ", "break;default:WARN_ON_ONCE(1);continue;}}kfree(rec);}void bpf_map_free_record(struct bpf_map *map)": "bpf_map_get_memcg(const struct bpf_map  map){if (map->objcg)return get_mem_cgroup_from_objcg(map->objcg);return root_mem_cgroup;}void  bpf_map_kmalloc_node(const struct bpf_map  map, size_t size, gfp_t flags,   int node){struct mem_cgroup  memcg,  old_memcg;void  ptr;memcg = bpf_map_get_memcg(map);old_memcg = set_active_memcg(memcg);ptr = kmalloc_node(size, flags | __GFP_ACCOUNT, node);set_active_memcg(old_memcg);mem_cgroup_put(memcg);return ptr;}void  bpf_map_kzalloc(const struct bpf_map  map, size_t size, gfp_t flags){struct mem_cgroup  memcg,  old_memcg;void  ptr;memcg = bpf_map_get_memcg(map);old_memcg = set_active_memcg(memcg);ptr = kzalloc(size, flags | __GFP_ACCOUNT);set_active_memcg(old_memcg);mem_cgroup_put(memcg);return ptr;}void  bpf_map_kvcalloc(struct bpf_map  map, size_t n, size_t size,       gfp_t flags){struct mem_cgroup  memcg,  old_memcg;void  ptr;memcg = bpf_map_get_memcg(map);old_memcg = set_active_memcg(memcg);ptr = kvcalloc(n, size, flags | __GFP_ACCOUNT);set_active_memcg(old_memcg);mem_cgroup_put(memcg);return ptr;}void __percpu  bpf_map_alloc_percpu(const struct bpf_map  map, size_t size,    size_t align, gfp_t flags){struct mem_cgroup  memcg,  old_memcg;void __percpu  ptr;memcg = bpf_map_get_memcg(map);old_memcg = set_active_memcg(memcg);ptr = __alloc_percpu_gfp(size, align, flags | __GFP_ACCOUNT);set_active_memcg(old_memcg);mem_cgroup_put(memcg);return ptr;}#elsestatic void bpf_map_save_memcg(struct bpf_map  map){}static void bpf_map_release_memcg(struct bpf_map  map){}#endifstatic int btf_field_cmp(const void  a, const void  b){const struct btf_field  f1 = a,  f2 = b;if (f1->offset < f2->offset)return -1;else if (f1->offset > f2->offset)return 1;return 0;}struct btf_field  btf_record_find(const struct btf_record  rec, u32 offset,  u32 field_mask){struct btf_field  field;if (IS_ERR_OR_NULL(rec) || !(rec->field_mask & field_mask))return NULL;field = bsearch(&offset, rec->fields, rec->cnt, sizeof(rec->fields[0]), btf_field_cmp);if (!field || !(field->type & field_mask))return NULL;return field;}void btf_record_free(struct btf_record  rec){int i;if (IS_ERR_OR_NULL(rec))return;for (i = 0; i < rec->cnt; i++) {switch (rec->fields[i].type) {case BPF_KPTR_UNREF:case BPF_KPTR_REF:if (rec->fields[i].kptr.module)module_put(rec->fields[i].kptr.module);btf_put(rec->fields[i].kptr.btf);break;case BPF_LIST_HEAD:case BPF_LIST_NODE:case BPF_RB_ROOT:case BPF_RB_NODE:case BPF_SPIN_LOCK:case BPF_TIMER:case BPF_REFCOUNT:  Nothing to release ", "void bpf_link_put(struct bpf_link *link)": "bpf_link_put_deferred(struct work_struct  work){struct bpf_link  link = container_of(work, struct bpf_link, work);bpf_link_free(link);}  bpf_link_put might be called from atomic context. It needs to be called   from sleepable context in order to acquire sleeping locks during the process. ", "case BPF_PROG_TEST_RUN:if (attr->test.data_in || attr->test.data_out ||    attr->test.ctx_out || attr->test.duration ||    attr->test.repeat || attr->test.flags)return -EINVAL;prog = bpf_prog_get_type(attr->test.prog_fd, BPF_PROG_TYPE_SYSCALL);if (IS_ERR(prog))return PTR_ERR(prog);if (attr->test.ctx_size_in < prog->aux->max_ctx_offset ||    attr->test.ctx_size_in > U16_MAX) ": "kern_sys_bpf(int cmd, union bpf_attr  attr, unsigned int size);int kern_sys_bpf(int cmd, union bpf_attr  attr, unsigned int size){struct bpf_prog   __maybe_unused prog;struct bpf_tramp_run_ctx __maybe_unused run_ctx;switch (cmd) {#ifdef CONFIG_BPF_JIT   __bpf_prog_enter_sleepable used by trampoline and JIT ", "int __cgroup_bpf_run_filter_skb(struct sock *sk,struct sk_buff *skb,enum cgroup_bpf_attach_type atype)": "__cgroup_bpf_run_filter_skb() - Run a program for packet filtering   @sk: The socket sending or receiving traffic   @skb: The skb that is being sent or received   @type: The type of program to be executed     If no socket is passed, or the socket is not of type INET or INET6,   this function does nothing and returns 0.     The program type passed in via @type must be suitable for network   filtering. No further check is performed to assert that.     For egress packets, this function can return:     NET_XMIT_SUCCESS    (0)- continue with packet output     NET_XMIT_DROP       (1)- drop packet and notify TCP to call cwr     NET_XMIT_CN         (2)- continue with packet output and notify TCP    to call cwr     -err- drop packet     For ingress packets, this function will return -EPERM if any   attached program was found and if it returned != 1 during execution.   Otherwise 0 is returned. ", "int __cgroup_bpf_run_filter_sock_addr(struct sock *sk,      struct sockaddr *uaddr,      enum cgroup_bpf_attach_type atype,      void *t_ctx,      u32 *flags)": "__cgroup_bpf_run_filter_sock_addr() - Run a program on a sock and                                         provided by user sockaddr   @sk: sock struct that will use sockaddr   @uaddr: sockaddr struct provided by user   @type: The type of program to be executed   @t_ctx: Pointer to attach type specific context   @flags: Pointer to u32 which contains higher bits of BPF program           return value (OR'ed together).     socket is expected to be of type INET or INET6.     This function will return %-EPERM if an attached program is found and   returned value != 1 during execution. In all other cases, 0 is returned. ", "int __cgroup_bpf_run_filter_sock_ops(struct sock *sk,     struct bpf_sock_ops_kern *sock_ops,     enum cgroup_bpf_attach_type atype)": "__cgroup_bpf_run_filter_sock_ops() - Run a program on a sock   @sk: socket to get cgroup from   @sock_ops: bpf_sock_ops_kern struct to pass to program. Contains   sk with connection information (IP addresses, etc.) May not contain   cgroup info if it is a req sock.   @type: The type of program to be executed     socket passed is expected to be of type INET or INET6.     The program type passed in via @type must be suitable for sock_ops   filtering. No further check is performed to assert that.     This function will return %-EPERM if any if an attached program was found   and if it returned != 1 during execution. In all other cases, 0 is returned. ", "kcsan_disable_current(); /* restore to 0, KCSAN still enabled ": "kcsan_enable_current(void){if (get_ctx()->disable_count-- == 0) {    Warn if kcsan_enable_current() calls are unbalanced with   kcsan_disable_current() calls, which causes disable_count to   become negative and should not happen. ", "++get_ctx()->atomic_nest_count;}EXPORT_SYMBOL(kcsan_nestable_atomic_begin": "kcsan_nestable_atomic_begin(void){    Do  not  check and warn if we are in a flat atomic region: nestable   and flat atomic regions are independent from each other.   See includelinuxkcsan.h: struct kcsan_ctx comments for more   comments. ", "kcsan_nestable_atomic_begin(); /* restore to 0 ": "kcsan_nestable_atomic_end(void){if (get_ctx()->atomic_nest_count-- == 0) {    Warn if kcsan_nestable_atomic_end() calls are unbalanced with   kcsan_nestable_atomic_begin() calls, which causes   atomic_nest_count to become negative and should not happen. ", "if (is_atomic(ctx, ptr, size, type))return false;if (this_cpu_dec_return(kcsan_skip) >= 0)return false;/* * NOTE: If we get here, kcsan_skip must always be reset in slow path * via reset_kcsan_skip() to avoid underflow. ": "kcsan_atomic_next for consecutive instruction stream. ", "INIT_LIST_HEAD(&sa->list);sa->ptr = ptr;sa->size = size;sa->type = type;sa->ip = _RET_IP_;if (!ctx->scoped_accesses.prev) /* Lazy initialize list head. ": "kcsan_begin_scoped_access(const volatile void  ptr, size_t size, int type,  struct kcsan_scoped_access  sa){struct kcsan_ctx  ctx = get_ctx();check_access(ptr, size, type, _RET_IP_);ctx->disable_count++;   Disable KCSAN, in case list debugging is on. ", "list_del(&sa->list);if (list_empty(&ctx->scoped_accesses))/* * Ensure we do not enter kcsan_check_scoped_accesses() * slow-path if unnecessary, and avoids requiring list_empty() * in the fast-path (to avoid a READ_ONCE() and potential * uaccess warning). ": "kcsan_end_scoped_access(struct kcsan_scoped_access  sa){struct kcsan_ctx  ctx = get_ctx();if (WARN(!ctx->scoped_accesses.prev, \"Unbalanced %s()?\", __func__))return;ctx->disable_count++;   Disable KCSAN, in case list debugging is on. ", "barrier();size = READ_ONCE(reorder_access->size);if (size)goto again;}}/* * Always checked last, right before returning from runtime; * if reorder_access is valid, checked after it was checked. ": "__tsan_func_exit(). Therefore we must read   and check size after the other fields. ", "void __tsan_atomic_signal_fence(int memorder);noinline void __tsan_atomic_signal_fence(int memorder)": "__tsan_atomic_signal_fence(), such instrumentation   can be disabled via the __no_kcsan function attribute (vs. an explicit call   which could not). When __no_kcsan is requested, __atomic_signal_fence()   generates no code.     Note: The result of using __atomic_signal_fence() with KCSAN enabled is   potentially limiting the compiler's ability to reorder operations; however,   if barriers were instrumented with explicit calls (without LTO), the compiler   couldn't optimize much anyway. The result of a hypothetical architecture   using __atomic_signal_fence() in normal code would be KCSAN false negatives. ", "size_t check_len = min_t(size_t, count, MAX_ENCODABLE_SIZE);check_access(s, check_len, KCSAN_ACCESS_WRITE, _RET_IP_);return memset(s, c, count);}#elsevoid *__tsan_memset(void *s, int c, size_t count) __alias(memset);#endifEXPORT_SYMBOL(__tsan_memset": "__tsan_memset(void  s, int c, size_t count);noinline void  __tsan_memset(void  s, int c, size_t count){    Instead of not setting up watchpoints where accessed size is greater   than MAX_ENCODABLE_SIZE, truncate checked size to MAX_ENCODABLE_SIZE. ", "if (entry->map_err_type == MAP_ERR_NOT_CHECKED) ": "debug_dma_mapping_error(struct device  dev, dma_addr_t dma_addr){struct dma_debug_entry ref;struct dma_debug_entry  entry;struct hash_bucket  bucket;unsigned long flags;if (unlikely(dma_debug_disabled()))return;ref.dev = dev;ref.dev_addr = dma_addr;bucket = get_hash_bucket(&ref, &flags);list_for_each_entry(entry, &bucket->list, list) {if (!exact_match(&ref, entry))continue;    The same physical address can be mapped multiple   times. Without a hardware IOMMU this results in the   same device addresses being put into the dma-debug   hash multiple times too. This can result in false   positives being reported. Therefore we implement a   best-fit algorithm here which updates the first entry   from the hash which fits the reference value and is   not currently listed as being checked. ", "void dmam_free_coherent(struct device *dev, size_t size, void *vaddr,dma_addr_t dma_handle)": "dma_free_attrs(dev, this->size, this->vaddr, this->dma_handle,this->attrs);}static int dmam_match(struct device  dev, void  res, void  match_data){struct dma_devres  this = res,  match = match_data;if (this->vaddr == match->vaddr) {WARN_ON(this->size != match->size ||this->dma_handle != match->dma_handle);return 1;}return 0;}     dmam_free_coherent - Managed dma_free_coherent()   @dev: Device to free coherent memory for   @size: Size of allocation   @vaddr: Virtual address of the memory to free   @dma_handle: DMA handle of the memory to free     Managed dma_free_coherent(). ", "void *dmam_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,gfp_t gfp, unsigned long attrs)": "dma_alloc_attrs()   @dev: Device to allocate non_coherent memory for   @size: Size of allocation   @dma_handle: Out argument for allocated DMA handle   @gfp: Allocation flags   @attrs: Flags in the DMA_ATTR_  namespace.     Managed dma_alloc_attrs().  Memory allocated using this function will be   automatically released on driver detach.     RETURNS:   Pointer to allocated memory on success, NULL on failure. ", "unsigned int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,    int nents, enum dma_data_direction dir, unsigned long attrs)": "dma_unmap_sg_attrs() should be used to unmap the buffer with the   original sg and original nents (not the value returned by this funciton). ", "bool dma_can_mmap(struct device *dev)": "dma_mmap_attrs() to   map DMA allocations to userspace. ", "mask = (dma_addr_t)mask;if (!dev->dma_mask || !dma_supported(dev, mask))return -EIO;arch_dma_set_mask(dev, mask);*dev->dma_mask = mask;return 0;}EXPORT_SYMBOL(dma_set_mask": "dma_set_mask(struct device  dev, u64 mask);#else#define arch_dma_set_mask(dev, mask)do { } while (0)#endifint dma_set_mask(struct device  dev, u64 mask){    Truncate the mask to the actually supported dma_addr_t width to   avoid generating unsupportable addresses. ", "mask = (dma_addr_t)mask;if (!dma_supported(dev, mask))return -EIO;dev->coherent_dma_mask = mask;return 0;}EXPORT_SYMBOL(dma_set_coherent_mask": "dma_set_coherent_mask(struct device  dev, u64 mask){    Truncate the mask to the actually supported dma_addr_t width to   avoid generating unsupportable addresses. ", "if (unlikely(kdb_trap_printk && kdb_printf_cpu < 0))return vkdb_printf(KDB_MSGSRC_PRINTK, fmt, args);#endif/* * Use the main logbuf even in NMI. But avoid calling console * drivers that might have their own locks. ": "vprintk(const char  fmt, va_list args){#ifdef CONFIG_KGDB_KDB  Allow to pass printk() to kdb but avoid a recursion. ", "#define DEVKMSG_LOG_MASK_DEFAULT0static unsigned int __read_mostly devkmsg_log = DEVKMSG_LOG_MASK_DEFAULT;static int __control_devkmsg(char *str)": "console_list_lock_held(void){lockdep_assert_held(&console_mutex);}EXPORT_SYMBOL(lockdep_assert_console_list_lock_held);#endif#ifdef CONFIG_DEBUG_LOCK_ALLOCbool console_srcu_read_lock_is_held(void){return srcu_read_lock_held(&console_srcu);}EXPORT_SYMBOL(console_srcu_read_lock_is_held);#endifenum devkmsg_log_bits {__DEVKMSG_LOG_BIT_ON = 0,__DEVKMSG_LOG_BIT_OFF,__DEVKMSG_LOG_BIT_LOCK,};enum devkmsg_log_masks {DEVKMSG_LOG_MASK_ON             = BIT(__DEVKMSG_LOG_BIT_ON),DEVKMSG_LOG_MASK_OFF            = BIT(__DEVKMSG_LOG_BIT_OFF),DEVKMSG_LOG_MASK_LOCK           = BIT(__DEVKMSG_LOG_BIT_LOCK),};  Keep both the 'on' and 'off' bits clear, i.e. ratelimit by default: ", "void console_list_unlock(void)": "console_list_unlock - Unlock the console list     Counterpart to console_list_lock() ", "int console_srcu_read_lock(void)": "console_srcu_read_unlock(). ", "struct file *file = iocb->ki_filp;struct devkmsg_user *user = file->private_data;size_t len = iov_iter_count(from);ssize_t ret = len;if (len > PRINTKRB_RECORD_MAX)return -EINVAL;/* Ignore when user logging is disabled. ": "vprintk_emit(facility, level, NULL, fmt, args);va_end(args);return r;}static ssize_t devkmsg_write(struct kiocb  iocb, struct iov_iter  from){char  buf,  line;int level = default_message_loglevel;int facility = 1;  LOG_USER ", "MESSAGE_LOGLEVEL_DEFAULT,/* default_message_loglevel ": "_printk[4] = {CONSOLE_LOGLEVEL_DEFAULT,  console_loglevel ", "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt#include <linux/kernel.h>#include <linux/mm.h>#include <linux/tty.h>#include <linux/tty_driver.h>#include <linux/console.h>#include <linux/init.h>#include <linux/jiffies.h>#include <linux/nmi.h>#include <linux/module.h>#include <linux/moduleparam.h>#include <linux/delay.h>#include <linux/smp.h>#include <linux/security.h>#include <linux/memblock.h>#include <linux/syscalls.h>#include <linux/crash_core.h>#include <linux/ratelimit.h>#include <linux/kmsg_dump.h>#include <linux/syslog.h>#include <linux/cpu.h>#include <linux/rculist.h>#include <linux/poll.h>#include <linux/irq_work.h>#include <linux/ctype.h>#include <linux/uio.h>#include <linux/sched/clock.h>#include <linux/sched/debug.h>#include <linux/sched/task_stack.h>#include <linux/uaccess.h>#include <asm/sections.h>#include <trace/events/initcall.h>#define CREATE_TRACE_POINTS#include <trace/events/printk.h>#include \"printk_ringbuffer.h\"#include \"console_cmdline.h\"#include \"braille.h\"#include \"internal.h\"int console_printk[4] = ": "console_lock  01Mar01 Andrew Morton ", "static int console_trylock_spinning(void)": "console_trylock_spinning - try to get console_lock by busy waiting     This allows to busy wait for the console_lock when the current   owner is running in specially marked sections. It means that   the current owner is running and cannot reschedule until it   is ready to lose the lock.     Return: 1 if we got the lock, 0 othrewise ", "#define printk_timefalse#define prb_read_valid(rb, seq, r)false#define prb_first_valid_seq(rb)0#define prb_next_seq(rb)0static u64 syslog_seq;static size_t record_print_text(const struct printk_record *r,bool syslog, bool time)": "console_unlock();preempt_enable();}wake_up_klogd();return printed_len;}EXPORT_SYMBOL(vprintk_emit);int vprintk_default(const char  fmt, va_list args){return vprintk_emit(0, LOGLEVEL_DEFAULT, NULL, fmt, args);}EXPORT_SYMBOL_GPL(vprintk_default);asmlinkage __visible int _printk(const char  fmt, ...){va_list args;int r;va_start(args, fmt);r = vprintk(fmt, args);va_end(args);return r;}EXPORT_SYMBOL(_printk);static bool pr_flush(int timeout_ms, bool reset_on_progress);static bool __pr_flush(struct console  con, int timeout_ms, bool reset_on_progress);#else   CONFIG_PRINTK ", "void __sched console_conditional_schedule(void)": "console_conditional_schedule - yield the CPU if required     If the console code is currently allowed to sleep, and   if this CPU should yield the CPU to another task, do   so here.     Must be called within console_lock();. ", "synchronize_srcu(&console_srcu);}EXPORT_SYMBOL(console_stop": "console_stop(struct console  console){__pr_flush(console, 1000, true);console_list_lock();console_srcu_write_flags(console, console->flags & ~CON_ENABLED);console_list_unlock();    Ensure that all SRCU list walks have completed. All contexts must   be able to see that this console is disabled so that (for example)   the caller can suspend the port without risk of another context   using the port. ", "WARN_ON_ONCE(debug_lockdep_rcu_enabled() &&     srcu_read_lock_held(&console_srcu));mutex_lock(&console_mutex);}EXPORT_SYMBOL(console_list_lock);/** * console_list_unlock - Unlock the console list * * Counterpart to console_list_lock() ": "console_force_preferred_locked(),   synchronize_srcu() is called with the console_list_lock held.   Therefore it is not allowed that the console_list_lock is taken   with the srcu_lock held.     Detecting if this context is really in the read-side critical   section is only possible if the appropriate debug options are   enabled. ", "bool printk_timed_ratelimit(unsigned long *caller_jiffies,unsigned int interval_msecs)": "printk_timed_ratelimit - caller-controlled printk ratelimiting   @caller_jiffies: pointer to caller's state   @interval_msecs: minimum interval between prints     printk_timed_ratelimit() returns true if more than @interval_msecs   milliseconds have elapsed since the last time printk_timed_ratelimit()   returned true. ", "void __printk_cpu_sync_wait(void)": "__printk_cpu_sync_wait() - Busy wait until the printk cpu-reentrant                              spinning lock is not owned by any CPU.     Context: Any context. ", "int __printk_cpu_sync_try_get(void)": "__printk_cpu_sync_try_get() - Try to acquire the printk cpu-reentrant                                 spinning lock.     If no processor has the lock, the calling processor takes the lock and   becomes the owner. If the calling processor is already the owner of the   lock, this function succeeds immediately.     Context: Any context. Expects interrupts to be disabled.   Return: 1 on success, otherwise 0. ", "old = atomic_cmpxchg_acquire(&printk_cpu_sync_owner, -1,     cpu); /* LMM(__printk_cpu_sync_try_get:A) ": "__printk_cpu_sync_put:B.     Memory barrier involvement:     If __printk_cpu_sync_try_get:A reads from __printk_cpu_sync_put:B,   then __printk_cpu_sync_put:A can never read from   __printk_cpu_sync_try_get:B.     Relies on:     RELEASE from __printk_cpu_sync_put:A to __printk_cpu_sync_put:B   of the previous CPU      matching   ACQUIRE from __printk_cpu_sync_try_get:A to   __printk_cpu_sync_try_get:B of this CPU ", "static void rdmacg_uncharge_hierarchy(struct rdma_cgroup *cg,     struct rdmacg_device *device,     struct rdma_cgroup *stop_cg,     enum rdmacg_resource_type index)": "rdmacg_uncharge_hierarchy - hierarchically uncharge rdma resource count   @cg: pointer to cg to uncharge and all parents in hierarchy   @device: pointer to rdmacg device   @stop_cg: while traversing hirerchy, when meet with stop_cg cgroup             stop uncharging   @index: index of the resource to uncharge in cg in given resource pool ", "int rdmacg_try_charge(struct rdma_cgroup **rdmacg,      struct rdmacg_device *device,      enum rdmacg_resource_type index)": "rdmacg_try_charge - hierarchically try to charge the rdma resource   @rdmacg: pointer to rdma cgroup which will own this resource   @device: pointer to rdmacg device   @index: index of the resource to charge in cgroup (resource pool)     This function follows charging resource in hierarchical way.   It will fail if the charge would cause the new value to exceed the   hierarchical limit.   Returns 0 if the charge succeeded, otherwise -EAGAIN, -ENOMEM or -EINVAL.   Returns pointer to rdmacg for this resource when charging is successful.     Charger needs to account resources on two criteria.   (a) per cgroup & (b) per device resource usage.   Per cgroup resource usage ensures that tasks of cgroup doesn't cross   the configured limits. Per device provides granular configuration   in multi device usage. It allocates resource pool in the hierarchy   for each parent it come across for first resource. Later on resource   pool will be available. Therefore it will be much faster thereon   to chargeuncharge. ", "void rdmacg_register_device(struct rdmacg_device *device)": "rdmacg_register_device - register rdmacg device to rdma controller.   @device: pointer to rdmacg device whose resources need to be accounted.     If IB stack wish a device to participate in rdma cgroup resource   tracking, it must invoke this API to register with rdma cgroup before   any user space application can start using the RDMA resources. ", "void rdmacg_unregister_device(struct rdmacg_device *device)": "rdmacg_unregister_device - unregister rdmacg device from rdma controller.   @device: pointer to rdmacg device which was previously registered with rdma            controller using rdmacg_register_device().     IB stack must invoke this after all the resources of the IB device   are destroyed and after ensuring that no more resources will be created   when this API is invoked. ", "while (irqd_irq_inprogress(&desc->irq_data))cpu_relax();/* Ok, that indicated we're done: double-check carefully. ": "synchronize_hardirq(struct irq_desc  desc, bool sync_chip){struct irq_data  irqd = irq_desc_get_irq_data(desc);bool inprogress;do {unsigned long flags;    Wait until we're out of the critical section.  This might   give the wrong answer due to the lack of memory barriers. ", "void synchronize_irq(unsigned int irq)": "synchronize_irq - wait for pending IRQ handlers (on other CPUs)  @irq: interrupt number to wait for    This function waits for any pending IRQ handlers for this interrupt  to complete before returning. If you use this function while  holding a resource the IRQ handler may need you will deadlock.    Can only be called from preemptible code as it might sleep when  an interrupt thread is associated to @irq.    It optionally makes sure (when the irq chip supports that method)  that the interrupt is not pending in any CPU and waiting for  service. ", "void disable_irq_nosync(unsigned int irq)": "disable_irq(struct irq_desc  desc){if (!desc->depth++)irq_disable(desc);}static int __disable_irq_nosync(unsigned int irq){unsigned long flags;struct irq_desc  desc = irq_get_desc_buslock(irq, &flags, IRQ_GET_DESC_CHECK_GLOBAL);if (!desc)return -EINVAL;__disable_irq(desc);irq_put_desc_busunlock(desc, flags);return 0;}    disable_irq_nosync - disable an irq without waiting  @irq: Interrupt to disable    Disable the selected interrupt line.  Disables and Enables are  nested.  Unlike disable_irq(), this function does not ensure existing  instances of the IRQ handler have completed before returning.    This function may be called from IRQ context. ", "irq_settings_set_noprobe(desc);/* * Call irq_startup() not irq_enable() here because the * interrupt might be marked NOAUTOEN. So irq_startup() * needs to be invoked when it gets enabled the first * time. If it was already started up, then irq_startup() * will invoke irq_enable() under the hood. ": "enable_irq(struct irq_desc  desc){switch (desc->depth) {case 0: err_out:WARN(1, KERN_WARNING \"Unbalanced enable for IRQ %d\\n\",     irq_desc_get_irq(desc));break;case 1: {if (desc->istate & IRQS_SUSPENDED)goto err_out;  Prevent probing on this irq: ", "int irq_set_irq_wake(unsigned int irq, unsigned int on)": "irq_set_irq_wake - control irq power management wakeup  @irq:interrupt to control  @on:enabledisable power management wakeup    Enabledisable power management wakeup mode, which is  disabled by default.  Enables and disables must match,  just as they match for non-wakeup mode support.    Wakeup mode lets this IRQ wake the system from sleep  states like \"suspend to RAM\".    Note: irq enabledisable state is completely orthogonal  to the enabledisable state of irq wake. An irq can be  disabled with disable_irq() and still wake the system as  long as the irq has wake enabled. If this does not hold,  then the underlying irq chip and the related driver need  to be investigated. ", "intirq_set_affinity_notifier(unsigned int irq, struct irq_affinity_notify *notify)": "free_irq(). ", "static irqreturn_t irq_default_primary_handler(int irq, void *dev_id)": "request_threaded_irq is called   with handler == NULL. Useful for oneshot interrupts. ", "void irq_domain_set_info(struct irq_domain *domain, unsigned int virq, irq_hw_number_t hwirq, const struct irq_chip *chip, void *chip_data, irq_flow_handler_t handler, void *handler_data, const char *handler_name)": "irq_domain_set_info - Set the complete data for a @virq in @domain   @domain:Interrupt domain to match   @virq:IRQ number   @hwirq:The hardware interrupt number   @chip:The associated interrupt chip   @chip_data:The associated interrupt chip data   @handler:The interrupt flow handler   @handler_data:The interrupt flow handler data   @handler_name:The interrupt handler name ", "int irq_set_chip(unsigned int irq, const struct irq_chip *chip)": "irq_set_chip - set the irq chip for an irq  @irq:irq number  @chip:pointer to irq chip description structure ", "int irq_set_irq_type(unsigned int irq, unsigned int type)": "irq_set_irq_type - set the irq trigger type for an irq  @irq:irq number  @type:IRQ_TYPE_{LEVEL,EDGE}_  value - see includelinuxirq.h ", "int irq_set_handler_data(unsigned int irq, void *data)": "irq_set_handler_data - set irq handler data for an irq  @irq:Interrupt number  @data:Pointer to interrupt specific data    Set the hardware irq controller data for an irq ", "int irq_set_chip_data(unsigned int irq, void *data)": "irq_set_chip_data - set irq chip data for an irq  @irq:Interrupt number  @data:Pointer to chip specific data    Set the hardware irq chip data for an irq ", "void handle_edge_irq(struct irq_desc *desc)": "handle_edge_irq - edge type IRQ handler  @desc:the interrupt description structure for this irq    Interrupt occurs on the falling andor rising edge of a hardware  signal. The occurrence is latched into the irq controller hardware  and must be acked in order to be reenabled. After the ack another  interrupt can happen on the same source even before the first one  is handled by the associated event handler. If this happens it  might be necessary to disable (mask) the interrupt depending on the  controller hardware. This requires to reenable the interrupt inside  of the loop which handles the interrupts which have arrived while  the handler was running. If all pending interrupts are handled, the  loop is left. ", "unsigned long probe_irq_on(void)": "probe_irq_on- begin an interrupt autodetect    Commence probing for an interrupt. The interrupts are scanned  and a mask of potential interrupt lines is returned.   ", "unsigned int probe_irq_mask(unsigned long val)": "probe_irq_mask - scan a bitmap of interrupt lines  @val:mask of interrupts to consider    Scan the interrupt lines and return a bitmap of active  autodetect interrupts. The interrupt probe logic state  is then returned to its previous value.    Note: we need to scan all the irq's even though we will  only return autodetect irq numbers - just so that we reset  them all to a known state. ", "int probe_irq_off(unsigned long val)": "probe_irq_off- end an interrupt autodetect  @val: mask of potential interrupts (unused)    Scans the unused interrupt lines and returns the line which  appears to have triggered the interrupt. If no interrupt was  found then zero is returned. If more than one interrupt is  found then minus the first candidate is returned to indicate  their is doubt.    The interrupt probe logic state is returned to its previous  value.    BUGS: When used in a module (which arguably shouldn't happen)  nothing prevents two IRQ probe callers from overlapping. The  results of this are non-optimal. ", "int devm_request_threaded_irq(struct device *dev, unsigned int irq,      irq_handler_t handler, irq_handler_t thread_fn,      unsigned long irqflags, const char *devname,      void *dev_id)": "devm_free_irq() must be used. ", "int devm_request_any_context_irq(struct device *dev, unsigned int irq,      irq_handler_t handler, unsigned long irqflags,      const char *devname, void *dev_id)": "devm_request_any_context_irq - allocate an interrupt line for a managed device  @dev: device to request interrupt for  @irq: Interrupt line to allocate  @handler: Function to be called when the IRQ occurs  @irqflags: Interrupt type flags  @devname: An ascii name for the claiming device, dev_name(dev) if NULL  @dev_id: A cookie passed back to the handler function    Except for the extra @dev argument, this function takes the  same arguments and performs the same function as  request_any_context_irq().  IRQs requested with this function will be  automatically freed on driver detach.    If an IRQ allocated with this function needs to be freed  separately, devm_free_irq() must be used. ", "void trace_hardirqs_on_prepare(void)": "trace_hardirqs_on() but without the lockdep invocation. This is   used in the low level entry code where the ordering vs. RCU is important   and lockdep uses a staged approach which splits the lockdep hardirq   tracking into a RCU on and a RCU off section. ", "void trace_hardirqs_off_finish(void)": "trace_hardirqs_off() but without the lockdep invocation. This is   used in the low level entry code where the ordering vs. RCU is important   and lockdep uses a staged approach which splits the lockdep hardirq   tracking into a RCU on and a RCU off section. ", "if (flags) ": "trace_print_flags_seq_u64(struct trace_seq  p, const char  delim,      unsigned long long flags,      const struct trace_print_flags_u64  flag_array){unsigned long long mask;const char  str;const char  ret = trace_seq_buffer_ptr(p);int i, first = 1;for (i = 0;  flag_array[i].name && flags; i++) {mask = flag_array[i].mask;if ((flags & mask) != mask)continue;str = flag_array[i].name;flags &= ~mask;if (!first && delim)trace_seq_puts(p, delim);elsefirst = 0;trace_seq_puts(p, str);}  check for left over flags ", "const char *trace_print_hex_seq(struct trace_seq *p, const unsigned char *buf, int buf_len,    bool concatenate)": "trace_print_hex_seq - print buffer as hex sequence   @p: trace seq struct to write to   @buf: The buffer to print   @buf_len: Length of @buf in bytes   @concatenate: Print @buf as single hex string or with spacing     Prints the passed buffer as a hex sequence either as a whole,   single hex string if @concatenate is true or with spacing after   each byte in case @concatenate is false. ", "char *trace_seq_acquire(struct trace_seq *s, unsigned int len)": "trace_seq_acquire - acquire seq buffer with size len   @s: trace sequence descriptor   @len: size of buffer to be acquired     acquire buffer with size of @len from trace_seq for output usage,   user can fill string into that buffer.     Returns start address of acquired buffer.     it allow multiple usage in one trace output function call. ", "if (is_constant) ": "ftrace_likely_update(struct ftrace_likely_data  f, int val,  int expect, int is_constant){unsigned long flags = user_access_save();  A constant is always correct ", "void ktime_get_real_ts64(struct timespec64 *ts)": "ktime_get_real_ts64 - Returns the time of day in a timespec64.   @ts:pointer to the timespec to be set     Returns the time of day in a timespec64 (WARN if suspended). ", "int do_settimeofday64(const struct timespec64 *ts)": "do_settimeofday64 - Sets the time of day.   @ts:     pointer to the timespec64 variable containing the new time     Sets the time of day to the new time and update NTP and notify hrtimers ", "void ktime_get_raw_ts64(struct timespec64 *ts)": "ktime_get_raw_ts64 - Returns the raw monotonic time in a timespec   @ts:pointer to the timespec64 to be set     Returns the raw monotonic time (completely un-modified by ntp) ", "void hardpps(const struct timespec64 *phase_ts, const struct timespec64 *raw_ts)": "hardpps() - Accessor function to NTP __hardpps function ", "kthread_run(clocksource_watchdog_kthread, NULL, \"kwatchdog\");}static void __clocksource_unstable(struct clocksource *cs)": "clocksource_change_rating(struct clocksource  cs, int rating);static void clocksource_watchdog_work(struct work_struct  work){    We cannot directly run clocksource_watchdog_kthread() here, because   clocksource_select() calls timekeeping_notify() which uses   stop_machine(). One cannot use stop_machine() from a workqueue() due   lock inversions wrt CPU hotplug.     Also, we only ever run this work once or twice during the lifetime   of the kernel, so there is no point in creating a more permanent   kthread for this.     If kthread_run fails the next watchdog scan over the   watchdog_list will find the unstable clock again. ", "int clocksource_unregister(struct clocksource *cs)": "clocksource_unregister - remove a registered clocksource   @cs:clocksource to be unregistered ", "void time64_to_tm(time64_t totalsecs, int offset, struct tm *result)": "time64_to_tm - converts the calendar time to local broken-down time     @totalsecs:the number of seconds elapsed since 00:00:00 on January 1, 1970,  Coordinated Universal Time (UTC).   @offset:offset seconds adding to totalsecs.   @result:pointer to struct tm variable to receive broken-down time ", "void init_timer_key(struct timer_list *timer,    void (*func)(struct timer_list *), unsigned int flags,    const char *name, struct lock_class_key *key)": "init_timer_key - initialize a timer   @timer: the timer to be initialized   @func: timer callback function   @flags: timer flags   @name: name of the timer   @key: lockdep class key of the fake lock used for tracking timer         sync lock dependencies     init_timer_key() must be done to a timer prior calling  any  of the   other timer functions. ", "int mod_timer_pending(struct timer_list *timer, unsigned long expires)": "mod_timer_pending - Modify a pending timer's timeout   @timer:The pending timer to be modified   @expires:New absolute timeout in jiffies     mod_timer_pending() is the same for pending timers as mod_timer(), but   will not activate inactive timers.     If @timer->function == NULL then the start operation is silently   discarded.     Return:     %0 - The timer was inactive and not modified or was in    shutdown state and the operation was discarded     %1 - The timer was active and requeued to expire at @expires ", "if (!(options & MOD_TIMER_NOTPENDING) && timer_pending(timer)) ": "mod_timer(struct timer_list  timer, unsigned long expires, unsigned int options){unsigned long clk = 0, flags, bucket_expiry;struct timer_base  base,  new_base;unsigned int idx = UINT_MAX;int ret = 0;debug_assert_init(timer);    This is a common optimization triggered by the networking code - if   the timer is re-modified to have the same timeout or ends up in the   same array bucket then just return: ", "int timer_reduce(struct timer_list *timer, unsigned long expires)": "timer_reduce - Modify a timer's timeout if it would reduce the timeout   @timer:The timer to be modified   @expires:New absolute timeout in jiffies     timer_reduce() is very similar to mod_timer(), except that it will only   modify an enqueued timer if that would reduce the expiration time. If   @timer is not enqueued it starts the timer.     If @timer->function == NULL then the start operation is silently   discarded.     Return:     %0 - The timer was inactive and started or was in shutdown    state and the operation was discarded     %1 - The timer was active and requeued to expire at @expires or    the timer was active and not modified because @expires    did not change the effective expiry time such that the    timer would expire earlier than already scheduled ", "static bool timer_fixup_init(void *addr, enum debug_obj_state state)": "add_timer(struct timer_base  base, struct timer_list  timer){unsigned long bucket_expiry;unsigned int idx;idx = calc_wheel_index(timer->expires, base->clk, &bucket_expiry);enqueue_timer(base, timer, idx, bucket_expiry);}#ifdef CONFIG_DEBUG_OBJECTS_TIMERSstatic const struct debug_obj_descr timer_debug_descr;struct timer_hint {void( function)(struct timer_list  t);longoffset;};#define TIMER_HINT(fn, container, timr, hintfn)\\{\\.function = fn,\\.offset  = offsetof(container, hintfn) -\\    offsetof(container, timr)\\}static const struct timer_hint timer_hints[] = {TIMER_HINT(delayed_work_timer_fn,   struct delayed_work, timer, work.func),TIMER_HINT(kthread_delayed_work_timer_fn,   struct kthread_delayed_work, timer, work.func),};static void  timer_debug_hint(void  addr){struct timer_list  timer = addr;int i;for (i = 0; i < ARRAY_SIZE(timer_hints); i++) {if (timer_hints[i].function == timer->function) {void (  fn)(void) = addr + timer_hints[i].offset;return  fn;}}return timer->function;}static bool timer_is_static_object(void  addr){struct timer_list  timer = addr;return (timer->entry.pprev == NULL &&timer->entry.next == TIMER_ENTRY_STATIC);}    fixup_init is called when:   - an active object is initialized ", "if (likely(base->running_timer != timer)) ": "timer_delete_sync() can't detect that the timer's   handler yet has not finished. This also guarantees that the   timer is serialized wrt itself. ", "static int __try_to_del_timer_sync(struct timer_list *timer, bool shutdown)": "try_to_del_timer_sync - Internal function: Try to deactivate a timer   @timer:Timer to deactivate   @shutdown:If true, this indicates that the timer is about to be  shutdown permanently.     If @shutdown is true then @timer->function is set to NULL under the   timer base lock which prevents further rearming of the timer. Any   attempt to rearm @timer after this function returns will be silently   ignored.     This function cannot guarantee that the timer cannot be rearmed   right after dropping the base lock if @shutdown is false. That   needs to be prevented by the calling code if necessary.     Return:     %0  - The timer was not pending     %1  - The timer was pending and deactivated     %-1 - The timer callback function is running on a different CPU ", "struct process_timer ": "schedule_timeout()'s timer is defined on the stack, it must store   the target task on the stack as well. ", "void msleep(unsigned int msecs)": "msleep - sleep safely even with waitqueue interruptions   @msecs: Time in milliseconds to sleep for ", "unsigned long msleep_interruptible(unsigned int msecs)": "msleep_interruptible - sleep waiting for signals   @msecs: Time in milliseconds to sleep for ", "void __sched usleep_range_state(unsigned long min, unsigned long max,unsigned int state)": "usleep_range_state - Sleep for an approximate time in a given state   @min:Minimum time in usecs to sleep   @max:Maximum time in usecs to sleep   @state:State of the current task that will be while sleeping     In non-atomic context where the exact wakeup time is flexible, use   usleep_range_state() instead of udelay().  The sleep improves responsiveness   by avoiding the CPU-hogging busy-wait of udelay(), and the range reduces   power usage by allowing hrtimers to take advantage of an already-   scheduled interrupt instead of scheduling a new one just for this sleep. ", "#include <linux/clocksource.h>#include <linux/jiffies.h>#include <linux/module.h>#include <linux/init.h>#include \"timekeeping.h\"#include \"tick-internal.h\"static u64 jiffies_read(struct clocksource *cs)": "jiffies based clocksource.     Copyright (C) 2004, 2005 IBM, John Stultz (johnstul@us.ibm.com) ", "BUILD_BUG_ON(HZ > USEC_PER_SEC);#if !(USEC_PER_SEC % HZ)return (USEC_PER_SEC / HZ) * j;#else# if BITS_PER_LONG == 32return (HZ_TO_USEC_MUL32 * j) >> HZ_TO_USEC_SHR32;# elsereturn (j * HZ_TO_USEC_NUM) / HZ_TO_USEC_DEN;# endif#endif}EXPORT_SYMBOL(jiffies_to_usecs": "jiffies_to_usecs(const unsigned long j){    Hz usually doesn't go much further MSEC_PER_SEC.   jiffies_to_usecs() and usecs_to_jiffies() depend on that. ", "time64_t mktime64(const unsigned int year0, const unsigned int mon0,const unsigned int day, const unsigned int hour,const unsigned int min, const unsigned int sec)": "mktime64 - Converts date to seconds.   Converts Gregorian date to seconds since 1970-01-01 00:00:00.   Assumes input in normal date format, i.e. 1980-12-31 23:59:59   => year=1980, mon=12, day=31, hour=23, min=59, sec=59.     [For the Julian calendar (which was used in Russia before 1917,   Britain & colonies before 1752, anywhere else before 1582,   and is still in use by some communities) leave out the   -year100+year400 terms, and add 10.]     This algorithm was first published by Gauss (I think).     A leap second can be indicated by calling this function with sec as   60 (allowable under ISO 8601).  The leap second is treated the same   as the following second since they don't exist in UNIX time.     An encoding of midnight at the end of the day as 24:00:00 - ie. midnight   tomorrow - (allowable under ISO 8601) is supported. ", "void set_normalized_timespec64(struct timespec64 *ts, time64_t sec, s64 nsec)": "ns_to_timespec64(nsec);struct __kernel_old_timeval tv;tv.tv_sec = ts.tv_sec;tv.tv_usec = (suseconds_t)ts.tv_nsec  1000;return tv;}EXPORT_SYMBOL(ns_to_kernel_old_timeval);     set_normalized_timespec64 - set timespec sec and nsec parts and normalize     @ts:pointer to timespec variable to be set   @sec:seconds to set   @nsec:nanoseconds to set     Set seconds and nanoseconds field of a timespec variable and   normalize to the timespec storage format     Note: The tv_nsec part is always in the range of  0 <= tv_nsec < NSEC_PER_SEC   For negative values only the tv_sec field is negative ! ", "unsigned long __msecs_to_jiffies(const unsigned int m)": "__msecs_to_jiffies: - convert milliseconds to jiffies   @m:time in milliseconds     conversion is done as follows:     - negative values mean 'infinite timeout' (MAX_JIFFY_OFFSET)     - 'too large' values [that would result in larger than     MAX_JIFFY_OFFSET values] mean 'infinite timeout' too.     - all other values are converted to jiffies by either multiplying     the input value by a factor or dividing it with a factor and     handling any 32-bit overflows.     for the details see __msecs_to_jiffies()     __msecs_to_jiffies() checks for the passed in value being a constant   via __builtin_constant_p() allowing gcc to eliminate most of the   code, __msecs_to_jiffies() is called if the value passed does not   allow constant folding and the actual conversion must be done at   runtime.   The _msecs_to_jiffies helpers are the HZ dependent conversion   routines found in includelinuxjiffies.h ", "u32 rem;value->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,    NSEC_PER_SEC, &rem);value->tv_nsec = rem;}EXPORT_SYMBOL(jiffies_to_timespec64": "jiffies_to_timespec64(const unsigned long jiffies, struct timespec64  value){    Convert jiffies to nanoseconds and separate with   one divide. ", "if (x >= ~0UL / HZ * USER_HZ)return ~0UL;/* .. but do try to contain it here ": "clock_t_to_jiffies(unsigned long x){#if (HZ % USER_HZ)==0if (x >= ~0UL  (HZ  USER_HZ))return ~0UL;return x   (HZ  USER_HZ);#else  Don't worry about loss of precision here .. ", "# endif#else/* * There are better ways that don't overflow early, * but even this doesn't overflow in hundreds of years * in 64 bits, so.. ": "jiffies_64_to_clock_t(u64 x){#if (TICK_NSEC % (NSEC_PER_SEC  USER_HZ)) == 0# if HZ < USER_HZx = div_u64(x   USER_HZ, HZ);# elif HZ > USER_HZx = div_u64(x, HZ  USER_HZ);# else  Nothing to do ", "u64 nsecs_to_jiffies64(u64 n)": "nsecs_to_jiffies64 - Convert nsecs in u64 to jiffies64     @n:nsecs in u64     Unlike {m,u}secs_to_jiffies, type of input is not unsigned int but u64.   And this doesn't return MAX_JIFFY_OFFSET since this function is designed   for scheduler, not for use in device drivers to calculate timeout value.     note:     NSEC_PER_SEC = 10^9 = (5^9   2^9) = (1953125   512)     ULLONG_MAX ns = 18446744073.709551615 secs = about 584 years ", "preempt_disable();/* * Make sure current didn't get patched between the above check and * preempt_disable(). ": "__klp_sched_try_switch(void){if (likely(!klp_patch_pending(current)))return;    This function is called from cond_resched() which is called in many   places throughout the kernel.  Using the klp_mutex here might   deadlock.     Instead, disable preemption to prevent racing with other callers of   klp_try_switch_task().  Thanks to task_call_func() they won't be   able to switch this task while it's running. ", "#define pr_fmt(fmt)\"gcov: \" fmt#include <linux/kernel.h>#include <linux/list.h>#include <linux/printk.h>#include <linux/ratelimit.h>#include <linux/slab.h>#include <linux/mm.h>#include \"gcov.h\"typedef void (*llvm_gcov_callback)(void);struct gcov_info ": "llvm_gcda_end_file()     This design is much more stateless and unstructured than gcc's, and is   intended to run at process exit.  This forces us to keep some local state   about which module we're dealing with at the moment.  On the other hand, it   also means we don't depend as much on how LLVM represents profiling data   internally.     See LLVM's libTransformsInstrumentationGCOVProfiling.cpp for more   details on how this works, particularly GCOVProfiler::emitProfileArcs(),   GCOVProfiler::insertCounterWriteout(), and   GCOVProfiler::insertFlush(). ", "void __gcov_init(struct gcov_info *info)": "__gcov_init is called by gcc-generated constructor code for each object   file compiled with -fprofile-arcs. ", "}EXPORT_SYMBOL(__gcov_flush": "__gcov_flush(void){  Unused. ", "}EXPORT_SYMBOL(__gcov_merge_add": "__gcov_merge_add(gcov_type  counters, unsigned int n_counters){  Unused. ", "}EXPORT_SYMBOL(__gcov_merge_single": "__gcov_merge_single(gcov_type  counters, unsigned int n_counters){  Unused. ", "}EXPORT_SYMBOL(__gcov_merge_delta": "__gcov_merge_delta(gcov_type  counters, unsigned int n_counters){  Unused. ", "}EXPORT_SYMBOL(__gcov_merge_ior": "__gcov_merge_ior(gcov_type  counters, unsigned int n_counters){  Unused. ", "}EXPORT_SYMBOL(__gcov_merge_time_profile": "__gcov_merge_time_profile(gcov_type  counters, unsigned int n_counters){  Unused. ", "}EXPORT_SYMBOL(__gcov_merge_icall_topn": "__gcov_merge_icall_topn(gcov_type  counters, unsigned int n_counters){  Unused. ", "}EXPORT_SYMBOL(__gcov_exit": "__gcov_exit(void){  Unused. ", "preempt_disable();if (!find_symbol(&fsa)) ": "module_layout\",.gplok= true,};    Since this should be found in kernel (which can't be removed), no   locking is necessary -- use preempt_disable() to placate lockdep. ", "int __request_module(bool wait, const char *fmt, ...)": "__request_module - try to load a kernel module   @wait: wait (or not) for the operation to complete   @fmt: printf style format string for the name of the module   @...: arguments as specified in the format string     Load a module using the user mode module loader. The function returns   zero on success or a negative errno code or positive exit code from   \"modprobe\" on failure. Note that a successful module load does not mean   the module did not then unload and exit on an error of its own. Callers   must check that the service they requested is now available not blindly   invoke it.     If module auto-loading support is disabled then this function   simply returns -ENOENT. ", "int module_refcount(struct module *mod)": "module_refcount() - return the refcount or -1 if unloading   @mod:the module we're checking     Return:  -1 if the module is in the process of unloading  otherwise the number of references in the kernel to the module ", "static inline int strong_try_module_get(struct module *mod)": "try_module_get(): 0 means success.   Otherwise an error is returned due to ongoing or failed   initialization etc. ", "static unsigned int find_sec(const struct load_info *info, const char *name)": "module_put_and_kthread_exit(struct module  mod, long code){module_put(mod);kthread_exit(code);}EXPORT_SYMBOL(__module_put_and_kthread_exit);  Find a module section: 0 means not found. ", "int kdbgetsymval(const char *symname, kdb_symtab_t *symtab)": "kdbgetsymval - Return the address of the given symbol.     Parameters:  symnameCharacter string containing symbol name        symtab  Structure to receive results   Returns:  0Symbol not found, symtab zero filled  1Symbol mapped to modulesymbolsection, data in symtab ", "struct file *sock_alloc_file(struct socket *sock, int flags, const char *dname)": "sock_alloc_file - Bind a &socket to a &file  @sock: socket  @flags: file status flags  @dname: protocol name    Returns the &file bound with @sock, implicitly storing it  in sock->file. If dname is %NULL, sets to \"\".    On failure @sock is released, and an ERR pointer is returned.    This function uses GFP_KERNEL internally. ", "struct socket *sock_from_file(struct file *file)": "sock_from_file - Return the &socket bounded to @file.  @file: file    On failure returns %NULL. ", "struct socket *sockfd_lookup(int fd, int *err)": "sockfd_lookup - Go from a file number to its socket slot  @fd: file handle  @err: pointer to an error code return    The file handle passed in is locked and the socket it is bound  to is returned. If an error occurs the err pointer is overwritten  with a negative errno code and NULL is returned. The function checks  for both invalid handles and passing a handle which is not a socket.    On a success the socket object pointer is returned. ", "#include <linux/bpf-cgroup.h>#include <linux/ethtool.h>#include <linux/mm.h>#include <linux/socket.h>#include <linux/file.h>#include <linux/splice.h>#include <linux/net.h>#include <linux/interrupt.h>#include <linux/thread_info.h>#include <linux/rcupdate.h>#include <linux/netdevice.h>#include <linux/proc_fs.h>#include <linux/seq_file.h>#include <linux/mutex.h>#include <linux/if_bridge.h>#include <linux/if_vlan.h>#include <linux/ptp_classify.h>#include <linux/init.h>#include <linux/poll.h>#include <linux/cache.h>#include <linux/module.h>#include <linux/highmem.h>#include <linux/mount.h>#include <linux/pseudo_fs.h>#include <linux/security.h>#include <linux/syscalls.h>#include <linux/compat.h>#include <linux/kmod.h>#include <linux/audit.h>#include <linux/wireless.h>#include <linux/nsproxy.h>#include <linux/magic.h>#include <linux/slab.h>#include <linux/xattr.h>#include <linux/nospec.h>#include <linux/indirect_call_wrapper.h>#include <linux/uaccess.h>#include <asm/unistd.h>#include <net/compat.h>#include <net/wext.h>#include <net/cls_cgroup.h>#include <net/sock.h>#include <linux/netfilter.h>#include <linux/if_tun.h>#include <linux/ipv6_route.h>#include <linux/route.h>#include <linux/termios.h>#include <linux/sockios.h>#include <net/busy_poll.h>#include <linux/errqueue.h>#include <linux/ptp_clock_kernel.h>#include <trace/events/sock.h>#ifdef CONFIG_NET_RX_BUSY_POLLunsigned int sysctl_net_busy_read __read_mostly;unsigned int sysctl_net_busy_poll __read_mostly;#endifstatic ssize_t sock_read_iter(struct kiocb *iocb, struct iov_iter *to);static ssize_t sock_write_iter(struct kiocb *iocb, struct iov_iter *from);static int sock_mmap(struct file *file, struct vm_area_struct *vma);static int sock_close(struct inode *inode, struct file *file);static __poll_t sock_poll(struct file *file,      struct poll_table_struct *wait);static long sock_ioctl(struct file *file, unsigned int cmd, unsigned long arg);#ifdef CONFIG_COMPATstatic long compat_sock_ioctl(struct file *file,      unsigned int cmd, unsigned long arg);#endifstatic int sock_fasync(int fd, struct file *filp, int on);static ssize_t sock_splice_read(struct file *file, loff_t *ppos,struct pipe_inode_info *pipe, size_t len,unsigned int flags);static void sock_splice_eof(struct file *file);#ifdef CONFIG_PROC_FSstatic void sock_show_fdinfo(struct seq_file *m, struct file *f)": "sock_release() public  for NetROM and future kernel nfsd type  stuff.  Alan Cox:sendmsgrecvmsg basics.  Tom Dyas:Export net symbols.  Marcin Dalecki:Fixed problems with CONFIG_NET=\"n\".  Alan Cox:Added thread locking to sys_  calls  for sockets. May have errors at the  moment.  Kevin Buhr:Fixed the dumb errors in the above.  Andi Kleen:Some small cleanups, optimizations,  and fixed a copy_from_user() bug.  Tigran Aivazian:sys_send(args) calls sys_sendto(args, NULL, 0)  Tigran Aivazian:Made listen(2) backlog sanity checks  protocol-independent    This module is effectively the top level interface to the BSD socket  paradigm.    Based upon Swansea University Computer Society NET3.039 ", "if (tsflags & SOF_TIMESTAMPING_BIND_PHC)flags |= SKBTX_HW_TSTAMP_USE_CYCLES;}if (tsflags & SOF_TIMESTAMPING_TX_SOFTWARE)flags |= SKBTX_SW_TSTAMP;if (tsflags & SOF_TIMESTAMPING_TX_SCHED)flags |= SKBTX_SCHED_TSTAMP;*tx_flags = flags;}EXPORT_SYMBOL(__sock_tx_timestamp": "__sock_tx_timestamp(__u16 tsflags, __u8  tx_flags){u8 flags =  tx_flags;if (tsflags & SOF_TIMESTAMPING_TX_HARDWARE) {flags |= SKBTX_HW_TSTAMP;  PTP hardware clocks can provide a free running cycle counter   as a time base for virtual clocks. Tell driver to use the   free running cycle counter for timestamp if socket is bound   to virtual clock. ", "int sock_sendmsg(struct socket *sock, struct msghdr *msg)": "sock_sendmsg_nosec(struct socket  sock, struct msghdr  msg){int ret = INDIRECT_CALL_INET(sock->ops->sendmsg, inet6_sendmsg,     inet_sendmsg, sock, msg,     msg_data_left(msg));BUG_ON(ret == -EIOCBQUEUED);if (trace_sock_send_length_enabled())call_trace_sock_send_length(sock->sk, ret, 0);return ret;}    sock_sendmsg - send a message through @sock  @sock: socket  @msg: message to send    Sends @msg through @sock, passing through LSM.  Returns the number of bytes sent, or an error code. ", "int kernel_sendmsg(struct socket *sock, struct msghdr *msg,   struct kvec *vec, size_t num, size_t size)": "kernel_sendmsg - send a message through @sock (kernel-space)  @sock: socket  @msg: message header  @vec: kernel vec  @num: vec array length  @size: total message data size    Builds the message data with @vec and sends it through @sock.  Returns the number of bytes sent, or an error code. ", "int kernel_sendmsg_locked(struct sock *sk, struct msghdr *msg,  struct kvec *vec, size_t num, size_t size)": "kernel_sendmsg_locked - send a message through @sock (kernel-space)  @sk: sock  @msg: message header  @vec: output sg array  @num: output sg array length  @size: total message data size    Builds the message data with @vec and sends it through @sock.  Returns the number of bytes sent, or an error code.  Caller must hold @sk. ", "int sock_recvmsg(struct socket *sock, struct msghdr *msg, int flags)": "sock_recvmsg_nosec(struct socket  sock, struct msghdr  msg,     int flags){int ret = INDIRECT_CALL_INET(sock->ops->recvmsg, inet6_recvmsg,     inet_recvmsg, sock, msg,     msg_data_left(msg), flags);if (trace_sock_recv_length_enabled())call_trace_sock_recv_length(sock->sk, ret, flags);return ret;}    sock_recvmsg - receive a message from @sock  @sock: socket  @msg: message to receive  @flags: message flags    Receives @msg from @sock, passing through LSM. Returns the total number  of bytes received, or an error. ", "int kernel_recvmsg(struct socket *sock, struct msghdr *msg,   struct kvec *vec, size_t num, size_t size, int flags)": "kernel_recvmsg - Receive a message from a socket (kernel space)  @sock: The socket to receive the message from  @msg: Received message  @vec: Input sg array for message data  @num: Size of input sg array  @size: Number of bytes to read  @flags: Message flags (MSG_DONTWAIT, etc...)    On return the msg structure contains the scattergather array passed in the  vec argument. The array is modified so that it consists of the unfilled  portion of the original array.    The returned value is the total number of bytes received, or an error. ", "int sock_create_lite(int family, int type, int protocol, struct socket **res)": "kernel_accept().  Returns 0 or an error. On failure @res is set to %NULL.  This function internally uses GFP_KERNEL. ", "int __sock_create(struct net *net, int family, int type, int protocol, struct socket **res, int kern)": "__sock_create - creates a socket  @net: net namespace  @family: protocol family (AF_INET, ...)  @type: communication type (SOCK_STREAM, ...)  @protocol: protocol (0, ...)  @res: new socket  @kern: boolean for kernel space sockets    Creates a new socket and assigns it to @res, passing through LSM.  Returns 0 or an error. On failure @res is set to %NULL. @kern must  be set to true if the socket resides in kernel space.  This function internally uses GFP_KERNEL. ", "int sock_create_kern(struct net *net, int family, int type, int protocol, struct socket **res)": "sock_create_kern - creates a socket (kernel space)  @net: net namespace  @family: protocol family (AF_INET, ...)  @type: communication type (SOCK_STREAM, ...)  @protocol: protocol (0, ...)  @res: new socket    A wrapper around __sock_create().  Returns 0 or an error. This function internally uses GFP_KERNEL. ", "int sock_register(const struct net_proto_family *ops)": "sock_register - add a socket protocol handler  @ops: description of protocol    This function is called by a protocol handler that wants to  advertise its address family, and have it linked into the  socket interface. The value ops->family corresponds to the  socket system call protocol family. ", "void sock_unregister(int family)": "sock_unregister - remove a protocol handler  @family: protocol family to remove    This function is called by a protocol handler that wants to  remove its address family, and have it unlinked from the  new socket creation.    If protocol handler is a module, then it can use module reference  counts to protect against new references. If protocol handler is not  a module then it needs to provide its own protection in  the ops->create routine. ", "static long sock_ioctl(struct file *file, unsigned cmd, unsigned long arg)": "put_user_ifreq(&ifr, argp))return -EFAULT;return err;}   With an ioctl, arg may well be a user mode pointer, but we don't know  what to do with it - that's up to the protocol still. ", "int kernel_bind(struct socket *sock, struct sockaddr *addr, int addrlen)": "kernel_bind - bind an address to a socket (kernel space)  @sock: socket  @addr: address  @addrlen: length of address    Returns 0 or an error. ", "int kernel_listen(struct socket *sock, int backlog)": "kernel_listen - move socket to listening state (kernel space)  @sock: socket  @backlog: pending connections queue size    Returns 0 or an error. ", "int kernel_connect(struct socket *sock, struct sockaddr *addr, int addrlen,   int flags)": "kernel_connect - connect a socket (kernel space)  @sock: socket  @addr: address  @addrlen: address length  @flags: flags (O_NONBLOCK, ...)    For datagram sockets, @addr is the address to which datagrams are sent  by default, and the only address from which datagrams are received.  For stream sockets, attempts to connect to @addr.  Returns 0 or an error code. ", "int kernel_getsockname(struct socket *sock, struct sockaddr *addr)": "kernel_getsockname - get the address which the socket is bound (kernel space)  @sock: socket  @addr: address holder     Fills the @addr pointer with the address which the socket is bound.  Returns the length of the address in bytes or an error code. ", "int kernel_getpeername(struct socket *sock, struct sockaddr *addr)": "kernel_getpeername - get the address which the socket is connected (kernel space)  @sock: socket  @addr: address holder     Fills the @addr pointer with the address which the socket is connected.  Returns the length of the address in bytes or an error code. ", "int kernel_sock_shutdown(struct socket *sock, enum sock_shutdown_cmd how)": "kernel_sock_shutdown - shut down part of a full-duplex connection (kernel space)  @sock: socket  @how: connection part    Returns 0 or an error. ", "u32 kernel_sock_ip_overhead(struct sock *sk)": "kernel_sock_ip_overhead - returns the IP overhead imposed by a socket  @sk: socket    This routine returns the IP overhead imposed by a socket i.e.  the length of the underlying IP header, depending on whether  this is an IPv4 or IPv6 socket and the length from IP options turned  on at the socket. Assumes that the caller has a lock on the socket. ", "int devm_register_netdev(struct device *dev, struct net_device *ndev)": "devm_register_netdev - resource managed variant of register_netdev()  @dev: managing device for this netdev - usually the parent device  @ndev: device to register    This is a devres variant of register_netdev() for which the unregister  function will be called automatically when the managing device is  detached. Note: the net_device used must also be resource managed by  the same struct device. ", "if (cmd->cmd_cb)cmd->cmd_cb(ddev, cmd->cb_context, ERR_PTR(-ENODEV));kfree(cmd->mdaa_params);kfree(cmd);}}EXPORT_SYMBOL(nfc_digital_unregister_device": "nfc_digital_unregister_device(struct nfc_digital_dev  ddev){struct digital_cmd  cmd,  n;nfc_unregister_device(ddev->nfc_dev);mutex_lock(&ddev->poll_lock);ddev->poll_tech_count = 0;mutex_unlock(&ddev->poll_lock);cancel_delayed_work_sync(&ddev->poll_work);cancel_work_sync(&ddev->cmd_work);cancel_work_sync(&ddev->cmd_complete_work);list_for_each_entry_safe(cmd, n, &ddev->cmd_queue, queue) {list_del(&cmd->queue);  Call the command callback if any and pass it a ENODEV error.   This gives a chance to the command issuer to free any   allocated buffer. ", "int nfc_fw_download_done(struct nfc_dev *dev, const char *firmware_name, u32 result)": "nfc_fw_download_done - inform that a firmware download was completed     @dev: The nfc device to which firmware was downloaded   @firmware_name: The firmware filename   @result: The positive value of a standard errno value ", "if (dev->dep_link_up == false) ": "nfc_tm_data_received(struct nfc_dev  dev, struct sk_buff  skb){  Only LLCP target mode for now ", "struct sk_buff *nfc_alloc_recv_skb(unsigned int size, gfp_t gfp)": "nfc_alloc_recv_skb - allocate a skb for data exchange responses     @size: size to allocate   @gfp: gfp flags ", "int nfc_targets_found(struct nfc_dev *dev,      struct nfc_target *targets, int n_targets)": "nfc_targets_found - inform that targets were found     @dev: The nfc device that found the targets   @targets: array of nfc targets found   @n_targets: targets array size     The device driver must call this function when one or many nfc targets   are found. After calling this function, the device driver must stop   polling for targets.   NOTE: This function can be called with targets=NULL and n_targets=0 to   notify a driver error, meaning that the polling operation cannot complete.   IMPORTANT: this function must not be called from an atomic context.   In addition, it must also not be called from a context that would prevent   the NFC Core to call other nfc ops entry point concurrently. ", "int nfc_target_lost(struct nfc_dev *dev, u32 target_idx)": "nfc_target_lost - inform that an activated target went out of field     @dev: The nfc device that had the activated target in field   @target_idx: the nfc index of the target     The device driver must call this function when the activated target   goes out of the field.   IMPORTANT: this function must not be called from an atomic context.   In addition, it must also not be called from a context that would prevent   the NFC Core to call other nfc ops entry point concurrently. ", "struct nfc_dev *nfc_allocate_device(const struct nfc_ops *ops,    u32 supported_protocols,    int tx_headroom, int tx_tailroom)": "nfc_allocate_device - allocate a new nfc device     @ops: device operations   @supported_protocols: NFC protocols supported by the device   @tx_headroom: reserved space at beginning of skb   @tx_tailroom: reserved space at end of skb ", "int nfc_register_device(struct nfc_dev *dev)": "nfc_register_device - register a nfc device in the nfc subsystem     @dev: The nfc device to register ", "void nfc_unregister_device(struct nfc_dev *dev)": "nfc_unregister_device - unregister a nfc device in the nfc subsystem     @dev: The nfc device to unregister ", "memset(skb->cb, 0, sizeof(skb->cb));if (WARN_ON(!dev->cur_cmd_info)) ": "nfc_vendor_cmd_reply(struct sk_buff  skb){struct nfc_dev  dev = ((void   )skb->cb)[0];void  hdr = ((void   )skb->cb)[1];  clear CB data for netlink core to own from now on ", "if (targets->hci_reader_gate == 0x00)targets->hci_reader_gate = gate;r = nfc_targets_found(hdev->ndev, targets, 1);exit:kfree(targets);kfree_skb(atqa_skb);kfree_skb(sak_skb);kfree_skb(uid_skb);return r;}EXPORT_SYMBOL(nfc_hci_target_discovered": "nfc_hci_target_discovered(struct nfc_hci_dev  hdev, u8 gate){struct nfc_target  targets;struct sk_buff  atqa_skb = NULL;struct sk_buff  sak_skb = NULL;struct sk_buff  uid_skb = NULL;int r;pr_debug(\"from gate %d\\n\", gate);targets = kzalloc(sizeof(struct nfc_target), GFP_KERNEL);if (targets == NULL)return -ENOMEM;switch (gate) {case NFC_HCI_RF_READER_A_GATE:r = nfc_hci_get_param(hdev, NFC_HCI_RF_READER_A_GATE,      NFC_HCI_RF_READER_A_ATQA, &atqa_skb);if (r < 0)goto exit;r = nfc_hci_get_param(hdev, NFC_HCI_RF_READER_A_GATE,      NFC_HCI_RF_READER_A_SAK, &sak_skb);if (r < 0)goto exit;if (atqa_skb->len != 2 || sak_skb->len != 1) {r = -EPROTO;goto exit;}targets->supported_protocols =nfc_hci_sak_to_protocol(sak_skb->data[0]);if (targets->supported_protocols == 0xffffffff) {r = -EPROTO;goto exit;}targets->sens_res = be16_to_cpu( (__be16  )atqa_skb->data);targets->sel_res = sak_skb->data[0];r = nfc_hci_get_param(hdev, NFC_HCI_RF_READER_A_GATE,      NFC_HCI_RF_READER_A_UID, &uid_skb);if (r < 0)goto exit;if (uid_skb->len == 0 || uid_skb->len > NFC_NFCID1_MAXSIZE) {r = -EPROTO;goto exit;}memcpy(targets->nfcid1, uid_skb->data, uid_skb->len);targets->nfcid1_len = uid_skb->len;if (hdev->ops->complete_target_discovered) {r = hdev->ops->complete_target_discovered(hdev, gate,  targets);if (r < 0)goto exit;}break;case NFC_HCI_RF_READER_B_GATE:targets->supported_protocols = NFC_PROTO_ISO14443_B_MASK;break;default:if (hdev->ops->target_from_gate)r = hdev->ops->target_from_gate(hdev, gate, targets);elser = -EPROTO;if (r < 0)goto exit;if (hdev->ops->complete_target_discovered) {r = hdev->ops->complete_target_discovered(hdev, gate,  targets);if (r < 0)goto exit;}break;}  if driver set the new gate, we will skip the old one ", "r = hdev->ops->load_session(hdev);if (r < 0)goto disconnect_all;} else ": "nfc_hci_driver_failure(hdev, r);}static void nfc_hci_cmd_timeout(struct timer_list  t){struct nfc_hci_dev  hdev = from_timer(hdev, t, cmd_timer);schedule_work(&hdev->msg_tx_work);}static int hci_dev_connect_gates(struct nfc_hci_dev  hdev, u8 gate_count, const struct nfc_hci_gate  gates){int r;while (gate_count--) {r = nfc_hci_connect_gate(hdev, NFC_HCI_HOST_CONTROLLER_ID, gates->gate, gates->pipe);if (r < 0)return r;gates++;}return 0;}static int hci_dev_session_init(struct nfc_hci_dev  hdev){struct sk_buff  skb = NULL;int r;if (hdev->init_data.gates[0].gate != NFC_HCI_ADMIN_GATE)return -EPROTO;r = nfc_hci_connect_gate(hdev, NFC_HCI_HOST_CONTROLLER_ID, hdev->init_data.gates[0].gate, hdev->init_data.gates[0].pipe);if (r < 0)goto exit;r = nfc_hci_get_param(hdev, NFC_HCI_ADMIN_GATE,      NFC_HCI_ADMIN_SESSION_IDENTITY, &skb);if (r < 0)goto disconnect_all;if (skb->len && skb->len == strlen(hdev->init_data.session_id) &&(memcmp(hdev->init_data.session_id, skb->data,   skb->len) == 0) && hdev->ops->load_session) {  Restore gate<->pipe table from some proprietary location. ", "pr_debug(\"idx=%d to gate %d\\n\", idx, gate);tmp = kmalloc(1 + param_len, GFP_KERNEL);if (tmp == NULL)return -ENOMEM;*tmp = idx;memcpy(tmp + 1, param, param_len);r = nfc_hci_send_cmd(hdev, gate, NFC_HCI_ANY_SET_PARAMETER,     tmp, param_len + 1, NULL);kfree(tmp);return r;}EXPORT_SYMBOL(nfc_hci_set_param": "nfc_hci_set_param(struct nfc_hci_dev  hdev, u8 gate, u8 idx,      const u8  param, size_t param_len){int r;u8  tmp;  TODO ELa: reg idx must be inserted before param, but we don't want   to ask the caller to do it to keep a simpler API.   For now, just create a new temporary param buffer. This is far from   optimal though, and the plan is to modify APIs to pass idx down to   nfc_hci_hcp_message_tx where the frame is actually built, thereby   eliminating the need for the temp allocation-copy here. ", "}return r;}hdev->pipes[pipe].gate = dest_gate;hdev->pipes[pipe].dest_host = dest_host;hdev->gate2pipe[dest_gate] = pipe;return 0;}EXPORT_SYMBOL(nfc_hci_connect_gate": "nfc_hci_connect_gate(struct nfc_hci_dev  hdev, u8 dest_host, u8 dest_gate, u8 pipe){bool pipe_created = false;int r;if (pipe == NFC_HCI_DO_NOT_CREATE_PIPE)return 0;if (hdev->gate2pipe[dest_gate] != NFC_HCI_INVALID_PIPE)return -EADDRINUSE;if (pipe != NFC_HCI_INVALID_PIPE)goto open_pipe;switch (dest_gate) {case NFC_HCI_LINK_MGMT_GATE:pipe = NFC_HCI_LINK_MGMT_PIPE;break;case NFC_HCI_ADMIN_GATE:pipe = NFC_HCI_ADMIN_PIPE;break;default:pipe = nfc_hci_create_pipe(hdev, dest_host, dest_gate, &r);if (pipe == NFC_HCI_INVALID_PIPE)return r;pipe_created = true;break;}open_pipe:r = nfc_hci_open_pipe(hdev, pipe);if (r < 0) {if (pipe_created)if (nfc_hci_delete_pipe(hdev, pipe) < 0) {  TODO: Cannot clean by deleting pipe...   -> inconsistent state ", "if (skb->len <= conn_info->max_pkt_payload_len) ": "nci_send_data(struct nci_dev  ndev, __u8 conn_id, struct sk_buff  skb){const struct nci_conn_info  conn_info;int rc = 0;pr_debug(\"conn_id 0x%x, plen %d\\n\", conn_id, skb->len);conn_info = nci_get_conn_info_by_conn_id(ndev, conn_id);if (!conn_info) {rc = -EPROTO;goto free_exit;}  check if the packet need to be fragmented ", "*num = 0;/* by default mapping is set to NCI_RF_INTERFACE_FRAME ": "nci_send_cmd(ndev, NCI_OP_CORE_RESET_CMD, 1, &cmd);}static void nci_init_req(struct nci_dev  ndev, const void  opt){u8 plen = 0;if (opt)plen = sizeof(struct nci_core_init_v2_cmd);nci_send_cmd(ndev, NCI_OP_CORE_INIT_CMD, plen, opt);}static void nci_init_complete_req(struct nci_dev  ndev, const void  opt){struct nci_rf_disc_map_cmd cmd;struct disc_map_config  cfg = cmd.mapping_configs;__u8  num = &cmd.num_mapping_configs;int i;  set rf mapping configurations ", "conn_info->data_exchange_cb = nci_nfcc_loopback_cb;conn_info->data_exchange_cb_context = ndev;skb = nci_skb_alloc(ndev, NCI_DATA_HDR_SIZE + data_len, GFP_KERNEL);if (!skb)return -ENOMEM;skb_reserve(skb, NCI_DATA_HDR_SIZE);skb_put_data(skb, data, data_len);loopback_data.conn_id = conn_id;loopback_data.data = skb;ndev->cur_conn_id = conn_id;r = nci_request(ndev, nci_send_data_req, &loopback_data,msecs_to_jiffies(NCI_DATA_TIMEOUT));if (r == NCI_STATUS_OK && resp)*resp = conn_info->rx_skb;return r;}EXPORT_SYMBOL(nci_nfcc_loopback": "nci_nfcc_loopback_cb(void  context, struct sk_buff  skb, int err){struct nci_dev  ndev = (struct nci_dev  )context;struct nci_conn_info  conn_info;conn_info = nci_get_conn_info_by_conn_id(ndev, ndev->cur_conn_id);if (!conn_info) {nci_req_complete(ndev, NCI_STATUS_REJECTED);return;}conn_info->rx_skb = skb;nci_req_complete(ndev, NCI_STATUS_OK);}int nci_nfcc_loopback(struct nci_dev  ndev, const void  data, size_t data_len,      struct sk_buff   resp){int r;struct nci_loopback_data loopback_data;struct nci_conn_info  conn_info;struct sk_buff  skb;int conn_id = nci_get_conn_info_by_dest_type_params(ndev,NCI_DESTINATION_NFCC_LOOPBACK, NULL);if (conn_id < 0) {r = nci_core_conn_create(ndev, NCI_DESTINATION_NFCC_LOOPBACK, 0, 0, NULL);if (r != NCI_STATUS_OK)return r;conn_id = nci_get_conn_info_by_dest_type_params(ndev,NCI_DESTINATION_NFCC_LOOPBACK,NULL);}conn_info = nci_get_conn_info_by_conn_id(ndev, conn_id);if (!conn_info)return -EPROTO;  store cb and context to be used on receiving data ", "conn_info->data_exchange_cb = nci_nfcc_loopback_cb;conn_info->data_exchange_cb_context = ndev;skb = nci_skb_alloc(ndev, NCI_DATA_HDR_SIZE + data_len, GFP_KERNEL);if (!skb)return -ENOMEM;skb_reserve(skb, NCI_DATA_HDR_SIZE);skb_put_data(skb, data, data_len);loopback_data.conn_id = conn_id;loopback_data.data = skb;ndev->cur_conn_id = conn_id;r = nci_request(ndev, nci_send_data_req, &loopback_data,msecs_to_jiffies(NCI_DATA_TIMEOUT));if (r == NCI_STATUS_OK && resp)*resp = conn_info->rx_skb;return r;}EXPORT_SYMBOL(nci_nfcc_loopback);static int nci_open_device(struct nci_dev *ndev)": "nci_set_config_param {__u8id;size_tlen;const __u8 val;};static void nci_set_config_req(struct nci_dev  ndev, const void  opt){const struct nci_set_config_param  param = opt;struct nci_core_set_config_cmd cmd;BUG_ON(param->len > NCI_MAX_PARAM_LEN);cmd.num_params = 1;cmd.param.id = param->id;cmd.param.len = param->len;memcpy(cmd.param.val, param->val, param->len);nci_send_cmd(ndev, NCI_OP_CORE_SET_CONFIG_CMD, (3 + param->len), &cmd);}struct nci_rf_discover_param {__u32im_protocols;__u32tm_protocols;};static void nci_rf_discover_req(struct nci_dev  ndev, const void  opt){const struct nci_rf_discover_param  param = opt;struct nci_rf_disc_cmd cmd;cmd.num_disc_configs = 0;if ((cmd.num_disc_configs < NCI_MAX_NUM_RF_CONFIGS) &&    (param->im_protocols & NFC_PROTO_JEWEL_MASK ||     param->im_protocols & NFC_PROTO_MIFARE_MASK ||     param->im_protocols & NFC_PROTO_ISO14443_MASK ||     param->im_protocols & NFC_PROTO_NFC_DEP_MASK)) {cmd.disc_configs[cmd.num_disc_configs].rf_tech_and_mode =NCI_NFC_A_PASSIVE_POLL_MODE;cmd.disc_configs[cmd.num_disc_configs].frequency = 1;cmd.num_disc_configs++;}if ((cmd.num_disc_configs < NCI_MAX_NUM_RF_CONFIGS) &&    (param->im_protocols & NFC_PROTO_ISO14443_B_MASK)) {cmd.disc_configs[cmd.num_disc_configs].rf_tech_and_mode =NCI_NFC_B_PASSIVE_POLL_MODE;cmd.disc_configs[cmd.num_disc_configs].frequency = 1;cmd.num_disc_configs++;}if ((cmd.num_disc_configs < NCI_MAX_NUM_RF_CONFIGS) &&    (param->im_protocols & NFC_PROTO_FELICA_MASK ||     param->im_protocols & NFC_PROTO_NFC_DEP_MASK)) {cmd.disc_configs[cmd.num_disc_configs].rf_tech_and_mode =NCI_NFC_F_PASSIVE_POLL_MODE;cmd.disc_configs[cmd.num_disc_configs].frequency = 1;cmd.num_disc_configs++;}if ((cmd.num_disc_configs < NCI_MAX_NUM_RF_CONFIGS) &&    (param->im_protocols & NFC_PROTO_ISO15693_MASK)) {cmd.disc_configs[cmd.num_disc_configs].rf_tech_and_mode =NCI_NFC_V_PASSIVE_POLL_MODE;cmd.disc_configs[cmd.num_disc_configs].frequency = 1;cmd.num_disc_configs++;}if ((cmd.num_disc_configs < NCI_MAX_NUM_RF_CONFIGS - 1) &&    (param->tm_protocols & NFC_PROTO_NFC_DEP_MASK)) {cmd.disc_configs[cmd.num_disc_configs].rf_tech_and_mode =NCI_NFC_A_PASSIVE_LISTEN_MODE;cmd.disc_configs[cmd.num_disc_configs].frequency = 1;cmd.num_disc_configs++;cmd.disc_configs[cmd.num_disc_configs].rf_tech_and_mode =NCI_NFC_F_PASSIVE_LISTEN_MODE;cmd.disc_configs[cmd.num_disc_configs].frequency = 1;cmd.num_disc_configs++;}nci_send_cmd(ndev, NCI_OP_RF_DISCOVER_CMD,     (1 + (cmd.num_disc_configs   sizeof(struct disc_config))),     &cmd);}struct nci_rf_discover_select_param {__u8rf_discovery_id;__u8rf_protocol;};static void nci_rf_discover_select_req(struct nci_dev  ndev, const void  opt){const struct nci_rf_discover_select_param  param = opt;struct nci_rf_discover_select_cmd cmd;cmd.rf_discovery_id = param->rf_discovery_id;cmd.rf_protocol = param->rf_protocol;switch (cmd.rf_protocol) {case NCI_RF_PROTOCOL_ISO_DEP:cmd.rf_interface = NCI_RF_INTERFACE_ISO_DEP;break;case NCI_RF_PROTOCOL_NFC_DEP:cmd.rf_interface = NCI_RF_INTERFACE_NFC_DEP;break;default:cmd.rf_interface = NCI_RF_INTERFACE_FRAME;break;}nci_send_cmd(ndev, NCI_OP_RF_DISCOVER_SELECT_CMD,     sizeof(struct nci_rf_discover_select_cmd), &cmd);}static void nci_rf_deactivate_req(struct nci_dev  ndev, const void  opt){struct nci_rf_deactivate_cmd cmd;cmd.type = (unsigned long)opt;nci_send_cmd(ndev, NCI_OP_RF_DEACTIVATE_CMD,     sizeof(struct nci_rf_deactivate_cmd), &cmd);}struct nci_cmd_param {__u16 opcode;size_t len;const __u8  payload;};static void nci_generic_req(struct nci_dev  ndev, const void  opt){const struct nci_cmd_param  param = opt;nci_send_cmd(ndev, param->opcode, param->len, param->payload);}int nci_prop_cmd(struct nci_dev  ndev, __u8 oid, size_t len, const __u8  payload){struct nci_cmd_param param;param.opcode = nci_opcode_pack(NCI_GID_PROPRIETARY, oid);param.len = len;param.payload = payload;return __nci_request(ndev, nci_generic_req, &param,     msecs_to_jiffies(NCI_CMD_TIMEOUT));}EXPORT_SYMBOL(nci_prop_cmd);int nci_core_cmd(struct nci_dev  ndev, __u16 opcode, size_t len, const __u8  payload){struct nci_cmd_param param;param.opcode = opcode;param.len = len;param.payload = payload;return __nci_request(ndev, nci_generic_req, &param,     msecs_to_jiffies(NCI_CMD_TIMEOUT));}EXPORT_SYMBOL(nci_core_cmd);int nci_core_reset(struct nci_dev  ndev){return __nci_request(ndev, nci_reset_req, (void  )0,     msecs_to_jiffies(NCI_RESET_TIMEOUT));}EXPORT_SYMBOL(nci_core_reset);int nci_core_init(struct nci_dev  ndev){return __nci_request(ndev, nci_init_req, (void  )0,     msecs_to_jiffies(NCI_INIT_TIMEOUT));}EXPORT_SYMBOL(nci_core_init);struct nci_loopback_data {u8 conn_id;struct sk_buff  data;};static void nci_send_data_req(struct nci_dev  ndev, const void  opt){const struct nci_loopback_data  data = opt;nci_send_data(ndev, data->conn_id, data->data);}static void nci_nfcc_loopback_cb(void  context, struct sk_buff  skb, int err){struct nci_dev  ndev = (struct nci_dev  )context;struct nci_conn_info  conn_info;conn_info = nci_get_conn_info_by_conn_id(ndev, ndev->cur_conn_id);if (!conn_info) {nci_req_complete(ndev, NCI_STATUS_REJECTED);return;}conn_info->rx_skb = skb;nci_req_complete(ndev, NCI_STATUS_OK);}int nci_nfcc_loopback(struct nci_dev  ndev, const void  data, size_t data_len,      struct sk_buff   resp){int r;struct nci_loopback_data loopback_data;struct nci_conn_info  conn_info;struct sk_buff  skb;int conn_id = nci_get_conn_info_by_dest_type_params(ndev,NCI_DESTINATION_NFCC_LOOPBACK, NULL);if (conn_id < 0) {r = nci_core_conn_create(ndev, NCI_DESTINATION_NFCC_LOOPBACK, 0, 0, NULL);if (r != NCI_STATUS_OK)return r;conn_id = nci_get_conn_info_by_dest_type_params(ndev,NCI_DESTINATION_NFCC_LOOPBACK,NULL);}conn_info = nci_get_conn_info_by_conn_id(ndev, conn_id);if (!conn_info)return -EPROTO;  store cb and context to be used on receiving data ", "void nci_req_complete(struct nci_dev *ndev, int result)": "nci_core_conn_create_cmd  cmd;};static void nci_cmd_work(struct work_struct  work);static void nci_rx_work(struct work_struct  work);static void nci_tx_work(struct work_struct  work);struct nci_conn_info  nci_get_conn_info_by_conn_id(struct nci_dev  ndev,   int conn_id){struct nci_conn_info  conn_info;list_for_each_entry(conn_info, &ndev->conn_info_list, list) {if (conn_info->conn_id == conn_id)return conn_info;}return NULL;}int nci_get_conn_info_by_dest_type_params(struct nci_dev  ndev, u8 dest_type,  const struct dest_spec_params  params){const struct nci_conn_info  conn_info;list_for_each_entry(conn_info, &ndev->conn_info_list, list) {if (conn_info->dest_type == dest_type) {if (!params)return conn_info->conn_id;if (params->id == conn_info->dest_params->id &&    params->protocol == conn_info->dest_params->protocol)return conn_info->conn_id;}}return -EINVAL;}EXPORT_SYMBOL(nci_get_conn_info_by_dest_type_params);  ---- NCI requests ---- ", "struct nci_dev *nci_allocate_device(const struct nci_ops *ops,    __u32 supported_protocols,    int tx_headroom, int tx_tailroom)": "nci_allocate_device - allocate a new nci device     @ops: device operations   @supported_protocols: NFC protocols supported by the device   @tx_headroom: Reserved space at beginning of skb   @tx_tailroom: Reserved space at end of skb ", "void nci_free_device(struct nci_dev *ndev)": "nci_free_device - deallocate nci device     @ndev: The nci device to deallocate ", "int nci_register_device(struct nci_dev *ndev)": "nci_register_device - register a nci device in the nfc subsystem     @ndev: The nci device to register ", "mutex_lock(&ndev->req_lock);if (!test_and_clear_bit(NCI_UP, &ndev->flags)) ": "nci_unregister_device ", "int nci_recv_frame(struct nci_dev *ndev, struct sk_buff *skb)": "nci_recv_frame - receive frame from NCI drivers     @ndev: The nci device   @skb: The sk_buff to receive ", "skb_orphan(skb);/* Send copy to sniffer ": "nci_send_frame(struct nci_dev  ndev, struct sk_buff  skb){pr_debug(\"len %d\\n\", skb->len);if (!ndev) {kfree_skb(skb);return -ENODEV;}  Get rid of skb owner, prior to sending to the driver. ", "}}return r;}ndev->hci_dev->pipes[pipe].gate = dest_gate;ndev->hci_dev->pipes[pipe].host = dest_host;ndev->hci_dev->gate2pipe[dest_gate] = pipe;return 0;}EXPORT_SYMBOL(nci_hci_connect_gate": "nci_hci_connect_gate(struct nci_dev  ndev, u8 dest_host, u8 dest_gate, u8 pipe){bool pipe_created = false;int r;if (pipe == NCI_HCI_DO_NOT_OPEN_PIPE)return 0;if (ndev->hci_dev->gate2pipe[dest_gate] != NCI_HCI_INVALID_PIPE)return -EADDRINUSE;if (pipe != NCI_HCI_INVALID_PIPE)goto open_pipe;switch (dest_gate) {case NCI_HCI_LINK_MGMT_GATE:pipe = NCI_HCI_LINK_MGMT_PIPE;break;case NCI_HCI_ADMIN_GATE:pipe = NCI_HCI_ADMIN_PIPE;break;default:pipe = nci_hci_create_pipe(ndev, dest_host, dest_gate, &r);if (pipe == NCI_HCI_INVALID_PIPE)return r;pipe_created = true;break;}open_pipe:r = nci_hci_open_pipe(ndev, pipe);if (r < 0) {if (pipe_created) {if (nci_hci_delete_pipe(ndev, pipe) < 0) {  TODO: Cannot clean by deleting pipe...   -> inconsistent state ", "r = ndev->ops->hci_load_session(ndev);} else ": "nci_hci_dev_session_init(struct nci_dev  ndev){struct nci_conn_info  conn_info;struct sk_buff  skb;int r;ndev->hci_dev->count_pipes = 0;ndev->hci_dev->expected_pipes = 0;conn_info = ndev->hci_dev->conn_info;if (!conn_info)return -EPROTO;conn_info->data_exchange_cb = nci_hci_data_received_cb;conn_info->data_exchange_cb_context = ndev;nci_hci_reset_pipes(ndev->hci_dev);if (ndev->hci_dev->init_data.gates[0].gate != NCI_HCI_ADMIN_GATE)return -EPROTO;r = nci_hci_connect_gate(ndev, ndev->hci_dev->init_data.gates[0].dest_host, ndev->hci_dev->init_data.gates[0].gate, ndev->hci_dev->init_data.gates[0].pipe);if (r < 0)return r;r = nci_hci_get_param(ndev, NCI_HCI_ADMIN_GATE,      NCI_HCI_ADMIN_PARAM_SESSION_IDENTITY, &skb);if (r < 0)return r;if (skb->len &&    skb->len == strlen(ndev->hci_dev->init_data.session_id) &&    !memcmp(ndev->hci_dev->init_data.session_id, skb->data, skb->len) &&    ndev->ops->hci_load_session) {  Restore gate<->pipe table from some proprietary location. ", "if (type == LLC_DEST_SAP || type == LLC_DEST_CONN)llc_type_handlers[type - 1] = handler;}void llc_remove_pack(int type)": "llc_add_pack(int type, void ( handler)(struct llc_sap  sap,    struct sk_buff  skb)){smp_wmb();   ensure initialisation is complete before it's called ", "if (handler)smp_wmb();llc_station_handler = handler;if (!handler)synchronize_net();}/** *llc_pdu_type - returns which LLC component must handle for PDU *@skb: input skb * *This function returns which LLC component must handle this PDU. ": "llc_set_station_handler(void ( handler)(struct sk_buff  skb)){  Ensure initialisation is complete before it's called ", "int llc_mac_hdr_init(struct sk_buff *skb,     const unsigned char *sa, const unsigned char *da)": "llc_mac_hdr_init - fills MAC header fields  @skb: Address of the frame to initialize its MAC header  @sa: The MAC source address  @da: The MAC destination address    Fills MAC header fields, depending on MAC type. Returns 0, If MAC type  is a valid type and initialization completes correctly 1, otherwise. ", "int llc_build_and_send_ui_pkt(struct llc_sap *sap, struct sk_buff *skb,      const unsigned char *dmac, unsigned char dsap)": "llc_build_and_send_ui_pkt - unitdata request interface for upper layers  @sap: sap to use  @skb: packet to send  @dmac: destination mac address  @dsap: destination sap    Upper layers calls this function when upper layer wants to send data  using connection-less mode communication (UI pdu).    Accept data frame from network layer to be sent using connection-  less mode communication; timeoutretries handled by network layer;  package primitive as an event and send to SAP event handler ", "static struct llc_sap *llc_sap_alloc(void)": "llc_sap_list);static DEFINE_SPINLOCK(llc_sap_list_lock);    llc_sap_alloc - allocates and initializes sap.    Allocates and initializes sap. ", "struct llc_sap *llc_sap_find(unsigned char sap_value)": "llc_sap_find(unsigned char sap_value){struct llc_sap  sap;list_for_each_entry(sap, &llc_sap_list, node)if (sap->laddr.lsap == sap_value)goto out;sap = NULL;out:return sap;}    llc_sap_find - searches a SAP in station  @sap_value: sap to be found    Searches for a sap in the sap list of the LLC's station upon the sap ID.  If the sap is found it will be refcounted and the user will have to do  a llc_sap_put after use.  Returns the sap or %NULL if not found. ", "struct llc_sap *llc_sap_open(unsigned char lsap,     int (*func)(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt, struct net_device *orig_dev))": "llc_sap_open - open interface to the upper layers.  @lsap: SAP number.  @func: rcv func for datalink protos    Interface function to upper layer. Each one who wants to get a SAP  (for example NetBEUI) should call this function. Returns the opened  SAP for success, NULL for failure. ", "void llc_sap_close(struct llc_sap *sap)": "llc_sap_close - close interface for upper layers.  @sap: SAP to be closed.    Close interface function to upper layer. Each one who wants to  close an open SAP (for example NetBEUI) should call this function.   Removes this sap from the list of saps in the station and then   frees the memory for this sap. ", "if (paclen == 0) ": "ax25_send_frame(struct sk_buff  skb, int paclen, const ax25_address  src, ax25_address  dest, ax25_digi  digi, struct net_device  dev){ax25_dev  ax25_dev;ax25_cb  ax25;    Take the default packet length for the device if zero is   specified. ", "ax25_address src_c;ax25_address dst_c;if ((ourskb = skb_copy(skb, GFP_ATOMIC)) == NULL) ": "ax25_ip_xmit(struct sk_buff  skb){struct sk_buff  ourskb;unsigned char  bp  = skb->data;ax25_route  route;struct net_device  dev = NULL;ax25_address  src,  dst;ax25_digi  digipeat = NULL;ax25_dev  ax25_dev;ax25_cb  ax25;char ip_mode = ' ';dst = (ax25_address  )(bp + 1);src = (ax25_address  )(bp + 8);ax25_route_lock_use();route = ax25_get_route(dst, NULL);if (route) {digipeat = route->digipeat;dev = route->dev;ip_mode = route->ip_mode;}if (dev == NULL)dev = skb->dev;if ((ax25_dev = ax25_dev_ax25dev(dev)) == NULL) {kfree_skb(skb);goto put;}if (bp[16] == AX25_P_IP) {if (ip_mode == 'V' || (ip_mode == ' ' && ax25_dev->values[AX25_VALUES_IPDEFMODE])) {   We copy the buffer and release the original thereby  keeping it straight    Note: we report 1 back so the caller will  not feed the frame direct to the physical device  We don't want that to happen. (It won't be upset  as we have pulled the frame from the queue by  freeing it).    NB: TCP modifies buffers that are still  on a device queue, thus we use skb_copy()        instead of using skb_clone() unless this  gets fixed. ", "return 1;ct++;}if ((a->ax25_call[ct] & 0x1E) == (b->ax25_call[ct] & 0x1E))/* SSID without control bit ": "ax25cmp(const ax25_address  a, const ax25_address  b){int ct = 0;while (ct < 6) {if ((a->ax25_call[ct] & 0xFE) != (b->ax25_call[ct] & 0xFE))  Clean off repeater bits ", "atomic_set(&chan->nesting, L2CAP_NESTING_NORMAL);write_lock(&chan_list_lock);list_add(&chan->global_l, &chan_list);write_unlock(&chan_list_lock);INIT_DELAYED_WORK(&chan->chan_timer, l2cap_chan_timeout);INIT_DELAYED_WORK(&chan->retrans_timer, l2cap_retrans_timeout);INIT_DELAYED_WORK(&chan->monitor_timer, l2cap_monitor_timeout);INIT_DELAYED_WORK(&chan->ack_timer, l2cap_ack_timeout);chan->state = BT_OPEN;kref_init(&chan->kref);/* This flag is cleared in l2cap_chan_ready() ": "l2cap_chan_close(chan, reason);chan->ops->close(chan);l2cap_chan_unlock(chan);l2cap_chan_put(chan);mutex_unlock(&conn->chan_lock);}struct l2cap_chan  l2cap_chan_create(void){struct l2cap_chan  chan;chan = kzalloc(sizeof( chan), GFP_ATOMIC);if (!chan)return NULL;skb_queue_head_init(&chan->tx_q);skb_queue_head_init(&chan->srej_q);mutex_init(&chan->lock);  Set default lock nesting level ", "hci_dev_lock(hdev);if (!list_empty(&user->list)) ": "l2cap_register_user(struct l2cap_conn  conn, struct l2cap_user  user){struct hci_dev  hdev = conn->hcon->hdev;int ret;  We need to check whether l2cap_conn is registered. If it is not, we   must not register the l2cap_user. l2cap_conn_del() is unregisters   l2cap_conn objects, but doesn't provide its own locking. Instead, it   relies on the parent hci_conn object to be locked. This itself relies   on the hci_dev object to be locked. So we must lock the hci device   here, too. ", "int l2cap_register_user(struct l2cap_conn *conn, struct l2cap_user *user)": "l2cap_unregister_user(). The l2cap_conn object might get destroyed at   any time if they don't. ", "if (err >= 0)return err;switch (err) ": "bt_status(int err){  Don't convert if already positive value ", "if (use_src) ": "hci_get_route(bdaddr_t  dst, bdaddr_t  src, uint8_t src_type){int use_src = bacmp(src, BDADDR_ANY);struct hci_dev  hdev = NULL,  d;BT_DBG(\"%pMR -> %pMR\", src, dst);read_lock(&hci_dev_list_lock);list_for_each_entry(d, &hci_dev_list, list) {if (!test_bit(HCI_UP, &d->flags) ||    hci_dev_test_flag(d, HCI_USER_CHANNEL) ||    d->dev_type != HCI_PRIMARY)continue;  Simple routing:     No source address - find interface with bdaddr != dst     Source address    - find interface with bdaddr == src ", "if (sec_level == BT_SECURITY_SDP)return 1;/* For non 2.1 devices and low security level we don't need the link   key. ": "hci_conn_security(struct hci_conn  conn, __u8 sec_level, __u8 auth_type,      bool initiator){BT_DBG(\"hcon %p\", conn);if (conn->type == LE_LINK)return smp_conn_security(conn, sec_level);  For sdp we don't need the link key. ", "if (sec_level != BT_SECURITY_HIGH && sec_level != BT_SECURITY_FIPS)return 1;/* Accept if secure or higher security level is already present ": "hci_conn_check_secure(struct hci_conn  conn, __u8 sec_level){BT_DBG(\"hcon %p\", conn);  Accept if non-secure or higher security level is required ", "if (sk)sock_put(sk);hdev->req_skb = skb_get(skb);}wake_up_interruptible(&hdev->req_wait_q);}static struct sk_buff *hci_cmd_sync_alloc(struct hci_dev *hdev, u16 opcode,  u32 plen, const void *param,  struct sock *sk)": "hci_cmd_sync_complete(struct hci_dev  hdev, u8 result, u16 opcode,  struct sk_buff  skb){bt_dev_dbg(hdev, \"result 0x%2.2x\", result);if (hdev->req_status != HCI_REQ_PEND)return;hdev->req_result = result;hdev->req_status = HCI_REQ_DONE;if (skb) {struct sock  sk = hci_skb_sk(skb);  Drop sk reference if set ", "if (!skb)return 0;status = skb->data[0];kfree_skb(skb);return status;}EXPORT_SYMBOL(__hci_cmd_sync_status_sk": "__hci_cmd_sync_status_sk(struct hci_dev  hdev, u16 opcode, u32 plen,     const void  param, u8 event, u32 timeout,     struct sock  sk){struct sk_buff  skb;u8 status;skb = __hci_cmd_sync_sk(hdev, opcode, plen, param, event, timeout, sk);if (IS_ERR(skb)) {if (!event)bt_dev_err(hdev, \"Opcode 0x%4x failed: %ld\", opcode,   PTR_ERR(skb));return PTR_ERR(skb);}  If command return a status event skb will be set to NULL as there are   no parameters, in case of failure IS_ERR(skb) would have be set to   the actual error would be found with PTR_ERR(skb). ", "if (!skb)return 0;status = skb->data[0];kfree_skb(skb);return status;}EXPORT_SYMBOL(__hci_cmd_sync_status": "__hci_cmd_sync_status_sk(struct hci_dev  hdev, u16 opcode, u32 plen,     const void  param, u8 event, u32 timeout,     struct sock  sk){struct sk_buff  skb;u8 status;skb = __hci_cmd_sync_sk(hdev, opcode, plen, param, event, timeout, sk);if (IS_ERR(skb)) {if (!event)bt_dev_err(hdev, \"Opcode 0x%4x failed: %ld\", opcode,   PTR_ERR(skb));return PTR_ERR(skb);}  If command return a status event skb will be set to NULL as there are   no parameters, in case of failure IS_ERR(skb) would have be set to   the actual error would be found with PTR_ERR(skb). ", "if (hdev->discovery.type == DISCOV_TYPE_LE)goto discov_stopped;if (hdev->discovery.type != DISCOV_TYPE_INTERLEAVED)goto _return;if (test_bit(HCI_QUIRK_SIMULTANEOUS_DISCOVERY, &hdev->quirks)) ": "hci_cmd_sync_queue(hdev, scan_disable_sync, NULL, NULL);if (status) {bt_dev_err(hdev, \"failed to disable LE scan: %d\", status);goto _return;}hdev->discovery.scan_start = 0;  If we were running LE only scan, change discovery state. If   we were running both LE and BREDR inquiry simultaneously,   and BREDR inquiry is already finished, stop discovery,   otherwise BREDR inquiry will stop discovery when finished.   If we will resolve remote device name, do not change   discovery state. ", "if (hdev->dump.state == HCI_DEVCOREDUMP_TIMEOUT) ": "hci_devcd_timeout() will report the available dump data. ", "void hci_devcd_rx(struct work_struct *work)": "hci_devcd_abort(). A devcoredump is                still generated with the available data indicating the abort                event and then the state machine is reset to the default state.          HCI_DEVCOREDUMP_TIMEOUT: A timeout timer for HCI_DEVCOREDUMP_TIMEOUT sec                is started during devcoredump initialization. Once the timeout                occurs, the driver is notified, a devcoredump is generated with                the available data indicating the timeout event and then the                state machine is reset to the default state.     The driver must register using hci_devcd_register() before using the hci   devcoredump APIs. ", "sock_hold(sk);lock_sock(sk);/* Check sk has not already been unlinked via * bt_accept_unlink() due to serialisation caused by sk locking ": "bt_accept_dequeue(struct sock  parent, struct socket  newsock){struct bt_sock  s,  n;struct sock  sk;BT_DBG(\"parent %p\", parent);restart:list_for_each_entry_safe(s, n, &bt_sk(parent)->accept_q, accept_q) {sk = (struct sock  )s;  Prevent early freeing of sk due to unlink and sock_kill ", "skb->len -= chunk;skb->data_len -= chunk;__skb_pull(frag, chunk);break;} else if (frag->len) ": "bt_sock_stream_recvmsg(struct socket  sock, struct msghdr  msg,   size_t size, int flags){struct sock  sk = sock->sk;int err = 0;size_t target, copied = 0;long timeo;if (flags & MSG_OOB)return -EOPNOTSUPP;BT_DBG(\"sk %p size %zu\", sk, size);lock_sock(sk);target = sock_rcvlowat(sk, flags & MSG_WAITALL, size);timeo  = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);do {struct sk_buff  skb;int chunk;skb = skb_dequeue(&sk->sk_receive_queue);if (!skb) {if (copied >= target)break;err = sock_error(sk);if (err)break;if (sk->sk_shutdown & RCV_SHUTDOWN)break;err = -EAGAIN;if (!timeo)break;timeo = bt_sock_data_wait(sk, timeo);if (signal_pending(current)) {err = sock_intr_errno(timeo);goto out;}continue;}chunk = min_t(unsigned int, skb->len, size);if (skb_copy_datagram_msg(skb, 0, msg, chunk)) {skb_queue_head(&sk->sk_receive_queue, skb);if (!copied)copied = -EFAULT;break;}copied += chunk;size   -= chunk;sock_recv_cmsgs(msg, sk, skb);if (!(flags & MSG_PEEK)) {int skb_len = skb_headlen(skb);if (chunk <= skb_len) {__skb_pull(skb, chunk);} else {struct sk_buff  frag;__skb_pull(skb, skb_len);chunk -= skb_len;skb_walk_frags(skb, frag) {if (chunk <= frag->len) {  Pulling partial data ", "alloc_size += sizeof_priv;}hdev = kzalloc(alloc_size, GFP_KERNEL);if (!hdev)return NULL;hdev->pkt_type  = (HCI_DM1 | HCI_DH1 | HCI_HV1);hdev->esco_type = (ESCO_HV1);hdev->link_mode = (HCI_LM_ACCEPT);hdev->num_iac = 0x01;/* One IAC support is mandatory ": "hci_alloc_dev_priv(int sizeof_priv){struct hci_dev  hdev;unsigned int alloc_size;alloc_size = sizeof( hdev);if (sizeof_priv) {  Fixme: May need ALIGN-ment? ", "put_device(&hdev->dev);}EXPORT_SYMBOL(hci_free_dev": "hci_free_dev(struct hci_dev  hdev){  will free via device release ", "switch (hdev->dev_type) ": "hci_register_dev(struct hci_dev  hdev){int id, error;if (!hdev->open || !hdev->close || !hdev->send)return -EINVAL;  Do not allow HCI_AMP devices to register at index 0,   so the index can be used as the AMP controller ID. ", "BUG_ON(!list_empty(&hdev->mgmt_pending));hci_sock_dev_event(hdev, HCI_DEV_UNREG);if (hdev->rfkill) ": "hci_unregister_dev(struct hci_dev  hdev){BT_DBG(\"%p name %s bus %d\", hdev, hdev->name, hdev->bus);mutex_lock(&hdev->unregister_lock);hci_dev_set_flag(hdev, HCI_UNREGISTER);mutex_unlock(&hdev->unregister_lock);write_lock(&hci_dev_list_lock);list_del(&hdev->list);write_unlock(&hci_dev_list_lock);cancel_work_sync(&hdev->power_on);hci_cmd_sync_clear(hdev);hci_unregister_suspend_notifier(hdev);msft_unregister(hdev);hci_dev_do_close(hdev);if (!test_bit(HCI_INIT, &hdev->flags) &&    !hci_dev_test_flag(hdev, HCI_SETUP) &&    !hci_dev_test_flag(hdev, HCI_CONFIG)) {hci_dev_lock(hdev);mgmt_index_removed(hdev);hci_dev_unlock(hdev);}  mgmt_index_removed should take care of emptying the   pending list ", "hci_dev_put(hdev);}EXPORT_SYMBOL(hci_unregister_dev);/* Release HCI device ": "hci_release_dev(). ", "struct hci_dev *hci_alloc_dev_priv(int sizeof_priv)": "hci_resume_dev(hdev);if (ret)bt_dev_err(hdev, \"Suspend notifier action (%lu) failed: %d\",   action, ret);return NOTIFY_DONE;}  Alloc HCI device ", "struct adv_info *hci_find_adv_instance(struct hci_dev *hdev, u8 instance)": "hci_reset_dev(hdev);}struct oob_data  hci_find_remote_oob_data(struct hci_dev  hdev,  bdaddr_t  bdaddr, u8 bdaddr_type){struct oob_data  data;list_for_each_entry(data, &hdev->remote_oob_data, list) {if (bacmp(bdaddr, &data->bdaddr) != 0)continue;if (data->bdaddr_type != bdaddr_type)continue;return data;}return NULL;}int hci_remove_remote_oob_data(struct hci_dev  hdev, bdaddr_t  bdaddr,       u8 bdaddr_type){struct oob_data  data;data = hci_find_remote_oob_data(hdev, bdaddr, bdaddr_type);if (!data)return -ENOENT;BT_DBG(\"%s removing %pMR (%u)\", hdev->name, bdaddr, bdaddr_type);list_del(&data->list);kfree(data);return 0;}void hci_remote_oob_data_clear(struct hci_dev  hdev){struct oob_data  data,  n;list_for_each_entry_safe(data, n, &hdev->remote_oob_data, list) {list_del(&data->list);kfree(data);}}int hci_add_remote_oob_data(struct hci_dev  hdev, bdaddr_t  bdaddr,    u8 bdaddr_type, u8  hash192, u8  rand192,    u8  hash256, u8  rand256){struct oob_data  data;data = hci_find_remote_oob_data(hdev, bdaddr, bdaddr_type);if (!data) {data = kmalloc(sizeof( data), GFP_KERNEL);if (!data)return -ENOMEM;bacpy(&data->bdaddr, bdaddr);data->bdaddr_type = bdaddr_type;list_add(&data->list, &hdev->remote_oob_data);}if (hash192 && rand192) {memcpy(data->hash192, hash192, sizeof(data->hash192));memcpy(data->rand192, rand192, sizeof(data->rand192));if (hash256 && rand256)data->present = 0x03;} else {memset(data->hash192, 0, sizeof(data->hash192));memset(data->rand192, 0, sizeof(data->rand192));if (hash256 && rand256)data->present = 0x02;elsedata->present = 0x00;}if (hash256 && rand256) {memcpy(data->hash256, hash256, sizeof(data->hash256));memcpy(data->rand256, rand256, sizeof(data->rand256));} else {memset(data->hash256, 0, sizeof(data->hash256));memset(data->rand256, 0, sizeof(data->rand256));if (hash192 && rand192)data->present = 0x01;}BT_DBG(\"%s for %pMR\", hdev->name, bdaddr);return 0;}  This function requires the caller holds hdev->lock ", "int hci_recv_frame(struct hci_dev *hdev, struct sk_buff *skb)": "hci_recv_frame(hdev, skb);}EXPORT_SYMBOL(hci_reset_dev);  Receive frame from HCI drivers ", "hci_skb_pkt_type(skb) = HCI_DIAG_PKT;/* Time stamp ": "hci_recv_diag(struct hci_dev  hdev, struct sk_buff  skb){  Mark as diagnostic packet ", "bt_dev_err(hdev, \"unresponded command not supported\");return -EINVAL;}skb = hci_prepare_cmd(hdev, opcode, plen, param);if (!skb) ": "__hci_cmd_send(struct hci_dev  hdev, u16 opcode, u32 plen,   const void  param){struct sk_buff  skb;if (hci_opcode_ogf(opcode) != 0x3f) {  A controller receiving a command shall respond with either   a Command Status Event or a Command Complete Event.   Therefore, all standard HCI commands must be sent via the   standard API, using hci_send_cmd or hci_cmd_sync helpers.   Some vendors do not comply with this rule for vendor-specific   commands and do not return any event. We want to support   unresponded commands for such cases only. ", "if (!ethtool_validate_speed(speed) ||    !ethtool_validate_duplex(duplex) ||    !ethtool_virtdev_validate_cmd(cmd))return -EINVAL;*dev_speed = speed;*dev_duplex = duplex;return 0;}EXPORT_SYMBOL(ethtool_virtdev_set_link_ksettings": "ethtool_virtdev_set_link_ksettings(struct net_device  dev,       const struct ethtool_link_ksettings  cmd,       u32  dev_speed, u8  dev_duplex){u32 speed;u8 duplex;speed = cmd->base.speed;duplex = cmd->base.duplex;  don't allow custom speed and duplex ", "flow->rule = flow_rule_alloc(1);if (!flow->rule) ": "ethtool_rx_flow_rule_create(const struct ethtool_rx_flow_spec_input  input){const struct ethtool_rx_flow_spec  fs = input->fs;struct ethtool_rx_flow_match  match;struct ethtool_rx_flow_rule  flow;struct flow_action_entry  act;flow = kzalloc(sizeof(struct ethtool_rx_flow_rule) +       sizeof(struct ethtool_rx_flow_match), GFP_KERNEL);if (!flow)return ERR_PTR(-ENOMEM);  ethtool_rx supports only one single action per rule. ", "static void ethnl_default_notify(struct net_device *dev, unsigned int cmd, const void *data)": "ethtool_notify(req_info.dev, ops->set_ntf_cmd, NULL);ret = 0;out_ops:ethnl_ops_complete(req_info.dev);out_rtnl:rtnl_unlock();out_dev:ethnl_parse_header_dev_put(&req_info);return ret;}static const struct ethnl_request_ops  ethnl_default_notify_ops[ETHTOOL_MSG_KERNEL_MAX + 1] = {[ETHTOOL_MSG_LINKINFO_NTF]= &ethnl_linkinfo_request_ops,[ETHTOOL_MSG_LINKMODES_NTF]= &ethnl_linkmodes_request_ops,[ETHTOOL_MSG_DEBUG_NTF]= &ethnl_debug_request_ops,[ETHTOOL_MSG_WOL_NTF]= &ethnl_wol_request_ops,[ETHTOOL_MSG_FEATURES_NTF]= &ethnl_features_request_ops,[ETHTOOL_MSG_PRIVFLAGS_NTF]= &ethnl_privflags_request_ops,[ETHTOOL_MSG_RINGS_NTF]= &ethnl_rings_request_ops,[ETHTOOL_MSG_CHANNELS_NTF]= &ethnl_channels_request_ops,[ETHTOOL_MSG_COALESCE_NTF]= &ethnl_coalesce_request_ops,[ETHTOOL_MSG_PAUSE_NTF]= &ethnl_pause_request_ops,[ETHTOOL_MSG_EEE_NTF]= &ethnl_eee_request_ops,[ETHTOOL_MSG_FEC_NTF]= &ethnl_fec_request_ops,[ETHTOOL_MSG_MODULE_NTF]= &ethnl_module_request_ops,[ETHTOOL_MSG_PLCA_NTF]= &ethnl_plca_cfg_request_ops,[ETHTOOL_MSG_MM_NTF]= &ethnl_mm_request_ops,};  default notification handler ", "static DEFINE_MUTEX(iucv_register_mutex);/* * Counter for number of non-smp capable handlers. ": "iucv_unregister. ", "struct iucv_cmd_control ": "iucv_path_quiesce and iucv_path_sever. ", "static int iucv_active_cpu = -1;/* * Mutex and wait queue for iucv_register/iucv_unregister. ": "iucv_path_sever called from tasklet. ", "struct iucv_cmd_purge ": "iucv_message_purge. ", "static int iucv_message_receive_iprmdata(struct iucv_path *path, struct iucv_message *msg, u8 flags, void *buffer, size_t size, size_t *residual)": "__iucv_message_receive   to receive RMDATA data stored in struct iucv_message. ", "struct iucv_cmd_db ": "iucv_message_reject, iucv_message_send, iucv_message_send2way   and iucv_declare_cpu. ", "struct iucv_cmd_dpl ": "iucv_message_send2way and iucv_message_reply. ", "int __iucv_message_send(struct iucv_path *path, struct iucv_message *msg,      u8 flags, u32 srccls, void *buffer, size_t size)": "__iucv_message_send   @path: address of iucv path structure   @msg: address of iucv msg structure   @flags: how the message is sent (IUCV_IPRMDATA, IUCV_IPPRTY, IUCV_IPBUFLST)   @srccls: source class of message   @buffer: address of send buffer or address of struct iucv_array   @size: length of send buffer     This function transmits data to another application. Data to be   transmitted is in a buffer and this is a one-way message and the   receiver will not reply to the message.     Locking:no locking     Returns the result from the CP IUCV call. ", "#include <linux/module.h>#include <linux/slab.h>#include <linux/types.h>#include <linux/kernel.h>#include <linux/errno.h>#include <linux/rtnetlink.h>#include <linux/skbuff.h>#include <net/pkt_cls.h>static LIST_HEAD(ematch_ops);static DEFINE_RWLOCK(ematch_mod_lock);static struct tcf_ematch_ops *tcf_em_lookup(u16 kind)": "tcf_em_unregister(&my_ops);        }          module_init(init_my_ematch);        module_exit(exit_my_ematch);       4) By now you should have two more seconds left, barely enough to        open up a beer to watch the compilation going. ", "int tcf_em_tree_validate(struct tcf_proto *tp, struct nlattr *nla, struct tcf_ematch_tree *tree)": "tcf_em_tree_validate - validate ematch config TLV and build ematch tree     @tp: classifier kind handle   @nla: ematch tree configuration TLV   @tree: destination ematch tree variable to store the resulting          ematch tree.     This function validates the given configuration TLV @nla and builds an   ematch tree in @tree. The resulting tree must later be copied into   the private classifier data using tcf_em_tree_change(). You MUST NOT   provide the ematch tree variable of the private classifier data directly,   the changes would not be locked properly.     Returns a negative error code if the configuration TLV contains errors. ", "void tcf_em_tree_destroy(struct tcf_ematch_tree *tree)": "tcf_em_tree_destroy(tree);return err;}EXPORT_SYMBOL(tcf_em_tree_validate);     tcf_em_tree_destroy - destroy an ematch tree     @tree: ematch tree to be deleted     This functions destroys an ematch tree previously created by   tcf_em_tree_validate()tcf_em_tree_change(). You must ensure that   the ematch tree is not in use before calling this function. ", "int tcf_em_tree_dump(struct sk_buff *skb, struct tcf_ematch_tree *tree, int tlv)": "tcf_em_tree_dump - dump ematch tree into a rtnl message     @skb: skb holding the rtnl message   @tree: ematch tree to be dumped   @tlv: TLV type to be used to encapsulate the tree     This function dumps a ematch tree into a rtnl message. It is valid to   call this function while the ematch tree is in use.     Returns -1 if the skb tailroom is insufficient. ", "if (strncmp(q->ops->id + 1, \"fifo\", 4) != 0)return 0;if (!q->ops->change)return 0;nla = kmalloc(nla_attr_size(sizeof(struct tc_fifo_qopt)), GFP_KERNEL);if (nla) ": "fifo_set_limit(struct Qdisc  q, unsigned int limit){struct nlattr  nla;int ret = -ENOMEM;  Hack to avoid sending change message to non-FIFO ", "if (p) ": "tcf_idr_release(struct tc_action  p, bool bind, bool strict){int ret = 0;  Release with strict==1 and bind==0 is only called through act API   interface (classifiers always bind). Only case when action with   positive reference count and zero bind count can exist is when it was   also created with act API (unbinding last classifier will destroy the   action if it was created by classifier). So only case when bind count   can be changed after initial check is when unbound action is   destroyed by act API while classifier binds to action with same id   concurrently. This result either creation of new action(same behavior   as before), or reusing existing action if concurrent process   increments reference count before action is deleted. Both scenarios   are acceptable. ", "return tcf_idr_create(tn, index, est, a, ops, bind,      !(flags & TCA_ACT_FLAGS_NO_PERCPU_STATS), flags);}EXPORT_SYMBOL(tcf_idr_create_from_flags": "tcf_idr_create_from_flags(struct tc_action_net  tn, u32 index,      struct nlattr  est, struct tc_action   a,      const struct tc_action_ops  ops, int bind,      u32 flags){  Set cpustats according to actions flags. ", "WARN_ON(!IS_ERR(idr_remove(&idrinfo->action_idr, index)));mutex_unlock(&idrinfo->lock);}EXPORT_SYMBOL(tcf_idr_cleanup": "tcf_idr_cleanup(struct tc_action_net  tn, u32 index){struct tcf_idrinfo  idrinfo = tn->idrinfo;mutex_lock(&idrinfo->lock);  Remove ERR_PTR(-EBUSY) allocated by tcf_idr_check_alloc ", "WARN_ON(!IS_ERR(idr_remove(&idrinfo->action_idr, index)));mutex_unlock(&idrinfo->lock);}EXPORT_SYMBOL(tcf_idr_cleanup);/* Check if action with specified index exists. If actions is found, increments * its reference and bind counters, and return 1. Otherwise insert temporary * error pointer (to prevent concurrent users from inserting actions with same * index) and return 0. ": "tcf_idr_check_alloc ", "ret = register_pernet_subsys(ops);if (ret)return ret;if (ops->id) ": "tcf_register_action(struct tc_action_ops  act,struct pernet_operations  ops){struct tc_action_ops  a;int ret;if (!act->act || !act->dump || !act->init)return -EINVAL;  We have to register pernet ops before making the action ops visible,   otherwise tcf_action_init_1() could get a partially initialized   netns. ", "int i;int ret = TC_ACT_OK;if (skb_skip_tc_classify(skb))return TC_ACT_OK;restart_act_graph:for (i = 0; i < nr_actions; i++) ": "tcf_action_exec(struct sk_buff  skb, struct tc_action   actions,    int nr_actions, struct tcf_result  res){u32 jmp_prgcnt = 0;u32 jmp_ttl = TCA_ACT_MAX_PRIO;  matches actions per filter ", "return tcf_idr_create(tn, index, est, a, ops, bind,      !(flags & TCA_ACT_FLAGS_NO_PERCPU_STATS), flags);}EXPORT_SYMBOL(tcf_idr_create_from_flags);/* Cleanup idr index that was allocated but not initialized. ": "tcf_action_dump_1(skb, p, 0, 0);if (err < 0) {index--;nlmsg_trim(skb, nest);goto done;}nla_nest_end(skb, nest);n_i++;if (!(act_flags & TCA_ACT_FLAG_LARGE_DUMP_ON) &&    n_i >= TCA_ACT_MAX_PRIO)goto done;}done:if (index >= 0)cb->args[0] = index + 1;mutex_unlock(&idrinfo->lock);if (n_i) {if (act_flags & TCA_ACT_FLAG_LARGE_DUMP_ON)cb->args[1] = n_i;}return n_i;nla_put_failure:nla_nest_cancel(skb, nest);goto done;}static int tcf_idr_release_unsafe(struct tc_action  p){if (atomic_read(&p->tcfa_bindcnt) > 0)return -EPERM;if (refcount_dec_and_test(&p->tcfa_refcnt)) {idr_remove(&p->idrinfo->action_idr, p->tcfa_index);tcf_action_cleanup(p);return ACT_P_DELETED;}return 0;}static int tcf_del_walker(struct tcf_idrinfo  idrinfo, struct sk_buff  skb,  const struct tc_action_ops  ops,  struct netlink_ext_ack  extack){struct nlattr  nest;int n_i = 0;int ret = -EINVAL;struct idr  idr = &idrinfo->action_idr;struct tc_action  p;unsigned long id = 1;unsigned long tmp;nest = nla_nest_start_noflag(skb, 0);if (nest == NULL)goto nla_put_failure;if (nla_put_string(skb, TCA_ACT_KIND, ops->kind))goto nla_put_failure;ret = 0;mutex_lock(&idrinfo->lock);idr_for_each_entry_ul(idr, p, tmp, id) {if (IS_ERR(p))continue;ret = tcf_idr_release_unsafe(p);if (ret == ACT_P_DELETED)module_put(ops->owner);else if (ret < 0)break;n_i++;}mutex_unlock(&idrinfo->lock);if (ret < 0) {if (n_i)NL_SET_ERR_MSG(extack, \"Unable to flush all TC actions\");elsegoto nla_put_failure;}ret = nla_put_u32(skb, TCA_FCNT, n_i);if (ret)goto nla_put_failure;nla_nest_end(skb, nest);return n_i;nla_put_failure:nla_nest_cancel(skb, nest);return ret;}int tcf_generic_walker(struct tc_action_net  tn, struct sk_buff  skb,       struct netlink_callback  cb, int type,       const struct tc_action_ops  ops,       struct netlink_ext_ack  extack){struct tcf_idrinfo  idrinfo = tn->idrinfo;if (type == RTM_DELACTION) {return tcf_del_walker(idrinfo, skb, ops, extack);} else if (type == RTM_GETACTION) {return tcf_dump_walker(idrinfo, skb, cb);} else {WARN(1, \"tcf_generic_walker: unknown command %d\\n\", type);NL_SET_ERR_MSG(extack, \"tcf_generic_walker: unknown command\");return -EINVAL;}}EXPORT_SYMBOL(tcf_generic_walker);int tcf_idr_search(struct tc_action_net  tn, struct tc_action   a, u32 index){struct tcf_idrinfo  idrinfo = tn->idrinfo;struct tc_action  p;mutex_lock(&idrinfo->lock);p = idr_find(&idrinfo->action_idr, index);if (IS_ERR(p))p = NULL;else if (p)refcount_inc(&p->tcfa_refcnt);mutex_unlock(&idrinfo->lock);if (p) { a = p;return true;}return false;}EXPORT_SYMBOL(tcf_idr_search);static int __tcf_generic_walker(struct net  net, struct sk_buff  skb,struct netlink_callback  cb, int type,const struct tc_action_ops  ops,struct netlink_ext_ack  extack){struct tc_action_net  tn = net_generic(net, ops->net_id);if (unlikely(ops->walk))return ops->walk(net, skb, cb, type, ops, extack);return tcf_generic_walker(tn, skb, cb, type, ops, extack);}static int __tcf_idr_search(struct net  net,    const struct tc_action_ops  ops,    struct tc_action   a, u32 index){struct tc_action_net  tn = net_generic(net, ops->net_id);if (unlikely(ops->lookup))return ops->lookup(net, a, index);return tcf_idr_search(tn, a, index);}static int tcf_idr_delete_index(struct tcf_idrinfo  idrinfo, u32 index){struct tc_action  p;int ret = 0;mutex_lock(&idrinfo->lock);p = idr_find(&idrinfo->action_idr, index);if (!p) {mutex_unlock(&idrinfo->lock);return -ENOENT;}if (!atomic_read(&p->tcfa_bindcnt)) {if (refcount_dec_and_test(&p->tcfa_refcnt)) {struct module  owner = p->ops->owner;WARN_ON(p != idr_remove(&idrinfo->action_idr,p->tcfa_index));mutex_unlock(&idrinfo->lock);tcf_action_cleanup(p);module_put(owner);return 0;}ret = 0;} else {ret = -EPERM;}mutex_unlock(&idrinfo->lock);return ret;}int tcf_idr_create(struct tc_action_net  tn, u32 index, struct nlattr  est,   struct tc_action   a, const struct tc_action_ops  ops,   int bind, bool cpustats, u32 flags){struct tc_action  p = kzalloc(ops->size, GFP_KERNEL);struct tcf_idrinfo  idrinfo = tn->idrinfo;int err = -ENOMEM;if (unlikely(!p))return -ENOMEM;refcount_set(&p->tcfa_refcnt, 1);if (bind)atomic_set(&p->tcfa_bindcnt, 1);if (cpustats) {p->cpu_bstats = netdev_alloc_pcpu_stats(struct gnet_stats_basic_sync);if (!p->cpu_bstats)goto err1;p->cpu_bstats_hw = netdev_alloc_pcpu_stats(struct gnet_stats_basic_sync);if (!p->cpu_bstats_hw)goto err2;p->cpu_qstats = alloc_percpu(struct gnet_stats_queue);if (!p->cpu_qstats)goto err3;}gnet_stats_basic_sync_init(&p->tcfa_bstats);gnet_stats_basic_sync_init(&p->tcfa_bstats_hw);spin_lock_init(&p->tcfa_lock);p->tcfa_index = index;p->tcfa_tm.install = jiffies;p->tcfa_tm.lastuse = jiffies;p->tcfa_tm.firstuse = 0;p->tcfa_flags = flags;if (est) {err = gen_new_estimator(&p->tcfa_bstats, p->cpu_bstats,&p->tcfa_rate_est,&p->tcfa_lock, false, est);if (err)goto err4;}p->idrinfo = idrinfo;__module_get(ops->owner);p->ops = ops; a = p;return 0;err4:free_percpu(p->cpu_qstats);err3:free_percpu(p->cpu_bstats_hw);err2:free_percpu(p->cpu_bstats);err1:kfree(p);return err;}EXPORT_SYMBOL(tcf_idr_create);int tcf_idr_create_from_flags(struct tc_action_net  tn, u32 index,      struct nlattr  est, struct tc_action   a,      const struct tc_action_ops  ops, int bind,      u32 flags){  Set cpustats according to actions flags. ", "if (softexpires - expires <= delta_ns)return;}hrtimer_start_range_ns(&wd->timer,       ns_to_ktime(expires),       delta_ns,       HRTIMER_MODE_ABS_PINNED);}EXPORT_SYMBOL(qdisc_watchdog_schedule_range_ns": "qdisc_watchdog_schedule_range_ns(struct qdisc_watchdog  wd, u64 expires,      u64 delta_ns){bool deactivated;rcu_read_lock();deactivated = test_bit(__QDISC_STATE_DEACTIVATED,       &qdisc_root_sleeping(wd->qdisc)->state);rcu_read_unlock();if (deactivated)return;if (hrtimer_is_queued(&wd->timer)) {u64 softexpires;softexpires = ktime_to_ns(hrtimer_get_softexpires(&wd->timer));  If timer is already set in [expires, expires + delta_ns],   do not reprogram it. ", "if (clhash->hashelems * 4 <= clhash->hashsize * 3)return;nsize = clhash->hashsize * 2;nmask = nsize - 1;nhash = qdisc_class_hash_alloc(nsize);if (nhash == NULL)return;ohash = clhash->hash;osize = clhash->hashsize;sch_tree_lock(sch);for (i = 0; i < osize; i++) ": "qdisc_class_hash_grow(struct Qdisc  sch, struct Qdisc_class_hash  clhash){struct Qdisc_class_common  cl;struct hlist_node  next;struct hlist_head  nhash,  ohash;unsigned int nsize, nmask, osize;unsigned int i, h;  Rehash when load factor exceeds 0.75 ", "notify = !sch->q.qlen && !WARN_ON_ONCE(!n &&       !qdisc_is_offloaded);/* TODO: perform the search on a per txq basis ": "qdisc_tree_reduce_backlog(struct Qdisc  sch, int n, int len){bool qdisc_is_offloaded = sch->flags & TCQ_F_OFFLOADED;const struct Qdisc_class_ops  cops;unsigned long cl;u32 parentid;bool notify;int drops;if (n == 0 && len == 0)return;drops = max_t(int, n, 0);rcu_read_lock();while ((parentid = sch->parent)) {if (TC_H_MAJ(parentid) == TC_H_MAJ(TC_H_INGRESS))break;if (sch->flags & TCQ_F_NOPARENT)break;  Notify parent qdisc only if child qdisc becomes empty.     If child was empty even before update then backlog   counter is screwed and we skip notification because   parent class is already passive.     If the original child was offloaded then it is allowed   to be seem as empty, so the parent is notified anyway. ", "if (!err || !new || new == &noop_qdisc)return;/* Don't report error if the parent, the old child and the new * one are not offloaded. ": "qdisc_offload_graft_helper(struct net_device  dev, struct Qdisc  sch,struct Qdisc  new, struct Qdisc  old,enum tc_setup_type type, void  type_data,struct netlink_ext_ack  extack){bool any_qdisc_is_offloaded;int err;if (!tc_can_offload(dev) || !dev->netdev_ops->ndo_setup_tc)return;err = dev->netdev_ops->ndo_setup_tc(dev, type, type_data);  Don't report error if the graft is part of destroy operation. ", "rcu_barrier();flush_workqueue(tc_filter_wq);write_lock(&cls_mod_lock);list_for_each_entry(t, &tcf_proto_base, head) ": "unregister_tcf_proto_ops(struct tcf_proto_ops  ops){struct tcf_proto_ops  t;int rc = -ENOENT;  Wait for outstanding call_rcu()s, if any, from a   tcf_proto_ops's destroy() handler. ", "while (chain && tcf_chain_held_by_acts_only(chain))chain = list_is_last(&chain->list, &block->chain_list) ?NULL : list_next_entry(chain, list);if (chain)tcf_chain_hold(chain);mutex_unlock(&block->lock);return chain;}/* Function to be used by all clients that want to iterate over all chains on * block. It properly obtains block->lock and takes reference to chain before * returning it. Users of this function must be tolerant to concurrent chain * insertion/deletion or ensure that no concurrent chain modification is * possible. Note that all netlink dump callbacks cannot guarantee to provide * consistent dump because rtnl lock is released each time skb is filled with * data and sent to user-space. ": "tcf_get_next_chain(struct tcf_block  block, struct tcf_chain  chain){mutex_lock(&block->lock);if (chain)chain = list_is_last(&chain->list, &block->chain_list) ?NULL : list_next_entry(chain, list);elsechain = list_first_entry_or_null(&block->chain_list, struct tcf_chain, list);  skip all action-only chains ", "prio = tp->prio + 1;tp = tcf_chain_dereference(chain->filter_chain, chain);for (; tp; tp = tcf_chain_dereference(tp->next, chain))if (!tp->deleting && tp->prio >= prio)break;} else ": "tcf_get_next_proto(struct tcf_chain  chain, struct tcf_proto  tp){u32 prio = 0;ASSERT_RTNL();mutex_lock(&chain->filter_chain_lock);if (!tp) {tp = tcf_chain_dereference(chain->filter_chain, chain);} else if (tcf_proto_is_deleting(tp)) {  'deleting' flag is set and chain->filter_chain_lock was   unlocked, which means next pointer could be invalid. Restart   search. ", "block = tcf_block_refcnt_get(net, ei->block_index);if (!block) ": "tcf_block_get_ext(struct tcf_block   p_block, struct Qdisc  q,      struct tcf_block_ext_info  ei,      struct netlink_ext_ack  extack){struct net  net = qdisc_net(q);struct tcf_block  block = NULL;int err;if (ei->block_index)  block_index not 0 means the shared block is requested ", "bool free_block = list_empty(&block->chain_list);mutex_unlock(&block->lock);if (tcf_block_shared(block))tcf_block_remove(block, block->net);if (q)tcf_block_offload_unbind(block, q, ei);if (free_block)tcf_block_destroy(block);elsetcf_block_flush_all_chains(block, rtnl_held);} else if (q) ": "tcf_block_put(struct tcf_block  block, struct Qdisc  q,    struct tcf_block_ext_info  ei, bool rtnl_held){if (refcount_dec_and_mutex_lock(&block->refcnt, &block->lock)) {  Flushingputting all chains will cause the block to be   deallocated when last chain is freed. However, if chain_list   is empty, block has to be manually deallocated. After block   reference counter reached 0, it is no longer possible to   increment it or add new chains to block. ", "if (unlikely(n->tp != tp || n->tp->chain != n->chain ||     !tp->ops->get_exts))return TC_ACT_SHOT;exts = tp->ops->get_exts(tp, n->handle);if (unlikely(!exts || n->exts != exts))return TC_ACT_SHOT;n = NULL;err = tcf_exts_exec_ex(skb, exts, act_index, res);} else ": "tcf_classify(struct sk_buff  skb, const struct tcf_proto  tp, const struct tcf_proto  orig_tp, struct tcf_result  res, bool compat_mode, struct tcf_exts_miss_cookie_node  n, int act_index, u32  last_executed_chain){#ifdef CONFIG_NET_CLS_ACTconst int max_reclassify_loop = 16;const struct tcf_proto  first_tp;int limit = 0;reclassify:#endiffor (; tp; tp = rcu_dereference_bh(tp->next)) {__be16 protocol = skb_protocol(skb, false);int err = 0;if (n) {struct tcf_exts  exts;if (n->tp_prio != tp->prio)continue;  We re-lookup the tp and chain based on index instead   of having hard refs and locks to them, so do a sanity   check if any of tp,chain,exts was replaced by the   time we got here with a cookie from hardware. ", "exts->net = net;exts->actions = kcalloc(TCA_ACT_MAX_PRIO, sizeof(struct tc_action *),GFP_KERNEL);if (!exts->actions)return -ENOMEM;#endifexts->action = action;exts->police = police;if (!use_action_miss)return 0;err = tcf_exts_miss_cookie_base_alloc(exts, tp, handle);if (err)goto err_miss_alloc;return 0;err_miss_alloc:tcf_exts_destroy(exts);#ifdef CONFIG_NET_CLS_ACTexts->actions = NULL;#endifreturn err;}EXPORT_SYMBOL(tcf_exts_init_ex": "tcf_exts_init_ex(struct tcf_exts  exts, struct net  net, int action,     int police, struct tcf_proto  tp, u32 handle,     bool use_action_miss){int err = 0;#ifdef CONFIG_NET_CLS_ACTexts->type = 0;exts->nr_actions = 0;exts->miss_cookie_node = NULL;  Note: we do not own yet a reference on net.   This reference might be taken later from tcf_exts_get_net(). ", "if (exts->type != TCA_OLD_COMPAT) ": "tcf_exts_dump(struct sk_buff  skb, struct tcf_exts  exts){#ifdef CONFIG_NET_CLS_ACTstruct nlattr  nest;if (exts->action && tcf_exts_has_actions(exts)) {    again for backward compatible mode - we want   to work with both old and new modes of entering   tc data even if iproute2  was newer - jhs ", "if (!rtnl_held && !take_rtnl && block->lockeddevcnt) ": "tc_setup_cb_destroy(struct tcf_block  block, struct tcf_proto  tp,enum tc_setup_type type, void  type_data, bool err_stop,u32  flags, unsigned int  in_hw_count, bool rtnl_held){bool take_rtnl = READ_ONCE(block->lockeddevcnt) && !rtnl_held;int ok_count;retry:if (take_rtnl)rtnl_lock();down_read(&block->cb_lock);  Need to obtain rtnl lock if block is bound to devs that require it.   In block bind code cb_lock is obtained while holding rtnl, so we must   obtain the locks in same order here. ", "if (block_index != qe->info.block_index) ": "tcf_qevent_validate_change(struct tcf_qevent  qe, struct nlattr  block_index_attr,       struct netlink_ext_ack  extack){u32 block_index;int err;if (!block_index_attr)return 0;err = tcf_qevent_parse_block_index(block_index_attr, &block_index, extack);if (err)return err;  Bounce newly-configured block or change in block. ", "static inline bool qdisc_restart(struct Qdisc *q, int *packets)": "netif_tx_lock serializes accesses to device driver.      qdisc_lock(q) and netif_tx_lock are mutually exclusive,    if one is grabbed, another must be free.     Note, that this procedure can be called by a watchdog timer     Returns to the caller:  0  - queue is empty or throttled.  >0 - queue is not empty.   ", "clear_bit(__QUEUE_STATE_FROZEN, &txq->state);netif_schedule_queue(txq);}}void netif_tx_unlock(struct net_device *dev)": "netif_tx_unlock(txq);}}void netif_tx_lock(struct net_device  dev){spin_lock(&dev->tx_global_lock);netif_freeze_queues(dev);}EXPORT_SYMBOL(netif_tx_lock);static void netif_unfreeze_queues(struct net_device  dev){unsigned int i;for (i = 0; i < dev->num_tx_queues; i++) {struct netdev_queue  txq = netdev_get_tx_queue(dev, i);  No need to grab the _xmit_lock here.  If the   queue is not stopped for another reason, we   force a schedule. ", "void netif_carrier_on(struct net_device *dev)": "netif_carrier_on - set carrier  @dev: network device     Device has detected acquisition of carrier. ", "void netif_carrier_off(struct net_device *dev)": "netif_carrier_off - clear carrier  @dev: network device     Device has detected loss of carrier. ", "smp_mb__after_atomic();/* Checking netif_xmit_frozen_or_stopped() again to * make sure STATE_MISSED is set if the STATE_MISSED * set by netif_tx_wake_queue()'s rescheduling of * net_tx_action() is cleared by the above clear_bit(). ": "pfifo_fast_ops;EXPORT_SYMBOL(default_qdisc_ops);static void qdisc_maybe_clear_missed(struct Qdisc  q,     const struct netdev_queue  txq){clear_bit(__QDISC_STATE_MISSED, &q->state);  Make sure the below netif_xmit_frozen_or_stopped()   checking happens after clearing STATE_MISSED. ", "void qdisc_reset(struct Qdisc *qdisc)": "qdisc_put(sch);return NULL;}EXPORT_SYMBOL(qdisc_create_dflt);  Under qdisc_lock(qdisc) and BH! ", "if (qdisc == NULL)qdisc = &noop_qdisc;rcu_assign_pointer(dev_queue->qdisc_sleeping, qdisc);rcu_assign_pointer(dev_queue->qdisc, &noop_qdisc);spin_unlock_bh(root_lock);return oqdisc;}EXPORT_SYMBOL(dev_graft_qdisc": "dev_graft_qdisc(struct netdev_queue  dev_queue,      struct Qdisc  qdisc){struct Qdisc  oqdisc = rtnl_dereference(dev_queue->qdisc_sleeping);spinlock_t  root_lock;root_lock = qdisc_lock(oqdisc);spin_lock_bh(root_lock);  ... and graft new one ", "if (rtnl_dereference(dev->qdisc) == &noop_qdisc)attach_default_qdiscs(dev);if (!netif_carrier_ok(dev))/* Delay activation until next carrier-on event ": "dev_activate(struct net_device  dev){int need_watchdog;  No queueing discipline is attached to device;   create default one for devices, which need queueing   and noqueue_qdisc for virtual interfaces ", "void dev_deactivate_many(struct list_head *head)": "dev_deactivate_queue(struct net_device  dev, struct netdev_queue  dev_queue, void  _qdisc_default){struct Qdisc  qdisc_default = _qdisc_default;struct Qdisc  qdisc;qdisc = rtnl_dereference(dev_queue->qdisc);if (qdisc) {qdisc_deactivate(qdisc);rcu_assign_pointer(dev_queue->qdisc, qdisc_default);}}static void dev_reset_queue(struct net_device  dev,    struct netdev_queue  dev_queue,    void  _unused){struct Qdisc  qdisc;bool nolock;qdisc = rtnl_dereference(dev_queue->qdisc_sleeping);if (!qdisc)return;nolock = qdisc->flags & TCQ_F_NOLOCK;if (nolock)spin_lock_bh(&qdisc->seqlock);spin_lock_bh(qdisc_lock(qdisc));qdisc_reset(qdisc);spin_unlock_bh(qdisc_lock(qdisc));if (nolock) {clear_bit(__QDISC_STATE_MISSED, &qdisc->state);clear_bit(__QDISC_STATE_DRAINING, &qdisc->state);spin_unlock_bh(&qdisc->seqlock);}}static bool some_qdisc_is_busy(struct net_device  dev){unsigned int i;for (i = 0; i < dev->num_tx_queues; i++) {struct netdev_queue  dev_queue;spinlock_t  root_lock;struct Qdisc  q;int val;dev_queue = netdev_get_tx_queue(dev, i);q = rtnl_dereference(dev_queue->qdisc_sleeping);root_lock = qdisc_lock(q);spin_lock_bh(root_lock);val = (qdisc_is_running(q) ||       test_bit(__QDISC_STATE_SCHED, &q->state));spin_unlock_bh(root_lock);if (val)return true;}return false;}     dev_deactivate_many - deactivate transmissions on several devices   @head: list of devices to deactivate    This function returns only when all outstanding transmissions  have completed, unless all devices are in dismantle phase. ", "if (qdisc != &noop_qdisc && !qdisc->handle)qdisc_hash_del(qdisc);}for (i = dev->real_num_tx_queues; i < new_real_tx; i++) ": "mq_change_real_num_tx(struct Qdisc  sch, unsigned int new_real_tx){#ifdef CONFIG_NET_SCHEDstruct net_device  dev = qdisc_dev(sch);struct Qdisc  qdisc;unsigned int i;for (i = new_real_tx; i < dev->real_num_tx_queues; i++) {qdisc = rtnl_dereference(netdev_get_tx_queue(dev, i)->qdisc_sleeping);  Only update the default qdiscs we created,   qdiscs with handles are always hashed. ", "static void psched_ratecfg_precompute__(u64 rate, u32 *mult, u8 *shift)": "psched_ratecfg_precompute__() - Pre-compute values for reciprocal division   @rate:   Rate to compute reciprocal division values of   @mult:   Multiplier for reciprocal division   @shift:  Shift for reciprocal division     The multiplier and shift for reciprocal division by rate are stored   in mult and shift.     The deal here is to replace a divide by a reciprocal one   in fast path (a reciprocal divide is a multiply and a shift)     Normal formula would be :    time_in_ns = (NSEC_PER_SEC   len)  rate_bps     We compute multshift to use instead :    time_in_ns = (len   mult) >> shift;     We try to get the highest possible mult value for accuracy,   but have to make sure no overflows will ever happen.     reciprocal_value() is not used here it doesn't handle 64-bit values. ", "struct mini_Qdisc *miniq_old =rcu_dereference_protected(*miniqp->p_miniq, 1);struct mini_Qdisc *miniq;if (!tp_head) ": "mini_qdisc_pair_swap(struct mini_Qdisc_pair  miniqp,  struct tcf_proto  tp_head){  Protected with chain0->filter_chain_lock.   Can't access chain directly because tp_head can be NULL. ", "if (rfkill->state & RFKILL_BLOCK_SW_PREV)rfkill->state |= RFKILL_BLOCK_SW;elserfkill->state &= ~RFKILL_BLOCK_SW;}rfkill->state &= ~RFKILL_BLOCK_SW_SETCALL;rfkill->state &= ~RFKILL_BLOCK_SW_PREV;curr = rfkill->state & RFKILL_BLOCK_SW;spin_unlock_irqrestore(&rfkill->lock, flags);rfkill_led_trigger_event(rfkill);rfkill_global_led_trigger_event();if (prev != curr)rfkill_event(rfkill);}static void rfkill_update_global_state(enum rfkill_type type, bool blocked)": "rfkill_set_sw_state was invoked. ", "swprev = !!(rfkill->state & RFKILL_BLOCK_SW);hwprev = !!(rfkill->state & RFKILL_BLOCK_HW);__rfkill_set_sw_state(rfkill, sw);if (hw)rfkill->state |= RFKILL_BLOCK_HW;elserfkill->state &= ~RFKILL_BLOCK_HW;spin_unlock_irqrestore(&rfkill->lock, flags);if (!rfkill->registered) ": "rfkill_set_states(struct rfkill  rfkill, bool sw, bool hw){unsigned long flags;bool swprev, hwprev;BUG_ON(!rfkill);spin_lock_irqsave(&rfkill->lock, flags);    No need to care about prevsetblock ... this is for uevent only   and that will get triggered by rfkill_set_block anyway. ", "int genl_register_family(struct genl_family *family)": "genl_register_family - register a generic netlink family   @family: generic netlink family     Registers the specified family after validating it first. Only one   family may be registered with the same family name or identifier.     The family's ops, multicast groups and module pointer must already   be assigned.     Return 0 on success or a negative error code. ", "int genl_unregister_family(const struct genl_family *family)": "genl_unregister_family - unregister generic netlink family   @family: generic netlink family     Unregisters the specified family.     Returns 0 on success or a negative error code. ", "void *genlmsg_put(struct sk_buff *skb, u32 portid, u32 seq,  const struct genl_family *family, int flags, u8 cmd)": "genlmsg_put - Add generic netlink header to netlink message   @skb: socket buffer holding the message   @portid: netlink portid the message is addressed to   @seq: sequence number (usually the one of the sender)   @family: generic netlink family   @flags: netlink message flags   @cmd: generic netlink command     Returns pointer to user specific header ", "if (!doit->policy && !dumpit->policy)return 0;hdr = ctrl_dumppolicy_prep(skb, cb);if (!hdr)return -ENOBUFS;nest_pol = nla_nest_start(skb, CTRL_ATTR_OP_POLICY);if (!nest_pol)goto err;nest_op = nla_nest_start(skb, doit->cmd);if (!nest_op)goto err;if (doit->policy) ": "genlmsg_multicast_allns(&genl_ctrl, msg, 0,0, GFP_ATOMIC);rcu_read_unlock();}return 0;}struct ctrl_dump_policy_ctx {struct netlink_policy_dump_state  state;const struct genl_family  rt;struct genl_op_iter  op_iter;u32 op;u16 fam_id;u8 dump_map:1,   single_op:1;};static const struct nla_policy ctrl_policy_policy[] = {[CTRL_ATTR_FAMILY_ID]= { .type = NLA_U16 },[CTRL_ATTR_FAMILY_NAME]= { .type = NLA_NUL_STRING,    .len = GENL_NAMSIZ - 1 },[CTRL_ATTR_OP]= { .type = NLA_U32 },};static int ctrl_dumppolicy_start(struct netlink_callback  cb){const struct genl_dumpit_info  info = genl_dumpit_info(cb);struct ctrl_dump_policy_ctx  ctx = (void  )cb->ctx;struct nlattr   tb = info->attrs;const struct genl_family  rt;struct genl_op_iter i;int err;BUILD_BUG_ON(sizeof( ctx) > sizeof(cb->ctx));if (!tb[CTRL_ATTR_FAMILY_ID] && !tb[CTRL_ATTR_FAMILY_NAME])return -EINVAL;if (tb[CTRL_ATTR_FAMILY_ID]) {ctx->fam_id = nla_get_u16(tb[CTRL_ATTR_FAMILY_ID]);} else {rt = genl_family_find_byname(nla_data(tb[CTRL_ATTR_FAMILY_NAME]));if (!rt)return -ENOENT;ctx->fam_id = rt->id;}rt = genl_family_find_byid(ctx->fam_id);if (!rt)return -ENOENT;ctx->rt = rt;if (tb[CTRL_ATTR_OP]) {struct genl_split_ops doit, dump;ctx->single_op = true;ctx->op = nla_get_u32(tb[CTRL_ATTR_OP]);err = genl_get_cmd_both(ctx->op, rt, &doit, &dump);if (err) {NL_SET_BAD_ATTR(cb->extack, tb[CTRL_ATTR_OP]);return err;}if (doit.policy) {err = netlink_policy_dump_add_policy(&ctx->state,     doit.policy,     doit.maxattr);if (err)goto err_free_state;}if (dump.policy) {err = netlink_policy_dump_add_policy(&ctx->state,     dump.policy,     dump.maxattr);if (err)goto err_free_state;}if (!ctx->state)return -ENODATA;ctx->dump_map = 1;return 0;}ctx->op_iter = kmalloc(sizeof( ctx->op_iter), GFP_KERNEL);if (!ctx->op_iter)return -ENOMEM;genl_op_iter_init(rt, ctx->op_iter);ctx->dump_map = genl_op_iter_next(ctx->op_iter);for (genl_op_iter_init(rt, &i); genl_op_iter_next(&i); ) {if (i.doit.policy) {err = netlink_policy_dump_add_policy(&ctx->state,     i.doit.policy,     i.doit.maxattr);if (err)goto err_free_state;}if (i.dumpit.policy) {err = netlink_policy_dump_add_policy(&ctx->state,     i.dumpit.policy,     i.dumpit.maxattr);if (err)goto err_free_state;}}if (!ctx->state) {err = -ENODATA;goto err_free_op_iter;}return 0;err_free_state:netlink_policy_dump_free(ctx->state);err_free_op_iter:kfree(ctx->op_iter);return err;}static void  ctrl_dumppolicy_prep(struct sk_buff  skb,  struct netlink_callback  cb){struct ctrl_dump_policy_ctx  ctx = (void  )cb->ctx;void  hdr;hdr = genlmsg_put(skb, NETLINK_CB(cb->skb).portid,  cb->nlh->nlmsg_seq, &genl_ctrl,  NLM_F_MULTI, CTRL_CMD_GETPOLICY);if (!hdr)return NULL;if (nla_put_u16(skb, CTRL_ATTR_FAMILY_ID, ctx->fam_id))return NULL;return hdr;}static int ctrl_dumppolicy_put_op(struct sk_buff  skb,  struct netlink_callback  cb,  struct genl_split_ops  doit,  struct genl_split_ops  dumpit){struct ctrl_dump_policy_ctx  ctx = (void  )cb->ctx;struct nlattr  nest_pol,  nest_op;void  hdr;int idx;  skip if we have nothing to show ", "bool __netlink_ns_capable(const struct netlink_skb_parms *nsp,struct user_namespace *user_ns, int cap)": "netlink_ns_capable - General netlink message capability test   @nsp: NETLINK_CB of the socket buffer holding a netlink command from userspace.   @user_ns: The user namespace of the capability to use   @cap: The capability to use     Test to see if the opener of the socket we received the message   from had when the netlink socket was created and the sender of the   message has the capability @cap in the user namespace @user_ns. ", "bool netlink_capable(const struct sk_buff *skb, int cap)": "netlink_capable - Netlink global message capability test   @skb: socket buffer holding a netlink command from userspace   @cap: The capability to use     Test to see if the opener of the socket we received the message   from had when the netlink socket was created and the sender of the   message has the capability @cap in all user namespaces. ", "bool netlink_net_capable(const struct sk_buff *skb, int cap)": "netlink_net_capable - Netlink network namespace message capability test   @skb: socket buffer holding a netlink command from userspace   @cap: The capability to use     Test to see if the opener of the socket we received the message   from had when the netlink socket was created and the sender of the   message has the capability @cap over the network namespace of   the socket we received the message from. ", "skb_orphan(p->skb2);}}if (p->skb2 == NULL) ": "netlink_broadcast_deliver(struct sock  sk, struct sk_buff  skb){struct netlink_sock  nlk = nlk_sk(sk);if (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&    !test_bit(NETLINK_S_CONGESTED, &nlk->state)) {netlink_skb_set_owner_r(skb, sk);__netlink_sendskb(sk, skb);return atomic_read(&sk->sk_rmem_alloc) > (sk->sk_rcvbuf >> 1);}return -1;}struct netlink_broadcast_data {struct sock  exclude_sk;struct net  net;u32 portid;u32 group;int failure;int delivery_failure;int congested;int delivered;gfp_t allocation;struct sk_buff  skb,  skb2;};static void do_one_broadcast(struct sock  sk,    struct netlink_broadcast_data  p){struct netlink_sock  nlk = nlk_sk(sk);int val;if (p->exclude_sk == sk)return;if (nlk->portid == p->portid || p->group - 1 >= nlk->ngroups ||    !test_bit(p->group - 1, nlk->groups))return;if (!net_eq(sock_net(sk), p->net)) {if (!(nlk->flags & NETLINK_F_LISTEN_ALL_NSID))return;if (!peernet_has_id(sock_net(sk), p->net))return;if (!file_ns_capable(sk->sk_socket->file, p->net->user_ns,     CAP_NET_BROADCAST))return;}if (p->failure) {netlink_overrun(sk);return;}sock_hold(sk);if (p->skb2 == NULL) {if (skb_shared(p->skb)) {p->skb2 = skb_clone(p->skb, p->allocation);} else {p->skb2 = skb_get(p->skb);    skb ownership may have been set when   delivered to a previous socket. ", "int netlink_set_err(struct sock *ssk, u32 portid, u32 group, int code)": "netlink_set_err_data {struct sock  exclude_sk;u32 portid;u32 group;int code;};static int do_one_set_err(struct sock  sk, struct netlink_set_err_data  p){struct netlink_sock  nlk = nlk_sk(sk);int ret = 0;if (sk == p->exclude_sk)goto out;if (!net_eq(sock_net(sk), sock_net(p->exclude_sk)))goto out;if (nlk->portid == p->portid || p->group - 1 >= nlk->ngroups ||    !test_bit(p->group - 1, nlk->groups))goto out;if (p->code == ENOBUFS && nlk->flags & NETLINK_F_RECV_NO_ENOBUFS) {ret = 1;goto out;}sk->sk_err = p->code;sk_error_report(sk);out:return ret;}     netlink_set_err - report error to broadcast listeners   @ssk: the kernel netlink socket, as returned by netlink_kernel_create()   @portid: the PORTID of a process that we want to skip (if any)   @group: the broadcast group that will notice the error   @code: error code, must be negative (as usual in kernelspace)     This function returns the number of broadcast listeners that have set the   NETLINK_NO_ENOBUFS socket option. ", "if (nlk->cb_running) ": "__netlink_dump_start(struct sock  ssk, struct sk_buff  skb, const struct nlmsghdr  nlh, struct netlink_dump_control  control){struct netlink_sock  nlk,  nlk2;struct netlink_callback  cb;struct sock  sk;int ret;refcount_inc(&skb->users);sk = netlink_lookup(sock_net(ssk), ssk->sk_protocol, NETLINK_CB(skb).portid);if (sk == NULL) {ret = -ECONNREFUSED;goto error_free;}nlk = nlk_sk(sk);mutex_lock(nlk->cb_mutex);  A dump is in progress... ", "if (!err)return tlvlen;if (extack->bad_attr)tlvlen += nla_total_size(sizeof(u32));if (extack->policy)tlvlen += netlink_policy_dump_attr_size_estimate(extack->policy);if (extack->miss_type)tlvlen += nla_total_size(sizeof(u32));if (extack->miss_nest)tlvlen += nla_total_size(sizeof(u32));return tlvlen;}static voidnetlink_ack_tlv_fill(struct sk_buff *in_skb, struct sk_buff *skb,     struct nlmsghdr *nlh, int err,     const struct netlink_ext_ack *extack)": "netlink_ack_tlv_len(struct netlink_sock  nlk, int err,    const struct netlink_ext_ack  extack){size_t tlvlen;if (!extack || !(nlk->flags & NETLINK_F_EXT_ACK))return 0;tlvlen = 0;if (extack->_msg)tlvlen += nla_total_size(strlen(extack->_msg) + 1);if (extack->cookie_len)tlvlen += nla_total_size(extack->cookie_len);  Following attributes are only reported as error (not warning) ", "if (!(nlh->nlmsg_flags & NLM_F_REQUEST))goto ack;/* Skip control messages ": "netlink_rcv_skb(struct sk_buff  skb, int ( cb)(struct sk_buff  ,   struct nlmsghdr  ,   struct netlink_ext_ack  )){struct netlink_ext_ack extack;struct nlmsghdr  nlh;int err;while (skb->len >= nlmsg_total_size(0)) {int msglen;memset(&extack, 0, sizeof(extack));nlh = nlmsg_hdr(skb);err = 0;if (nlh->nlmsg_len < NLMSG_HDRLEN || skb->len < nlh->nlmsg_len)return 0;  Only requests are handled by the kernel ", "int nlmsg_notify(struct sock *sk, struct sk_buff *skb, u32 portid, unsigned int group, int report, gfp_t flags)": "nlmsg_notify - send a notification netlink message   @sk: netlink socket to use   @skb: notification message   @portid: destination netlink portid for reports or 0   @group: destination multicast group or 0   @report: 1 to report back, 0 to disable   @flags: allocation flags ", "datalen = skb->len - protoff;udph->len = htons(datalen);/* fix udp checksum if udp checksum was previously calculated ": "nf_nat_mangle_udp_packet(struct sk_buff  skb, struct nf_conn  ct, enum ip_conntrack_info ctinfo, unsigned int protoff, unsigned int match_offset, unsigned int match_len, const char  rep_buffer, unsigned int rep_len){struct udphdr  udph;int datalen, oldlen;if (skb_ensure_writable(skb, skb->len))return false;if (rep_len > match_len &&    rep_len - match_len > skb_tailroom(skb) &&    !enlarge_skb(skb, rep_len - match_len))return false;udph = (void  )skb->data + protoff;oldlen = skb->len - protoff;mangle_contents(skb, protoff + sizeof( udph),match_offset, match_len, rep_buffer, rep_len);  update the length of the UDP packet ", "BUG_ON(ct->status & IPS_NAT_DONE_MASK);/* Change src to where master sends to ": "nf_nat_follow_master(struct nf_conn  ct,  struct nf_conntrack_expect  exp){struct nf_nat_range2 range;  This must be a fresh one. ", "if (nf_ct_is_confirmed(ct))return NF_ACCEPT;WARN_ON(maniptype != NF_NAT_MANIP_SRC &&maniptype != NF_NAT_MANIP_DST);if (WARN_ON(nf_nat_initialized(ct, maniptype)))return NF_DROP;/* What we've got will look like inverse of reply. Normally * this is what is in the conntrack, except for prior * manipulations (future optimization: if num_manips == 0, * orig_tp = ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple) ": "nf_nat_setup_info(struct nf_conn  ct,  const struct nf_nat_range2  range,  enum nf_nat_manip_type maniptype){struct net  net = nf_ct_net(ct);struct nf_conntrack_tuple curr_tuple, new_tuple;  Can't setup nat info for confirmed ct. ", "WARN_ON(rcu_access_pointer(nf_queue_handler));rcu_assign_pointer(nf_queue_handler, qh);}EXPORT_SYMBOL(nf_register_queue_handler": "nf_register_queue_handler(const struct nf_queue_handler  qh){  should never happen, we only have one queueing backend in kernel ", "int nf_queue(struct sk_buff *skb, struct nf_hook_state *state,     unsigned int index, unsigned int verdict)": "nf_reinject(). ", "static bool nf_remove_net_hook(struct nf_hook_entries *old,       const struct nf_hook_ops *unreg)": "nf_register_net_hook(struct net  net, int pf,  const struct nf_hook_ops  reg){struct nf_hook_entries  p,  new_hooks;struct nf_hook_entries __rcu   pp;int err;switch (pf) {case NFPROTO_NETDEV:#ifndef CONFIG_NETFILTER_INGRESSif (reg->hooknum == NF_NETDEV_INGRESS)return -EOPNOTSUPP;#endif#ifndef CONFIG_NETFILTER_EGRESSif (reg->hooknum == NF_NETDEV_EGRESS)return -EOPNOTSUPP;#endifif ((reg->hooknum != NF_NETDEV_INGRESS &&     reg->hooknum != NF_NETDEV_EGRESS) ||    !reg->dev || dev_net(reg->dev) != net)return -EINVAL;break;case NFPROTO_INET:if (reg->hooknum != NF_INET_INGRESS)break;err = nf_ingress_check(net, reg, NF_INET_INGRESS);if (err < 0)return err;break;}pp = nf_hook_entry_head(net, pf, reg->hooknum, reg->dev);if (!pp)return -EINVAL;mutex_lock(&nf_hook_mutex);p = nf_entry_dereference( pp);new_hooks = nf_hook_entries_grow(p, reg);if (!IS_ERR(new_hooks)) {hooks_validate(new_hooks);rcu_assign_pointer( pp, new_hooks);}mutex_unlock(&nf_hook_mutex);if (IS_ERR(new_hooks))return PTR_ERR(new_hooks);#ifdef CONFIG_NETFILTER_INGRESSif (nf_ingress_hook(reg, pf))net_inc_ingress_queue();#endif#ifdef CONFIG_NETFILTER_EGRESSif (nf_egress_hook(reg, pf))net_inc_egress_queue();#endifnf_static_key_inc(reg, pf);BUG_ON(p == new_hooks);nf_hook_entries_free(p);return 0;}    nf_remove_net_hook - remove a hook from blob     @oldp: current address of hook blob   @unreg: hook to unregister     This cannot fail, hook unregistration must always succeed.   Therefore replace the to-be-removed hook with a dummy hook. ", "}static const struct nf_hook_ops dummy_ops = ": "nf_hook_slow call next hook ", "list_splice(&sublist, head);}EXPORT_SYMBOL(nf_hook_slow_list": "nf_hook_slow_list(struct list_head  head, struct nf_hook_state  state,       const struct nf_hook_entries  e){struct sk_buff  skb,  next;struct list_head sublist;int ret;INIT_LIST_HEAD(&sublist);list_for_each_entry_safe(skb, next, head, list) {skb_list_del_init(skb);ret = nf_hook_slow(skb, state, e, 0);if (ret == 1)list_add_tail(&skb->list, &sublist);}  Put passed packets back on main list ", "}}mutex_unlock(&xt[af].mutex);if (af != NFPROTO_UNSPEC)/* Try searching again in the family-independent list ": "xt_find_match(u8 af, const char  name, u8 revision){struct xt_match  m;int err = -ENOENT;if (strnlen(name, XT_EXTENSION_MAXNAMELEN) == XT_EXTENSION_MAXNAMELEN)return ERR_PTR(-EINVAL);mutex_lock(&xt[af].mutex);list_for_each_entry(m, &xt[af].match, list) {if (strcmp(m->name, name) == 0) {if (m->revision == revision) {if (try_module_get(m->me)) {mutex_unlock(&xt[af].mutex);return m;}} elseerr = -EPROTOTYPE;   Found something. ", "int xt_check_proc_name(const char *name, unsigned int size)": "xt_check_proc_name - check that name is suitable for proc file creation     @name: file name candidate   @size: length of buffer     some x_tables modules wish to create a file in proc.   This function makes sure that the name is suitable for this   purpose, it checks that name is NUL terminated and isn't a 'special'   name, like \"..\".     returns negative number on error or 0 if name is useable. ", "int xt_check_table_hooks(const struct xt_table_info *info, unsigned int valid_hooks)": "xt_check_table_hooks - check hook entry points are sane     @info xt_table_info to check   @valid_hooks - hook entry points that we can enter from     Validates that the hook entry and underflows points are set up.     Return: 0 on success, negative errno on failure. ", "BUILD_BUG_ON(sizeof(struct compat_xt_entry_match) != sizeof(struct xt_entry_match));return xt_check_entry_match(elems, base + target_offset,    __alignof__(struct compat_xt_entry_match));}EXPORT_SYMBOL(xt_compat_check_entry_offsets": "xt_compat_check_entry_offsets(const void  base, const char  elems,  unsigned int target_offset,  unsigned int next_offset){long size_of_base_struct = elems - (const char  )base;const struct compat_xt_entry_target  t;const char  e = base;if (target_offset < size_of_base_struct)return -EINVAL;if (target_offset + sizeof( t) > next_offset)return -EINVAL;t = (void  )(e + target_offset);if (t->u.target_size < sizeof( t))return -EINVAL;if (target_offset + t->u.target_size > next_offset)return -EINVAL;if (strcmp(t->u.user.name, XT_STANDARD_TARGET) == 0) {const struct compat_xt_standard_target  st = (const void  )t;if (COMPAT_XT_ALIGN(target_offset + sizeof( st)) != next_offset)return -EINVAL;if (!verdict_ok(st->verdict))return -EINVAL;} else if (strcmp(t->u.user.name, XT_ERROR_TARGET) == 0) {const struct compat_xt_error_target  et = (const void  )t;if (!error_tg_ok(t->u.target_size, sizeof( et), et->errorname, sizeof(et->errorname)))return -EINVAL;}  compat_xt_entry match has less strict alignment requirements,   otherwise they are identical.  In case of padding differences   we need to add compat version of xt_check_entry_match. ", "int xt_check_entry_offsets(const void *base,   const char *elems,   unsigned int target_offset,   unsigned int next_offset)": "xt_check_entry_offsets - validate arpipip6t_entry     @base: pointer to arpipip6t_entry   @elems: pointer to first xt_entry_match, i.e. ip(6)t_entry->elems   @target_offset: the arpipip6_t->target_offset   @next_offset: the arpipip6_t->next_offset     validates that target_offset and next_offset are sane and that all   match sizes (if any) align with the target offset.     This function does not validate the targets or matches themselves, it   only tests that all the offsets and sizes are correct, that all   match structures are aligned, and that the last structure ends where   the target structure begins.     Also see xt_compat_check_entry_offsets for CONFIG_NETFILTER_XTABLES_COMPAT version.     The arpipip6t_entry structure @base must have passed following tests:   - it must point to a valid memory location   - base to base + next_offset must be accessible, i.e. not exceed allocated     length.     A well-formed entry looks like this:     ip(6)t_entry   match [mtdata]  match [mtdata] target [tgdata] ip(6)t_entry   e->elems[]-----'                              |               |                  matchsize                      |               |                                  matchsize      |               |                                                 |               |   target_offset---------------------------------'               |   next_offset---------------------------------------------------'     elems[]: flexible array member at end of ip(6)arpt_entry struct.            This is where matches (if any) and the target reside.   target_offset: beginning of target.   next_offset: start of the next rule; also: size of this rule.   Since targets have a minimum size, target_offset + minlen <= next_offset.     Every match stores its size, sum of sizes must not exceed target_offset.     Return: 0 on success, negative errno on failure. ", "unsigned int *xt_alloc_entry_offsets(unsigned int size)": "xt_alloc_entry_offsets - allocate array to store rule head offsets     @size: number of entries     Return: NULL or zeroed kmalloc'd or vmalloc'd array ", "bool xt_find_jump_offset(const unsigned int *offsets, unsigned int target, unsigned int size)": "xt_find_jump_offset - check if target is a valid jump offset     @offsets: array containing all valid rule start offsets of a rule blob   @target: the jump target to search for   @size: entries in @offset ", "WARN_ON(nf_ct_is_confirmed(ct));/* struct nf_ct_ext uses u8 to store offsets/size ": "nf_ct_ext_add(struct nf_conn  ct, enum nf_ct_ext_id id, gfp_t gfp){unsigned int newlen, newoff, oldlen, alloc;struct nf_ct_ext  new;  Conntrack must not be confirmed to avoid races on reallocation. ", "struct nf_conn *nf_ct_tmpl_alloc(struct net *net, const struct nf_conntrack_zone *zone, gfp_t flags)": "nf_ct_destroy() ", "netdev->dcbnl_ops->getpgtccfgrx(netdev,i - DCB_PG_ATTR_TC_0, &prio,&pgid, &tc_pct, &up_map);} else ": "dcb_setapp(netdev, &app);}ret = nla_put_u8(skb, DCB_ATTR_APP, ret);dcbnl_cee_notify(netdev, RTM_SETDCB, DCB_CMD_SAPP, seq, 0);return ret;}static int __dcbnl_pg_getcfg(struct net_device  netdev, struct nlmsghdr  nlh,     struct nlattr   tb, struct sk_buff  skb, int dir){struct nlattr  pg_nest,  param_nest,  data;struct nlattr  pg_tb[DCB_PG_ATTR_MAX + 1];struct nlattr  param_tb[DCB_TC_ATTR_PARAM_MAX + 1];u8 prio, pgid, tc_pct, up_map;int ret;int getall = 0;int i;if (!tb[DCB_ATTR_PG_CFG])return -EINVAL;if (!netdev->dcbnl_ops->getpgtccfgtx ||    !netdev->dcbnl_ops->getpgtccfgrx ||    !netdev->dcbnl_ops->getpgbwgcfgtx ||    !netdev->dcbnl_ops->getpgbwgcfgrx)return -EOPNOTSUPP;ret = nla_parse_nested_deprecated(pg_tb, DCB_PG_ATTR_MAX,  tb[DCB_ATTR_PG_CFG], dcbnl_pg_nest,  NULL);if (ret)return ret;pg_nest = nla_nest_start_noflag(skb, DCB_ATTR_PG_CFG);if (!pg_nest)return -EMSGSIZE;if (pg_tb[DCB_PG_ATTR_TC_ALL])getall = 1;for (i = DCB_PG_ATTR_TC_0; i <= DCB_PG_ATTR_TC_7; i++) {if (!getall && !pg_tb[i])continue;if (pg_tb[DCB_PG_ATTR_TC_ALL])data = pg_tb[DCB_PG_ATTR_TC_ALL];elsedata = pg_tb[i];ret = nla_parse_nested_deprecated(param_tb,  DCB_TC_ATTR_PARAM_MAX, data,  dcbnl_tc_param_nest, NULL);if (ret)goto err_pg;param_nest = nla_nest_start_noflag(skb, i);if (!param_nest)goto err_pg;pgid = DCB_ATTR_VALUE_UNDEFINED;prio = DCB_ATTR_VALUE_UNDEFINED;tc_pct = DCB_ATTR_VALUE_UNDEFINED;up_map = DCB_ATTR_VALUE_UNDEFINED;if (dir) {  Rx ", "if ((!app_tb[DCB_APP_ATTR_IDTYPE]) ||    (!app_tb[DCB_APP_ATTR_ID]) ||    (!app_tb[DCB_APP_ATTR_PRIORITY]))return -EINVAL;/* either by eth type or by socket number ": "dcb_getapp(netdev, &app);}app_nest = nla_nest_start_noflag(skb, DCB_ATTR_APP);if (!app_nest)return -EMSGSIZE;ret = nla_put_u8(skb, DCB_APP_ATTR_IDTYPE, idtype);if (ret)goto out_cancel;ret = nla_put_u16(skb, DCB_APP_ATTR_ID, id);if (ret)goto out_cancel;ret = nla_put_u8(skb, DCB_APP_ATTR_PRIORITY, up);if (ret)goto out_cancel;nla_nest_end(skb, app_nest);return 0;out_cancel:nla_nest_cancel(skb, app_nest);return ret;}static int dcbnl_setapp(struct net_device  netdev, struct nlmsghdr  nlh,u32 seq, struct nlattr   tb, struct sk_buff  skb){int ret;u16 id;u8 up, idtype;struct nlattr  app_tb[DCB_APP_ATTR_MAX + 1];if (!tb[DCB_ATTR_APP])return -EINVAL;ret = nla_parse_nested_deprecated(app_tb, DCB_APP_ATTR_MAX,  tb[DCB_ATTR_APP], dcbnl_app_nest,  NULL);if (ret)return ret;  all must be non-null ", "u8 dcb_ieee_getapp_mask(struct net_device *dev, struct dcb_app *app)": "dcb_ieee_getapp_mask - retrieve the IEEE DCB application priority   @dev: network interface   @app: where to store the retrieve application data     Helper routine which on success returns a non-zero 802.1Qaz user   priority bitmap otherwise returns 0 to indicate the dcb_app was   not found in APP list. ", "for (i = 0; i < nselectors; i++) ": "dcb_ieee_setapp);if (err)goto err;}if (ieee[DCB_ATTR_DCB_APP_TRUST_TABLE]) {u8 selectors[IEEE_8021QAZ_APP_SEL_MAX + 1] = {0};struct nlattr  attr;int nselectors = 0;int rem;if (!ops->dcbnl_setapptrust) {err = -EOPNOTSUPP;goto err;}nla_for_each_nested(attr, ieee[DCB_ATTR_DCB_APP_TRUST_TABLE],    rem) {enum ieee_attrs_app type = nla_type(attr);u8 selector;int i;if (!dcbnl_app_attr_type_validate(type) ||    nla_len(attr) != 1 ||    nselectors >= sizeof(selectors)) {err = -EINVAL;goto err;}selector = nla_get_u8(attr);if (!dcbnl_app_selector_validate(type, selector)) {err = -EINVAL;goto err;}  Duplicate selector ? ", "static int dcbnl_getdcbx(struct net_device *netdev, struct nlmsghdr *nlh, u32 seq, struct nlattr **tb, struct sk_buff *skb)": "dcb_ieee_delapp);if (err)goto err;}if (ieee[DCB_ATTR_DCB_REWR_TABLE]) {err = dcbnl_app_table_setdel(ieee[DCB_ATTR_DCB_REWR_TABLE],     netdev,     ops->dcbnl_delrewr ?: dcb_delrewr);if (err)goto err;}err:err = nla_put_u8(skb, DCB_ATTR_IEEE, err);dcbnl_ieee_notify(netdev, RTM_SETDCB, DCB_CMD_IEEE_DEL, seq, 0);return err;}  DCBX configuration ", "void dcb_getrewr_prio_pcp_mask_map(const struct net_device *dev,   struct dcb_rewr_prio_pcp_map *p_map)": "dcb_getrewr_prio_pcp_mask_map - For a given device, find mapping from   priorities to the PCP and DEI values assigned to that priority. ", "void dcb_getrewr_prio_dscp_mask_map(const struct net_device *dev,    struct dcb_ieee_app_prio_map *p_map)": "dcb_getrewr_prio_dscp_mask_map - For a given device, find mapping from   priorities to the DSCP values assigned to that priority. ", "void dcb_ieee_getapp_prio_dscp_mask_map(const struct net_device *dev,struct dcb_ieee_app_prio_map *p_map)": "dcb_ieee_getapp_prio_dscp_mask_map - For a given device, find mapping from   priorities to the DSCP values assigned to that priority. Initialize p_map   such that each map element holds a bit mask of DSCP values configured for   that priority by APP entries. ", "voiddcb_ieee_getapp_dscp_prio_mask_map(const struct net_device *dev,   struct dcb_ieee_app_dscp_map *p_map)": "dcb_ieee_getapp_dscp_prio_mask_map - For a given device, find mapping from   DSCP values to the priorities assigned to that DSCP value. Initialize p_map   such that each map element holds a bit mask of priorities configured for a   given DSCP value by APP entries. ", "u8 dcb_ieee_getapp_default_prio_mask(const struct net_device *dev)": "dcb_ieee_getapp_default_prio_mask - For a given device, find all APP entries   of the form {$PRIO, ETHERTYPE, 0} and construct a bit mask of all default   priorities set by these entries. ", "if (S_ISSOCK(inode->i_mode) && !(filp->f_mode & FMODE_PATH)) ": "unix_get_socket(struct file  filp){struct sock  u_sock = NULL;struct inode  inode = file_inode(filp);  Socket ? ", "UNIXCB(skb).fp = scm_fp_dup(scm->fp);if (!UNIXCB(skb).fp)return -ENOMEM;for (i = scm->fp->count - 1; i >= 0; i--)unix_inflight(scm->fp->user, scm->fp->fp[i]);return 0;}EXPORT_SYMBOL(unix_attach_fds": "unix_attach_fds(struct scm_cookie  scm, struct sk_buff  skb){int i;if (too_many_unix_fds(current))return -ETOOMANYREFS;    Need to duplicate file references for the sake of garbage   collection.  Otherwise a socket in the fps might become a   candidate for GC while the skb is not yet queued. ", "/* So fscking what? fput() had been SMP-safe since the last Summer ": "unix_destruct_scm(struct sk_buff  skb){struct scm_cookie scm;memset(&scm, 0, sizeof(scm));scm.pid  = UNIXCB(skb).pid;if (UNIXCB(skb).fp)unix_detach_fds(&scm, skb);  Alas, it calls VFS ", "if (!between48(seq, dccp_rsk(req)->dreq_iss, dccp_rsk(req)->dreq_gss)) ": "dccp_req_err(struct sock  sk, u64 seq){struct request_sock  req = inet_reqsk(sk);struct net  net = sock_net(sk);    ICMPs are not backlogged, hence we cannot get an established   socket here. ", "while (refcount_read(&lapb->refcnt) > 2)usleep_range(1, 10);spin_lock_bh(&lapb->lock);lapb_stop_t1timer(lapb);lapb_stop_t2timer(lapb);lapb_clear_queues(lapb);spin_unlock_bh(&lapb->lock);/* Wait for running timers to stop ": "lapb_unregister(struct net_device  dev){struct lapb_cb  lapb;int rc = LAPB_BADTOKEN;write_lock_bh(&lapb_list_lock);lapb = __lapb_devtostruct(dev);if (!lapb)goto out;lapb_put(lapb);  Wait for other refs to \"lapb\" to drop ", "WARN_ON(cr.ap_mld_addr && !cr.links[link_id].addr);BUG_ON(!cr.links[link_id].bss->channel);if (cr.links[link_id].bss->channel->band == NL80211_BAND_S1GHZ) ": "cfg80211_rx_assoc_resp(struct net_device  dev,    struct cfg80211_rx_assoc_resp  data){struct wireless_dev  wdev = dev->ieee80211_ptr;struct wiphy  wiphy = wdev->wiphy;struct cfg80211_registered_device  rdev = wiphy_to_rdev(wiphy);struct ieee80211_mgmt  mgmt = (struct ieee80211_mgmt  )data->buf;struct cfg80211_connect_resp_params cr = {.timeout_reason = NL80211_TIMEOUT_UNSPECIFIED,.req_ie = data->req_ies,.req_ie_len = data->req_ies_len,.resp_ie = mgmt->u.assoc_resp.variable,.resp_ie_len = data->len -       offsetof(struct ieee80211_mgmt,u.assoc_resp.variable),.status = le16_to_cpu(mgmt->u.assoc_resp.status_code),.ap_mld_addr = data->ap_mld_addr,};unsigned int link_id;for (link_id = 0; link_id < ARRAY_SIZE(data->links); link_id++) {cr.links[link_id].status = data->links[link_id].status;WARN_ON_ONCE(cr.links[link_id].status != WLAN_STATUS_SUCCESS &&     (!cr.ap_mld_addr || !cr.links[link_id].bss));cr.links[link_id].bss = data->links[link_id].bss;if (!cr.links[link_id].bss)continue;cr.links[link_id].bssid = data->links[link_id].bss->bssid;cr.links[link_id].addr = data->links[link_id].addr;  need to have local link addresses for MLO connections ", "/* Indicate the received Action frame to user space ": "cfg80211_rx_mgmt_ext(struct wireless_dev  wdev,  struct cfg80211_rx_info  info){struct wiphy  wiphy = wdev->wiphy;struct cfg80211_registered_device  rdev = wiphy_to_rdev(wiphy);struct cfg80211_mgmt_registration  reg;const struct ieee80211_txrx_stypes  stypes =&wiphy->mgmt_stypes[wdev->iftype];struct ieee80211_mgmt  mgmt = (void  )info->buf;const u8  data;int data_len;bool result = false;__le16 ftype = mgmt->frame_control &cpu_to_le16(IEEE80211_FCTL_FTYPE | IEEE80211_FCTL_STYPE);u16 stype;trace_cfg80211_rx_mgmt(wdev, info);stype = (le16_to_cpu(mgmt->frame_control) & IEEE80211_FCTL_STYPE) >> 4;if (!(stypes->rx & BIT(stype))) {trace_cfg80211_return_bool(false);return false;}data = info->buf + ieee80211_hdrlen(mgmt->frame_control);data_len = info->len - ieee80211_hdrlen(mgmt->frame_control);spin_lock_bh(&rdev->mgmt_registrations_lock);list_for_each_entry(reg, &wdev->mgmt_registrations, list) {if (reg->frame_type != ftype)continue;if (reg->match_len > data_len)continue;if (memcmp(reg->match, data, reg->match_len))continue;  found match! ", "cfg80211_set_dfs_state(wiphy, chandef, NL80211_DFS_UNAVAILABLE);if (offchan)queue_work(cfg80211_wq, &rdev->background_cac_abort_wk);cfg80211_sched_dfs_chan_update(rdev);nl80211_radar_notify(rdev, chandef, NL80211_RADAR_DETECTED, NULL, gfp);memcpy(&rdev->radar_chandef, chandef, sizeof(struct cfg80211_chan_def));queue_work(cfg80211_wq, &rdev->propagate_radar_detect_wk);}EXPORT_SYMBOL(__cfg80211_radar_event": "__cfg80211_radar_event(struct wiphy  wiphy,    struct cfg80211_chan_def  chandef,    bool offchan, gfp_t gfp){struct cfg80211_registered_device  rdev = wiphy_to_rdev(wiphy);trace_cfg80211_radar_event(wiphy, chandef, offchan);  only set the chandef supplied channel to unavailable, in   case the radar is detected on only one of multiple channels   spanned by the chandef. ", "if (wdev->valid_links)return;trace_cfg80211_cac_event(netdev, event);if (WARN_ON(!wdev->cac_started && event != NL80211_RADAR_CAC_STARTED))return;switch (event) ": "cfg80211_cac_event(struct net_device  netdev,const struct cfg80211_chan_def  chandef,enum nl80211_radar_event event, gfp_t gfp){struct wireless_dev  wdev = netdev->ieee80211_ptr;struct wiphy  wiphy = wdev->wiphy;struct cfg80211_registered_device  rdev = wiphy_to_rdev(wiphy);unsigned long timeout;  not yet supported ", "if (chandef->center_freq1 - chandef->center_freq2 == 80 ||    chandef->center_freq2 - chandef->center_freq1 == 80)return false;break;default:if (chandef->center_freq2)return false;break;}switch (chandef->width) ": "cfg80211_chandef_valid(const struct cfg80211_chan_def  chandef){u32 control_freq, oper_freq;int oper_width, control_width;if (!chandef->chan)return false;if (chandef->freq1_offset >= 1000)return false;control_freq = chandef->chan->center_freq;switch (chandef->width) {case NL80211_CHAN_WIDTH_5:case NL80211_CHAN_WIDTH_10:case NL80211_CHAN_WIDTH_20:case NL80211_CHAN_WIDTH_20_NOHT:if (ieee80211_chandef_to_khz(chandef) !=    ieee80211_channel_to_khz(chandef->chan))return false;if (chandef->center_freq2)return false;break;case NL80211_CHAN_WIDTH_1:case NL80211_CHAN_WIDTH_2:case NL80211_CHAN_WIDTH_4:case NL80211_CHAN_WIDTH_8:case NL80211_CHAN_WIDTH_16:if (chandef->chan->band != NL80211_BAND_S1GHZ)return false;control_freq = ieee80211_channel_to_khz(chandef->chan);oper_freq = ieee80211_chandef_to_khz(chandef);control_width = nl80211_chan_width_to_mhz(ieee80211_s1g_channel_width(chandef->chan));oper_width = cfg80211_chandef_get_width(chandef);if (oper_width < 0 || control_width < 0)return false;if (chandef->center_freq2)return false;if (control_freq + MHZ_TO_KHZ(control_width)  2 >    oper_freq + MHZ_TO_KHZ(oper_width)  2)return false;if (control_freq - MHZ_TO_KHZ(control_width)  2 <    oper_freq - MHZ_TO_KHZ(oper_width)  2)return false;break;case NL80211_CHAN_WIDTH_80P80:if (!chandef->center_freq2)return false;  adjacent is not allowed -- that's a 160 MHz channel ", "if (cfg80211_chandef_identical(c1, c2))return c1;/* otherwise, must have same control channel ": "cfg80211_chandef_compatible(const struct cfg80211_chan_def  c1,    const struct cfg80211_chan_def  c2){u32 c1_pri40, c1_pri80, c2_pri40, c2_pri80, c1_pri160, c2_pri160;  If they are identical, return ", "if (width > 20)prohibited_flags |= IEEE80211_CHAN_NO_OFDM;/* 5 and 10 MHz are only defined for the OFDM PHY ": "cfg80211_chandef_usable(struct wiphy  wiphy,     const struct cfg80211_chan_def  chandef,     u32 prohibited_flags){struct ieee80211_sta_ht_cap  ht_cap;struct ieee80211_sta_vht_cap  vht_cap;struct ieee80211_edmg  edmg_cap;u32 width, control_freq, cap;bool ext_nss_cap, support_80_80 = false, support_320 = false;const struct ieee80211_sband_iftype_data  iftd;struct ieee80211_supported_band  sband;int i;if (WARN_ON(!cfg80211_chandef_valid(chandef)))return false;ht_cap = &wiphy->bands[chandef->chan->band]->ht_cap;vht_cap = &wiphy->bands[chandef->chan->band]->vht_cap;edmg_cap = &wiphy->bands[chandef->chan->band]->edmg_cap;ext_nss_cap = __le16_to_cpu(vht_cap->vht_mcs.tx_highest) &IEEE80211_VHT_EXT_NSS_BW_CAPABLE;if (edmg_cap->channels &&    !cfg80211_edmg_usable(wiphy,  chandef->edmg.channels,  chandef->edmg.bw_config,  chandef->chan->hw_value,  edmg_cap))return false;control_freq = chandef->chan->center_freq;switch (chandef->width) {case NL80211_CHAN_WIDTH_1:width = 1;break;case NL80211_CHAN_WIDTH_2:width = 2;break;case NL80211_CHAN_WIDTH_4:width = 4;break;case NL80211_CHAN_WIDTH_8:width = 8;break;case NL80211_CHAN_WIDTH_16:width = 16;break;case NL80211_CHAN_WIDTH_5:width = 5;break;case NL80211_CHAN_WIDTH_10:prohibited_flags |= IEEE80211_CHAN_NO_10MHZ;width = 10;break;case NL80211_CHAN_WIDTH_20:if (!ht_cap->ht_supported &&    chandef->chan->band != NL80211_BAND_6GHZ)return false;fallthrough;case NL80211_CHAN_WIDTH_20_NOHT:prohibited_flags |= IEEE80211_CHAN_NO_20MHZ;width = 20;break;case NL80211_CHAN_WIDTH_40:width = 40;if (chandef->chan->band == NL80211_BAND_6GHZ)break;if (!ht_cap->ht_supported)return false;if (!(ht_cap->cap & IEEE80211_HT_CAP_SUP_WIDTH_20_40) ||    ht_cap->cap & IEEE80211_HT_CAP_40MHZ_INTOLERANT)return false;if (chandef->center_freq1 < control_freq &&    chandef->chan->flags & IEEE80211_CHAN_NO_HT40MINUS)return false;if (chandef->center_freq1 > control_freq &&    chandef->chan->flags & IEEE80211_CHAN_NO_HT40PLUS)return false;break;case NL80211_CHAN_WIDTH_80P80:cap = vht_cap->cap;support_80_80 =(cap & IEEE80211_VHT_CAP_SUPP_CHAN_WIDTH_160_80PLUS80MHZ) ||(cap & IEEE80211_VHT_CAP_SUPP_CHAN_WIDTH_160MHZ && cap & IEEE80211_VHT_CAP_EXT_NSS_BW_MASK) ||(ext_nss_cap && u32_get_bits(cap, IEEE80211_VHT_CAP_EXT_NSS_BW_MASK) > 1);if (chandef->chan->band != NL80211_BAND_6GHZ && !support_80_80)return false;fallthrough;case NL80211_CHAN_WIDTH_80:prohibited_flags |= IEEE80211_CHAN_NO_80MHZ;width = 80;if (chandef->chan->band == NL80211_BAND_6GHZ)break;if (!vht_cap->vht_supported)return false;break;case NL80211_CHAN_WIDTH_160:prohibited_flags |= IEEE80211_CHAN_NO_160MHZ;width = 160;if (chandef->chan->band == NL80211_BAND_6GHZ)break;if (!vht_cap->vht_supported)return false;cap = vht_cap->cap & IEEE80211_VHT_CAP_SUPP_CHAN_WIDTH_MASK;if (cap != IEEE80211_VHT_CAP_SUPP_CHAN_WIDTH_160MHZ &&    cap != IEEE80211_VHT_CAP_SUPP_CHAN_WIDTH_160_80PLUS80MHZ &&    !(ext_nss_cap &&      (vht_cap->cap & IEEE80211_VHT_CAP_EXT_NSS_BW_MASK)))return false;break;case NL80211_CHAN_WIDTH_320:prohibited_flags |= IEEE80211_CHAN_NO_320MHZ;width = 320;if (chandef->chan->band != NL80211_BAND_6GHZ)return false;sband = wiphy->bands[NL80211_BAND_6GHZ];if (!sband)return false;for (i = 0; i < sband->n_iftype_data; i++) {iftd = &sband->iftype_data[i];if (!iftd->eht_cap.has_eht)continue;if (iftd->eht_cap.eht_cap_elem.phy_cap_info[0] &    IEEE80211_EHT_PHY_CAP0_320MHZ_IN_6GHZ) {support_320 = true;break;}}if (!support_320)return false;break;default:WARN_ON_ONCE(1);return false;}    TODO: What if there are only certain 8016080+80 MHz channels   allowed by the driver, or only certain combinations?   For 40 MHz the driver can set the NO_HT40 flags, but for   80160 MHz and in particular 80+80 MHz this isn't really   feasible and we only have NO_80MHZNO_160MHZ so far but   no way to cover 80+80 MHz or more complex restrictions.   Note that such restrictions also need to be advertised to   userspace, for example for P2P channel selection. ", "prohibited_flags = IEEE80211_CHAN_DISABLED;}res = cfg80211_chandef_usable(wiphy, chandef, prohibited_flags);trace_cfg80211_return_bool(res);return res;}bool cfg80211_reg_can_beacon(struct wiphy *wiphy,     struct cfg80211_chan_def *chandef,     enum nl80211_iftype iftype)": "cfg80211_reg_can_beacon(struct wiphy  wiphy,     struct cfg80211_chan_def  chandef,     enum nl80211_iftype iftype,     bool check_no_ir){bool res;u32 prohibited_flags = IEEE80211_CHAN_DISABLED |       IEEE80211_CHAN_RADAR;trace_cfg80211_reg_can_beacon(wiphy, chandef, iftype, check_no_ir);if (check_no_ir)prohibited_flags |= IEEE80211_CHAN_NO_IR;if (cfg80211_chandef_dfs_required(wiphy, chandef, iftype) > 0 &&    cfg80211_chandef_dfs_available(wiphy, chandef)) {  We can skip IEEE80211_CHAN_NO_IR if chandef dfs available ", "check_no_ir = !cfg80211_ir_permissive_chan(wiphy, iftype,   chandef->chan);return _cfg80211_reg_can_beacon(wiphy, chandef, iftype, check_no_ir);}EXPORT_SYMBOL(cfg80211_reg_can_beacon_relax": "cfg80211_reg_can_beacon_relax(struct wiphy  wiphy,   struct cfg80211_chan_def  chandef,   enum nl80211_iftype iftype){struct cfg80211_registered_device  rdev = wiphy_to_rdev(wiphy);bool check_no_ir;lockdep_assert_held(&rdev->wiphy.mtx);    Under certain conditions suggested by some regulatory bodies a   GOSTA can IR on channels marked with IEEE80211_NO_IR. Set this flag   only if such relaxations are not enabled and the conditions are not   met. ", "if (link_id || wdev->valid_links & BIT(0)) ": "wdev_chandef(struct wireless_dev  wdev,       unsigned int link_id){    We need to sort out the locking here - in some cases   where we get here we really just don't care (yet)   about the valid links, but in others we do. But we   get here with various driver cases, so we cannot   easily require the wdev mutex. ", "if (*bitmap & (u16)BIT((chandef->chan->center_freq - start_freq) / 20))return false;for (i = 0; i < per_bw_puncturing[idx].len; i++)if (per_bw_puncturing[idx].valid_values[i] == *bitmap)return true;return false;}EXPORT_SYMBOL(cfg80211_valid_disable_subchannel_bitmap": "cfg80211_valid_disable_subchannel_bitmap(u16  bitmap,      const struct cfg80211_chan_def  chandef){u32 idx, i, start_freq;switch (chandef->width) {case NL80211_CHAN_WIDTH_80:idx = 0;start_freq = chandef->center_freq1 - 40;break;case NL80211_CHAN_WIDTH_160:idx = 1;start_freq = chandef->center_freq1 - 80;break;case NL80211_CHAN_WIDTH_320:idx = 2;start_freq = chandef->center_freq1 - 160;break;default: bitmap = 0;break;}if (! bitmap)return true;  check if primary channel is punctured ", "if (!spydata)return -EOPNOTSUPP;/* Disable spy collection while we copy the addresses. * While we copy addresses, any call to wireless_spy_update() * will NOP. This is OK, as anyway the addresses are changing. ": "iw_handler_set_spy(struct net_device  dev,       struct iw_request_info  info,       union iwreq_data  wrqu,       char  extra){struct iw_spy_data  spydata = get_spydata(dev);struct sockaddr  address = (struct sockaddr  ) extra;  Make sure driver is not buggy or using the old API ", "if (!spydata)return -EOPNOTSUPP;wrqu->data.length = spydata->spy_number;/* Copy addresses. ": "iw_handler_get_spy(struct net_device  dev,       struct iw_request_info  info,       union iwreq_data  wrqu,       char  extra){struct iw_spy_data  spydata = get_spydata(dev);struct sockaddr  address = (struct sockaddr  ) extra;inti;  Make sure driver is not buggy or using the old API ", "if (!spydata)return -EOPNOTSUPP;/* Just do it ": "iw_handler_get_thrspy(struct net_device  dev,  struct iw_request_info  info,  union iwreq_data  wrqu,  char  extra){struct iw_spy_data  spydata = get_spydata(dev);struct iw_thrspy  threshold = (struct iw_thrspy  ) extra;  Make sure driver is not buggy or using the old API ", "spydata->spy_number = 0;/* We want to operate without locking, because wireless_spy_update() * most likely will happen in the interrupt handler, and therefore * have its own locking constraints and needs performance. * The rtnl_lock() make sure we don't race with the other iw_handlers. * This make sure wireless_spy_update() \"see\" that the spy list * is temporarily disabled. ": "wireless_spy_update()   will NOP. This is OK, as anyway the addresses are changing. ", "spin_lock_irqsave(info->lock, flags);if (!info->crypt_quiesced) ": "lib80211_crypt_delayed_deinit(struct lib80211_crypt_info  info,    struct lib80211_crypt_data   crypt){struct lib80211_crypt_data  tmp;unsigned long flags;if ( crypt == NULL)return;tmp =  crypt; crypt = NULL;  must not run ops->deinit() while there may be pending encrypt or   decrypt operations. Use a list of delayed deinits to avoid needing   locking. ", "id_len = non_inherit_elem->data[1];if (non_inherit_elem->datalen < 3 + id_len)return true;ext_id_len = non_inherit_elem->data[2 + id_len];if (non_inherit_elem->datalen < 3 + id_len + ext_id_len)return true;if (elem->id == WLAN_EID_EXTENSION) ": "cfg80211_is_element_inherited(const struct element  elem,   const struct element  non_inherit_elem){u8 id_len, ext_id_len, i, loop_len, id;const u8  list;if (elem->id == WLAN_EID_MULTIPLE_BSSID)return false;if (elem->id == WLAN_EID_EXTENSION && elem->datalen > 1 &&    elem->data[0] == WLAN_EID_EXT_EHT_MULTI_LINK)return false;if (!non_inherit_elem || non_inherit_elem->datalen < 2)return true;    non inheritance element format is:   ext ID (56) | IDs list len | list | extension IDs list len | list   Both lists are optional. Both lengths are mandatory.   This means valid length is:   elem_len = 1 (extension ID) + 2 (list len fields) + list lengths ", "if (wdev->netdev)cfg80211_sme_scan_done(wdev->netdev);if (!request->info.aborted &&    request->flags & NL80211_SCAN_FLAG_FLUSH) ": "cfg80211_scan_done(struct cfg80211_registered_device  rdev,   bool send_message){struct cfg80211_scan_request  request,  rdev_req;struct wireless_dev  wdev;struct sk_buff  msg;#ifdef CONFIG_CFG80211_WEXTunion iwreq_data wrqu;#endiflockdep_assert_held(&rdev->wiphy.mtx);if (rdev->scan_msg) {nl80211_send_scan_msg(rdev, rdev->scan_msg);rdev->scan_msg = NULL;return;}rdev_req = rdev->scan_req;if (!rdev_req)return;wdev = rdev_req->wdev;request = rdev->int_scan_req ? rdev->int_scan_req : rdev_req;if (wdev_running(wdev) &&    (rdev->wiphy.flags & WIPHY_FLAG_SPLIT_SCAN_6GHZ) &&    !rdev_req->scan_6ghz && !request->info.aborted &&    !cfg80211_scan_6ghz(rdev))return;    This must be before sending the other events!   Otherwise, wpa_supplicant gets completely confused with   wext events. ", "spin_lock_bh(&rdev->bss_lock);__cfg80211_bss_expire(rdev, req->scan_start);spin_unlock_bh(&rdev->bss_lock);req->scan_start = jiffies;}nl80211_send_sched_scan(req,NL80211_CMD_SCHED_SCAN_RESULTS);}}wiphy_unlock(&rdev->wiphy);}void cfg80211_sched_scan_results(struct wiphy *wiphy, u64 reqid)": "cfg80211_sched_scan_results_wk(struct work_struct  work){struct cfg80211_registered_device  rdev;struct cfg80211_sched_scan_request  req,  tmp;rdev = container_of(work, struct cfg80211_registered_device,   sched_scan_res_wk);wiphy_lock(&rdev->wiphy);list_for_each_entry_safe(req, tmp, &rdev->sched_scan_req_list, list) {if (req->report_results) {req->report_results = false;if (req->flags & NL80211_SCAN_FLAG_FLUSH) {  flush entries from previous scans ", "if (!sub &&    cfg80211_is_element_inherited(parent, non_inherit_elem)) ": "cfg80211_find_elem_match(id, subie, subie_len,       &ext_id, match_len, 0);  Copy from parent if not in subie and inherited ", "if (time_after(now, bss->ts + IEEE80211_SCAN_RESULT_EXPIRE) &&    !atomic_read(&bss->hold))continue;if (is_bss(&bss->pub, bssid, ssid, ssid_len)) ": "cfg80211_get_bss(struct wiphy  wiphy,      struct ieee80211_channel  channel,      const u8  bssid,      const u8  ssid, size_t ssid_len,      enum ieee80211_bss_type bss_type,      enum ieee80211_privacy privacy){struct cfg80211_registered_device  rdev = wiphy_to_rdev(wiphy);struct cfg80211_internal_bss  bss,  res = NULL;unsigned long now = jiffies;int bss_privacy;trace_cfg80211_get_bss(wiphy, channel, bssid, ssid, ssid_len, bss_type,       privacy);spin_lock_bh(&rdev->bss_lock);list_for_each_entry(bss, &rdev->bss_list, list) {if (!cfg80211_bss_type_match(bss->pub.capability,     bss->pub.channel->band, bss_type))continue;bss_privacy = (bss->pub.capability & WLAN_CAPABILITY_PRIVACY);if ((privacy == IEEE80211_PRIVACY_ON && !bss_privacy) ||    (privacy == IEEE80211_PRIVACY_OFF && bss_privacy))continue;if (channel && bss->pub.channel != channel)continue;if (!is_valid_ether_addr(bss->pub.bssid))continue;  Don't get expired BSS structs ", "next = (void *)(elem->data + elem->datalen);elem_datalen = elem->datalen;if (elem->id == WLAN_EID_EXTENSION) ": "cfg80211_put_bss(wiphy, bss);}}out:kfree(new_ie);kfree(profile);}ssize_t cfg80211_defragment_element(const struct element  elem, const u8  ies,    size_t ieslen, u8  data, size_t data_len,    u8 frag_id){const struct element  next;ssize_t copied;u8 elem_datalen;if (!elem)return -EINVAL;  elem might be invalid after the memmove ", "if (ieee80211_is_s1g_beacon(mgmt->frame_control))return res;inform_data.ftype = ieee80211_is_beacon(mgmt->frame_control) ?CFG80211_BSS_FTYPE_BEACON : CFG80211_BSS_FTYPE_PRESP;memcpy(inform_data.bssid, mgmt->bssid, ETH_ALEN);inform_data.tsf = le64_to_cpu(mgmt->u.probe_resp.timestamp);inform_data.beacon_interval =le16_to_cpu(mgmt->u.probe_resp.beacon_int);/* process each non-transmitting bss ": "cfg80211_inform_bss_frame_data(struct wiphy  wiphy,       struct cfg80211_inform_bss  data,       struct ieee80211_mgmt  mgmt, size_t len,       gfp_t gfp){struct cfg80211_inform_single_bss_data inform_data = {.drv_data = data,.ie = mgmt->u.probe_resp.variable,.ielen = len - offsetof(struct ieee80211_mgmt,u.probe_resp.variable),};struct cfg80211_bss  res;res = cfg80211_inform_single_bss_frame_data(wiphy, data, mgmt,    len, gfp);if (!res)return NULL;  don't do any further MBSSIDML handling for S1G ", "if (!bss->pub.hidden_beacon_bss)return false;/* * if it's a probe response entry break its * link to the other entries in the group ": "cfg80211_unlink_bss(struct cfg80211_registered_device  rdev,  struct cfg80211_internal_bss  bss){lockdep_assert_held(&rdev->bss_lock);if (!list_empty(&bss->hidden_list)) {    don't remove the beacon entry if it has   probe responses associated with it ", "if (!band_rule_found)band_rule_found = freq_in_rule_band(fr, center_freq);bw_fits = cfg80211_does_bw_fit_range(fr, center_freq, bw);if (band_rule_found && bw_fits)return rr;}if (!band_rule_found)return ERR_PTR(-ERANGE);return ERR_PTR(-EINVAL);}static const struct ieee80211_reg_rule *__freq_reg_info(struct wiphy *wiphy, u32 center_freq, u32 min_bw)": "freq_reg_info_regd(u32 center_freq,   const struct ieee80211_regdomain  regd, u32 bw){int i;bool band_rule_found = false;bool bw_fits = false;if (!regd)return ERR_PTR(-EINVAL);for (i = 0; i < regd->n_reg_rules; i++) {const struct ieee80211_reg_rule  rr;const struct ieee80211_freq_range  fr = NULL;rr = &regd->reg_rules[i];fr = &rr->freq_range;    We only need to know if one frequency rule was   in center_freq's band, that's enough, so let's   not overwrite it once found ", "WARN_ON(!bands_set);new_regd = reg_copy_regd(regd);if (IS_ERR(new_regd))return;rtnl_lock();wiphy_lock(wiphy);tmp = get_wiphy_regdom(wiphy);rcu_assign_pointer(wiphy->regd, new_regd);rcu_free_regdom(tmp);wiphy_unlock(wiphy);rtnl_unlock();}EXPORT_SYMBOL(wiphy_apply_custom_regulatory": "wiphy_apply_custom_regulatory(struct wiphy  wiphy,   const struct ieee80211_regdomain  regd){const struct ieee80211_regdomain  new_regd,  tmp;enum nl80211_band band;unsigned int bands_set = 0;WARN(!(wiphy->regulatory_flags & REGULATORY_CUSTOM_REG),     \"wiphy should have REGULATORY_CUSTOM_REG\\n\");wiphy->regulatory_flags |= REGULATORY_CUSTOM_REG;for (band = 0; band < NUM_NL80211_BANDS; band++) {if (!wiphy->bands[band])continue;handle_band_custom(wiphy, wiphy->bands[band], regd);bands_set++;}    no point in calling this if it won't have any effect   on your device's supported bands. ", "int regulatory_hint_user(const char *alpha2, enum nl80211_user_reg_hint_type user_reg_hint_type)": "regulatory_hint_core(const char  alpha2){struct regulatory_request  request;request = kzalloc(sizeof(struct regulatory_request), GFP_KERNEL);if (!request)return -ENOMEM;request->alpha2[0] = alpha2[0];request->alpha2[1] = alpha2[1];request->initiator = NL80211_REGDOM_SET_BY_CORE;request->wiphy_idx = WIPHY_IDX_INVALID;queue_regulatory_request(request);return 0;}  User hints ", "reg_process_self_managed_hint(wiphy);reg_check_channels();return 0;}EXPORT_SYMBOL(regulatory_set_wiphy_regd_sync": "regulatory_set_wiphy_regd_sync(struct wiphy  wiphy,   struct ieee80211_regdomain  rd){int ret;ASSERT_RTNL();ret = __regulatory_set_wiphy_regd(wiphy, rd);if (ret)return ret;  process the request immediately ", "atomic_dec(&wiphy_counter);kfree(rdev);return NULL;}/* atomic_inc_return makes it start at 1, make it start at 0 ": "wiphy_new_nm(const struct cfg80211_ops  ops, int sizeof_priv,   const char  requested_name){static atomic_t wiphy_counter = ATOMIC_INIT(0);struct cfg80211_registered_device  rdev;int alloc_size;WARN_ON(ops->add_key && (!ops->del_key || !ops->set_default_key));WARN_ON(ops->auth && (!ops->assoc || !ops->deauth || !ops->disassoc));WARN_ON(ops->connect && !ops->disconnect);WARN_ON(ops->join_ibss && !ops->leave_ibss);WARN_ON(ops->add_virtual_intf && !ops->del_virtual_intf);WARN_ON(ops->add_station && !ops->del_station);WARN_ON(ops->add_mpath && !ops->del_mpath);WARN_ON(ops->join_mesh && !ops->leave_mesh);WARN_ON(ops->start_p2p_device && !ops->stop_p2p_device);WARN_ON(ops->start_ap && !ops->stop_ap);WARN_ON(ops->join_ocb && !ops->leave_ocb);WARN_ON(ops->suspend && !ops->resume);WARN_ON(ops->sched_scan_start && !ops->sched_scan_stop);WARN_ON(ops->remain_on_channel && !ops->cancel_remain_on_channel);WARN_ON(ops->tdls_channel_switch && !ops->tdls_cancel_channel_switch);WARN_ON(ops->add_tx_ts && !ops->del_tx_ts);alloc_size = sizeof( rdev) + sizeof_priv;rdev = kzalloc(alloc_size, GFP_KERNEL);if (!rdev)return NULL;rdev->ops = ops;rdev->wiphy_idx = atomic_inc_return(&wiphy_counter);if (unlikely(rdev->wiphy_idx < 0)) {  ugh, wrapped! ", "if (WARN_ON(wiphy->bss_select_support &&    (wiphy->bss_select_support & ~(BIT(__NL80211_BSS_SELECT_ATTR_AFTER_LAST) - 2))))return -EINVAL;if (WARN_ON(wiphy_ext_feature_isset(&rdev->wiphy,    NL80211_EXT_FEATURE_4WAY_HANDSHAKE_STA_1X) &&    (!rdev->ops->set_pmk || !rdev->ops->del_pmk)))return -EINVAL;if (WARN_ON(!(rdev->wiphy.flags & WIPHY_FLAG_SUPPORTS_FW_ROAM) &&    rdev->ops->update_connect_params))return -EINVAL;if (wiphy->addresses)memcpy(wiphy->perm_addr, wiphy->addresses[0].addr, ETH_ALEN);/* sanity check ifmodes ": "wiphy_register(struct wiphy  wiphy){struct cfg80211_registered_device  rdev = wiphy_to_rdev(wiphy);int res;enum nl80211_band band;struct ieee80211_supported_band  sband;bool have_band = false;int i;u16 ifmodes = wiphy->interface_modes;#ifdef CONFIG_PMif (WARN_ON(wiphy->wowlan &&    (wiphy->wowlan->flags & WIPHY_WOWLAN_GTK_REKEY_FAILURE) &&    !(wiphy->wowlan->flags & WIPHY_WOWLAN_SUPPORTS_GTK_REKEY)))return -EINVAL;if (WARN_ON(wiphy->wowlan &&    !wiphy->wowlan->flags && !wiphy->wowlan->n_patterns &&    !wiphy->wowlan->tcp))return -EINVAL;#endifif (WARN_ON((wiphy->features & NL80211_FEATURE_TDLS_CHANNEL_SWITCH) &&    (!rdev->ops->tdls_channel_switch ||     !rdev->ops->tdls_cancel_channel_switch)))return -EINVAL;if (WARN_ON((wiphy->interface_modes & BIT(NL80211_IFTYPE_NAN)) &&    (!rdev->ops->start_nan || !rdev->ops->stop_nan ||     !rdev->ops->add_nan_func || !rdev->ops->del_nan_func ||     !(wiphy->nan_supported_bands & BIT(NL80211_BAND_2GHZ)))))return -EINVAL;if (WARN_ON(wiphy->interface_modes & BIT(NL80211_IFTYPE_WDS)))return -EINVAL;if (WARN_ON(wiphy->pmsr_capa && !wiphy->pmsr_capa->ftm.supported))return -EINVAL;if (wiphy->pmsr_capa && wiphy->pmsr_capa->ftm.supported) {if (WARN_ON(!wiphy->pmsr_capa->ftm.asap &&    !wiphy->pmsr_capa->ftm.non_asap))return -EINVAL;if (WARN_ON(!wiphy->pmsr_capa->ftm.preambles ||    !wiphy->pmsr_capa->ftm.bandwidths))return -EINVAL;if (WARN_ON(wiphy->pmsr_capa->ftm.preambles &~(BIT(NL80211_PREAMBLE_LEGACY) |  BIT(NL80211_PREAMBLE_HT) |  BIT(NL80211_PREAMBLE_VHT) |  BIT(NL80211_PREAMBLE_HE) |  BIT(NL80211_PREAMBLE_DMG))))return -EINVAL;if (WARN_ON((wiphy->pmsr_capa->ftm.trigger_based ||     wiphy->pmsr_capa->ftm.non_trigger_based) &&    !(wiphy->pmsr_capa->ftm.preambles &      BIT(NL80211_PREAMBLE_HE))))return -EINVAL;if (WARN_ON(wiphy->pmsr_capa->ftm.bandwidths &~(BIT(NL80211_CHAN_WIDTH_20_NOHT) |  BIT(NL80211_CHAN_WIDTH_20) |  BIT(NL80211_CHAN_WIDTH_40) |  BIT(NL80211_CHAN_WIDTH_80) |  BIT(NL80211_CHAN_WIDTH_80P80) |  BIT(NL80211_CHAN_WIDTH_160) |  BIT(NL80211_CHAN_WIDTH_5) |  BIT(NL80211_CHAN_WIDTH_10))))return -EINVAL;}if (WARN_ON((wiphy->regulatory_flags & REGULATORY_WIPHY_SELF_MANAGED) &&    (wiphy->regulatory_flags &(REGULATORY_CUSTOM_REG | REGULATORY_STRICT_REG | REGULATORY_COUNTRY_IE_FOLLOW_POWER | REGULATORY_COUNTRY_IE_IGNORE))))return -EINVAL;if (WARN_ON(wiphy->coalesce &&    (!wiphy->coalesce->n_rules ||     !wiphy->coalesce->n_patterns) &&    (!wiphy->coalesce->pattern_min_len ||     wiphy->coalesce->pattern_min_len >wiphy->coalesce->pattern_max_len)))return -EINVAL;if (WARN_ON(wiphy->ap_sme_capa &&    !(wiphy->flags & WIPHY_FLAG_HAVE_AP_SME)))return -EINVAL;if (WARN_ON(wiphy->addresses && !wiphy->n_addresses))return -EINVAL;if (WARN_ON(wiphy->addresses &&    !is_zero_ether_addr(wiphy->perm_addr) &&    memcmp(wiphy->perm_addr, wiphy->addresses[0].addr,   ETH_ALEN)))return -EINVAL;if (WARN_ON(wiphy->max_acl_mac_addrs &&    (!(wiphy->flags & WIPHY_FLAG_HAVE_AP_SME) ||     !rdev->ops->set_mac_acl)))return -EINVAL;  assure only valid behaviours are flagged by driver   hence subtract 2 as bit 0 is invalid. ", "debugfs_remove_recursive(rdev->wiphy.debugfsdir);list_del_rcu(&rdev->list);synchronize_rcu();/* * If this device got a regulatory hint tell core its * free to listen now to a new shiny device regulatory hint ": "wiphy_unregister(&rdev->wiphy);return res;}return 0;}EXPORT_SYMBOL(wiphy_register);void wiphy_rfkill_start_polling(struct wiphy  wiphy){struct cfg80211_registered_device  rdev = wiphy_to_rdev(wiphy);if (!rdev->ops->rfkill_poll)return;rdev->rfkill_ops.poll = cfg80211_rfkill_poll;rfkill_resume_polling(wiphy->rfkill);}EXPORT_SYMBOL(wiphy_rfkill_start_polling);void cfg80211_process_wiphy_works(struct cfg80211_registered_device  rdev){unsigned int runaway_limit = 100;unsigned long flags;lockdep_assert_held(&rdev->wiphy.mtx);spin_lock_irqsave(&rdev->wiphy_work_lock, flags);while (!list_empty(&rdev->wiphy_work_list)) {struct wiphy_work  wk;wk = list_first_entry(&rdev->wiphy_work_list,      struct wiphy_work, entry);list_del_init(&wk->entry);spin_unlock_irqrestore(&rdev->wiphy_work_lock, flags);wk->func(&rdev->wiphy, wk);spin_lock_irqsave(&rdev->wiphy_work_lock, flags);if (WARN_ON(--runaway_limit == 0))INIT_LIST_HEAD(&rdev->wiphy_work_list);}spin_unlock_irqrestore(&rdev->wiphy_work_lock, flags);}void wiphy_unregister(struct wiphy  wiphy){struct cfg80211_registered_device  rdev = wiphy_to_rdev(wiphy);wait_event(rdev->dev_wait, ({int __count;wiphy_lock(&rdev->wiphy);__count = rdev->opencount;wiphy_unlock(&rdev->wiphy);__count == 0; }));if (rdev->wiphy.rfkill)rfkill_unregister(rdev->wiphy.rfkill);rtnl_lock();wiphy_lock(&rdev->wiphy);nl80211_notify_wiphy(rdev, NL80211_CMD_DEL_WIPHY);rdev->wiphy.registered = false;WARN_ON(!list_empty(&rdev->wiphy.wdev_list));    First remove the hardware from everywhere, this makes   it impossible to find from userspace. ", "rdev->wiphy.retry_short = 7;rdev->wiphy.retry_long = 4;rdev->wiphy.frag_threshold = (u32) -1;rdev->wiphy.rts_threshold = (u32) -1;rdev->wiphy.coverage_class = 0;rdev->wiphy.max_num_csa_counters = 1;rdev->wiphy.max_sched_scan_plans = 1;rdev->wiphy.max_sched_scan_plan_interval = U32_MAX;return &rdev->wiphy;}EXPORT_SYMBOL(wiphy_new_nm);static int wiphy_verify_combinations(struct wiphy *wiphy)": "wiphy_free(&rdev->wiphy);return NULL;}INIT_WORK(&rdev->wiphy_work, cfg80211_wiphy_work);INIT_LIST_HEAD(&rdev->wiphy_work_list);spin_lock_init(&rdev->wiphy_work_lock);INIT_WORK(&rdev->rfkill_block, cfg80211_rfkill_block_work);INIT_WORK(&rdev->conn_work, cfg80211_conn_work);INIT_WORK(&rdev->event_work, cfg80211_event_work);INIT_WORK(&rdev->background_cac_abort_wk,  cfg80211_background_cac_abort_wk);INIT_DELAYED_WORK(&rdev->background_cac_done_wk,  cfg80211_background_cac_done_wk);init_waitqueue_head(&rdev->dev_wait);    Initialize wiphy parameters to IEEE 802.11 MIB default values.   Fragmentation and RTS threshold are disabled by default with the   special -1 value. ", "cfg80211_process_wdev_events(wdev);if (wdev->iftype == NL80211_IFTYPE_STATION ||    wdev->iftype == NL80211_IFTYPE_P2P_CLIENT) ": "cfg80211_unregister_wdev(struct wireless_dev  wdev,      bool unregister_netdev){struct cfg80211_registered_device  rdev = wiphy_to_rdev(wdev->wiphy);unsigned int link_id;ASSERT_RTNL();lockdep_assert_held(&rdev->wiphy.mtx);nl80211_notify_iface(rdev, wdev, NL80211_CMD_DEL_INTERFACE);wdev->registered = false;if (wdev->netdev) {sysfs_remove_link(&wdev->netdev->dev.kobj, \"phy80211\");if (unregister_netdev)unregister_netdevice(wdev->netdev);}list_del_rcu(&wdev->list);synchronize_net();rdev->devlist_generation++;cfg80211_mlme_purge_registrations(wdev);switch (wdev->iftype) {case NL80211_IFTYPE_P2P_DEVICE:cfg80211_stop_p2p_device(rdev, wdev);break;case NL80211_IFTYPE_NAN:cfg80211_stop_nan(rdev, wdev);break;default:break;}#ifdef CONFIG_CFG80211_WEXTkfree_sensitive(wdev->wext.keys);wdev->wext.keys = NULL;#endifcfg80211_cqm_config_free(wdev);    Ensure that all events have been processed and   freed. ", "wdev->registered = true;wdev->registering = true;ret = register_netdevice(dev);if (ret)goto out;cfg80211_register_wdev(rdev, wdev);ret = 0;out:wdev->registering = false;if (ret)wdev->registered = false;return ret;}EXPORT_SYMBOL(cfg80211_register_netdevice": "cfg80211_register_netdevice(struct net_device  dev){struct wireless_dev  wdev = dev->ieee80211_ptr;struct cfg80211_registered_device  rdev;int ret;ASSERT_RTNL();if (WARN_ON(!wdev))return -EINVAL;rdev = wiphy_to_rdev(wdev->wiphy);lockdep_assert_held(&rdev->wiphy.mtx);  we'll take care of this ", "int event_len;/* Its size ": "wireless_send_event(struct net_device  dev, unsigned intcmd, union iwreq_data  wrqu, const char  extra){const struct iw_ioctl_description  descr = NULL;int extra_len = 0;struct iw_event   event;  Mallocated whole event ", "if (likely((stream + event_len) < ends)) ": "iwe_stream_add_point(struct iw_request_info  info, char  stream,   char  ends, struct iw_event  iwe, char  extra){int event_len = iwe_stream_point_len(info) + iwe->u.data.length;int point_len = iwe_stream_point_len(info);int lcp_len   = iwe_stream_lcp_len(info);  Check if it's possible ", "event_len -= IW_EV_LCP_LEN;/* Check if it's possible ": "iwe_stream_add_value(struct iw_request_info  info, char  event,   char  value, char  ends, struct iw_event  iwe,   int event_len){int lcp_len = iwe_stream_lcp_len(info);  Don't duplicate LCP ", "BUILD_BUG_ON(NL80211_STA_FLAG_MAX != 7);switch (statype) ": "cfg80211_check_station_change(struct wiphy  wiphy,  struct station_parameters  params,  enum cfg80211_station_type statype){if (params->listen_interval != -1 &&    statype != CFG80211_STA_AP_CLIENT_UNASSOC)return -EINVAL;if (params->support_p2p_ps != -1 &&    statype != CFG80211_STA_AP_CLIENT_UNASSOC)return -EINVAL;if (params->aid &&    !(params->sta_flags_set & BIT(NL80211_STA_FLAG_TDLS_PEER)) &&    statype != CFG80211_STA_AP_CLIENT_UNASSOC)return -EINVAL;  When you run into this, adjust the code below for the new flag ", "memset(skb->cb, 0, sizeof(skb->cb));nla_nest_end(skb, data);genlmsg_end(skb, hdr);if (nlhdr->nlmsg_pid) ": "__cfg80211_send_event_skb(struct sk_buff  skb, gfp_t gfp){struct cfg80211_registered_device  rdev = ((void   )skb->cb)[0];void  hdr = ((void   )skb->cb)[1];struct nlmsghdr  nlhdr = nlmsg_hdr(skb);struct nlattr  data = ((void   )skb->cb)[2];enum nl80211_multicast_groups mcgrp = NL80211_MCGRP_TESTMODE;  clear CB data for netlink core to own from now on ", "if (chan <= 0)return 0; /* not supported ": "ieee80211_channel_to_freq_khz(int chan, enum nl80211_band band){  see 802.11 17.3.8.3.2 and Annex J   there are overlapping channel numbers in 5GHz and 2GHz bands ", "if (chan->flags & IEEE80211_CHAN_1MHZ)return NL80211_CHAN_WIDTH_1;else if (chan->flags & IEEE80211_CHAN_2MHZ)return NL80211_CHAN_WIDTH_2;else if (chan->flags & IEEE80211_CHAN_4MHZ)return NL80211_CHAN_WIDTH_4;else if (chan->flags & IEEE80211_CHAN_8MHZ)return NL80211_CHAN_WIDTH_8;else if (chan->flags & IEEE80211_CHAN_16MHZ)return NL80211_CHAN_WIDTH_16;pr_err(\"unknown channel width for channel at %dKHz?\\n\",       ieee80211_channel_to_khz(chan));return NL80211_CHAN_WIDTH_1;}EXPORT_SYMBOL(ieee80211_s1g_channel_width": "ieee80211_s1g_channel_width(const struct ieee80211_channel  chan){if (WARN_ON(!chan || chan->band != NL80211_BAND_S1GHZ))return NL80211_CHAN_WIDTH_20_NOHT; S1G defines a single allowed channel width per channel.   Extract that width here. ", "freq = KHZ_TO_MHZ(freq);/* see 802.11 17.3.8.3.2 and Annex J ": "ieee80211_freq_khz_to_channel(u32 freq){  TODO: just handle MHz for now ", "if ((fc & cpu_to_le16(0x00E0)) == cpu_to_le16(0x00C0))hdrlen = 10;elsehdrlen = 16;}out:return hdrlen;}EXPORT_SYMBOL(ieee80211_hdrlen": "ieee80211_hdrlen(__le16 fc){unsigned int hdrlen = 24;if (ieee80211_is_ext(fc)) {hdrlen = 4;goto out;}if (ieee80211_is_data(fc)) {if (ieee80211_has_a4(fc))hdrlen = 30;if (ieee80211_is_data_qos(fc)) {hdrlen += IEEE80211_QOS_CTL_LEN;if (ieee80211_has_order(fc))hdrlen += IEEE80211_HT_CTL_LEN;}goto out;}if (ieee80211_is_mgmt(fc)) {if (ieee80211_has_order(fc))hdrlen += IEEE80211_HT_CTL_LEN;goto out;}if (ieee80211_is_ctl(fc)) {    ACK and CTS are 10 bytes, all others 16. To see how   to get this condition consider     subtype mask:   0b0000000011110000 (0x00F0)     ACK subtype:    0b0000000011010000 (0x00D0)     CTS subtype:    0b0000000011000000 (0x00C0)     bits that matter:         ^^^      (0x00E0)     value of those: 0b0000000011000000 (0x00C0) ", "switch (ae) ": "ieee80211_get_mesh_hdrlen(u8 flags){int ae = flags & MESH_FLAGS_AE;  802.11-2012, 8.2.4.7.3 ", "memcpy(tmp.h_dest, ieee80211_get_DA(hdr), ETH_ALEN);memcpy(tmp.h_source, ieee80211_get_SA(hdr), ETH_ALEN);switch (hdr->frame_control &cpu_to_le16(IEEE80211_FCTL_TODS | IEEE80211_FCTL_FROMDS)) ": "bridge_tunnel_header))return false; proto =  hdr_proto;return true;}EXPORT_SYMBOL(ieee80211_get_8023_tunnel_proto);int ieee80211_strip_8023_mesh_hdr(struct sk_buff  skb){const void  mesh_addr;struct {struct ethhdr eth;u8 flags;} payload;int hdrlen;int ret;ret = skb_copy_bits(skb, 0, &payload, sizeof(payload));if (ret)return ret;hdrlen = sizeof(payload.eth) + __ieee80211_get_mesh_hdrlen(payload.flags);if (likely(pskb_may_pull(skb, hdrlen + 8) &&   ieee80211_get_8023_tunnel_proto(skb->data + hdrlen,   &payload.eth.h_proto)))hdrlen += ETH_ALEN + 2;else if (!pskb_may_pull(skb, hdrlen))return -EINVAL;elsepayload.eth.h_proto = htons(skb->len - hdrlen);mesh_addr = skb->data + sizeof(payload.eth) + ETH_ALEN;switch (payload.flags & MESH_FLAGS_AE) {case MESH_FLAGS_AE_A4:memcpy(&payload.eth.h_source, mesh_addr, ETH_ALEN);break;case MESH_FLAGS_AE_A5_A6:memcpy(&payload.eth, mesh_addr, 2   ETH_ALEN);break;default:break;}pskb_pull(skb, hdrlen - sizeof(payload.eth));memcpy(skb->data, &payload.eth, sizeof(payload.eth));return 0;}EXPORT_SYMBOL(ieee80211_strip_8023_mesh_hdr);int ieee80211_data_to_8023_exthdr(struct sk_buff  skb, struct ethhdr  ehdr,  const u8  addr, enum nl80211_iftype iftype,  u8 data_offset, bool is_amsdu){struct ieee80211_hdr  hdr = (struct ieee80211_hdr  ) skb->data;struct {u8 hdr[ETH_ALEN] __aligned(2);__be16 proto;} payload;struct ethhdr tmp;u16 hdrlen;if (unlikely(!ieee80211_is_data_present(hdr->frame_control)))return -1;hdrlen = ieee80211_hdrlen(hdr->frame_control) + data_offset;if (skb->len < hdrlen)return -1;  convert IEEE 802.11 header + possible LLC headers into Ethernet   header   IEEE 802.11 address fields:   ToDS FromDS Addr1 Addr2 Addr3 Addr4     0     0   DA    SA    BSSID na     0     1   DA    BSSID SA    na     1     0   BSSID SA    DA    na     1     1   RA    TA    DA    SA ", "remaining = skb->len - offset;if (subframe_len > remaining)goto purge;/* mitigate A-MSDU aggregation injection attacks ": "ieee80211_amsdu_to_8023s(struct sk_buff  skb, struct sk_buff_head  list,      const u8  addr, enum nl80211_iftype iftype,      const unsigned int extra_headroom,      const u8  check_da, const u8  check_sa,      u8 mesh_control){unsigned int hlen = ALIGN(extra_headroom, 4);struct sk_buff  frame = NULL;int offset = 0, remaining;struct {struct ethhdr eth;uint8_t flags;} hdr;bool reuse_frag = skb->head_frag && !skb_has_frag_list(skb);bool reuse_skb = false;bool last = false;int copy_len = sizeof(hdr.eth);if (iftype == NL80211_IFTYPE_MESH_POINT)copy_len = sizeof(hdr);while (!last) {unsigned int subframe_len;int len, mesh_len = 0;u8 padding;skb_copy_bits(skb, offset, &hdr, copy_len);if (iftype == NL80211_IFTYPE_MESH_POINT)mesh_len = __ieee80211_get_mesh_hdrlen(hdr.flags);len = ieee80211_amsdu_subframe_length(&hdr.eth.h_proto, hdr.flags,      mesh_control);subframe_len = sizeof(struct ethhdr) + len;padding = (4 - subframe_len) & 0x3;  the last MSDU has no padding ", "if (skb->priority >= 256 && skb->priority <= 263) ": "cfg80211_classify8021d(struct sk_buff  skb,    struct cfg80211_qos_map  qos_map){unsigned int dscp;unsigned char vlan_priority;unsigned int ret;  skb->priority values from 256->263 are magic values to   directly indicate a specific 802.1d priority.  This is used   to allow 802.1d priority to be passed directly in from VLAN   tags, etc. ", "if (WARN_ON_ONCE(rate->mcs >= 32))return 0;modulation = rate->mcs & 7;streams = (rate->mcs >> 3) + 1;bitrate = (rate->bw == RATE_INFO_BW_40) ? 13500000 : 6500000;if (modulation < 4)bitrate *= (modulation + 1);else if (modulation == 4)bitrate *= (modulation + 2);elsebitrate *= (modulation + 3);bitrate *= streams;if (rate->flags & RATE_INFO_FLAGS_SHORT_GI)bitrate = (bitrate / 9) * 10;/* do NOT round down here ": "cfg80211_calculate_bitrate_ht(struct rate_info  rate){int modulation, streams, bitrate;  the formula below does only work for MCS values smaller than 32 ", "if (iedata[0] != 0x50 || iedata[1] != 0x6f ||    iedata[2] != 0x9a || iedata[3] != 0x09)goto cont;iedatalen -= 4;iedata += 4;/* check attribute continuation into this IE ": "cfg80211_get_p2p_attr(const u8  ies, unsigned int len,  enum ieee80211_p2p_attr_id attr,  u8  buf, unsigned int bufsize){u8  out = buf;u16 attr_remaining = 0;bool desired_attr = false;u16 desired_len = 0;while (len > 0) {unsigned int iedatalen;unsigned int copy;const u8  iedata;if (len < 2)return -EILSEQ;iedatalen = ies[1];if (iedatalen + 2 > len)return -EILSEQ;if (ies[0] != WLAN_EID_VENDOR_SPECIFIC)goto cont;if (iedatalen < 4)goto cont;iedata = ies + 2;  check WFA OUI, P2P subtype ", "if (chandef->width == NL80211_CHAN_WIDTH_40) ": "ieee80211_chandef_to_operating_class(struct cfg80211_chan_def  chandef,  u8  op_class){u8 vht_opclass;u32 freq = chandef->center_freq1;if (freq >= 2412 && freq <= 2472) {if (chandef->width > NL80211_CHAN_WIDTH_40)return false;  2.407 GHz, channels 1..13 ", "if (beacon_int < 10 || beacon_int > 10000)return -EINVAL;return 0;}int cfg80211_iter_combinations(struct wiphy *wiphy,       struct iface_combination_params *params,       void (*iter)(const struct ieee80211_iface_combination *c,    void *data),       void *data)": "cfg80211_check_combinations(), in which case we'll validate more   through the cfg80211_calculate_bi_data() call and code in   cfg80211_iter_combinations(). ", "skb = dev_alloc_skb(sizeof(*msg));if (!skb)return;msg = skb_put(skb, sizeof(*msg));/* 802.2 Type 1 Logical Link Control (LLC) Exchange Identifier (XID) * Update response frame; IEEE Std 802.2-1998, 5.4.1.2.1 ": "cfg80211_send_layer2_update(struct net_device  dev, const u8  addr){struct iapp_layer2_update  msg;struct sk_buff  skb;  Send Level 2 Update Frame to update forwarding tables in layer 2   bridge devices ", "for (i = 7; i >= 0; i--) ": "ieee80211_get_vht_max_nss(struct ieee80211_vht_cap  cap,      enum ieee80211_vht_chanwidth bw,      int mcs, bool ext_nss_bw_capable,      unsigned int max_vht_nss){u16 map = le16_to_cpu(cap->supp_mcs.rx_mcs_map);int ext_nss_bw;int supp_width;int i, mcs_encoding;if (map == 0xffff)return 0;if (WARN_ON(mcs > 9 || max_vht_nss > 8))return 0;if (mcs <= 7)mcs_encoding = 0;else if (mcs == 8)mcs_encoding = 1;elsemcs_encoding = 2;if (!max_vht_nss) {  find max_vht_nss for the given MCS ", "if ((all_iftypes & used_iftypes) != used_iftypes)goto cont;if (beacon_int_gcd) ": "cfg80211_iftype_allowed(wiphy, iftype, 0, 1))used_iftypes |= BIT(iftype);}for (i = 0; i < wiphy->n_iface_combinations; i++) {const struct ieee80211_iface_combination  c;struct ieee80211_iface_limit  limits;u32 all_iftypes = 0;c = &wiphy->iface_combinations[i];if (num_interfaces > c->max_interfaces)continue;if (params->num_different_channels > c->num_different_channels)continue;limits = kmemdup(c->limits, sizeof(limits[0])   c->n_limits, GFP_KERNEL);if (!limits)return -ENOMEM;for (iftype = 0; iftype < NUM_NL80211_IFTYPES; iftype++) {if (cfg80211_iftype_allowed(wiphy, iftype, 0, 1))continue;for (j = 0; j < c->n_limits; j++) {all_iftypes |= limits[j].types;if (!(limits[j].types & BIT(iftype)))continue;if (limits[j].max < params->iftype_num[iftype])goto cont;limits[j].max -= params->iftype_num[iftype];}}if (params->radar_detect !=(c->radar_detect_widths & params->radar_detect))goto cont;if (params->radar_detect && c->radar_detect_regions &&    !(c->radar_detect_regions & BIT(region)))goto cont;  Finally check that all iftypes that we're currently   using are actually part of this combination. If they   aren't then we can't use this combination and have   to continue to the next. ", "void cfg80211_roamed(struct net_device *dev, struct cfg80211_roam_info *info,     gfp_t gfp)": "cfg80211_roamed(struct wireless_dev  wdev,       struct cfg80211_roam_info  info){#ifdef CONFIG_CFG80211_WEXTunion iwreq_data wrqu;#endifunsigned int link;const u8  connected_addr;ASSERT_WDEV_LOCK(wdev);if (WARN_ON(wdev->iftype != NL80211_IFTYPE_STATION &&    wdev->iftype != NL80211_IFTYPE_P2P_CLIENT))goto out;if (WARN_ON(!wdev->connected))goto out;if (info->valid_links) {if (WARN_ON(!info->ap_mld_addr))goto out;for_each_valid_link(info, link) {if (WARN_ON(!info->links[link].addr))goto out;}}cfg80211_wdev_release_bsses(wdev);for_each_valid_link(info, link) {if (WARN_ON(!info->links[link].bss))goto out;}memset(wdev->links, 0, sizeof(wdev->links));wdev->valid_links = info->valid_links;for_each_valid_link(info, link) {cfg80211_hold_bss(bss_from_pub(info->links[link].bss));wdev->links[link].client.current_bss =bss_from_pub(info->links[link].bss);}connected_addr = info->valid_links ? info->ap_mld_addr : info->links[0].bss->bssid;ether_addr_copy(wdev->u.client.connected_addr, connected_addr);if (info->valid_links) {for_each_valid_link(info, link)memcpy(wdev->links[link].addr, info->links[link].addr,       ETH_ALEN);}wdev->unprot_beacon_reported = 0;nl80211_send_roamed(wiphy_to_rdev(wdev->wiphy),    wdev->netdev, info, GFP_KERNEL);#ifdef CONFIG_CFG80211_WEXTif (!info->valid_links) {if (info->req_ie) {memset(&wrqu, 0, sizeof(wrqu));wrqu.data.length = info->req_ie_len;wireless_send_event(wdev->netdev, IWEVASSOCREQIE,    &wrqu, info->req_ie);}if (info->resp_ie) {memset(&wrqu, 0, sizeof(wrqu));wrqu.data.length = info->resp_ie_len;wireless_send_event(wdev->netdev, IWEVASSOCRESPIE,    &wrqu, info->resp_ie);}memset(&wrqu, 0, sizeof(wrqu));wrqu.ap_addr.sa_family = ARPHRD_ETHER;memcpy(wrqu.ap_addr.sa_data, connected_addr, ETH_ALEN);memcpy(wdev->wext.prev_bssid, connected_addr, ETH_ALEN);wdev->wext.prev_bssid_valid = true;wireless_send_event(wdev->netdev, SIOCGIWAP, &wrqu, NULL);}#endifreturn;out:for_each_valid_link(info, link)cfg80211_put_bss(wdev->wiphy, info->links[link].bss);}  Consumes info->links.bss object(s) one way or another ", "spin_lock_irqsave(&wdev->event_lock, flags);list_add_tail(&ev->list, &wdev->event_list);spin_unlock_irqrestore(&wdev->event_lock, flags);queue_work(cfg80211_wq, &rdev->event_work);}EXPORT_SYMBOL(cfg80211_port_authorized": "cfg80211_port_authorized(struct wireless_dev  wdev, const u8  bssid,const u8  td_bitmap, u8 td_bitmap_len){ASSERT_WDEV_LOCK(wdev);if (WARN_ON(wdev->iftype != NL80211_IFTYPE_STATION &&    wdev->iftype != NL80211_IFTYPE_P2P_CLIENT))return;if (WARN_ON(!wdev->connected) ||    WARN_ON(!ether_addr_equal(wdev->u.client.connected_addr, bssid)))return;nl80211_send_port_authorized(wiphy_to_rdev(wdev->wiphy), wdev->netdev,     bssid, td_bitmap, td_bitmap_len);}void cfg80211_port_authorized(struct net_device  dev, const u8  bssid,      const u8  td_bitmap, u8 td_bitmap_len, gfp_t gfp){struct wireless_dev  wdev = dev->ieee80211_ptr;struct cfg80211_registered_device  rdev = wiphy_to_rdev(wdev->wiphy);struct cfg80211_event  ev;unsigned long flags;if (WARN_ON(!bssid))return;ev = kzalloc(sizeof( ev) + td_bitmap_len, gfp);if (!ev)return;ev->type = EVENT_PORT_AUTHORIZED;memcpy(ev->pa.bssid, bssid, ETH_ALEN);ev->pa.td_bitmap = ((u8  )ev) + sizeof( ev);ev->pa.td_bitmap_len = td_bitmap_len;memcpy((void  )ev->pa.td_bitmap, td_bitmap, td_bitmap_len);    Use the wdev event list so that if there are pending   connectedroamed events, they will be reported first. ", "if (rdev->ops->crit_proto_stop && rdev->crit_proto_nlportid) ": "cfg80211_disconnected(struct net_device  dev, const u8  ie,     size_t ie_len, u16 reason, bool from_ap){struct wireless_dev  wdev = dev->ieee80211_ptr;struct cfg80211_registered_device  rdev = wiphy_to_rdev(wdev->wiphy);int i;#ifdef CONFIG_CFG80211_WEXTunion iwreq_data wrqu;#endifASSERT_WDEV_LOCK(wdev);if (WARN_ON(wdev->iftype != NL80211_IFTYPE_STATION &&    wdev->iftype != NL80211_IFTYPE_P2P_CLIENT))return;cfg80211_wdev_release_bsses(wdev);wdev->connected = false;wdev->u.client.ssid_len = 0;wdev->conn_owner_nlportid = 0;kfree_sensitive(wdev->connect_keys);wdev->connect_keys = NULL;nl80211_send_disconnected(rdev, dev, reason, ie, ie_len, from_ap);  stop critical protocol if supported ", "int ieee80211_radiotap_iterator_init(struct ieee80211_radiotap_iterator *iterator,struct ieee80211_radiotap_header *radiotap_header,int max_length, const struct ieee80211_radiotap_vendor_namespaces *vns)": "ieee80211_radiotap_iterator_next() to visit every radiotap   argument which is present in the header.  It knows about extended   present headers and handles them.     How to use:   call __ieee80211_radiotap_iterator_init() to init a semi-opaque iterator   struct ieee80211_radiotap_iterator (no need to init the struct beforehand)   checking for a good 0 return code.  Then loop calling   __ieee80211_radiotap_iterator_next()... it returns either 0,   -ENOENT if there are no more args to parse, or -EINVAL if there is a problem.   The iterator's @this_arg member points to the start of the argument   associated with the current argument index that is present, which can be   found in the iterator's @this_arg_index member.  This arg index corresponds   to the IEEE80211_RADIOTAP_... defines.     Radiotap header length:   You can find the CPU-endian total radiotap header length in   iterator->max_length after executing ieee80211_radiotap_iterator_init()   successfully.     Alignment Gotcha:   You must take care when dereferencing iterator.this_arg   for multibyte types... the pointer is not aligned.  Use   get_unaligned((type  )iterator.this_arg) to dereference   iterator.this_arg for type \"type\" safely on all arches.     Example code:   See Documentationnetworkingradiotap-headers.rst ", "synchronize_rcu();if (adap_layer->ctrlcmd != NULL)adap_layer->ctrlcmd(adap_layer, CAIF_CTRLCMD_DEINIT_RSP, 0);return 0;}EXPORT_SYMBOL(caif_disconnect_client": "caif_disconnect_client(struct net  net, struct cflayer  adap_layer){u8 channel_id;struct cfcnfg  cfg = get_cfcnfg(net);caif_assert(adap_layer != NULL);cfctrl_cancel_req(cfg->ctrl, adap_layer);channel_id = adap_layer->id;if (channel_id != 0) {struct cflayer  servl;servl = cfmuxl_remove_uplayer(cfg->mux, channel_id);cfctrl_linkdown_req(cfg->ctrl, channel_id, adap_layer);if (servl != NULL)layer_set_up(servl, NULL);} elsepr_debug(\"nothing to disconnect\\n\");  Do RCU sync before initiating cleanup ", "cfctrl_enum_req(cfg->ctrl, param.phyid);return cfctrl_linkup_request(cfg->ctrl, &param, adap_layer);unlock:rcu_read_unlock();return err;}EXPORT_SYMBOL(caif_connect_client": "caif_connect_client(struct net  net, struct caif_connect_request  conn_req,struct cflayer  adap_layer, int  ifindex,int  proto_head, int  proto_tail){struct cflayer  frml;struct cfcnfg_phyinfo  phy;int err;struct cfctrl_link_param param;struct cfcnfg  cfg = get_cfcnfg(net);rcu_read_lock();err = caif_connect_req_to_link_param(cfg, conn_req, &param);if (err)goto unlock;phy = cfcnfg_get_phyinfo_rcu(cfg, param.phyid);if (!phy) {err = -ENODEV;goto unlock;}err = -EINVAL;if (adap_layer == NULL) {pr_err(\"adap_layer is zero\\n\");goto unlock;}if (adap_layer->receive == NULL) {pr_err(\"adap_layer->receive is NULL\\n\");goto unlock;}if (adap_layer->ctrlcmd == NULL) {pr_err(\"adap_layer->ctrlcmd == NULL\\n\");goto unlock;}err = -ENODEV;frml = phy->frm_layer;if (frml == NULL) {pr_err(\"Specified PHY type does not exist!\\n\");goto unlock;}caif_assert(param.phyid == phy->id);caif_assert(phy->frm_layer->id ==     param.phyid);caif_assert(phy->phy_layer->id ==     param.phyid); ifindex = phy->ifindex; proto_tail = 2; proto_head = protohead[param.linktype] + phy->head_room;rcu_read_unlock();  FIXME: ENUMERATE INITIALLY WHEN ACTIVATING PHYSICAL INTERFACE ", "for (i = 0; i < 7; i++) ": "cfcnfg_add_phy_layer(struct cfcnfg  cnfg,     struct net_device  dev, struct cflayer  phy_layer,     enum cfcnfg_phy_preference pref,     struct cflayer  link_support,     bool fcs, int head_room){struct cflayer  frml;struct cfcnfg_phyinfo  phyinfo = NULL;int i, res = 0;u8 phyid;mutex_lock(&cnfg->lock);  CAIF protocol allow maximum 6 link-layers ", "if (cffrml_refcnt_read(phyinfo->frm_layer) != 0) ": "cfcnfg_del_phy_layer(struct cfcnfg  cnfg, struct cflayer  phy_layer){struct cflayer  frml,  frml_dn;u16 phyid;struct cfcnfg_phyinfo  phyinfo;might_sleep();mutex_lock(&cnfg->lock);phyid = phy_layer->id;phyinfo = cfcnfg_get_phyinfo_rcu(cnfg, phyid);if (phyinfo == NULL) {mutex_unlock(&cnfg->lock);return 0;}caif_assert(phyid == phyinfo->id);caif_assert(phy_layer == phyinfo->phy_layer);caif_assert(phy_layer->id == phyid);caif_assert(phyinfo->frm_layer->id == phyid);list_del_rcu(&phyinfo->node);synchronize_rcu();  Fail if reference count is not zero ", "if (unlikely(skb_tailroom(skb) < len)) ": "cfpkt_add_head(pkt, data, len);}int cfpkt_extr_head(struct cfpkt  pkt, void  data, u16 len){struct sk_buff  skb = pkt_to_skb(pkt);u8  from;if (unlikely(is_erronous(pkt)))return -EPROTO;if (unlikely(len > skb->len)) {PKT_ERROR(pkt, \"read beyond end of packet\\n\");return -EPROTO;}if (unlikely(len > skb_headlen(skb))) {if (unlikely(skb_linearize(skb) != 0)) {PKT_ERROR(pkt, \"linearize failed\\n\");return -EPROTO;}}from = skb_pull(skb, len);from -= len;if (data)memcpy(data, from, len);return 0;}EXPORT_SYMBOL(cfpkt_extr_head);int cfpkt_extr_trail(struct cfpkt  pkt, void  dta, u16 len){struct sk_buff  skb = pkt_to_skb(pkt);u8  data = dta;u8  from;if (unlikely(is_erronous(pkt)))return -EPROTO;if (unlikely(skb_linearize(skb) != 0)) {PKT_ERROR(pkt, \"linearize failed\\n\");return -EPROTO;}if (unlikely(skb->data + len > skb_tail_pointer(skb))) {PKT_ERROR(pkt, \"read beyond end of packet\\n\");return -EPROTO;}from = skb_tail_pointer(skb) - len;skb_trim(skb, skb->len - len);memcpy(data, from, len);return 0;}int cfpkt_pad_trail(struct cfpkt  pkt, u16 len){return cfpkt_add_body(pkt, NULL, len);}int cfpkt_add_body(struct cfpkt  pkt, const void  data, u16 len){struct sk_buff  skb = pkt_to_skb(pkt);struct sk_buff  lastskb;u8  to;u16 addlen = 0;if (unlikely(is_erronous(pkt)))return -EPROTO;lastskb = skb;  Check whether we need to add space at the tail ", "void xdr_truncate_encode(struct xdr_stream *xdr, size_t len)": "xdr_truncate_encode - truncate an encode buffer   @xdr: pointer to xdr_stream   @len: new length of buffer     Truncates the xdr stream, so that xdr->buf->len == len,   and xdr->p points at offset len from the start of the buffer, and   head, tail, and page lengths are adjusted to correspond.     If this means moving xdr->p to a different buffer, we assume that   the end pointer should be set to the end of the current page,   except in the case of the head buffer when we assume the head   buffer's current length represents the end of the available buffer.     This is  not  safe to use on a buffer that already has inlined page   cache pages (as in a zero-copy server read reply), except for the   simple case of truncating from one position in the tail to another.   ", "int xdr_restrict_buflen(struct xdr_stream *xdr, int newbuflen)": "xdr_restrict_buflen - decrease available buffer space   @xdr: pointer to xdr_stream   @newbuflen: new maximum number of bytes available     Adjust our idea of how much space is available in the buffer.   If we've already used too much space in the buffer, returns -1.   If the available space is already smaller than newbuflen, returns 0   and does nothing.  Otherwise, adjusts xdr->buf->buflen to newbuflen   and ensures xdr->end is set at most offset newbuflen from the start   of the buffer. ", "int gss_mech_flavor2info(rpc_authflavor_t pseudoflavor, struct rpcsec_gss_info *info)": "gss_mech_put(gm);return pseudoflavor;}     gss_mech_flavor2info - look up a GSS tuple for a given pseudoflavor   @pseudoflavor: GSS pseudoflavor to match   @info: rpcsec_gss_info structure to fill in     Returns zero and fills in \"info\" if pseudoflavor matches a   supported mechanism.  Otherwise a negative errno is returned. ", "pool->cached_need_wakeup = XDP_WAKEUP_TX;dev_hold(netdev);if (force_copy)/* For copy-mode, we are done. ": "xp_alloc_tx_descs(struct xsk_buff_pool  pool, struct xdp_sock  xs){pool->tx_descs = kvcalloc(xs->tx->nentries, sizeof( pool->tx_descs),  GFP_KERNEL);if (!pool->tx_descs)return -ENOMEM;return 0;}struct xsk_buff_pool  xp_create_and_assign_umem(struct xdp_sock  xs,struct xdp_umem  umem){bool unaligned = umem->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;struct xsk_buff_pool  pool;struct xdp_buff_xsk  xskb;u32 i, entries;entries = unaligned ? umem->chunks : 0;pool = kvzalloc(struct_size(pool, free_heads, entries),GFP_KERNEL);if (!pool)goto out;pool->heads = kvcalloc(umem->chunks, sizeof( pool->heads), GFP_KERNEL);if (!pool->heads)goto out;if (xs->tx)if (xp_alloc_tx_descs(pool, xs))goto out;pool->chunk_mask = ~((u64)umem->chunk_size - 1);pool->addrs_cnt = umem->size;pool->heads_cnt = umem->chunks;pool->free_heads_cnt = umem->chunks;pool->headroom = umem->headroom;pool->chunk_size = umem->chunk_size;pool->chunk_shift = ffs(umem->chunk_size) - 1;pool->unaligned = unaligned;pool->frame_len = umem->chunk_size - umem->headroom -XDP_PACKET_HEADROOM;pool->umem = umem;pool->addrs = umem->addrs;INIT_LIST_HEAD(&pool->free_list);INIT_LIST_HEAD(&pool->xsk_tx_list);spin_lock_init(&pool->xsk_tx_list_lock);spin_lock_init(&pool->cq_lock);refcount_set(&pool->users, 1);pool->fq = xs->fq_tmp;pool->cq = xs->cq_tmp;for (i = 0; i < pool->free_heads_cnt; i++) {xskb = &pool->heads[i];xskb->pool = pool;xskb->xdp.frame_sz = umem->chunk_size - umem->headroom;INIT_LIST_HEAD(&xskb->free_list_node);if (pool->unaligned)pool->free_heads[i] = xskb;elsexp_init_xskb_addr(xskb, pool, i   pool->chunk_size);}return pool;out:xp_destroy(pool);return NULL;}void xp_set_rxq_info(struct xsk_buff_pool  pool, struct xdp_rxq_info  rxq){u32 i;for (i = 0; i < pool->heads_cnt; i++)pool->heads[i].xdp.rxq = rxq;}EXPORT_SYMBOL(xp_set_rxq_info);static void xp_disable_drv_zc(struct xsk_buff_pool  pool){struct netdev_bpf bpf;int err;ASSERT_RTNL();if (pool->umem->zc) {bpf.command = XDP_SETUP_XSK_POOL;bpf.xsk.pool = NULL;bpf.xsk.queue_id = pool->queue_id;err = pool->netdev->netdev_ops->ndo_bpf(pool->netdev, &bpf);if (err)WARN(1, \"Failed to disable zero-copy!\\n\");}}#define NETDEV_XDP_ACT_ZC(NETDEV_XDP_ACT_BASIC |\\ NETDEV_XDP_ACT_REDIRECT |\\ NETDEV_XDP_ACT_XSK_ZEROCOPY)int xp_assign_dev(struct xsk_buff_pool  pool,  struct net_device  netdev, u16 queue_id, u16 flags){bool force_zc, force_copy;struct netdev_bpf bpf;int err = 0;ASSERT_RTNL();force_zc = flags & XDP_ZEROCOPY;force_copy = flags & XDP_COPY;if (force_zc && force_copy)return -EINVAL;if (xsk_get_pool_from_qid(netdev, queue_id))return -EBUSY;pool->netdev = netdev;pool->queue_id = queue_id;err = xsk_reg_pool_at_qid(netdev, pool, queue_id);if (err)return err;if (flags & XDP_USE_NEED_WAKEUP)pool->uses_need_wakeup = true;  Tx needs to be explicitly woken up the first time.  Also   for supporting drivers that do not implement this   feature. They will always have to call sendto() or poll(). ", "buff = xp_alloc(pool);if (buff)*xdp = buff;return !!buff;}if (unlikely(pool->free_list_cnt)) ": "xp_alloc_batch(struct xsk_buff_pool  pool, struct xdp_buff   xdp, u32 max){u32 nb_entries1 = 0, nb_entries2;if (unlikely(pool->dma_need_sync)) {struct xdp_buff  buff;  Slow path ", "if (xskq_prod_reserve_addr(pool->cq, desc->addr))goto out;xskq_cons_release(xs->tx);rcu_read_unlock();return true;}out:rcu_read_unlock();return false;}EXPORT_SYMBOL(xsk_tx_peek_desc": "xsk_tx_peek_desc(struct xsk_buff_pool  pool, struct xdp_desc  desc){struct xdp_sock  xs;rcu_read_lock();list_for_each_entry_rcu(xs, &pool->xsk_tx_list, tx_list) {if (!xskq_cons_peek_desc(xs->tx, desc, pool)) {xs->tx->queue_empty_descs++;continue;}  This is the backpressure mechanism for the Tx path.   Reserve space in the completion queue and only proceed   if there is space in it. This avoids having to implement   any buffering in the Tx path. ", "rcu_read_unlock();return xsk_tx_peek_release_fallback(pool, nb_pkts);}xs = list_first_or_null_rcu(&pool->xsk_tx_list, struct xdp_sock, tx_list);if (!xs) ": "xsk_tx_peek_release_desc_batch(struct xsk_buff_pool  pool, u32 nb_pkts){struct xdp_sock  xs;rcu_read_lock();if (!list_is_singular(&pool->xsk_tx_list)) {  Fallback to the non-batched version ", "newinfo->chainstack =vmalloc(array_size(nr_cpu_ids,   sizeof(*(newinfo->chainstack))));if (!newinfo->chainstack)return -ENOMEM;for_each_possible_cpu(i) ": "ebt_register_table()   if an error occurs ", "table = kmemdup(input_table, sizeof(struct ebt_table), GFP_KERNEL);if (!table) ": "ebt_unregister_table(struct net  net, struct ebt_table  table){mutex_lock(&ebt_mutex);list_del(&table->list);mutex_unlock(&ebt_mutex);audit_log_nfcfg(table->name, AF_BRIDGE, table->private->nentries,AUDIT_XT_OP_UNREGISTER, GFP_KERNEL);EBT_ENTRY_ITERATE(table->private->entries, table->private->entries_size,  ebt_cleanup_entry, net, NULL);if (table->private->nentries)module_put(table->me);vfree(table->private->entries);ebt_free_table_info(table->private);vfree(table->private);kfree(table->ops);kfree(table);}int ebt_register_table(struct net  net, const struct ebt_table  input_table,       const struct nf_hook_ops  template_ops){struct ebt_pernet  ebt_net = net_generic(net, ebt_pernet_id);struct ebt_table_info  newinfo;struct ebt_table  t,  table;struct nf_hook_ops  ops;unsigned int num_ops;struct ebt_replace_kernel  repl;int ret, i, countersize;void  p;if (input_table == NULL || (repl = input_table->table) == NULL ||    repl->entries == NULL || repl->entries_size == 0 ||    repl->counters != NULL || input_table->private != NULL)return -EINVAL;  Don't add one table to multiple lists. ", "base = private->entries;i = 0;while (i < nentries) ": "ebt_do_table(void  priv, struct sk_buff  skb,  const struct nf_hook_state  state){struct ebt_table  table = priv;unsigned int hook = state->hook;int i, nentries;struct ebt_entry  point;struct ebt_counter  counter_base,  cb_base;const struct ebt_entry_target  t;int verdict, sp = 0;struct ebt_chainstack  cs;struct ebt_entries  chaininfo;const char  base;const struct ebt_table_info  private;struct xt_action_param acpar;acpar.state   = state;acpar.hotdrop = false;read_lock_bh(&table->lock);private = table->private;cb_base = COUNTER_BASE(private->counters, private->nentries,   smp_processor_id());if (private->chainstack)cs = private->chainstack[smp_processor_id()];elsecs = NULL;chaininfo = private->hook_entry[hook];nentries = private->hook_entry[hook]->nentries;point = (struct ebt_entry  )(private->hook_entry[hook]->data);counter_base = cb_base + private->hook_entry[hook]->counter_offset;  base for chain jumps ", "if (!(sk_filter_state & (1 << sk->sk_state)))return 0;attrs = nla_nest_start_noflag(skb, TIPC_NLA_SOCK);if (!attrs)goto msg_cancel;if (__tipc_nl_add_sk_info(skb, tsk))goto attr_msg_cancel;if (nla_put_u32(skb, TIPC_NLA_SOCK_TYPE, (u32)sk->sk_type) ||    nla_put_u32(skb, TIPC_NLA_SOCK_TIPC_STATE, (u32)sk->sk_state) ||    nla_put_u32(skb, TIPC_NLA_SOCK_INO, sock_i_ino(sk)) ||    nla_put_u32(skb, TIPC_NLA_SOCK_UID,from_kuid_munged(sk_user_ns(NETLINK_CB(cb->skb).sk), sock_i_uid(sk))) ||    nla_put_u64_64bit(skb, TIPC_NLA_SOCK_COOKIE,      tipc_diag_gen_cookie(sk),      TIPC_NLA_SOCK_PAD))goto attr_msg_cancel;stat = nla_nest_start_noflag(skb, TIPC_NLA_SOCK_STAT);if (!stat)goto attr_msg_cancel;if (nla_put_u32(skb, TIPC_NLA_SOCK_STAT_RCVQ,skb_queue_len(&sk->sk_receive_queue)) ||    nla_put_u32(skb, TIPC_NLA_SOCK_STAT_SENDQ,skb_queue_len(&sk->sk_write_queue)) ||    nla_put_u32(skb, TIPC_NLA_SOCK_STAT_DROP,atomic_read(&sk->sk_drops)))goto stat_msg_cancel;if (tsk->cong_link_cnt &&    nla_put_flag(skb, TIPC_NLA_SOCK_STAT_LINK_CONG))goto stat_msg_cancel;if (tsk_conn_cong(tsk) &&    nla_put_flag(skb, TIPC_NLA_SOCK_STAT_CONN_CONG))goto stat_msg_cancel;nla_nest_end(skb, stat);if (tsk->group)if (tipc_group_fill_sock_diag(tsk->group, skb))goto stat_msg_cancel;nla_nest_end(skb, attrs);return 0;stat_msg_cancel:nla_nest_cancel(skb, stat);attr_msg_cancel:nla_nest_cancel(skb, attrs);msg_cancel:return -EMSGSIZE;}EXPORT_SYMBOL(tipc_sk_fill_sock_diag": "tipc_sk_fill_sock_diag(struct sk_buff  skb, struct netlink_callback  cb,   struct tipc_sock  tsk, u32 sk_filter_state,   u64 ( tipc_diag_gen_cookie)(struct sock  sk)){struct sock  sk = &tsk->sk;struct nlattr  attrs;struct nlattr  stat; filter response w.r.t sk_state", "#include <linux/module.h>#include <linux/types.h>#include <linux/kernel.h>#include <linux/string.h>#include <linux/mm.h>#include <linux/socket.h>#include <linux/in.h>#include <linux/inet.h>#include <linux/ip.h>#include <linux/netdevice.h>#include <linux/nvmem-consumer.h>#include <linux/etherdevice.h>#include <linux/skbuff.h>#include <linux/errno.h>#include <linux/init.h>#include <linux/if_ether.h>#include <linux/of_net.h>#include <linux/pci.h>#include <linux/property.h>#include <net/dst.h>#include <net/arp.h>#include <net/sock.h>#include <net/ipv6.h>#include <net/ip.h>#include <net/dsa.h>#include <net/flow_dissector.h>#include <net/gro.h>#include <linux/uaccess.h>#include <net/pkt_sched.h>/** * eth_header - create the Ethernet header * @skb:buffer to alter * @dev:source device * @type:Ethernet type field * @daddr: destination address (NULL leave destination address) * @saddr: source address (NULL use device source address) * @len:   packet length (<= skb->len) * * * Set the protocol type. For a packet of type ETH_P_802_3/2 we put the length * in here instead. ": "ether_setup() - use netdev_boot_setup(). ", "u32 eth_get_headlen(const struct net_device *dev, const void *data, u32 len)": "eth_get_headlen - determine the length of header for an ethernet frame   @dev: pointer to network device   @data: pointer to start of frame   @len: total length of frame     Make a best effort attempt to pull the length for all of the headers for   a given frame in a linear buffer. ", "__be16 eth_type_trans(struct sk_buff *skb, struct net_device *dev)": "eth_type_trans - determine the packet's protocol ID.   @skb: received socket data   @dev: receiving network device     The rule here is that we   assume 802.3 if the type field is short enough to be a length.   This is normal practice and works for any 'now in use' protocol. ", "int eth_header_parse(const struct sk_buff *skb, unsigned char *haddr)": "eth_header_parse - extract hardware address from packet   @skb: packet to extract header from   @haddr: destination buffer ", "int eth_header_cache(const struct neighbour *neigh, struct hh_cache *hh, __be16 type)": "eth_header_cache - fill cache entry from neighbour   @neigh: source neighbour   @hh: destination cache entry   @type: Ethernet type field     Create an Ethernet header template from the neighbour. ", "void eth_header_cache_update(struct hh_cache *hh,     const struct net_device *dev,     const unsigned char *haddr)": "eth_header_cache_update - update cache entry   @hh: destination cache entry   @dev: network device   @haddr: new hardware address     Called by Address Resolution module to notify changes in address. ", "__be16 eth_header_parse_protocol(const struct sk_buff *skb)": "eth_header_parse_protocol - extract protocol from L2 header   @skb: packet to extract protocol from ", "int eth_prepare_mac_addr_change(struct net_device *dev, void *p)": "eth_prepare_mac_addr_change - prepare for mac change   @dev: network device   @p: socket address ", "void eth_commit_mac_addr_change(struct net_device *dev, void *p)": "eth_commit_mac_addr_change - commit mac change   @dev: network device   @p: socket address ", "int eth_mac_addr(struct net_device *dev, void *p)": "eth_mac_addr - set new Ethernet hardware address   @dev: network device   @p: socket address     Change hardware address of device.     This doesn't change hardware matching, so needs to be overridden   for most real devices. ", "struct net_device *alloc_etherdev_mqs(int sizeof_priv, unsigned int txqs,      unsigned int rxqs)": "alloc_etherdev_mqs - Allocates and sets up an Ethernet device   @sizeof_priv: Size of additional driver-private structure to be allocated  for this Ethernet device   @txqs: The number of TX queues this device has.   @rxqs: The number of RX queues this device has.     Fill in the fields of the device structure with Ethernet-generic   values. Basically does everything except registering the device.     Constructs a new net device, complete with a private data area of   size (sizeof_priv).  A 32-byte (not bit) alignment is enforced for   this private data area. ", "int platform_get_ethdev_address(struct device *dev, struct net_device *netdev)": "platform_get_ethdev_address - Set netdev's MAC address from a given device   @dev:Pointer to the device   @netdev:Pointer to netdev to write the address to     Wrapper around eth_platform_get_mac_address() which writes the address   directly to netdev->dev_addr. ", "int fwnode_get_mac_address(struct fwnode_handle *fwnode, char *addr)": "fwnode_get_mac_address - Get the MAC from the firmware node   @fwnode:Pointer to the firmware node   @addr:Address of buffer to store the MAC in     Search the firmware node for the best MAC address to use.  'mac-address' is   checked first, because that is supposed to contain to \"most recent\" MAC   address. If that isn't set, then 'local-mac-address' is checked next,   because that is the default address.  If that isn't set, then the obsolete   'address' is checked, just in case we're using an old device tree.     Note that the 'address' property is supposed to contain a virtual address of   the register set, but some DTS files have redefined that property to be the   MAC address.     All-zero MAC addresses are rejected, because those could be properties that   exist in the firmware tables, but were not updated by the firmware.  For   example, the DTS could define 'mac-address' and 'local-mac-address', with   zero MAC addresses.  Some older U-Boots only initialized 'local-mac-address'.   In this case, the real MAC is in 'local-mac-address', and 'mac-address'   exists but is all zeros. ", "int device_get_mac_address(struct device *dev, char *addr)": "device_get_mac_address - Get the MAC for a given device   @dev:Pointer to the device   @addr:Address of buffer to store the MAC in ", "int device_get_ethdev_address(struct device *dev, struct net_device *netdev)": "device_get_ethdev_address - Set netdev's MAC address from a given device   @dev:Pointer to the device   @netdev:Pointer to netdev to write the address to     Wrapper around device_get_mac_address() which writes the address   directly to netdev->dev_addr. ", "tx = ieee80211_vif_is_mld(&sdata->vif) ||!sdata->deflink.csa_block_tx;if (!ifmgd->driver_disconnect) ": "ieee80211_disconnect(struct ieee80211_sub_if_data  sdata){struct ieee80211_local  local = sdata->local;struct ieee80211_if_managed  ifmgd = &sdata->u.mgd;u8 frame_buf[IEEE80211_DEAUTH_FRAME_LEN];bool tx;if (!ifmgd->associated)return;  in MLO assume we have a link where we can TX the frame ", "if (!(*changed & BSS_CHANGED_BANDWIDTH) &&    extracted == link->conf->eht_puncturing)return true;if (!cfg80211_valid_disable_subchannel_bitmap(&bitmap,      &link->conf->chandef)) ": "ieee80211_cqm_rssi_notify(&sdata->vif,NL80211_CQM_RSSI_THRESHOLD_EVENT_LOW,sig, GFP_KERNEL);} else if (sig > thold &&   (last_event == 0 || sig > last_event + hyst)) {link->u.mgd.last_cqm_event_signal = sig;ieee80211_cqm_rssi_notify(&sdata->vif,NL80211_CQM_RSSI_THRESHOLD_EVENT_HIGH,sig, GFP_KERNEL);}}if (bss_conf->cqm_rssi_low &&    link->u.mgd.count_beacon_signal >= IEEE80211_SIGNAL_AVE_MIN_COUNT) {int sig = -ewma_beacon_signal_read(&link->u.mgd.ave_beacon_signal);int last_event = link->u.mgd.last_cqm_event_signal;int low = bss_conf->cqm_rssi_low;int high = bss_conf->cqm_rssi_high;if (sig < low &&    (last_event == 0 || last_event >= low)) {link->u.mgd.last_cqm_event_signal = sig;ieee80211_cqm_rssi_notify(&sdata->vif,NL80211_CQM_RSSI_THRESHOLD_EVENT_LOW,sig, GFP_KERNEL);} else if (sig > high &&   (last_event == 0 || last_event <= high)) {link->u.mgd.last_cqm_event_signal = sig;ieee80211_cqm_rssi_notify(&sdata->vif,NL80211_CQM_RSSI_THRESHOLD_EVENT_HIGH,sig, GFP_KERNEL);}}}static bool ieee80211_rx_our_beacon(const u8  tx_bssid,    struct cfg80211_bss  bss){if (ether_addr_equal(tx_bssid, bss->bssid))return true;if (!bss->transmitted_bss)return false;return ether_addr_equal(tx_bssid, bss->transmitted_bss->bssid);}static bool ieee80211_config_puncturing(struct ieee80211_link_data  link,const struct ieee80211_eht_operation  eht_oper,u64  changed){u16 bitmap = 0, extracted;if ((eht_oper->params & IEEE80211_EHT_OPER_INFO_PRESENT) &&    (eht_oper->params &     IEEE80211_EHT_OPER_DISABLED_SUBCHANNEL_BITMAP_PRESENT)) {const struct ieee80211_eht_operation_info  info =(void  )eht_oper->optional;const u8  disable_subchannel_bitmap = info->optional;bitmap = get_unaligned_le16(disable_subchannel_bitmap);}extracted = ieee80211_extract_dis_subch_bmap(eht_oper,     &link->conf->chandef,     bitmap);  accept if there are no changes ", "if (ifmgd->flags & IEEE80211_STA_CONNECTION_POLL)already = true;ifmgd->flags |= IEEE80211_STA_CONNECTION_POLL;mutex_unlock(&sdata->local->mtx);if (already)goto out;mutex_lock(&sdata->local->iflist_mtx);ieee80211_recalc_ps(sdata->local);mutex_unlock(&sdata->local->iflist_mtx);ifmgd->probe_send_count = 0;ieee80211_mgd_probe_ap_send(sdata); out:sdata_unlock(sdata);}struct sk_buff *ieee80211_ap_probereq_get(struct ieee80211_hw *hw,  struct ieee80211_vif *vif)": "ieee80211_cqm_beacon_loss_notify(&sdata->vif, GFP_KERNEL);}    The driverour work has already reported this event or the   connection monitoring has kicked in and we have already sent   a probe request. Or maybe the AP died and the driver keeps   reporting until we disassociate...     In either case we have to ignore the current call to this   function (except for setting the correct probe reason bit)   because otherwise we would reset the timer every time and   never check whether we received a probe response! ", "sdata->u.mgd.rssi_min_thold = rssi_min_thold*16;sdata->u.mgd.rssi_max_thold = rssi_max_thold*16;}void ieee80211_enable_rssi_reports(struct ieee80211_vif *vif,    int rssi_min_thold,    int rssi_max_thold)": "ieee80211_enable_rssi_reports(struct ieee80211_sub_if_data  sdata,    int rssi_min_thold,    int rssi_max_thold){trace_api_enable_rssi_reports(sdata, rssi_min_thold, rssi_max_thold);if (WARN_ON(sdata->vif.type != NL80211_IFTYPE_STATION))return;    Scale up threshold values before storing it, as the RSSI averaging   algorithm uses a scaled up value as well. Change this scaling   factor if the RSSI averaging algorithm changes. ", "WARN_ON(1);mutex_unlock(&rate_ctrl_mutex);return -EALREADY;}}alg = kzalloc(sizeof(*alg), GFP_KERNEL);if (alg == NULL) ": "ieee80211_rate_control_register(const struct rate_control_ops  ops){struct rate_control_alg  alg;if (!ops->name)return -EINVAL;mutex_lock(&rate_ctrl_mutex);list_for_each_entry(alg, &rate_ctrl_algs, list) {if (!strcmp(alg->ops->name, ops->name)) {  don't register an algorithm twice ", "old = rcu_dereference_protected(pubsta->rates, true);rcu_assign_pointer(pubsta->rates, rates);if (old)kfree_rcu(old, rcu_head);if (sta->uploaded)drv_sta_rate_tbl_update(hw_to_local(hw), sta->sdata, pubsta);ieee80211_sta_set_expected_throughput(pubsta, sta_get_expected_throughput(sta));return 0;}EXPORT_SYMBOL(rate_control_set_rates": "rate_control_set_rates(struct ieee80211_hw  hw,   struct ieee80211_sta  pubsta,   struct ieee80211_sta_rates  rates){struct sta_info  sta = container_of(pubsta, struct sta_info, sta);struct ieee80211_sta_rates  old;struct ieee80211_supported_band  sband;sband = ieee80211_get_sband(sta->sdata);if (!sband)return -EINVAL;rate_control_apply_mask_ratetbl(sta, sband, rates);    mac80211 guarantees that this function will not be called   concurrently, so the following RCU access is safe, even without   extra locking. This can not be checked easily, so we just set   the condition to true. ", "if (WARN_ON(!local->scanning && !aborted))aborted = true;if (WARN_ON(!local->scan_req))return;scan_sdata = rcu_dereference_protected(local->scan_sdata,       lockdep_is_held(&local->mtx));if (hw_scan && !aborted &&    !ieee80211_hw_check(&local->hw, SINGLE_SCAN_ON_ALL_BANDS) &&    ieee80211_prep_hw_scan(scan_sdata)) ": "ieee80211_scan_completed(struct ieee80211_hw  hw, bool aborted){struct ieee80211_local  local = hw_to_local(hw);bool hw_scan = test_bit(SCAN_HW_SCANNING, &local->scanning);bool was_scanning = local->scanning;struct cfg80211_scan_request  scan_req;struct ieee80211_sub_if_data  scan_sdata;struct ieee80211_sub_if_data  sdata;lockdep_assert_held(&local->mtx);    It's ok to abort a not-yet-running scan (that   we have one at all will be verified by checking   local->scan_req next), but not to complete it   successfully. ", "if (local->in_reconfig)return;schedule_work(&local->sched_scan_stopped_work);}EXPORT_SYMBOL(ieee80211_sched_scan_stopped": "ieee80211_sched_scan_stopped_work(struct work_struct  work){struct ieee80211_local  local =container_of(work, struct ieee80211_local,     sched_scan_stopped_work);ieee80211_sched_scan_end(local);}void ieee80211_sched_scan_stopped(struct ieee80211_hw  hw){struct ieee80211_local  local = hw_to_local(hw);trace_api_sched_scan_stopped(local);    this shouldn't really happen, so for simplicity   simply ignore it, and let mac80211 reconfigure   the sched scan later on. ", "in_ps = test_sta_flag(sta, WLAN_STA_PS_STA);if ((start && in_ps) || (!start && !in_ps))return -EINVAL;if (start)sta_ps_start(sta);elsesta_ps_end(sta);return 0;}EXPORT_SYMBOL(ieee80211_sta_ps_transition": "ieee80211_sta_ps_transition(struct ieee80211_sta  pubsta, bool start){struct sta_info  sta = container_of(pubsta, struct sta_info, sta);bool in_ps;WARN_ON(!ieee80211_hw_check(&sta->local->hw, AP_LINK_PS));  Don't let the same PS state be set twice ", "if (!(sta->sta.uapsd_queues & ieee80211_ac_to_qos_mask[ac]) &&    tid != IEEE80211_NUM_TIDS)return;/* if we are in a service period, do nothing ": "ieee80211_sta_uapsd_trigger(struct ieee80211_sta  pubsta, u8 tid){struct sta_info  sta = container_of(pubsta, struct sta_info, sta);int ac = ieee80211_ac_from_tid(tid);    If this AC is not trigger-enabled do nothing unless the   driver is calling us after it already checked.     NB: This couldshould check a separate bitmap of trigger-   enabled queues, but for now we only implement uAPSD wo   TSPEC changes to the ACs, so they're always the same. ", ".security_idx = tid,.seqno_idx = tid,};int i, diff;if (WARN_ON(!pubsta || tid >= IEEE80211_NUM_TIDS))return;__skb_queue_head_init(&frames);sta = container_of(pubsta, struct sta_info, sta);if (!ieee80211_rx_data_set_sta(&rx, sta, -1))return;rcu_read_lock();tid_agg_rx = rcu_dereference(sta->ampdu_mlme.tid_rx[tid]);if (!tid_agg_rx)goto out;spin_lock_bh(&tid_agg_rx->reorder_lock);if (received_mpdus >= IEEE80211_SN_MODULO >> 1) ": "ieee80211_mark_rx_ba_filtered_frames(struct ieee80211_sta  pubsta, u8 tid,  u16 ssn, u64 filtered,  u16 received_mpdus){struct sta_info  sta;struct tid_ampdu_rx  tid_agg_rx;struct sk_buff_head frames;struct ieee80211_rx_data rx = {  This is OK -- must be QoS data frame ", "if (unlikely(local->quiescing || local->suspended))goto drop;/* We might be during a HW reconfig, prevent Rx for the same reason ": "ieee80211_rx_list(struct ieee80211_hw  hw, struct ieee80211_sta  pubsta,       struct sk_buff  skb, struct list_head  list){struct ieee80211_local  local = hw_to_local(hw);struct ieee80211_rate  rate = NULL;struct ieee80211_supported_band  sband;struct ieee80211_rx_status  status = IEEE80211_SKB_RXCB(skb);struct ieee80211_hdr  hdr = (struct ieee80211_hdr  )skb->data;WARN_ON_ONCE(softirq_count() == 0);if (WARN_ON(status->band >= NUM_NL80211_BANDS))goto drop;sband = local->hw.wiphy->bands[status->band];if (WARN_ON(!sband))goto drop;    If we're suspending, it is possible although not too likely   that we'd be receiving frames after having already partially   quiesced the stack. We can't process such frames then since   that might, for example, cause stations to be added or other   driver callbacks be invoked. ", "rcu_read_lock();ieee80211_rx_list(hw, pubsta, skb, &list);rcu_read_unlock();if (!napi) ": "ieee80211_rx_napi(struct ieee80211_hw  hw, struct ieee80211_sta  pubsta,       struct sk_buff  skb, struct napi_struct  napi){struct sk_buff  tmp;LIST_HEAD(list);    key references and virtual interfaces are protected using RCU   and this requires that we are in a read-side RCU section during   receive processing ", "struct ieee80211_sub_if_data *iter;list_for_each_entry_rcu(iter, &local->interfaces, list) ": "ieee80211_csa_finish(struct ieee80211_vif  vif){struct ieee80211_sub_if_data  sdata = vif_to_sdata(vif);struct ieee80211_local  local = sdata->local;rcu_read_lock();if (vif->mbssid_tx_vif == vif) {  Trigger ieee80211_csa_finish() on the non-transmitting   interfaces when channel switch is received on   transmitting interface ", "if (initiator == WLAN_BACK_RECIPIENT && tx)ieee80211_send_delba(sta->sdata, sta->sta.addr,     tid, WLAN_BACK_RECIPIENT, reason);/* * return here in case tid_rx is not assigned - which will happen if * IEEE80211_HW_SUPPORTS_REORDERING_BUFFER is set. ": "ieee80211_stop_rx_ba_session(struct sta_info  sta, u16 tid,     u16 initiator, u16 reason, bool tx){struct ieee80211_local  local = sta->local;struct tid_ampdu_rx  tid_rx;struct ieee80211_ampdu_params params = {.sta = &sta->sta,.action = IEEE80211_AMPDU_RX_STOP,.tid = tid,.amsdu = false,.timeout = 0,.ssn = 0,};lockdep_assert_held(&sta->ampdu_mlme.mtx);tid_rx = rcu_dereference_protected(sta->ampdu_mlme.tid_rx[tid],lockdep_is_held(&sta->ampdu_mlme.mtx));if (!test_bit(tid, sta->ampdu_mlme.agg_session_valid))return;RCU_INIT_POINTER(sta->ampdu_mlme.tid_rx[tid], NULL);__clear_bit(tid, sta->ampdu_mlme.agg_session_valid);ht_dbg(sta->sdata,       \"Rx BA session stop requested for %pM tid %u %s reason: %d\\n\",       sta->sta.addr, tid,       initiator == WLAN_BACK_RECIPIENT ? \"recipient\" : \"initiator\",       (int)reason);if (drv_ampdu_action(local, sta->sdata, &params))sdata_info(sta->sdata,   \"HW problem - can not stop rx aggregation for %pM tid %d\\n\",   sta->sta.addr, tid);  check if this is a self generated aggregation halt ", "if (key->sta && key->sta->removed)continue;if (!(key->flags & KEY_FLAG_UPLOADED_TO_HARDWARE))continue;iter(hw, &sdata->vif,     key->sta ? &key->sta->sta : NULL,     &key->conf, iter_data);}}void ieee80211_iter_keys_rcu(struct ieee80211_hw *hw,     struct ieee80211_vif *vif,     void (*iter)(struct ieee80211_hw *hw,  struct ieee80211_vif *vif,  struct ieee80211_sta *sta,  struct ieee80211_key_conf *key,  void *data),     void *iter_data)": "ieee80211_iter_keys_rcu(struct ieee80211_hw  hw, struct ieee80211_sub_if_data  sdata, void ( iter)(struct ieee80211_hw  hw,      struct ieee80211_vif  vif,      struct ieee80211_sta  sta,      struct ieee80211_key_conf  key,      void  data), void  iter_data){struct ieee80211_key  key;list_for_each_entry_rcu(key, &sdata->key_list, list) {  skip keys of station in removal process ", "ieee80211_sta_update_pending_airtime(local, sta,     skb_get_queue_mapping(skb),     tx_time_est,     true);ieee80211_info_set_tx_time_est(IEEE80211_SKB_CB(skb), 0);}if (!status->info)goto free;rates_idx = ieee80211_tx_get_rates(hw, info, &retry_count);acked = !!(info->flags & IEEE80211_TX_STAT_ACK);noack_success = !!(info->flags & IEEE80211_TX_STAT_NOACK_TRANSMITTED);ack_signal_valid =!!(info->status.flags & IEEE80211_TX_STATUS_ACK_SIGNAL_VALID);if (pubsta) ": "ieee80211_tx_status_ext(hw, &status);rcu_read_unlock();}EXPORT_SYMBOL(ieee80211_tx_status);void ieee80211_tx_status_ext(struct ieee80211_hw  hw,     struct ieee80211_tx_status  status){struct ieee80211_local  local = hw_to_local(hw);struct ieee80211_tx_info  info = status->info;struct ieee80211_sta  pubsta = status->sta;struct sk_buff  skb = status->skb;struct sta_info  sta = NULL;int rates_idx, retry_count;bool acked, noack_success, ack_signal_valid;u16 tx_time_est;if (pubsta) {sta = container_of(pubsta, struct sta_info, sta);if (status->n_rates)sta->deflink.tx_stats.last_rate_info =status->rates[status->n_rates - 1].rate_idx;}if (skb && (tx_time_est =    ieee80211_info_get_tx_time_est(IEEE80211_SKB_CB(skb))) > 0) {  Do this here to avoid the expensive lookup of the sta   in ieee80211_report_used_skb(). ", "memset(&info->control, 0, sizeof(info->control));info->control.jiffies = jiffies;info->control.vif = &sta->sdata->vif;info->control.flags |= IEEE80211_TX_INTCFL_NEED_TXPROCESSING;info->flags |= IEEE80211_TX_INTFL_RETRANSMISSION;info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;sta->deflink.status_stats.filtered++;/* * Clear more-data bit on filtered frames, it might be set * but later frames might time out so it might have to be * clear again ... It's all rather unlikely (this frame * should time out first, right?) but let's not confuse * peers unnecessarily. ": "ieee80211_free_txskb(hw, skb);tmp--;I802_DEBUG_INC(local->tx_status_drop);}tasklet_schedule(&local->tasklet);}EXPORT_SYMBOL(ieee80211_tx_status_irqsafe);static void ieee80211_handle_filtered_frame(struct ieee80211_local  local,    struct sta_info  sta,    struct sk_buff  skb){struct ieee80211_tx_info  info = IEEE80211_SKB_CB(skb);struct ieee80211_hdr  hdr = (void  )skb->data;int ac;if (info->flags & (IEEE80211_TX_CTL_NO_PS_BUFFER |   IEEE80211_TX_CTL_AMPDU |   IEEE80211_TX_CTL_HW_80211_ENCAP)) {ieee80211_free_txskb(&local->hw, skb);return;}    This skb 'survived' a round-trip through the driver, and   hopefully the driver didn't mangle it too badly. However,   we can definitely not rely on the control information   being correct. Clear it so we don't get junk there, and   indicate that it needs new processing, but must not be   modifiedencrypted again. ", "skb2 = __skb_dequeue(&tx.skbs);if (WARN_ON(skb2 != skb || !skb_queue_empty(&tx.skbs))) ": "ieee80211_tx_prepare_skb(struct ieee80211_hw  hw,      struct ieee80211_vif  vif, struct sk_buff  skb,      int band, struct ieee80211_sta   sta){struct ieee80211_sub_if_data  sdata = vif_to_sdata(vif);struct ieee80211_tx_info  info = IEEE80211_SKB_CB(skb);struct ieee80211_tx_data tx;struct sk_buff  skb2;if (ieee80211_tx_prepare(sdata, &tx, NULL, skb) == TX_DROP)return false;info->band = band;info->control.vif = vif;info->hw_queue = vif->hw_queue[skb_get_queue_mapping(skb)];if (invoke_tx_handlers(&tx))return false;if (sta) {if (tx.sta) sta = &tx.sta->sta;else sta = NULL;}  this function isn't suitable for fragmented data frames ", "set_bit(IEEE80211_TXQ_DIRTY, &txqi->flags);return NULL;}spin_lock_bh(&fq->lock);/* Make sure fragments stay together. ": "ieee80211_txq_airtime_check(hw, txq))return NULL;begin:spin_lock_irqsave(&local->queue_stop_reason_lock, flags);q_stopped = local->queue_stop_reasons[q];spin_unlock_irqrestore(&local->queue_stop_reason_lock, flags);if (unlikely(q_stopped)) {  mark for waking later ", "if (txqi->txq.sta && local->airtime_flags && has_queue &&    wiphy_ext_feature_isset(local->hw.wiphy,    NL80211_EXT_FEATURE_AIRTIME_FAIRNESS))list_add(&txqi->schedule_order, &local->active_txqs[txq->ac]);elselist_add_tail(&txqi->schedule_order,      &local->active_txqs[txq->ac]);if (has_queue)ieee80211_txq_set_active(txqi);}spin_unlock_bh(&local->active_txq_lock[txq->ac]);}EXPORT_SYMBOL(__ieee80211_schedule_txq": "__ieee80211_schedule_txq(struct ieee80211_hw  hw,      struct ieee80211_txq  txq,      bool force){struct ieee80211_local  local = hw_to_local(hw);struct txq_info  txqi = to_txq_info(txq);bool has_queue;spin_lock_bh(&local->active_txq_lock[txq->ac]);has_queue = force || txq_has_queue(txq);if (list_empty(&txqi->schedule_order) &&    (has_queue || ieee80211_txq_keep_active(txqi))) {  If airtime accounting is active, always enqueue STAs at the   head of the list to ensure that they only get moved to the   back by the airtime DRR scheduler once they have a negative   deficit. A station that already has a negative deficit will   get immediately moved to the back of the list on the next   call to ieee80211_next_txq(). ", "WARN_ON_ONCE(!beacon->cntdwn_current_counter);return beacon->cntdwn_current_counter;}u8 ieee80211_beacon_update_cntdwn(struct ieee80211_vif *vif)": "ieee80211_beacon_update_cntdwn(struct beacon_data  beacon){beacon->cntdwn_current_counter--;  the counter should never reach 0 ", "if (ema_index <= IEEE80211_INCLUDE_ALL_MBSSID_ELEMS)ema_index = beacon->mbssid_ies->cnt;} else ": "ieee80211_beacon_free_ema_list(ema);return NULL;}#define IEEE80211_INCLUDE_ALL_MBSSID_ELEMS -1static struct sk_buff  __ieee80211_beacon_get(struct ieee80211_hw  hw,       struct ieee80211_vif  vif,       struct ieee80211_mutable_offsets  offs,       bool is_template,       unsigned int link_id,       int ema_index,       struct ieee80211_ema_beacons   ema_beacons){struct ieee80211_local  local = hw_to_local(hw);struct beacon_data  beacon = NULL;struct sk_buff  skb = NULL;struct ieee80211_sub_if_data  sdata = NULL;struct ieee80211_chanctx_conf  chanctx_conf;struct ieee80211_link_data  link;rcu_read_lock();sdata = vif_to_sdata(vif);link = rcu_dereference(sdata->link[link_id]);if (!link)goto out;chanctx_conf =rcu_dereference(link->conf->chanctx_conf);if (!ieee80211_sdata_running(sdata) || !chanctx_conf)goto out;if (offs)memset(offs, 0, sizeof( offs));if (sdata->vif.type == NL80211_IFTYPE_AP) {beacon = rcu_dereference(link->u.ap.beacon);if (!beacon)goto out;if (ema_beacons) { ema_beacons =ieee80211_beacon_get_ap_ema_list(hw, vif, link, offs, is_template, beacon, chanctx_conf);} else {if (beacon->mbssid_ies && beacon->mbssid_ies->cnt) {if (ema_index >= beacon->mbssid_ies->cnt)goto out;   End of MBSSID elements ", "copy = skb_copy(bcn, GFP_ATOMIC);if (!copy)return bcn;shift = ieee80211_vif_get_shift(vif);ieee80211_tx_monitor(hw_to_local(hw), copy, 1, shift, false, NULL);return bcn;}EXPORT_SYMBOL(ieee80211_beacon_get_tim": "ieee80211_beacon_get_tim(struct ieee80211_hw  hw, struct ieee80211_vif  vif, u16  tim_offset, u16  tim_length, unsigned int link_id){struct ieee80211_mutable_offsets offs = {};struct sk_buff  bcn = __ieee80211_beacon_get(hw, vif, &offs, false,     link_id,     IEEE80211_INCLUDE_ALL_MBSSID_ELEMS,     NULL);struct sk_buff  copy;int shift;if (!bcn)return bcn;if (tim_offset) tim_offset = offs.tim_offset;if (tim_length) tim_length = offs.tim_length;if (ieee80211_hw_check(hw, BEACON_TX_STATUS) ||    !hw_to_local(hw)->monitors)return bcn;  send a copy to monitor interfaces ", "pspoll->aid |= cpu_to_le16(1 << 15 | 1 << 14);memcpy(pspoll->bssid, sdata->deflink.u.mgd.bssid, ETH_ALEN);memcpy(pspoll->ta, vif->addr, ETH_ALEN);return skb;}EXPORT_SYMBOL(ieee80211_pspoll_get": "ieee80211_pspoll_get(struct ieee80211_hw  hw,     struct ieee80211_vif  vif){struct ieee80211_sub_if_data  sdata;struct ieee80211_pspoll  pspoll;struct ieee80211_local  local;struct sk_buff  skb;if (WARN_ON(vif->type != NL80211_IFTYPE_STATION))return NULL;sdata = vif_to_sdata(vif);local = sdata->local;skb = dev_alloc_skb(local->hw.extra_tx_headroom + sizeof( pspoll));if (!skb)return NULL;skb_reserve(skb, local->hw.extra_tx_headroom);pspoll = skb_put_zero(skb, sizeof( pspoll));pspoll->frame_control = cpu_to_le16(IEEE80211_FTYPE_CTL |    IEEE80211_STYPE_PSPOLL);pspoll->aid = cpu_to_le16(sdata->vif.cfg.aid);  aid in PS-Poll has its two MSBs each set to 1 ", "switch (sdata->vif.type) ": "ieee80211_unreserve_tid(struct ieee80211_sta  pubsta, u8 tid){struct sta_info  sta = container_of(pubsta, struct sta_info, sta);struct ieee80211_sub_if_data  sdata = sta->sdata;lockdep_assert_held(&sdata->local->sta_mtx);  only some cases are supported right now ", "ieee80211_stop_queues_by_reason(hw, IEEE80211_MAX_QUEUE_MAP,IEEE80211_QUEUE_STOP_REASON_SUSPEND,false);/* * Stop all Rx during the reconfig. We don't want state changes * or driver callbacks while this is in progress. ": "ieee80211_restart_hw(struct ieee80211_hw  hw){struct ieee80211_local  local = hw_to_local(hw);trace_api_restart_hw(local);wiphy_info(hw->wiphy,   \"Hardware restart was requested\\n\");  use this reason, ieee80211_reconfig will unblock it ", "i = !!ops->add_chanctx + !!ops->remove_chanctx +    !!ops->change_chanctx + !!ops->assign_vif_chanctx +    !!ops->unassign_vif_chanctx;if (WARN_ON(i != 0 && i != 5))return NULL;use_chanctx = i == 5;/* Ensure 32-byte alignment of our private data and hw private data. * We use the wiphy priv data for both our ieee80211_local and for * the driver's private data * * In memory it'll be like this: * * +-------------------------+ * | struct wiphy    | * +-------------------------+ * | struct ieee80211_local  | * +-------------------------+ * | driver's private data   | * +-------------------------+ * ": "ieee80211_alloc_hw_nm(size_t priv_data_len,   const struct ieee80211_ops  ops,   const char  requested_name){struct ieee80211_local  local;int priv_size, i;struct wiphy  wiphy;bool use_chanctx;if (WARN_ON(!ops->tx || !ops->start || !ops->stop || !ops->config ||    !ops->add_interface || !ops->remove_interface ||    !ops->configure_filter || !ops->wake_tx_queue))return NULL;if (WARN_ON(ops->sta_state && (ops->sta_add || ops->sta_remove)))return NULL;if (WARN_ON(!!ops->link_info_changed != !!ops->vif_cfg_changed ||    (ops->link_info_changed && ops->bss_info_changed)))return NULL;  check all or no channel context operations exist ", "if (WARN_ON(!local->use_chanctx))return -EINVAL;if (WARN_ON(!local->ops->link_info_changed))return -EINVAL;if (WARN_ON(!ieee80211_hw_check(hw, HAS_RATE_CONTROL)))return -EINVAL;if (WARN_ON(!ieee80211_hw_check(hw, AMPDU_AGGREGATION)))return -EINVAL;if (WARN_ON(ieee80211_hw_check(hw, HOST_BROADCAST_PS_BUFFERING)))return -EINVAL;if (WARN_ON(ieee80211_hw_check(hw, SUPPORTS_PS) &&    (!ieee80211_hw_check(hw, SUPPORTS_DYNAMIC_PS) ||     ieee80211_hw_check(hw, PS_NULLFUNC_STACK))))return -EINVAL;if (WARN_ON(!ieee80211_hw_check(hw, MFP_CAPABLE)))return -EINVAL;if (WARN_ON(!ieee80211_hw_check(hw, CONNECTION_MONITOR)))return -EINVAL;if (WARN_ON(ieee80211_hw_check(hw, NEED_DTIM_BEFORE_ASSOC)))return -EINVAL;if (WARN_ON(ieee80211_hw_check(hw, TIMING_BEACON_ONLY)))return -EINVAL;if (WARN_ON(!ieee80211_hw_check(hw, AP_LINK_PS)))return -EINVAL;if (WARN_ON(ieee80211_hw_check(hw, DEAUTH_NEED_MGD_TX_PREP)))return -EINVAL;}#ifdef CONFIG_PMif (hw->wiphy->wowlan && (!local->ops->suspend || !local->ops->resume))return -EINVAL;#endifif (!local->use_chanctx) ": "ieee80211_register_hw(struct ieee80211_hw  hw){struct ieee80211_local  local = hw_to_local(hw);int result, i;enum nl80211_band band;int channels, max_bitrates;bool supp_ht, supp_vht, supp_he, supp_eht;struct cfg80211_chan_def dflt_chandef = {};if (ieee80211_hw_check(hw, QUEUE_CONTROL) &&    (local->hw.offchannel_tx_hw_queue == IEEE80211_INVAL_HW_QUEUE ||     local->hw.offchannel_tx_hw_queue >= local->hw.queues))return -EINVAL;if ((hw->wiphy->features & NL80211_FEATURE_TDLS_CHANNEL_SWITCH) &&    (!local->ops->tdls_channel_switch ||     !local->ops->tdls_cancel_channel_switch ||     !local->ops->tdls_recv_channel_switch))return -EOPNOTSUPP;if (WARN_ON(ieee80211_hw_check(hw, SUPPORTS_TX_FRAG) &&    !local->ops->set_frag_threshold))return -EINVAL;if (WARN_ON(local->hw.wiphy->interface_modes &BIT(NL80211_IFTYPE_NAN) &&    (!local->ops->start_nan || !local->ops->stop_nan)))return -EINVAL;if (hw->wiphy->flags & WIPHY_FLAG_SUPPORTS_MLO) {    For drivers capable of doing MLO, assume modern driver   or firmware facilities, so software doesn't have to do   as much, e.g. monitoring beacons would be hard if we   might not even know which link is active at which time. ", "ieee80211_remove_interfaces(local);rtnl_unlock();cancel_delayed_work_sync(&local->roc_work);cancel_work_sync(&local->restart_work);cancel_work_sync(&local->reconfig_filter);flush_work(&local->sched_scan_stopped_work);flush_work(&local->radar_detected_work);ieee80211_clear_tx_pending(local);rate_control_deinitialize(local);if (skb_queue_len(&local->skb_queue) ||    skb_queue_len(&local->skb_queue_unreliable))wiphy_warn(local->hw.wiphy, \"skb_queue not empty\\n\");skb_queue_purge(&local->skb_queue);skb_queue_purge(&local->skb_queue_unreliable);wiphy_unregister(local->hw.wiphy);destroy_workqueue(local->workqueue);ieee80211_led_exit(local);kfree(local->int_scan_req);}EXPORT_SYMBOL(ieee80211_unregister_hw": "ieee80211_unregister_hw(struct ieee80211_hw  hw){struct ieee80211_local  local = hw_to_local(hw);tasklet_kill(&local->tx_pending_tasklet);tasklet_kill(&local->tasklet);#ifdef CONFIG_INETunregister_inetaddr_notifier(&local->ifa_notifier);#endif#if IS_ENABLED(CONFIG_IPV6)unregister_inet6addr_notifier(&local->ifa6_notifier);#endifrtnl_lock();    At this point, interface list manipulations are fine   because the driver cannot be handing us frames any   more and the tasklet is killed. ", "return NULL;if (ieee80211_has_a4(fc))return NULL;if (ieee80211_has_tods(fc))return hdr->addr1;if (ieee80211_has_fromds(fc))return hdr->addr2;return hdr->addr3;}if (ieee80211_is_s1g_beacon(fc)) ": "ieee80211_get_bssid(struct ieee80211_hdr  hdr, size_t len,enum nl80211_iftype type){__le16 fc = hdr->frame_control;if (ieee80211_is_data(fc)) {if (len < 24)   drop incorrect hdr len (data) ", "dur = ieee80211_frame_duration(sband->band, 10, bitrate,       erp, short_preamble, shift);/* Data frame duration ": "ieee80211_rts_duration(struct ieee80211_hw  hw,      struct ieee80211_vif  vif, size_t frame_len,      const struct ieee80211_tx_info  frame_txctl){struct ieee80211_local  local = hw_to_local(hw);struct ieee80211_rate  rate;struct ieee80211_sub_if_data  sdata;bool short_preamble;int erp, shift = 0, bitrate;u16 dur;struct ieee80211_supported_band  sband;sband = local->hw.wiphy->bands[frame_txctl->band];short_preamble = false;rate = &sband->bitrates[frame_txctl->control.rts_cts_rate_idx];erp = 0;if (vif) {sdata = vif_to_sdata(vif);short_preamble = sdata->vif.bss_conf.use_short_preamble;if (sdata->deflink.operating_11g_mode)erp = rate->flags & IEEE80211_RATE_ERP_G;shift = ieee80211_vif_get_shift(vif);}bitrate = DIV_ROUND_UP(rate->bitrate, 1 << shift);  CTS duration ", "dur = ieee80211_frame_duration(sband->band, frame_len, bitrate,       erp, short_preamble, shift);if (!(frame_txctl->flags & IEEE80211_TX_CTL_NO_ACK)) ": "ieee80211_ctstoself_duration(struct ieee80211_hw  hw,    struct ieee80211_vif  vif,    size_t frame_len,    const struct ieee80211_tx_info  frame_txctl){struct ieee80211_local  local = hw_to_local(hw);struct ieee80211_rate  rate;struct ieee80211_sub_if_data  sdata;bool short_preamble;int erp, shift = 0, bitrate;u16 dur;struct ieee80211_supported_band  sband;sband = local->hw.wiphy->bands[frame_txctl->band];short_preamble = false;rate = &sband->bitrates[frame_txctl->control.rts_cts_rate_idx];erp = 0;if (vif) {sdata = vif_to_sdata(vif);short_preamble = sdata->vif.bss_conf.use_short_preamble;if (sdata->deflink.operating_11g_mode)erp = rate->flags & IEEE80211_RATE_ERP_G;shift = ieee80211_vif_get_shift(vif);}bitrate = DIV_ROUND_UP(rate->bitrate, 1 << shift);  Data frame duration ", "ieee80211_txq_schedule_start(hw, txq->ac);while ((queue = ieee80211_next_txq(hw, txq->ac))) ": "ieee80211_handle_wake_tx_queue(struct ieee80211_hw  hw,    struct ieee80211_txq  txq){struct ieee80211_local  local = hw_to_local(hw);struct ieee80211_sub_if_data  sdata = vif_to_sdata(txq->vif);struct ieee80211_txq  queue;spin_lock(&local->handle_wake_tx_queue_lock);  Use ieee80211_next_txq() for airtime fairness accounting ", "return;if (!skb_queue_empty(&local->pending[queue]))tasklet_schedule(&local->tx_pending_tasklet);/* * Calling _ieee80211_wake_txqs here can be a problem because it may * release queue_stop_reason_lock which has been taken by * __ieee80211_wake_queue's caller. It is certainly not very nice to * release someone's lock, but it is fine because all the callers of * __ieee80211_wake_queue call it right before releasing the lock. ": "ieee80211_wake_queue(struct ieee80211_hw  hw, int queue,   enum queue_stop_reason reason,   bool refcounted,   unsigned long  flags){struct ieee80211_local  local = hw_to_local(hw);trace_wake_queue(local, queue, reason);if (WARN_ON(queue >= hw->queues))return;if (!test_bit(reason, &local->queue_stop_reasons[queue]))return;if (!refcounted) {local->q_stop_reasons[queue][reason] = 0;} else {local->q_stop_reasons[queue][reason]--;if (WARN_ON(local->q_stop_reasons[queue][reason] < 0))local->q_stop_reasons[queue][reason] = 0;}if (local->q_stop_reasons[queue][reason] == 0)__clear_bit(reason, &local->queue_stop_reasons[queue]);if (local->queue_stop_reasons[queue] != 0)  someone still has this queue stopped ", "clear_sta_flag(sta, WLAN_STA_PS_DRIVER);ieee80211_queue_work(hw, &sta->drv_deliver_wk);} else ": "ieee80211_sta_block_awake(struct ieee80211_hw  hw,       struct ieee80211_sta  pubsta, bool block){struct sta_info  sta = container_of(pubsta, struct sta_info, sta);trace_api_sta_block_awake(sta->local, pubsta, block);if (block) {set_sta_flag(sta, WLAN_STA_PS_DRIVER);ieee80211_clear_fast_xmit(sta);return;}if (!test_sta_flag(sta, WLAN_STA_PS_DRIVER))return;if (!test_sta_flag(sta, WLAN_STA_PS_STA)) {set_sta_flag(sta, WLAN_STA_PS_DELIVER);clear_sta_flag(sta, WLAN_STA_PS_DRIVER);ieee80211_queue_work(hw, &sta->drv_deliver_wk);} else if (test_sta_flag(sta, WLAN_STA_PSPOLL) ||   test_sta_flag(sta, WLAN_STA_UAPSD)) {  must be asleep in this case ", "void sta_info_free(struct ieee80211_local *local, struct sta_info *sta)": "ieee80211_sta_recalc_aggregates(&sta->sta);}     sta_info_free - free STA     @local: pointer to the global information   @sta: STA info to free     This function must undo everything done by sta_info_alloc()   that may happen before sta_info_insert(). It may only be   called when sta_info_insert() has not been attempted (and   if that fails, the station is freed anyway.) ", "static void ieee80211_send_addba_request(struct ieee80211_sub_if_data *sdata, const u8 *da, u16 tid, u8 dialog_token, u16 start_seq_num, u16 agg_size, u16 timeout)": "ieee80211_stop_tx_ba_cb_irqsafe().   Note that the sta can get destroyed before the BA tear down is   complete. ", "int can_send(struct sk_buff *skb, int loop)": "can_send - transmit a CAN frame (optional with local loopback)   @skb: pointer to socket buffer with CAN frame in data section   @loop: loopback for listeners on local CAN sockets (recommended default!)     Due to the loopback this routine must not be called from hardirq context.     Return:    0 on success    -ENETDOWN when the selected interface is down    -ENOBUFS on full driver queue (see net_xmit_errno())    -ENOMEM when local loopback failed at calling skb_clone()    -EPERM when trying to send on a non-CAN interface    -EMSGSIZE CAN frame size is bigger than CAN interface MTU    -EINVAL when the skb->data does not contain a valid CAN frame ", "int can_rx_register(struct net *net, struct net_device *dev, canid_t can_id,    canid_t mask, void (*func)(struct sk_buff *, void *),    void *data, char *ident, struct sock *sk)": "can_rx_register - subscribe CAN frames from a specific interface   @net: the applicable net namespace   @dev: pointer to netdevice (NULL => subscribe from 'all' CAN devices list)   @can_id: CAN identifier (see description)   @mask: CAN mask (see description)   @func: callback function on filter match   @data: returned parameter for callback function   @ident: string for calling module identification   @sk: socket pointer (might be NULL)     Description:    Invokes the callback function with the received sk_buff and the given    parameter 'data' on a matching receive filter. A filter matches, when              <received_can_id> & mask == can_id & mask      The filter can be inverted (CAN_INV_FILTER bit set in can_id) or it can    filter for error message frames (CAN_ERR_FLAG bit set in mask).      The provided pointer to the sk_buff is guaranteed to be valid as long as    the callback function is running. The callback function must  not  free    the given sk_buff while processing it's task. When the given sk_buff is    needed after the end of the callback function it must be cloned inside    the callback function with skb_clone().     Return:    0 on success    -ENOMEM on missing cache mem to create subscription entry    -ENODEV unknown device ", "void can_rx_unregister(struct net *net, struct net_device *dev, canid_t can_id,       canid_t mask, void (*func)(struct sk_buff *, void *),       void *data)": "can_rx_unregister - unsubscribe CAN frames from a specific interface   @net: the applicable net namespace   @dev: pointer to netdevice (NULL => unsubscribe from 'all' CAN devices list)   @can_id: CAN identifier   @mask: CAN mask   @func: callback function on filter match   @data: returned parameter for callback function     Description:    Removes subscription entry depending on given (subscription) values. ", "int can_proto_register(const struct can_proto *cp)": "can_proto_register - register CAN transport protocol   @cp: pointer to CAN protocol structure     Return:    0 on success    -EINVAL invalid (out of range) protocol number    -EBUSY  protocol already in use    -ENOBUF if proto_register() fails ", "void can_proto_unregister(const struct can_proto *cp)": "can_proto_unregister - unregister CAN transport protocol   @cp: pointer to CAN protocol structure ", "struct net_device *upper_dev;upper_dev = netdev_master_upper_dev_get_rcu(dev);if (upper_dev)return __vlan_find_dev_deep_rcu(upper_dev,    vlan_proto, vlan_id);}return NULL;}EXPORT_SYMBOL(__vlan_find_dev_deep_rcu": "__vlan_find_dev_deep_rcu(struct net_device  dev,__be16 vlan_proto, u16 vlan_id){struct vlan_info  vlan_info = rcu_dereference(dev->vlan_info);if (vlan_info) {return vlan_group_get_device(&vlan_info->grp,     vlan_proto, vlan_id);} else {    Lower devices of master uppers (bonding, team) do not have   grp assigned to themselves. Grp is assigned to upper device   instead. ", "const struct netlbl_calipso_ops *netlbl_calipso_ops_register(const struct netlbl_calipso_ops *ops)": "netlbl_calipso_ops_register - Register the CALIPSO operations   @ops: ops to register     Description:   Register the CALIPSO packet engine operations.   ", "int netlbl_catmap_walk(struct netlbl_lsm_catmap *catmap, u32 offset)": "netlbl_catmap_walk - Walk a LSM secattr catmap looking for a bit   @catmap: the category bitmap   @offset: the offset to start searching at, in bits     Description:   This function walks a LSM secattr category bitmap starting at @offset and   returns the spot of the first set bit or -ENOENT if no bits are set.   ", "int netlbl_catmap_setbit(struct netlbl_lsm_catmap **catmap, u32 bit, gfp_t flags)": "netlbl_catmap_setbit - Set a bit in a LSM secattr catmap   @catmap: pointer to the category bitmap   @bit: the bit to set   @flags: memory allocation flags     Description:   Set the bit specified by @bit in @catmap.  Returns zero on success,   negative values on failure.   ", "int netlbl_bitmap_walk(const unsigned char *bitmap, u32 bitmap_len,       u32 offset, u8 state)": "netlbl_bitmap_walk - Walk a bitmap looking for a bit   @bitmap: the bitmap   @bitmap_len: length in bits   @offset: starting offset   @state: if non-zero, look for a set (1) bit else look for a cleared (0) bit     Description:   Starting at @offset, walk the bitmap from left to right until either the   desired bit is found or we reach the end.  Return the bit offset, -1 if   not found, or -2 if error. ", "void netlbl_bitmap_setbit(unsigned char *bitmap, u32 bit, u8 state)": "netlbl_bitmap_setbit - Sets a single bit in a bitmap   @bitmap: the bitmap   @bit: the bit   @state: if non-zero, set the bit (1) else clear the bit (0)     Description:   Set a single bit in the bitmask.  Returns zero on success, negative values   on error. ", "struct audit_buffer *netlbl_audit_start(int type,struct netlbl_audit *audit_info)": "netlbl_audit_start - Start an audit message   @type: audit message type   @audit_info: NetLabel audit information     Description:   Start an audit message using the type specified in @type and fill the audit   message with some fields common to all NetLabel audit messages.  This   function should only be used by protocol engines, not LSMs.  Returns a   pointer to the audit buffer on success, NULL on failure.   ", "int rxrpc_kernel_send_data(struct socket *sock, struct rxrpc_call *call,   struct msghdr *msg, size_t len,   rxrpc_notify_end_tx_t notify_end_tx)": "rxrpc_kernel_send_data - Allow a kernel service to send data on a call   @sock: The socket the call is on   @call: The call to send data through   @msg: The data to send   @len: The amount of data to send   @notify_end_tx: Notification that the last packet is queued.     Allow a kernel service to send data on a call.  The call must be in an state   appropriate to sending data.  No control data should be supplied in @msg,   nor should an address be supplied.  MSG_MORE should be flagged if there's   more data to come, otherwise this data will end the transmission phase. ", "bool rxrpc_kernel_abort_call(struct socket *sock, struct rxrpc_call *call,     u32 abort_code, int error, enum rxrpc_abort_reason why)": "rxrpc_kernel_abort_call - Allow a kernel service to abort a call   @sock: The socket the call is on   @call: The call to be aborted   @abort_code: The abort code to stick into the ABORT packet   @error: Local error value   @why: Indication as to why.     Allow a kernel service to abort a call, if it's still in an abortable state   and return true if the call was aborted, false if it was already complete. ", "void rxrpc_kernel_set_tx_length(struct socket *sock, struct rxrpc_call *call,s64 tx_total_len)": "rxrpc_kernel_set_tx_length - Set the total Tx length on a call   @sock: The socket the call is on   @call: The call to be informed   @tx_total_len: The amount of data to be transmitted for this call     Allow a kernel service to set the total transmit length on a call.  This   allows buffer-to-packet encrypt-and-copy to be performed.     This function is primarily for use for setting the reply length since the   request length can be set when beginning the call. ", "struct rxrpc_call *rxrpc_kernel_begin_call(struct socket *sock,   struct sockaddr_rxrpc *srx,   struct key *key,   unsigned long user_call_ID,   s64 tx_total_len,   u32 hard_timeout,   gfp_t gfp,   rxrpc_notify_rx_t notify_rx,   bool upgrade,   enum rxrpc_interruptibility interruptibility,   unsigned int debug_id)": "rxrpc_kernel_begin_call - Allow a kernel service to begin a call   @sock: The socket on which to make the call   @srx: The address of the peer to contact   @key: The security context to use (defaults to socket setting)   @user_call_ID: The ID to use   @tx_total_len: Total length of data to transmit during the call (or -1)   @hard_timeout: The maximum lifespan of the call in sec   @gfp: The allocation constraints   @notify_rx: Where to send notifications instead of socket queue   @upgrade: Request service upgrade for call   @interruptibility: The call is interruptible, or can be canceled.   @debug_id: The debug ID for tracing to be assigned to the call     Allow a kernel service to begin a call on the nominated socket.  This just   sets up all the internal tracking structures and allocates connection and   call IDs as appropriate.  The call to be used is returned.     The default socket destination address and security may be overridden by   supplying @srx and @key. ", "void rxrpc_kernel_shutdown_call(struct socket *sock, struct rxrpc_call *call)": "rxrpc_kernel_shutdown_call - Allow a kernel service to shut down a call it was using   @sock: The socket the call is on   @call: The call to end     Allow a kernel service to shut down a call it was using.  The call must be   complete before this is called (the call should be aborted if necessary). ", "void rxrpc_kernel_put_call(struct socket *sock, struct rxrpc_call *call)": "rxrpc_kernel_put_call - Release a reference to a call   @sock: The socket the call is on   @call: The call to put     Drop the application's ref on an rxrpc call. ", "bool rxrpc_kernel_check_life(const struct socket *sock,     const struct rxrpc_call *call)": "rxrpc_kernel_check_life - Check to see whether a call is still alive   @sock: The socket the call is on   @call: The call to check     Allow a kernel service to find out whether a call is still alive - whether   it has completed successfully and all received data has been consumed. ", "u32 rxrpc_kernel_get_epoch(struct socket *sock, struct rxrpc_call *call)": "rxrpc_kernel_get_epoch - Retrieve the epoch value from a call.   @sock: The socket the call is on   @call: The call to query     Allow a kernel service to retrieve the epoch value from a service call to   see if the client at the other end rebooted. ", "void rxrpc_kernel_new_call_notification(struct socket *sock,rxrpc_notify_new_call_t notify_new_call,rxrpc_discard_new_call_t discard_new_call)": "rxrpc_kernel_new_call_notification - Get notifications of new calls   @sock: The socket to intercept received messages on   @notify_new_call: Function to be called when new calls appear   @discard_new_call: Function to discard preallocated calls     Allow a kernel service to be given notifications about new calls. ", "void rxrpc_kernel_set_max_life(struct socket *sock, struct rxrpc_call *call,       unsigned long hard_timeout)": "rxrpc_kernel_set_max_life - Set maximum lifespan on a call   @sock: The socket the call is on   @call: The call to configure   @hard_timeout: The maximum lifespan of the call in jiffies     Set the maximum lifespan of a call.  The call will end with ETIME or   ETIMEDOUT if it takes longer than this. ", "int rxrpc_kernel_charge_accept(struct socket *sock,       rxrpc_notify_rx_t notify_rx,       rxrpc_user_attach_call_t user_attach_call,       unsigned long user_call_ID, gfp_t gfp,       unsigned int debug_id)": "rxrpc_kernel_charge_accept - Charge up socket with preallocated calls   @sock: The socket on which to preallocate   @notify_rx: Event notification function for the call   @user_attach_call: Func to attach call to user_call_ID   @user_call_ID: The tag to attach to the preallocated call   @gfp: The allocation conditions.   @debug_id: The tracing debug ID.     Charge up the socket with preallocated calls, each with a user ID.  A   function should be provided to effect the attachment from the user's side.   The user is given a ref to hold on the call.     Note that the call may be come connected before this function returns. ", "struct key *rxrpc_get_null_key(const char *keyname)": "rxrpc_get_null_key - Generate a null RxRPC key   @keyname: The name to give the key.     Generate a null RxRPC key that can be used to indicate anonymous security is   required for a particular domain. ", "int rxrpc_sock_set_security_keyring(struct sock *sk, struct key *keyring)": "rxrpc_sock_set_security_keyring - Set the security keyring for a kernel service   @sk: The socket to set the keyring on   @keyring: The keyring to set     Set the server security keyring on an rxrpc socket.  This is used to provide   the encryption keys for a kernel service. ", "void rxrpc_kernel_get_peer(struct socket *sock, struct rxrpc_call *call,   struct sockaddr_rxrpc *_srx)": "rxrpc_kernel_get_peer - Get the peer address of a call   @sock: The socket on which the call is in progress.   @call: The call to query   @_srx: Where to place the result     Get the address of the remote peer in a call. ", "bool rxrpc_kernel_get_srtt(struct socket *sock, struct rxrpc_call *call,   u32 *_srtt)": "rxrpc_kernel_get_srtt - Get a call's peer smoothed RTT   @sock: The socket on which the call is in progress.   @call: The call to query   @_srtt: Where to store the SRTT value.     Get the call's peer smoothed RTT in uS. ", "int rxrpc_kernel_recv_data(struct socket *sock, struct rxrpc_call *call,   struct iov_iter *iter, size_t *_len,   bool want_more, u32 *_abort, u16 *_service)": "rxrpc_kernel_recv_data - Allow a kernel service to receive datainfo   @sock: The socket that the call exists on   @call: The call to send data through   @iter: The buffer to receive into   @_len: The amount of data we want to receive (decreased on return)   @want_more: True if more data is expected to be read   @_abort: Where the abort code is stored if -ECONNABORTED is returned   @_service: Where to store the actual service ID (may be upgraded)     Allow a kernel service to receive data and pick up information about the   state of a call.  Returns 0 if got what was asked for and there's more   available, 1 if we got what was asked for and we're at the end of the data   and -EAGAIN if we need more data.     Note that we may return -EAGAIN to drain empty packets at the end of the   data, even if we've already copied over the requested data.      _abort should also be initialised to 0. ", "struct net_device *alloc_fcdev(int sizeof_priv)": "alloc_fcdev - Register fibre channel device   @sizeof_priv: Size of additional driver-private structure to be allocated  for this fibre channel device     Fill in the fields of the device structure with fibre channel-generic values.     Constructs a new net device, complete with a private data area of   size @sizeof_priv.  A 32-byte (not bit) alignment is enforced for   this private data area. ", "skb->dev = dev;skb_reset_mac_header(skb);/* point to frame control (FC) ": "fddi_type_trans(struct sk_buff  skb, struct net_device  dev){struct fddihdr  fddi = (struct fddihdr  )skb->data;__be16 type;    Set mac.raw field to point to FC byte, set data field to point   to start of packet data.  Assume 802.2 SNAP frames for now. ", "struct net_device *alloc_fddidev(int sizeof_priv)": "alloc_fddidev - Register FDDI device   @sizeof_priv: Size of additional driver-private structure to be allocated  for this FDDI device     Fill in the fields of the device structure with FDDI-generic values.     Constructs a new net device, complete with a private data area of   size @sizeof_priv.  A 32-byte (not bit) alignment is enforced for   this private data area. ", "skb->dev = dev;skb_reset_mac_header(skb);hip = (struct hippi_hdr *)skb_mac_header(skb);skb_pull(skb, HIPPI_HLEN);/* * No fancy promisc stuff here now. ": "hippi_type_trans(struct sk_buff  skb, struct net_device  dev){struct hippi_hdr  hip;    This is actually wrong ... question is if we really should   set the raw address here. ", "NEIGH_VAR_INIT(p, MCAST_PROBES, 0);/* In IPv6 unicast probes are valid even on NBMA,* because they are encapsulated in normal IPv6 protocol.* Should be a generic flag.": "hippi_neigh_setup_dev(struct net_device  dev, struct neigh_parms  p){  Never send broadcastmulticast ARP messages ", "struct net_device *alloc_hippi_dev(int sizeof_priv)": "alloc_hippi_dev - Register HIPPI device   @sizeof_priv: Size of additional driver-private structure to be allocated  for this HIPPI device     Fill in the fields of the device structure with HIPPI-generic values.     Constructs a new net device, complete with a private data area of   size @sizeof_priv.  A 32-byte (not bit) alignment is enforced for   this private data area. ", "if (dev->type == ARPHRD_LOCALTLK) ": "aarp_send_ddp(struct net_device  dev, struct sk_buff  skb,  struct atalk_addr  sa, void  hwaddr){static char ddp_eth_multicast[ETH_ALEN] ={ 0x09, 0x00, 0x07, 0xFF, 0xFF, 0xFF };int hash;struct aarp_entry  a;skb_reset_network_header(skb);  Check for LocalTalk first ", "static void atrtr_set_default(struct net_device *dev)": "atrtr_get_dev(struct atalk_addr  sa){struct atalk_route  atr = atrtr_find(sa);return atr ? atr->dev : NULL;}  Set up a default router ", "read_lock_bh(&atalk_interfaces_lock);for (iface = atalk_interfaces; iface; iface = iface->next) ": "atalk_find_dev_addr(struct net_device  dev){struct atalk_iface  iface = dev->atalk_ptr;return iface ? &iface->address : NULL;}static struct atalk_addr  atalk_find_primary(void){struct atalk_iface  fiface = NULL;struct atalk_addr  retval;struct atalk_iface  iface;    Return a point-to-point interface only if   there is no non-ptp interface available. ", "struct net_device *alloc_ltalkdev(int sizeof_priv)": "alloc_ltalkdev - Allocates and sets up an localtalk device   @sizeof_priv: Size of additional driver-private structure to be allocated  for this localtalk device     Fill in the fields of the device structure with localtalk-generic   values. Basically does everything except registering the device.     Constructs a new net device, complete with a private data area of   size @sizeof_priv.  A 32-byte (not bit) alignment is enforced for   this private data area. ", "atomic_dec(&wpan_phy_counter);kfree(rdev);return NULL;}/* atomic_inc_return makes it start at 1, make it start at 0 ": "wpan_phy_new(const struct cfg802154_ops  ops, size_t priv_size){static atomic_t wpan_phy_counter = ATOMIC_INIT(0);struct cfg802154_registered_device  rdev;size_t alloc_size;alloc_size = sizeof( rdev) + priv_size;rdev = kzalloc(alloc_size, GFP_KERNEL);if (!rdev)return NULL;rdev->ops = ops;rdev->wpan_phy_idx = atomic_inc_return(&wpan_phy_counter);if (unlikely(rdev->wpan_phy_idx < 0)) {  ugh, wrapped! ", "rtnl_unlock();/* TODO nl802154 phy notify ": "wpan_phy_register(struct wpan_phy  phy){struct cfg802154_registered_device  rdev = wpan_phy_to_rdev(phy);int ret;rtnl_lock();ret = device_add(&phy->dev);if (ret) {rtnl_unlock();return ret;}list_add_rcu(&rdev->list, &cfg802154_rdev_list);cfg802154_rdev_list_generation++;  TODO phy registered lock ", "/* TODO phy registered lock ": "wpan_phy_unregister(struct wpan_phy  phy){struct cfg802154_registered_device  rdev = wpan_phy_to_rdev(phy);wait_event(rdev->dev_wait, ({int __count;rtnl_lock();__count = rdev->opencount;rtnl_unlock();__count == 0; }));rtnl_lock();  TODO nl802154 phy notify ", "priv_size = ALIGN(sizeof(*local), NETDEV_ALIGN) + priv_data_len;phy = wpan_phy_new(&mac802154_config_ops, priv_size);if (!phy) ": "ieee802154_alloc_hw(size_t priv_data_len, const struct ieee802154_ops  ops){struct wpan_phy  phy;struct ieee802154_local  local;size_t priv_size;if (WARN_ON(!ops || !(ops->xmit_async || ops->xmit_sync) || !ops->ed ||    !ops->start || !ops->stop || !ops->set_channel))return NULL;  Ensure 32-byte alignment of our private data and hw private data.   We use the wpan_phy priv data for both our ieee802154_local and for   the driver's private data     in memory it'll be like this:     +-------------------------+   | struct wpan_phy         |   +-------------------------+   | struct ieee802154_local |   +-------------------------+   | driver's private data   |   +-------------------------+     Due to ieee802154 layer isn't aware of driver and MAC structures,   so lets align them here. ", "duration = 50 * NSEC_PER_USEC;else if (BIT(channel) & 0x7FE)/* 915 MHz BPSK802.15.4-2003: 40 ksym/s ": "ieee802154_configure_durations(struct wpan_phy  phy,    unsigned int page, unsigned int channel){u32 duration = 0;switch (page) {case 0:if (BIT(channel) & 0x1)  868 MHz BPSK 802.15.4-2003: 20 ksyms ", "if (hw->flags & IEEE802154_HW_TX_OMIT_CKSUM)max_sifs_size = IEEE802154_MAX_SIFS_FRAME_SIZE -IEEE802154_FCS_LEN;elsemax_sifs_size = IEEE802154_MAX_SIFS_FRAME_SIZE;if (skb->len > max_sifs_size)hrtimer_start(&local->ifs_timer,      hw->phy->lifs_period * NSEC_PER_USEC,      HRTIMER_MODE_REL);elsehrtimer_start(&local->ifs_timer,      hw->phy->sifs_period * NSEC_PER_USEC,      HRTIMER_MODE_REL);} else ": "ieee802154_xmit_complete(struct ieee802154_hw  hw, struct sk_buff  skb,      bool ifs_handling){struct ieee802154_local  local = hw_to_local(hw);local->tx_result = IEEE802154_SUCCESS;if (ifs_handling) {u8 max_sifs_size;  If transceiver sets CRC on his own we need to use lifs   threshold len above 16 otherwise 18, because it's not   part of skb->len. ", "if (*window_clamp == 0)(*window_clamp) = (U16_MAX << TCP_MAX_WSCALE);space = min(*window_clamp, space);/* Quantize space offering to a multiple of mss if possible. ": "tcp_select_initial_window(const struct sock  sk, int __space, __u32 mss,       __u32  rcv_wnd, __u32  window_clamp,       int wscale_ok, __u8  rcv_wscale,       __u32 init_rcv_wnd){unsigned int space = (__space < 0 ? 0 : __space);  If no clamp set the clamp to the max possible scaled window ", "void tcp_release_cb(struct sock *sk)": "tcp_release_cb - tcp release_sock() callback   @sk: socket     called from release_sock() to perform protocol dependent   actions before socket release. ", "mss_now = pmtu - icsk->icsk_af_ops->net_header_len - sizeof(struct tcphdr);/* IPv6 adds a frag_hdr in case RTAX_FEATURE_ALLFRAG is set ": "tcp_mtu_to_mss(struct sock  sk, int pmtu){const struct tcp_sock  tp = tcp_sk(sk);const struct inet_connection_sock  icsk = inet_csk(sk);int mss_now;  Calculate base mss without TCP options:   It is MMS_S - sizeof(tcphdr) of rfc1122 ", "if (icsk->icsk_af_ops->net_frag_header_len) ": "tcp_mss_to_mtu(struct sock  sk, int mss){const struct tcp_sock  tp = tcp_sk(sk);const struct inet_connection_sock  icsk = inet_csk(sk);int mtu;mtu = mss +      tp->tcp_header_len +      icsk->icsk_ext_hdr_len +      icsk->icsk_af_ops->net_header_len;  IPv6 adds a frag_hdr in case RTAX_FEATURE_ALLFRAG is set ", "icsk->icsk_pmtu_cookie = pmtu;if (icsk->icsk_mtup.enabled)mss_now = min(mss_now, tcp_mtu_to_mss(sk, icsk->icsk_mtup.search_low));tp->mss_cache = mss_now;return mss_now;}EXPORT_SYMBOL(tcp_sync_mss": "tcp_sync_mss(struct sock  sk, u32 pmtu){struct tcp_sock  tp = tcp_sk(sk);struct inet_connection_sock  icsk = inet_csk(sk);int mss_now;if (icsk->icsk_mtup.search_high > pmtu)icsk->icsk_mtup.search_high = pmtu;mss_now = tcp_mtu_to_mss(sk, pmtu);mss_now = tcp_bound_to_half_wnd(tp, mss_now);  And store cached results ", "struct sk_buff *tcp_make_synack(const struct sock *sk, struct dst_entry *dst,struct request_sock *req,struct tcp_fastopen_cookie *foc,enum tcp_synack_type synack_type,struct sk_buff *syn_skb)": "tcp_make_synack - Allocate one skb and build a SYNACK packet.   @sk: listener socket   @dst: dst entry attached to the SYNACK. It is consumed and caller         should not use it again.   @req: request_sock pointer   @foc: cookie for tcp fast open   @synack_type: Type of synack to prepare   @syn_skb: SYN packet just received.  It could be NULL for rtx case. ", "tp->tcp_header_len = sizeof(struct tcphdr);if (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_timestamps))tp->tcp_header_len += TCPOLEN_TSTAMP_ALIGNED;#ifdef CONFIG_TCP_MD5SIGif (tp->af_specific->md5_lookup(sk, sk))tp->tcp_header_len += TCPOLEN_MD5SIG_ALIGNED;#endif/* If user gave his TCP_MAXSEG, record it to clamp ": "tcp_connect_init(struct sock  sk){const struct dst_entry  dst = __sk_dst_get(sk);struct tcp_sock  tp = tcp_sk(sk);__u8 rcv_wscale;u32 rcv_wnd;  We'll fix this up when we get a response from the other end.   See tcp_input.c:tcp_rcv_state_process case TCP_SYN_SENT. ", "if (READ_ONCE(sk->sk_txrehash) == SOCK_TXREHASH_ENABLED)WRITE_ONCE(tcp_rsk(req)->txhash, net_tx_rndhash());res = af_ops->send_synack(sk, NULL, &fl, req, NULL, TCP_SYNACK_NORMAL,  NULL);if (!res) ": "tcp_rtx_synack(const struct sock  sk, struct request_sock  req){const struct tcp_request_sock_ops  af_ops = tcp_rsk(req)->af_specific;struct flowi fl;int res;  Paired with WRITE_ONCE() in sock_setsockopt() ", "u32 options = tcp_opt->rcv_tsecr;if (!tcp_opt->saw_tstamp)  ": "cookie_timestamp_decode(const struct net  net,     struct tcp_options_received  tcp_opt){  echoed timestamp, lowest bits contain options ", "struct net_device *__ip_dev_find(struct net *net, __be32 addr, bool devref)": "__ip_dev_find - find the first device with a given source address.   @net: the net namespace   @addr: the source address   @devref: if true, take a reference on the found device     If a caller uses devref=false, it should be protected by RCU, or RTNL ", "if (master_idx &&    (dev = dev_get_by_index_rcu(net, master_idx)) &&    (in_dev = __in_dev_get_rcu(dev))) ": "inet_select_addr(const struct net_device  dev, __be32 dst, int scope){const struct in_ifaddr  ifa;__be32 addr = 0;unsigned char localnet_scope = RT_SCOPE_HOST;struct in_device  in_dev;struct net  net = dev_net(dev);int master_idx;rcu_read_lock();in_dev = __in_dev_get_rcu(dev);if (!in_dev)goto no_in_dev;if (unlikely(IN_DEV_ROUTE_LOCALNET(in_dev)))localnet_scope = RT_SCOPE_LINK;in_dev_for_each_ifa_rcu(ifa, in_dev) {if (ifa->ifa_flags & IFA_F_SECONDARY)continue;if (min(ifa->ifa_scope, localnet_scope) > scope)continue;if (!dst || inet_ifa_match(dst, ifa)) {addr = ifa->ifa_local;break;}if (!addr)addr = ifa->ifa_local;}if (addr)goto out_unlock;no_in_dev:master_idx = l3mdev_master_ifindex_rcu(dev);  For VRFs, the VRF device takes the place of the loopback device,   with addresses on it being preferred.  Note in such cases the   loopback device will be among the devices that fail the master_idx   equality check in the loop below. ", "if (!inet->inet_rcv_saddr) ": "ip4_datagram_connect(struct sock  sk, struct sockaddr  uaddr, int addr_len){struct inet_sock  inet = inet_sk(sk);struct sockaddr_in  usin = (struct sockaddr_in  ) uaddr;struct flowi4  fl4;struct rtable  rt;__be32 saddr;int oif;int err;if (addr_len < sizeof( usin))return -EINVAL;if (usin->sin_family != AF_INET)return -EAFNOSUPPORT;sk_dst_reset(sk);oif = sk->sk_bound_dev_if;saddr = inet->inet_saddr;if (ipv4_is_multicast(usin->sin_addr.s_addr)) {if (!oif || netif_index_is_l3_master(sock_net(sk), oif))oif = inet->mc_index;if (!saddr)saddr = inet->mc_addr;} else if (!oif) {oif = inet->uc_index;}fl4 = &inet->cork.fl.u.ip4;rt = ip_route_connect(fl4, usin->sin_addr.s_addr, saddr, oif,      sk->sk_protocol, inet->inet_sport,      usin->sin_port, sk);if (IS_ERR(rt)) {err = PTR_ERR(rt);if (err == -ENETUNREACH)IP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);goto out;}if ((rt->rt_flags & RTCF_BROADCAST) && !sock_flag(sk, SOCK_BROADCAST)) {ip_rt_put(rt);err = -EACCES;goto out;}if (!inet->inet_saddr)inet->inet_saddr = fl4->saddr;  Update source address ", "int ip_local_deliver(struct sk_buff *skb)": "ip_local_deliver_finish(struct net  net, struct sock  sk, struct sk_buff  skb){skb_clear_delivery_time(skb);__skb_pull(skb, skb_network_header_len(skb));rcu_read_lock();ip_protocol_deliver_rcu(net, skb, ip_hdr(skb)->protocol);rcu_read_unlock();return 0;}    Deliver IP Packets to the higher protocol layers. ", "if (flags & IP_CMSG_PKTINFO) ": "ip_cmsg_recv_offset(struct msghdr  msg, struct sock  sk, struct sk_buff  skb, int tlen, int offset){struct inet_sock  inet = inet_sk(sk);unsigned int flags = inet->cmsg_flags;  Ordered by supposed usage frequency ", "if (optname == IP_ROUTER_ALERT)return ip_ra_control(sk, val ? 1 : 0, NULL);if (ip_mroute_opt(optname))return ip_mroute_setsockopt(sk, optname, optval, optlen);err = 0;if (needs_rtnl)rtnl_lock();sockopt_lock_sock(sk);switch (optname) ": "ip_setsockopt(struct sock  sk, int level, int optname,     sockptr_t optval, unsigned int optlen){struct inet_sock  inet = inet_sk(sk);struct net  net = sock_net(sk);int val = 0, err;bool needs_rtnl = setsockopt_needs_rtnl(optname);switch (optname) {case IP_PKTINFO:case IP_RECVTTL:case IP_RECVOPTS:case IP_RECVTOS:case IP_RETOPTS:case IP_TOS:case IP_TTL:case IP_HDRINCL:case IP_MTU_DISCOVER:case IP_RECVERR:case IP_ROUTER_ALERT:case IP_FREEBIND:case IP_PASSSEC:case IP_TRANSPARENT:case IP_MINTTL:case IP_NODEFRAG:case IP_BIND_ADDRESS_NO_PORT:case IP_UNICAST_IF:case IP_MULTICAST_TTL:case IP_MULTICAST_ALL:case IP_MULTICAST_LOOP:case IP_RECVORIGDSTADDR:case IP_CHECKSUM:case IP_RECVFRAGSIZE:case IP_RECVERR_RFC4884:case IP_LOCAL_PORT_RANGE:if (optlen >= sizeof(int)) {if (copy_from_sockptr(&val, optval, sizeof(val)))return -EFAULT;} else if (optlen >= sizeof(char)) {unsigned char ucval;if (copy_from_sockptr(&ucval, optval, sizeof(ucval)))return -EFAULT;val = (int) ucval;}}  If optlen==0, it is equivalent to val == 0 ", "if (err == -ENOPROTOOPT && optname != IP_PKTOPTIONS &&!ip_mroute_opt(optname)) ": "ip_getsockopt(struct sock  sk, int level, int optname,     sockptr_t optval, sockptr_t optlen){struct inet_sock  inet = inet_sk(sk);bool needs_rtnl = getsockopt_needs_rtnl(optname);int val, err = 0;int len;if (level != SOL_IP)return -EOPNOTSUPP;if (ip_mroute_opt(optname))return ip_mroute_getsockopt(sk, optname, optval, optlen);if (copy_from_sockptr(&len, optlen, sizeof(int)))return -EFAULT;if (len < 0)return -EINVAL;if (needs_rtnl)rtnl_lock();sockopt_lock_sock(sk);switch (optname) {case IP_OPTIONS:{unsigned char optbuf[sizeof(struct ip_options)+40];struct ip_options  opt = (struct ip_options  )optbuf;struct ip_options_rcu  inet_opt;inet_opt = rcu_dereference_protected(inet->inet_opt,     lockdep_sock_is_held(sk));opt->optlen = 0;if (inet_opt)memcpy(optbuf, &inet_opt->opt,       sizeof(struct ip_options) +       inet_opt->opt.optlen);sockopt_release_sock(sk);if (opt->optlen == 0) {len = 0;return copy_to_sockptr(optlen, &len, sizeof(int));}ip_options_undo(opt);len = min_t(unsigned int, len, opt->optlen);if (copy_to_sockptr(optlen, &len, sizeof(int)))return -EFAULT;if (copy_to_sockptr(optval, opt->__data, len))return -EFAULT;return 0;}case IP_PKTINFO:val = (inet->cmsg_flags & IP_CMSG_PKTINFO) != 0;break;case IP_RECVTTL:val = (inet->cmsg_flags & IP_CMSG_TTL) != 0;break;case IP_RECVTOS:val = (inet->cmsg_flags & IP_CMSG_TOS) != 0;break;case IP_RECVOPTS:val = (inet->cmsg_flags & IP_CMSG_RECVOPTS) != 0;break;case IP_RETOPTS:val = (inet->cmsg_flags & IP_CMSG_RETOPTS) != 0;break;case IP_PASSSEC:val = (inet->cmsg_flags & IP_CMSG_PASSSEC) != 0;break;case IP_RECVORIGDSTADDR:val = (inet->cmsg_flags & IP_CMSG_ORIGDSTADDR) != 0;break;case IP_CHECKSUM:val = (inet->cmsg_flags & IP_CMSG_CHECKSUM) != 0;break;case IP_RECVFRAGSIZE:val = (inet->cmsg_flags & IP_CMSG_RECVFRAGSIZE) != 0;break;case IP_TOS:val = inet->tos;break;case IP_TTL:{struct net  net = sock_net(sk);val = (inet->uc_ttl == -1 ?       READ_ONCE(net->ipv4.sysctl_ip_default_ttl) :       inet->uc_ttl);break;}case IP_HDRINCL:val = inet->hdrincl;break;case IP_NODEFRAG:val = inet->nodefrag;break;case IP_BIND_ADDRESS_NO_PORT:val = inet->bind_address_no_port;break;case IP_MTU_DISCOVER:val = inet->pmtudisc;break;case IP_MTU:{struct dst_entry  dst;val = 0;dst = sk_dst_get(sk);if (dst) {val = dst_mtu(dst);dst_release(dst);}if (!val) {sockopt_release_sock(sk);return -ENOTCONN;}break;}case IP_RECVERR:val = inet->recverr;break;case IP_RECVERR_RFC4884:val = inet->recverr_rfc4884;break;case IP_MULTICAST_TTL:val = inet->mc_ttl;break;case IP_MULTICAST_LOOP:val = inet->mc_loop;break;case IP_UNICAST_IF:val = (__force int)htonl((__u32) inet->uc_index);break;case IP_MULTICAST_IF:{struct in_addr addr;len = min_t(unsigned int, len, sizeof(struct in_addr));addr.s_addr = inet->mc_addr;sockopt_release_sock(sk);if (copy_to_sockptr(optlen, &len, sizeof(int)))return -EFAULT;if (copy_to_sockptr(optval, &addr, len))return -EFAULT;return 0;}case IP_MSFILTER:{struct ip_msfilter msf;if (len < IP_MSFILTER_SIZE(0)) {err = -EINVAL;goto out;}if (copy_from_sockptr(&msf, optval, IP_MSFILTER_SIZE(0))) {err = -EFAULT;goto out;}err = ip_mc_msfget(sk, &msf, optval, optlen);goto out;}case MCAST_MSFILTER:if (in_compat_syscall())err = compat_ip_get_mcast_msfilter(sk, optval, optlen,   len);elseerr = ip_get_mcast_msfilter(sk, optval, optlen, len);goto out;case IP_MULTICAST_ALL:val = inet->mc_all;break;case IP_PKTOPTIONS:{struct msghdr msg;sockopt_release_sock(sk);if (sk->sk_type != SOCK_STREAM)return -ENOPROTOOPT;if (optval.is_kernel) {msg.msg_control_is_user = false;msg.msg_control = optval.kernel;} else {msg.msg_control_is_user = true;msg.msg_control_user = optval.user;}msg.msg_controllen = len;msg.msg_flags = in_compat_syscall() ? MSG_CMSG_COMPAT : 0;if (inet->cmsg_flags & IP_CMSG_PKTINFO) {struct in_pktinfo info;info.ipi_addr.s_addr = inet->inet_rcv_saddr;info.ipi_spec_dst.s_addr = inet->inet_rcv_saddr;info.ipi_ifindex = inet->mc_index;put_cmsg(&msg, SOL_IP, IP_PKTINFO, sizeof(info), &info);}if (inet->cmsg_flags & IP_CMSG_TTL) {int hlim = inet->mc_ttl;put_cmsg(&msg, SOL_IP, IP_TTL, sizeof(hlim), &hlim);}if (inet->cmsg_flags & IP_CMSG_TOS) {int tos = inet->rcv_tos;put_cmsg(&msg, SOL_IP, IP_TOS, sizeof(tos), &tos);}len -= msg.msg_controllen;return copy_to_sockptr(optlen, &len, sizeof(int));}case IP_FREEBIND:val = inet->freebind;break;case IP_TRANSPARENT:val = inet->transparent;break;case IP_MINTTL:val = inet->min_ttl;break;case IP_LOCAL_PORT_RANGE:val = inet->local_port_range.hi << 16 | inet->local_port_range.lo;break;case IP_PROTOCOL:val = inet_sk(sk)->inet_num;break;default:sockopt_release_sock(sk);return -ENOPROTOOPT;}sockopt_release_sock(sk);if (len < sizeof(int) && len > 0 && val >= 0 && val <= 255) {unsigned char ucval = (unsigned char)val;len = 1;if (copy_to_sockptr(optlen, &len, sizeof(int)))return -EFAULT;if (copy_to_sockptr(optval, &ucval, 1))return -EFAULT;} else {len = min_t(unsigned int, sizeof(int), len);if (copy_to_sockptr(optlen, &len, sizeof(int)))return -EFAULT;if (copy_to_sockptr(optval, &val, len))return -EFAULT;}return 0;out:sockopt_release_sock(sk);if (needs_rtnl)rtnl_unlock();return err;}int ip_getsockopt(struct sock  sk, int level,  int optname, char __user  optval, int __user  optlen){int err;err = do_ip_getsockopt(sk, level, optname,       USER_SOCKPTR(optval), USER_SOCKPTR(optlen));#if IS_ENABLED(CONFIG_BPFILTER_UMH)if (optname >= BPFILTER_IPT_SO_GET_INFO &&    optname < BPFILTER_IPT_GET_MAX)err = bpfilter_ip_get_sockopt(sk, optname, optval, optlen);#endif#ifdef CONFIG_NETFILTER  we need to exclude all possible ENOPROTOOPTs except default case ", "static void ip_expire(struct timer_list *t)": "ip_defrag_user_in_between(user, IP_DEFRAG_CONNTRACK_IN, __IP_DEFRAG_CONNTRACK_IN_END) ||       ip_defrag_user_in_between(user, IP_DEFRAG_CONNTRACK_BRIDGE_IN, __IP_DEFRAG_CONNTRACK_BRIDGE_IN);}    Oops, a fragment queue timed out.  Kill it and send an ICMP reply. ", "*sport = e->sport ? : udp_flow_src_port(dev_net(skb->dev),skb, 0, 0, false);hdrlen = sizeof(struct guehdr) + optlen;skb_push(skb, hdrlen);guehdr = (struct guehdr *)skb->data;guehdr->control = 0;guehdr->version = 0;guehdr->hlen = optlen >> 2;guehdr->flags = 0;guehdr->proto_ctype = *protocol;data = &guehdr[1];if (need_priv) ": "__gue_build_header(struct sk_buff  skb, struct ip_tunnel_encap  e,       u8  protocol, __be16  sport, int type){struct guehdr  guehdr;size_t hdrlen, optlen = 0;void  data;bool need_priv = false;int err;if ((e->flags & TUNNEL_ENCAP_FLAG_REMCSUM) &&    skb->ip_summed == CHECKSUM_PARTIAL) {optlen += GUE_PLEN_REMCSUM;type |= SKB_GSO_TUNNEL_REMCSUM;need_priv = true;}optlen += need_priv ? GUE_LEN_PRIV : 0;err = iptunnel_handle_offloads(skb, type);if (err)return err;  Get source port (based on flow hash) before skb_push ", "error = -EINVAL;if (sk->sk_state != TCP_LISTEN)goto out_err;/* Find already established connection ": "inet_csk_accept(struct sock  sk, int flags, int  err, bool kern){struct inet_connection_sock  icsk = inet_csk(sk);struct request_sock_queue  queue = &icsk->icsk_accept_queue;struct request_sock  req;struct sock  newsk;int error;lock_sock(sk);  We need to make sure that this socket is listening,   and that it has something pending. ", "WARN_ON(!sk_unhashed(sk));/* If it has not 0 inet_sk(sk)->inet_num, it must be bound ": "inet_csk_destroy_sock(struct sock  sk){WARN_ON(sk->sk_state != TCP_CLOSE);WARN_ON(!sock_flag(sk, SOCK_DEAD));  It cannot be in hash table! ", "bh_unlock_sock(sk);sock_put(sk);inet_csk_prepare_for_destroy_sock(sk);inet_sk(sk)->inet_num = 0;}EXPORT_SYMBOL(inet_csk_prepare_forced_close": "inet_csk_prepare_forced_close(struct sock  sk)__releases(&sk->sk_lock.slock){  sk_clone_lock locked the socket and set refcnt to 2 ", "struct sock *inet_csk_clone_lock(const struct sock *sk, const struct request_sock *req, const gfp_t priority)": "inet_csk_reqsk_queue_added(sk);}EXPORT_SYMBOL_GPL(inet_csk_reqsk_queue_hash_add);static void inet_clone_ulp(const struct request_sock  req, struct sock  newsk,   const gfp_t priority){struct inet_connection_sock  icsk = inet_csk(newsk);if (!icsk->icsk_ulp_ops)return;icsk->icsk_ulp_ops->clone(req, newsk, priority);}    inet_csk_clone_lock - clone an inet socket, and lock its clone  @sk: the socket to clone  @req: request_sock  @priority: for allocation (%GFP_KERNEL, %GFP_ATOMIC, etc)    Caller must unlock socket even in error path (bh_unlock_sock(newsk)) ", "struct request_sock *nreq;/* hold a refcnt for the nreq->rsk_listener * which is assigned in inet_reqsk_clone() ": "inet_csk_complete_hashdance(struct sock  sk, struct sock  child, struct request_sock  req, bool own_req){if (own_req) {inet_csk_reqsk_queue_drop(req->rsk_listener, req);reqsk_queue_removed(&inet_csk(req->rsk_listener)->icsk_accept_queue, req);if (sk != req->rsk_listener) {  another listening sk has been selected,   migrate the req to it. ", "tp->fastopen_req = kzalloc(sizeof(*tp->fastopen_req),   sk->sk_allocation);if (tp->fastopen_req)tp->fastopen_req->cookie = cookie;else*err = -ENOBUFS;}return false;}EXPORT_SYMBOL(tcp_fastopen_defer_connect": "tcp_fastopen_defer_connect(struct sock  sk, int  err){struct tcp_fastopen_cookie cookie = { .len = 0 };struct tcp_sock  tp = tcp_sk(sk);u16 mss;if (tp->fastopen_connect && !tp->fastopen_req) {if (tcp_fastopen_cookie_check(sk, &mss, &cookie)) {inet_sk(sk)->defer_connect = 1;return true;}  Alloc fastopen_req in order for FO option to be included   in SYN ", "/* Out of window, send ACK ": "tcp_timewait_state_process(struct inet_timewait_sock  tw, struct sk_buff  skb,   const struct tcphdr  th){struct tcp_options_received tmp_opt;struct tcp_timewait_sock  tcptw = tcp_twsk((struct sock  )tw);bool paws_reject = false;tmp_opt.saw_tstamp = 0;if (th->doff > (sizeof( th) >> 2) && tcptw->tw_ts_recent_stamp) {tcp_parse_options(twsk_net(tw), skb, &tmp_opt, 0, NULL);if (tmp_opt.saw_tstamp) {if (tmp_opt.rcv_tsecr)tmp_opt.rcv_tsecr -= tcptw->tw_ts_offset;tmp_opt.ts_recent= tcptw->tw_ts_recent;tmp_opt.ts_recent_stamp= tcptw->tw_ts_recent_stamp;paws_reject = tcp_paws_reject(&tmp_opt, th->rst);}}if (tw->tw_substate == TCP_FIN_WAIT2) {  Just repeat all the checks of tcp_rcv_state_process() ", "tcptw->tw_md5_key = NULL;if (!static_branch_unlikely(&tcp_md5_needed.key))return;key = tp->af_specific->md5_lookup(sk, sk);if (key) ": "tcp_time_wait_init(struct sock  sk, struct tcp_timewait_sock  tcptw){#ifdef CONFIG_TCP_MD5SIGconst struct tcp_sock  tp = tcp_sk(sk);struct tcp_md5sig_key  key;    The timewait bucket does not have the key DB from the   sock structure. We just make a quick copy of the   md5 key being used (if indeed we are using one)   so the timewait ack generating code has the key. ", "req->rsk_window_clamp = window_clamp ? : dst_metric(dst, RTAX_WINDOW);/* limit the window selection if the user enforce a smaller rx buffer ": "tcp_openreq_init_rwin(struct request_sock  req,   const struct sock  sk_listener,   const struct dst_entry  dst){struct inet_request_sock  ireq = inet_rsk(req);const struct tcp_sock  tp = tcp_sk(sk_listener);int full_space = tcp_full_space(sk_listener);u32 window_clamp;__u8 rcv_wscale;u32 rcv_wnd;int mss;mss = tcp_mss_clamp(tp, dst_metric_advmss(dst));window_clamp = READ_ONCE(tp->window_clamp);  Set this up on the first call only ", "newtp->pred_flags = 0;seq = treq->rcv_isn + 1;newtp->rcv_wup = seq;WRITE_ONCE(newtp->copied_seq, seq);WRITE_ONCE(newtp->rcv_nxt, seq);newtp->segs_in = 1;seq = treq->snt_isn + 1;newtp->snd_sml = newtp->snd_una = seq;WRITE_ONCE(newtp->snd_nxt, seq);newtp->snd_up = seq;INIT_LIST_HEAD(&newtp->tsq_node);INIT_LIST_HEAD(&newtp->tsorted_sent_queue);tcp_init_wl(newtp, treq->rcv_isn);minmax_reset(&newtp->rtt_min, tcp_jiffies32, ~0U);newicsk->icsk_ack.lrcvtime = tcp_jiffies32;newtp->lsndtime = tcp_jiffies32;newsk->sk_txhash = READ_ONCE(treq->txhash);newtp->total_retrans = req->num_retrans;tcp_init_xmit_timers(newsk);WRITE_ONCE(newtp->write_seq, newtp->pushed_seq = treq->snt_isn + 1);if (sock_flag(newsk, SOCK_KEEPOPEN))inet_csk_reset_keepalive_timer(newsk,       keepalive_time_when(newtp));newtp->rx_opt.tstamp_ok = ireq->tstamp_ok;newtp->rx_opt.sack_ok = ireq->sack_ok;newtp->window_clamp = req->rsk_window_clamp;newtp->rcv_ssthresh = req->rsk_rcv_wnd;newtp->rcv_wnd = req->rsk_rcv_wnd;newtp->rx_opt.wscale_ok = ireq->wscale_ok;if (newtp->rx_opt.wscale_ok) ": "tcp_create_openreq_child(const struct sock  sk,      struct request_sock  req,      struct sk_buff  skb){struct sock  newsk = inet_csk_clone_lock(sk, req, GFP_ATOMIC);const struct inet_request_sock  ireq = inet_rsk(req);struct tcp_request_sock  treq = tcp_rsk(req);struct inet_connection_sock  newicsk;const struct tcp_sock  oldtp;struct tcp_sock  newtp;u32 seq;if (!newsk)return NULL;newicsk = inet_csk(newsk);newtp = tcp_sk(newsk);oldtp = tcp_sk(sk);smc_check_reset_syn_req(oldtp, req, newtp);  Now setup tcp_sock ", "tmp_opt.ts_recent_stamp = ktime_get_seconds() - reqsk_timeout(req, TCP_RTO_MAX) / HZ;paws_reject = tcp_paws_reject(&tmp_opt, th->rst);}}/* Check for pure retransmitted SYN. ": "tcp_check_req(struct sock  sk, struct sk_buff  skb,   struct request_sock  req,   bool fastopen, bool  req_stolen){struct tcp_options_received tmp_opt;struct sock  child;const struct tcphdr  th = tcp_hdr(skb);__be32 flg = tcp_flag_word(th) & (TCP_FLAG_RST|TCP_FLAG_SYN|TCP_FLAG_ACK);bool paws_reject = false;bool own_req;tmp_opt.saw_tstamp = 0;if (th->doff > (sizeof(struct tcphdr)>>2)) {tcp_parse_options(sock_net(sk), skb, &tmp_opt, 0, NULL);if (tmp_opt.saw_tstamp) {tmp_opt.ts_recent = READ_ONCE(req->ts_recent);if (tmp_opt.rcv_tsecr)tmp_opt.rcv_tsecr -= tcp_rsk(req)->ts_off;  We do not store true stamp, but it is not required,   it can be estimated (approximately)   from another data. ", "sk_mark_napi_id_set(child, skb);tcp_segs_in(tcp_sk(child), skb);if (!sock_owned_by_user(child)) ": "tcp_child_process(struct sock  parent, struct sock  child,      struct sk_buff  skb)__releases(&((child)->sk_lock.slock)){int ret = 0;int state = child->sk_state;  record sk_napi_id and sk_rx_queue_mapping of child. ", "if (old_state != TCP_LISTEN) ": "inet_listen(struct socket  sock, int backlog){struct sock  sk = sock->sk;unsigned char old_state;int err, tcp_fastopen;lock_sock(sk);err = -EINVAL;if (sock->state != SS_UNCONNECTED || sock->type != SOCK_STREAM)goto out;old_state = sk->sk_state;if (!((1 << old_state) & (TCPF_CLOSE | TCPF_LISTEN)))goto out;WRITE_ONCE(sk->sk_max_ack_backlog, backlog);  Really, if the socket is already in listen state   we can only allow the backlog to be adjusted. ", "ip_mc_drop_socket(sk);/* If linger is set, we don't return until the close * is complete.  Otherwise we return immediately. The * actually closing is done the same either way. * * If the close is due to the process exiting, we never * linger.. ": "inet_release(struct socket  sock){struct sock  sk = sock->sk;if (sk) {long timeout;if (!sk->sk_kern_sock)BPF_CGROUP_RUN_PROG_INET_SOCK_RELEASE(sk);  Applications forget to leave groups before exiting ", "if (sk->sk_prot->bind) ": "inet_bind(struct socket  sock, struct sockaddr  uaddr, int addr_len){struct sock  sk = sock->sk;u32 flags = BIND_WITH_LOCK;int err;  If the socket has its own bind function then use it. (RAW) ", "prot = READ_ONCE(sk->sk_prot);if (uaddr->sa_family == AF_UNSPEC)return prot->disconnect(sk, flags);if (BPF_CGROUP_PRE_CONNECT_ENABLED(sk)) ": "inet_dgram_connect(struct socket  sock, struct sockaddr  uaddr,       int addr_len, int flags){struct sock  sk = sock->sk;const struct proto  prot;int err;if (addr_len < sizeof(uaddr->sa_family))return -EINVAL;  IPV6_ADDRFORM can change sk->sk_prot under us. ", "if (uaddr) ": "__inet_stream_connect(struct socket  sock, struct sockaddr  uaddr,  int addr_len, int flags, int is_sendmsg){struct sock  sk = sock->sk;int err;long timeo;    uaddr can be NULL and addr_len can be 0 if:   sk is a TCP fastopen active socket and   TCP_FASTOPEN_CONNECT sockopt is set and   we already have a valid cookie for this socket.   In this case, user can call write() after connect().   write() will invoke tcp_sendmsg_fastopen() which calls   __inet_stream_connect(). ", "#define pr_fmt(fmt) \"IPv4: \" fmt#include <linux/err.h>#include <linux/errno.h>#include <linux/types.h>#include <linux/socket.h>#include <linux/in.h>#include <linux/kernel.h>#include <linux/kmod.h>#include <linux/sched.h>#include <linux/timer.h>#include <linux/string.h>#include <linux/sockios.h>#include <linux/net.h>#include <linux/capability.h>#include <linux/fcntl.h>#include <linux/mm.h>#include <linux/interrupt.h>#include <linux/stat.h>#include <linux/init.h>#include <linux/poll.h>#include <linux/netfilter_ipv4.h>#include <linux/random.h>#include <linux/slab.h>#include <linux/uaccess.h>#include <linux/inet.h>#include <linux/igmp.h>#include <linux/inetdevice.h>#include <linux/netdevice.h>#include <net/checksum.h>#include <net/ip.h>#include <net/protocol.h>#include <net/arp.h>#include <net/route.h>#include <net/ip_fib.h>#include <net/inet_connection_sock.h>#include <net/gro.h>#include <net/gso.h>#include <net/tcp.h>#include <net/udp.h>#include <net/udplite.h>#include <net/ping.h>#include <linux/skbuff.h>#include <net/sock.h>#include <net/raw.h>#include <net/icmp.h>#include <net/inet_common.h>#include <net/ip_tunnels.h>#include <net/xfrm.h>#include <net/net_namespace.h>#include <net/secure_seq.h>#ifdef CONFIG_IP_MROUTE#include <linux/mroute.h>#endif#include <net/l3mdev.h>#include <net/compat.h>#include <trace/events/sock.h>/* The inetsw table contains everything that inet_create needs to * build a new socket. ": "inet_stream_connect TCP race. ", "int inet_accept(struct socket *sock, struct socket *newsock, int flags,bool kern)": "inet_accept(struct socket  sock, struct socket  newsock, struct sock  newsk){sock_rps_record_flow(newsk);WARN_ON(!((1 << newsk->sk_state) &  (TCPF_ESTABLISHED | TCPF_SYN_RECV |  TCPF_CLOSE_WAIT | TCPF_CLOSE)));if (test_bit(SOCK_SUPPORT_ZC, &sock->flags))set_bit(SOCK_SUPPORT_ZC, &newsock->flags);sock_graft(newsk, newsock);newsock->state = SS_CONNECTED;}   Accept a pending connection. The TCP layer now gives BSD semantics. ", "how++; /* maps 0->1 has the advantage of making bit 1 rcvs and       1->2 bit 2 snds.       2->3 ": "inet_shutdown(struct socket  sock, int how){struct sock  sk = sock->sk;int err = 0;  This should really check to make sure   the socket is a TCP socket. (WHY AC...) ", "last_perm = &inetsw[p->type];list_for_each(lh, &inetsw[p->type]) ": "inet_register_protosw(struct inet_protosw  p){struct list_head  lh;struct inet_protosw  answer;int protocol = p->protocol;struct list_head  last_perm;spin_lock_bh(&inetsw_lock);if (p->type >= SOCK_MAX)goto out_illegal;  If we are trying to override a permanent protocol, bail. ", "if (rt)return 0;/* Reroute. ": "inet_sk_rebuild_header(struct sock  sk){struct inet_sock  inet = inet_sk(sk);struct rtable  rt = (struct rtable  )__sk_dst_check(sk, 0);__be32 daddr;struct ip_options_rcu  inet_opt;struct flowi4  fl4;int err;  Route is OK, nothing to do. ", "__be32 inet_current_timestamp(void)": "inet_current_timestamp - Return IP network timestamp     Return milliseconds since midnight in network byte order. ", "if (!encap_type)return 1;/* If this is a paged skb, make sure we pull up * whatever data we need to look at. ": "xfrm4_udp_encap_rcv(struct sock  sk, struct sk_buff  skb){struct udp_sock  up = udp_sk(sk);struct udphdr  uh;struct iphdr  iph;int iphlen, len;__u8  udpdata;__be32  udpdata32;__u16 encap_type = up->encap_type;  if this is not encapsulated socket, then just return now ", "int xfrm4_udp_encap_rcv(struct sock *sk, struct sk_buff *skb)": "xfrm4_rcv_encap_finish2(struct net  net, struct sock  sk,   struct sk_buff  skb){return dst_input(skb);}static inline int xfrm4_rcv_encap_finish(struct net  net, struct sock  sk, struct sk_buff  skb){if (!skb_dst(skb)) {const struct iphdr  iph = ip_hdr(skb);if (ip_route_input_noref(skb, iph->daddr, iph->saddr, iph->tos, skb->dev))goto drop;}if (xfrm_trans_queue(skb, xfrm4_rcv_encap_finish2))goto drop;return 0;drop:kfree_skb(skb);return NET_RX_DROP;}int xfrm4_transport_finish(struct sk_buff  skb, int async){struct xfrm_offload  xo = xfrm_offload(skb);struct iphdr  iph = ip_hdr(skb);iph->protocol = XFRM_MODE_SKB_CB(skb)->protocol;#ifndef CONFIG_NETFILTERif (!async)return -iph->protocol;#endif__skb_push(skb, skb->data - skb_network_header(skb));iph->tot_len = htons(skb->len);ip_send_check(iph);if (xo && (xo->flags & XFRM_GRO)) {skb_mac_header_rebuild(skb);skb_reset_transport_header(skb);return 0;}NF_HOOK(NFPROTO_IPV4, NF_INET_PRE_ROUTING,dev_net(skb->dev), NULL, skb, skb->dev, NULL,xfrm4_rcv_encap_finish);return 0;}  If it's a keepalive packet, then just eat it.   If it's an encapsulated packet, then pass it to the   IPsec xfrm input.   Returns 0 if skb passed to xfrm or was dropped.   Returns >0 if skb should be passed to UDP.   Returns <0 if skb should be resubmitted (-ret is protocol) ", "rcu_read_lock();inet_opt = rcu_dereference(inet->inet_opt);fl4 = &fl->u.ip4;rt = skb_rtable(skb);if (rt)goto packet_routed;/* Make sure we can route this packet. ": "__ip_queue_xmit(struct sock  sk, struct sk_buff  skb, struct flowi  fl,    __u8 tos){struct inet_sock  inet = inet_sk(sk);struct net  net = sock_net(sk);struct ip_options_rcu  inet_opt;struct flowi4  fl4;struct rtable  rt;struct iphdr  iph;int res;  Skip all of this if the packet is already routed,   f.e. by something like SCTP. ", "#include <linux/uaccess.h>#include <linux/module.h>#include <linux/types.h>#include <linux/kernel.h>#include <linux/mm.h>#include <linux/string.h>#include <linux/errno.h>#include <linux/highmem.h>#include <linux/slab.h>#include <linux/socket.h>#include <linux/sockios.h>#include <linux/in.h>#include <linux/inet.h>#include <linux/netdevice.h>#include <linux/etherdevice.h>#include <linux/proc_fs.h>#include <linux/stat.h>#include <linux/init.h>#include <net/snmp.h>#include <net/ip.h>#include <net/protocol.h>#include <net/route.h>#include <net/xfrm.h>#include <linux/skbuff.h>#include <net/sock.h>#include <net/arp.h>#include <net/icmp.h>#include <net/checksum.h>#include <net/gso.h>#include <net/inetpeer.h>#include <net/inet_ecn.h>#include <net/lwtunnel.h>#include <linux/bpf-cgroup.h>#include <linux/igmp.h>#include <linux/netfilter_ipv4.h>#include <linux/netfilter_bridge.h>#include <linux/netlink.h>#include <linux/tcp.h>static intip_fragment(struct net *net, struct sock *sk, struct sk_buff *skb,    unsigned int mtu,    int (*output)(struct net *, struct sock *, struct sk_buff *));/* Generate a checksum for an outgoing IP datagram. ": "ip_queue_xmit  (in case if packet not accepted by  output firewall rules)  Mike McLagan:Routing by source  Alexey Kuznetsov:use new route cache  Andi Kleen:Fix broken PMTU recovery and remove  some redundant tests.  Vitaly E. Lavrov:Transparent proxy revived after year coma.  Andi Kleen: Replace ip_reply with ip_send_reply.  Andi Kleen:Split fast and slow ip_build_xmit path  for decreased register pressure on x86  and more readability.  Marc Boucher:When call_out_firewall returns FW_QUEUE,  silently drop skb instead of failing with -EPERM.  Detlev Wengorz:Copy protocol for fragments.  Hirokazu Takahashi:HW checksumming for outgoing UDP  datagrams.  Hirokazu Takahashi:sendfile() on UDP works now. ", "ip_send_check(iph);}EXPORT_SYMBOL(ip_fraglist_prepare": "ip_fraglist_prepare(struct sk_buff  skb, struct ip_fraglist_iter  iter){unsigned int hlen = iter->hlen;struct iphdr  iph = iter->iph;struct sk_buff  frag;frag = iter->frag;frag->ip_summed = CHECKSUM_NONE;skb_reset_transport_header(frag);__skb_push(frag, hlen);skb_reset_network_header(frag);memcpy(skb_network_header(frag), iph, hlen);iter->iph = ip_hdr(frag);iph = iter->iph;iph->tot_len = htons(frag->len);ip_copy_metadata(frag, skb);iter->offset += skb->len - hlen;iph->frag_off = htons(iter->offset >> 3);if (frag->next)iph->frag_off |= htons(IP_MF);  Ready, complete checksum ", "state->ptr = hlen;/* Where to start from ": "ip6_frag_init(struct sk_buff  skb, unsigned int hlen, unsigned int mtu,   unsigned short needed_tailroom, int hdr_room, u8  prevhdr,   u8 nexthdr, __be32 frag_id, struct ip6_frag_state  state){state->prevhdr = prevhdr;state->nexthdr = nexthdr;state->frag_id = frag_id;state->hlen = hlen;state->mtu = mtu;state->left = skb->len - hlen;  Space per frame ", "if (len > state->mtu)len = state->mtu;/* IF: we are not sending up to and including the packet end   then align the next start on an eight byte boundary ": "ip6_frag_next(struct sk_buff  skb, struct ip6_frag_state  state){u8  prevhdr = state->prevhdr,  fragnexthdr_offset;struct sk_buff  frag;struct frag_hdr  fh;unsigned int len;len = state->left;  IF: it doesn't fit, use 'mtu' - the data space left ", "ip_send_check(iph);}EXPORT_SYMBOL(ip_fraglist_prepare);void ip_frag_init(struct sk_buff *skb, unsigned int hlen,  unsigned int ll_rs, unsigned int mtu, bool DF,  struct ip_frag_state *state)": "ip_do_fragment(net, sk, skb, output);if (unlikely(!skb->ignore_df ||     (IPCB(skb)->frag_max_size &&      IPCB(skb)->frag_max_size > mtu))) {IP_INC_STATS(net, IPSTATS_MIB_FRAGFAILS);icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED,  htonl(mtu));kfree_skb(skb);return -EMSGSIZE;}return ip_do_fragment(net, sk, skb, output);}void ip_fraglist_init(struct sk_buff  skb, struct iphdr  iph,      unsigned int hlen, struct ip_fraglist_iter  iter){unsigned int first_len = skb_pagelen(skb);iter->frag = skb_shinfo(skb)->frag_list;skb_frag_list_init(skb);iter->offset = 0;iter->iph = iph;iter->hlen = hlen;skb->data_len = first_len - skb_headlen(skb);skb->len = first_len;iph->tot_len = htons(first_len);iph->frag_off = htons(IP_MF);ip_send_check(iph);}EXPORT_SYMBOL(ip_fraglist_init);void ip_fraglist_prepare(struct sk_buff  skb, struct ip_fraglist_iter  iter){unsigned int hlen = iter->hlen;struct iphdr  iph = iter->iph;struct sk_buff  frag;frag = iter->frag;frag->ip_summed = CHECKSUM_NONE;skb_reset_transport_header(frag);__skb_push(frag, hlen);skb_reset_network_header(frag);memcpy(skb_network_header(frag), iph, hlen);iter->iph = ip_hdr(frag);iph = iter->iph;iph->tot_len = htons(frag->len);ip_copy_metadata(frag, skb);iter->offset += skb->len - hlen;iph->frag_off = htons(iter->offset >> 3);if (frag->next)iph->frag_off |= htons(IP_MF);  Ready, complete checksum ", "[IPFRAG_ECN_CE | IPFRAG_ECN_ECT_0]= INET_ECN_CE,[IPFRAG_ECN_CE | IPFRAG_ECN_ECT_1]= INET_ECN_CE,[IPFRAG_ECN_CE | IPFRAG_ECN_ECT_0 | IPFRAG_ECN_ECT_1]= INET_ECN_CE,/* invalid combinations : drop frame ": "ip_frag_ecn_table[16] = {  at least one fragment had CE, and others ECT_0 or ECT_1 ", "if (!READ_ONCE(fqdir->dead)) ": "inet_frag_kill(struct inet_frag_queue  fq){if (del_timer(&fq->timer))refcount_dec(&fq->refcnt);if (!(fq->flags & INET_FRAG_COMPLETE)) {struct fqdir  fqdir = fq->fqdir;fq->flags |= INET_FRAG_COMPLETE;rcu_read_lock();  The RCU read lock provides a memory barrier   guaranteeing that if fqdir->dead is false then   the hash table destruction will not start until   after we unlock.  Paired with fqdir_pre_exit(). ", "kill_list = llist_del_all(&fqdir_free_list);/* We need to make sure all ongoing call_rcu(..., inet_frag_destroy_rcu) * have completed, since they need to dereference fqdir. * Would it not be nice to have kfree_rcu_barrier() ? :) ": "inet_frag_destroy(fq);}static LLIST_HEAD(fqdir_free_list);static void fqdir_free_fn(struct work_struct  work){struct llist_node  kill_list;struct fqdir  fqdir,  tmp;struct inet_frags  f;  Atomically snapshot the list of fqdirs to free ", "long high_thresh = READ_ONCE(fqdir->high_thresh);struct inet_frag_queue *fq = NULL, *prev;if (!high_thresh || frag_mem_limit(fqdir) > high_thresh)return NULL;rcu_read_lock();prev = rhashtable_lookup(&fqdir->rhashtable, key, fqdir->f->rhash_params);if (!prev)fq = inet_frag_create(fqdir, key, &prev);if (!IS_ERR_OR_NULL(prev)) ": "inet_frag_find(struct fqdir  fqdir, void  key){  This pairs with WRITE_ONCE() in fqdir_pre_exit(). ", "if (!last)fragrun_create(q, skb);  /* First fragment. ": "inet_frag_queue_insert(struct inet_frag_queue  q, struct sk_buff  skb,   int offset, int end){struct sk_buff  last = q->fragments_tail;  RFC5722, Section 4, amended by Errata ID : 3089                            When reassembling an IPv6 datagram, if     one or more its constituent fragments is determined to be an     overlapping fragment, the entire datagram (and any constituent     fragments) MUST be silently discarded.     Duplicates, however, should be ignored (i.e. skb dropped, but the   queuefragments kept for later reassembly). ", "if (skb_unclone(head, GFP_ATOMIC))return NULL;delta += head->truesize;if (delta)add_frag_mem_limit(q->fqdir, delta);/* If the first fragment is fragmented itself, we split * it to two chunks: the first with data and paged part * and the second, holding only fragments. ": "inet_frag_reasm_prepare(struct inet_frag_queue  q, struct sk_buff  skb,      struct sk_buff  parent){struct sk_buff  fp,  head = skb_rb_first(&q->rb_fragments);struct sk_buff   nextp;int delta;if (head != skb) {fp = skb_clone(skb, GFP_ATOMIC);if (!fp)return NULL;FRAG_CB(fp)->next_frag = FRAG_CB(skb)->next_frag;if (RB_EMPTY_NODE(&skb->rbnode))FRAG_CB(parent)->next_frag = fp;elserb_replace_node(&skb->rbnode, &fp->rbnode,&q->rb_fragments);if (q->fragments_tail == skb)q->fragments_tail = fp;skb_morph(skb, head);FRAG_CB(skb)->next_frag = FRAG_CB(head)->next_frag;rb_replace_node(&head->rbnode, &skb->rbnode,&q->rb_fragments);consume_skb(head);head = skb;}WARN_ON(head->ip_defrag_offset != 0);delta = -head->truesize;  Head of list must not be cloned. ", "fp = FRAG_CB(head)->next_frag;rbn = rb_next(&head->rbnode);rb_erase(&head->rbnode, &q->rb_fragments);sum_truesize = head->truesize;while (rbn || fp) ": "inet_frag_reasm_finish(struct inet_frag_queue  q, struct sk_buff  head,    void  reasm_data, bool try_coalesce){struct sk_buff   nextp = reasm_data;struct rb_node  rbn;struct sk_buff  fp;int sum_truesize;skb_push(head, head->data - skb_network_header(head));  Traverse the tree in order, to build frag_list. ", "fl4.daddr = iph->daddr;fl4.saddr = saddr;fl4.flowi4_tos = RT_TOS(iph->tos);fl4.flowi4_oif = sk ? sk->sk_bound_dev_if : 0;fl4.flowi4_l3mdev = l3mdev_master_ifindex(dev);fl4.flowi4_mark = skb->mark;fl4.flowi4_flags = flags;fib4_rules_early_flow_dissect(net, skb, &fl4, &flkeys);rt = ip_route_output_key(net, &fl4);if (IS_ERR(rt))return PTR_ERR(rt);/* Drop old route. ": "ip_route_me_harder(struct net  net, struct sock  sk, struct sk_buff  skb, unsigned int addr_type){const struct iphdr  iph = ip_hdr(skb);struct rtable  rt;struct flowi4 fl4 = {};__be32 saddr = iph->saddr;__u8 flags;struct net_device  dev = skb_dst(skb)->dev;struct flow_keys flkeys;unsigned int hh_len;sk = sk_to_full_sk(sk);flags = sk ? inet_sk_flowi_flags(sk) : 0;if (addr_type == RTN_UNSPEC)addr_type = inet_addr_type_dev_table(net, dev, saddr);if (addr_type == RTN_LOCAL || addr_type == RTN_UNICAST)flags |= FLOWI_FLAG_ANYSRC;elsesaddr = 0;  some non-standard hacks like ipt_REJECT.c:send_reset() can cause   packets with foreign saddr to appear on the NF_INET_LOCAL_OUT hook. ", ".fatal = 0,},": "icmp_err_convert[] = {{.errno = ENETUNREACH,  ICMP_NET_UNREACH ", "bool icmp_global_allow(void)": "icmp_global_allow - Are we allowed to send one more ICMP message ?     Uses a token bucket to limit our ICMP messages to ~sysctl_icmp_msgs_per_sec.   Returns false if we reached the limit and can not send another packet.   Note: called with BH disabled ", "iph = ip_hdr(skb_in);if ((u8 *)iph < skb_in->head ||    (skb_network_header(skb_in) + sizeof(*iph)) >    skb_tail_pointer(skb_in))goto out;/* *No replies to physical multicast/broadcast ": "__icmp_send(struct sk_buff  skb_in, int type, int code, __be32 info, const struct ip_options  opt){struct iphdr  iph;int room;struct icmp_bxm icmp_param;struct rtable  rt = skb_rtable(skb_in);struct ipcm_cookie ipc;struct flowi4 fl4;__be32 saddr;u8  tos;u32 mark;struct net  net;struct sock  sk;if (!rt)goto out;if (rt->dst.dev)net = dev_net(rt->dst.dev);else if (skb_in->dev)net = dev_net(skb_in->dev);elsegoto out;   Find the original header. It is expected to be valid, of course.  Check this, icmp_send is called from the most obscure devices  sometimes. ", "if (unlikely(siphash_key_is_zero(&net->ipv4.ip_id_key)))get_random_bytes(&net->ipv4.ip_id_key, sizeof(net->ipv4.ip_id_key));hash = siphash_3u32((__force u32)iph->daddr,    (__force u32)iph->saddr,    iph->protocol,    &net->ipv4.ip_id_key);id = ip_idents_reserve(hash, segs);iph->id = htons(id);}EXPORT_SYMBOL(__ip_select_ident": "__ip_select_ident(struct net  net, struct iphdr  iph, int segs){u32 hash, id;  Note the following code is not safe, but this is okay. ", "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt#include <linux/module.h>#include <linux/types.h>#include <linux/string.h>#include <linux/kernel.h>#include <linux/capability.h>#include <linux/socket.h>#include <linux/sockios.h>#include <linux/errno.h>#include <linux/in.h>#include <linux/mm.h>#include <linux/inet.h>#include <linux/inetdevice.h>#include <linux/netdevice.h>#include <linux/etherdevice.h>#include <linux/fddidevice.h>#include <linux/if_arp.h>#include <linux/skbuff.h>#include <linux/proc_fs.h>#include <linux/seq_file.h>#include <linux/stat.h>#include <linux/init.h>#include <linux/net.h>#include <linux/rcupdate.h>#include <linux/slab.h>#ifdef CONFIG_SYSCTL#include <linux/sysctl.h>#endif#include <net/net_namespace.h>#include <net/ip.h>#include <net/icmp.h>#include <net/route.h>#include <net/protocol.h>#include <net/tcp.h>#include <net/sock.h>#include <net/arp.h>#include <net/ax25.h>#include <net/netrom.h>#include <net/dst_metadata.h>#include <net/ip_tunnels.h>#include <linux/uaccess.h>#include <linux/netfilter_arp.h>/* *Interface to generic neighbour cache. ": "arp_xmit so intermediate drivers like  bonding can change the skb before  sending (e.g. insert 8021q tag).  Harald Welte:convert to make use of jenkins hash  Jesper D. Brouer:       Proxy ARP PVLAN RFC 3069 support. ", "int udp_lib_get_port(struct sock *sk, unsigned short snum,     unsigned int hash2_nulladdr)": "udp_lib_get_port  -  UDP-Lite port lookup for IPv4 and IPv6      @sk:          socket struct in question    @snum:        port number to look up    @hash2_nulladdr: AF-dependent hash value in secondary hash chains,                     with NULL address ", "if (msg->msg_flags & MSG_OOB) /* Mirror BSD error message compatibility ": "udp_sendmsg(struct sock  sk, struct msghdr  msg, size_t len){struct inet_sock  inet = inet_sk(sk);struct udp_sock  up = udp_sk(sk);DECLARE_SOCKADDR(struct sockaddr_in  , usin, msg->msg_name);struct flowi4 fl4_stack;struct flowi4  fl4;int ulen = len;struct ipcm_cookie ipc;struct rtable  rt = NULL;int free = 0;int connected = 0;__be32 daddr, faddr, saddr;u8 tos, scope;__be16 dport;int err, is_udplite = IS_UDPLITE(sk);int corkreq = READ_ONCE(up->corkflag) || msg->msg_flags&MSG_MORE;int ( getfrag)(void  , char  , int, int, int, struct sk_buff  );struct sk_buff  skb;struct ip_options_data opt_copy;if (len > 0xFFFF)return -EMSGSIZE;   Check the flags. ", "spin_lock(&sk_queue->lock);skb_queue_splice_tail_init(sk_queue, queue);skb = __skb_try_recv_from_queue(sk, queue, flags, off,err, &last);if (skb && !(flags & MSG_PEEK))udp_skb_dtor_locked(sk, skb);spin_unlock(&sk_queue->lock);spin_unlock_bh(&queue->lock);if (skb)return skb;busy_check:if (!sk_can_busy_loop(sk))break;sk_busy_loop(sk, flags & MSG_DONTWAIT);} while (!skb_queue_empty_lockless(sk_queue));/* sk_queue is empty, reader_queue may contain peeked packets ": "__skb_recv_udp(struct sock  sk, unsigned int flags,       int  off, int  err){struct sk_buff_head  sk_queue = &sk->sk_receive_queue;struct sk_buff_head  queue;struct sk_buff  last;long timeo;int error;queue = &udp_sk(sk)->reader_queue;timeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);do {struct sk_buff  skb;error = sock_error(sk);if (error)break;error = -EAGAIN;do {spin_lock_bh(&queue->lock);skb = __skb_try_recv_from_queue(sk, queue, flags, off,err, &last);if (skb) {if (!(flags & MSG_PEEK))udp_skb_destructor(sk, skb);spin_unlock_bh(&queue->lock);return skb;}if (skb_queue_empty_lockless(sk_queue)) {spin_unlock_bh(&queue->lock);goto busy_check;}  refill the reader queue and walk it again   keep both queues locked to avoid re-acquiring   the sk_receive_queue lock if fwd memory scheduling   is needed. ", "if (addr_len < sizeof(struct sockaddr_in))return -EINVAL;return BPF_CGROUP_RUN_PROG_INET4_CONNECT_LOCK(sk, uaddr);}EXPORT_SYMBOL(udp_pre_connect": "udp_pre_connect(struct sock  sk, struct sockaddr  uaddr, int addr_len){  This check is replicated from __ip4_datagram_connect() and   intended to prevent BPF program called below from accessing bytes   that are out of the bound specified by user in addr_len. ", "sk->sk_state = TCP_CLOSE;inet->inet_daddr = 0;inet->inet_dport = 0;sock_rps_reset_rxhash(sk);sk->sk_bound_dev_if = 0;if (!(sk->sk_userlocks & SOCK_BINDADDR_LOCK)) ": "udp_disconnect(struct sock  sk, int flags){struct inet_sock  inet = inet_sk(sk);   1003.1g - break association. ", "spin_lock_bh(&hslot->lock);if (rcu_access_pointer(sk->sk_reuseport_cb))reuseport_detach_sock(sk);if (hslot2 != nhslot2) ": "udp_lib_rehash(struct sock  sk, u16 newhash){if (sk_hashed(sk)) {struct udp_table  udptable = udp_get_table_prot(sk);struct udp_hslot  hslot,  hslot2,  nhslot2;hslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);nhslot2 = udp_hashslot2(udptable, newhash);udp_sk(sk)->udp_portaddr_hash = newhash;if (hslot2 != nhslot2 ||    rcu_access_pointer(sk->sk_reuseport_cb)) {hslot = udp_hashslot(udptable, sock_net(sk),     udp_sk(sk)->udp_port_hash);  we must lock primary chain too ", "WRITE_ONCE(up->forward_threshold, sk->sk_rcvbuf >> 2);sockopt_release_sock(sk);}return err;}if (optlen < sizeof(int))return -EINVAL;if (copy_from_sockptr(&val, optval, sizeof(val)))return -EFAULT;valbool = val ? 1 : 0;switch (optname) ": "udp_lib_setsockopt(struct sock  sk, int level, int optname,       sockptr_t optval, unsigned int optlen,       int ( push_pending_frames)(struct sock  )){struct udp_sock  up = udp_sk(sk);int val, valbool;int err = 0;int is_udplite = IS_UDPLITE(sk);if (level == SOL_SOCKET) {err = sk_setsockopt(sk, level, optname, optval, optlen);if (optname == SO_RCVBUF || optname == SO_RCVBUFFORCE) {sockopt_lock_sock(sk);  paired with READ_ONCE in udp_rmem_release() ", "case UDPLITE_SEND_CSCOV:val = up->pcslen;break;case UDPLITE_RECV_CSCOV:val = up->pcrlen;break;default:return -ENOPROTOOPT;}if (put_user(len, optlen))return -EFAULT;if (copy_to_user(optval, &val, len))return -EFAULT;return 0;}EXPORT_SYMBOL(udp_lib_getsockopt": "udp_lib_getsockopt(struct sock  sk, int level, int optname,       char __user  optval, int __user  optlen){struct udp_sock  up = udp_sk(sk);int val, len;if (get_user(len, optlen))return -EFAULT;len = min_t(unsigned int, len, sizeof(int));if (len < 0)return -EINVAL;switch (optname) {case UDP_CORK:val = READ_ONCE(up->corkflag);break;case UDP_ENCAP:val = up->encap_type;break;case UDP_NO_CHECK6_TX:val = up->no_check6_tx;break;case UDP_NO_CHECK6_RX:val = up->no_check6_rx;break;case UDP_SEGMENT:val = READ_ONCE(up->gso_size);break;case UDP_GRO:val = up->gro_enabled;break;  The following two cannot be changed on UDP sockets, the return is   always 0 (which corresponds to the full checksum coverage of UDP). ", "__poll_t udp_poll(struct file *file, struct socket *sock, poll_table *wait)": "udp_poll - wait for a UDP event.  @file: - file struct  @sock: - socket  @wait: - poll table    This is same as datagram poll, except for the special case of  blocking sockets. If application is using a blocking fd  and a packet with checksum error is in the queue;  then it could get return from select indicating data available  but then block when reading it. Add special case code  to work around these arguably broken applications. ", "static void udp4_format_sock(struct sock *sp, struct seq_file *f,int bucket)": "udp_seq_ops;#endifstatic struct udp_table  udp_get_table_seq(struct seq_file  seq,   struct net  net){const struct udp_seq_afinfo  afinfo;#ifdef CONFIG_BPF_SYSCALLif (seq->op == &bpf_iter_udp_seq_ops)return net->ipv4.udp_table;#endifafinfo = pde_data(file_inode(seq->file));return afinfo->udp_table ? : net->ipv4.udp_table;}static struct sock  udp_get_first(struct seq_file  seq, int start){struct udp_iter_state  state = seq->private;struct net  net = seq_file_net(seq);struct udp_table  udptable;struct sock  sk;udptable = udp_get_table_seq(seq, net);for (state->bucket = start; state->bucket <= udptable->mask;     ++state->bucket) {struct udp_hslot  hslot = &udptable->hash[state->bucket];if (hlist_empty(&hslot->head))continue;spin_lock_bh(&hslot->lock);sk_for_each(sk, &hslot->head) {if (seq_sk_match(seq, sk))goto found;}spin_unlock_bh(&hslot->lock);}sk = NULL;found:return sk;}static struct sock  udp_get_next(struct seq_file  seq, struct sock  sk){struct udp_iter_state  state = seq->private;struct net  net = seq_file_net(seq);struct udp_table  udptable;do {sk = sk_next(sk);} while (sk && !seq_sk_match(seq, sk));if (!sk) {udptable = udp_get_table_seq(seq, net);if (state->bucket <= udptable->mask)spin_unlock_bh(&udptable->hash[state->bucket].lock);return udp_get_first(seq, state->bucket + 1);}return sk;}static struct sock  udp_get_idx(struct seq_file  seq, loff_t pos){struct sock  sk = udp_get_first(seq, 0);if (sk)while (pos && (sk = udp_get_next(seq, sk)) != NULL)--pos;return pos ? NULL : sk;}void  udp_seq_start(struct seq_file  seq, loff_t  pos){struct udp_iter_state  state = seq->private;state->bucket = MAX_UDP_PORTS;return  pos ? udp_get_idx(seq,  pos-1) : SEQ_START_TOKEN;}EXPORT_SYMBOL(udp_seq_start);void  udp_seq_next(struct seq_file  seq, void  v, loff_t  pos){struct sock  sk;if (v == SEQ_START_TOKEN)sk = udp_get_idx(seq, 0);elsesk = udp_get_next(seq, v);++ pos;return sk;}EXPORT_SYMBOL(udp_seq_next);void udp_seq_stop(struct seq_file  seq, void  v){struct udp_iter_state  state = seq->private;struct udp_table  udptable;udptable = udp_get_table_seq(seq, seq_file_net(seq));if (state->bucket <= udptable->mask)spin_unlock_bh(&udptable->hash[state->bucket].lock);}EXPORT_SYMBOL(udp_seq_stop);  ------------------------------------------------------------------------ ", "if (opt->srr) ": "__ip_options_compile(struct net  net, struct ip_options  opt, struct sk_buff  skb, __be32  info){__be32 spec_dst = htonl(INADDR_ANY);unsigned char  pp_ptr = NULL;struct rtable  rt = NULL;unsigned char  optptr;unsigned char  iph;int optlen, l;if (skb) {rt = skb_rtable(skb);optptr = (unsigned char  )&(ip_hdr(skb)[1]);} elseoptptr = opt->__data;iph = optptr - sizeof(struct iphdr);for (l = opt->optlen; l > 0; ) {switch ( optptr) {case IPOPT_END:for (optptr++, l--; l > 0; optptr++, l--) {if ( optptr != IPOPT_END) { optptr = IPOPT_END;opt->is_changed = 1;}}goto eol;case IPOPT_NOOP:l--;optptr++;continue;}if (unlikely(l < 2)) {pp_ptr = optptr;goto error;}optlen = optptr[1];if (optlen < 2 || optlen > l) {pp_ptr = optptr;goto error;}switch ( optptr) {case IPOPT_SSRR:case IPOPT_LSRR:if (optlen < 3) {pp_ptr = optptr + 1;goto error;}if (optptr[2] < 4) {pp_ptr = optptr + 2;goto error;}  NB: cf RFC-1812 5.2.4.1 ", "static void spec_dst_fill(__be32 *spec_dst, struct sk_buff *skb)": "ip_options_compile() to call fib_compute_spec_dst()   at most one time. ", "iph->daddr = nexthop;opt->is_changed = 1;}if (srrptr <= srrspace) ": "ip_options_rcv_srr(struct sk_buff  skb, struct net_device  dev){struct ip_options  opt = &(IPCB(skb)->opt);int srrspace, srrptr;__be32 nexthop;struct iphdr  iph = ip_hdr(skb);unsigned char  optptr = skb_network_header(skb) + opt->srr;struct rtable  rt = skb_rtable(skb);struct rtable  rt2;unsigned long orefdst;int err;if (!rt)return 0;if (skb->pkt_type != PACKET_HOST)return -EINVAL;if (rt->rt_type == RTN_UNICAST) {if (!opt->is_strictroute)return 0;icmp_send(skb, ICMP_PARAMETERPROB, 0, htonl(16<<24));return -EINVAL;}if (rt->rt_type != RTN_LOCAL)return -EINVAL;for (srrptr = optptr[2], srrspace = optptr[1]; srrptr <= srrspace; srrptr += 4) {if (srrptr + 3 > srrspace) {icmp_send(skb, ICMP_PARAMETERPROB, 0, htonl((opt->srr+2)<<24));return -EINVAL;}memcpy(&nexthop, &optptr[srrptr-1], 4);orefdst = skb->_skb_refdst;skb_dst_set(skb, NULL);err = ip_route_input(skb, nexthop, iph->saddr, iph->tos, dev);rt2 = skb_rtable(skb);if (err || (rt2->rt_type != RTN_UNICAST && rt2->rt_type != RTN_LOCAL)) {skb_dst_drop(skb);skb->_skb_refdst = orefdst;return -EINVAL;}refdst_drop(orefdst);if (rt2->rt_type != RTN_LOCAL)break;  Superfast 8) loopback forward ", "res_table = rcu_dereference(nhg->res_table);if (num_buckets != res_table->num_nh_buckets)goto out;for (i = 0; i < num_buckets; i++) ": "nexthop_res_grp_activity_update(struct net  net, u32 id, u16 num_buckets,     unsigned long  activity){struct nh_res_table  res_table;struct nexthop  nexthop;struct nh_group  nhg;u16 i;rcu_read_lock();nexthop = nexthop_find_by_id(net, id);if (!nexthop || !nexthop->is_group)goto out;nhg = rcu_dereference(nexthop->nh_grp);if (!nhg->resilient)goto out;  Instead of silently ignoring some buckets, demand that the sizes   be the same. ", "im->sfmode = mode;im->sfcount[mode] = 1;refcount_set(&im->refcnt, 1);spin_lock_init(&im->lock);#ifdef CONFIG_IP_MULTICASTtimer_setup(&im->timer, igmp_timer_expire, 0);#endifim->next_rcu = in_dev->mc_list;in_dev->mc_count++;rcu_assign_pointer(in_dev->mc_list, im);ip_mc_hash_add(in_dev, im);#ifdef CONFIG_IP_MULTICASTigmpv3_del_delrec(in_dev, im);#endifigmp_group_added(im);if (!in_dev->dead)ip_rt_multicast_event(in_dev);out:return;}void __ip_mc_inc_group(struct in_device *in_dev, __be32 addr, gfp_t gfp)": "ip_mc_inc_group(struct in_device  in_dev, __be32 addr,unsigned int mode, gfp_t gfp){struct ip_mc_list  im;ASSERT_RTNL();for_each_pmc_rtnl(in_dev, im) {if (im->multiaddr == addr) {im->users++;ip_mc_add_src(in_dev, &addr, mode, 0, NULL, 0);goto out;}}im = kzalloc(sizeof( im), gfp);if (!im)goto out;im->users = 1;im->interface = in_dev;in_dev_hold(in_dev);im->multiaddr = addr;  initial mode is (EX, empty) ", "if (transport_len != sizeof(struct igmphdr)) ": "ip_mc_check_igmp_reportv3(struct sk_buff  skb){unsigned int len = skb_transport_offset(skb);len += sizeof(struct igmpv3_report);return ip_mc_may_pull(skb, len) ? 0 : -EINVAL;}static int ip_mc_check_igmp_query(struct sk_buff  skb){unsigned int transport_len = ip_transport_len(skb);unsigned int len;  IGMPv{1,2}? ", "int ip_mc_join_group(struct sock *sk, struct ip_mreqn *imr)": "ip_mc_join_group(struct sock  sk, struct ip_mreqn  imr,      unsigned int mode){__be32 addr = imr->imr_multiaddr.s_addr;struct ip_mc_socklist  iml,  i;struct in_device  in_dev;struct inet_sock  inet = inet_sk(sk);struct net  net = sock_net(sk);int ifindex;int count = 0;int err;ASSERT_RTNL();if (!ipv4_is_multicast(addr))return -EINVAL;in_dev = ip_mc_find_dev(net, imr);if (!in_dev) {err = -ENODEV;goto done;}err = -EADDRINUSE;ifindex = imr->imr_ifindex;for_each_pmc_rtnl(inet, i) {if (i->multi.imr_multiaddr.s_addr == addr &&    i->multi.imr_ifindex == ifindex)goto done;count++;}err = -ENOBUFS;if (count >= READ_ONCE(net->ipv4.sysctl_igmp_max_memberships))goto done;iml = sock_kmalloc(sk, sizeof( iml), GFP_KERNEL);if (!iml)goto done;memcpy(&iml->multi, imr, sizeof( imr));iml->next_rcu = inet->mc_list;iml->sflist = NULL;iml->sfmode = mode;rcu_assign_pointer(inet->mc_list, iml);____ip_mc_inc_group(in_dev, addr, mode, GFP_KERNEL);err = 0;done:return err;}  Join ASM (Any-Source Multicast) group ", "atomic_sub(sizeof(*iml), &sk->sk_omem_alloc);kfree_rcu(iml, rcu);return 0;}out:return ret;}EXPORT_SYMBOL(ip_mc_leave_group": "ip_mc_leave_group(struct sock  sk, struct ip_mreqn  imr){struct inet_sock  inet = inet_sk(sk);struct ip_mc_socklist  iml;struct ip_mc_socklist __rcu   imlp;struct in_device  in_dev;struct net  net = sock_net(sk);__be32 group = imr->imr_multiaddr.s_addr;u32 ifindex;int ret = -EADDRNOTAVAIL;ASSERT_RTNL();in_dev = ip_mc_find_dev(net, imr);if (!imr->imr_ifindex && !imr->imr_address.s_addr && !in_dev) {ret = -ENODEV;goto out;}ifindex = imr->imr_ifindex;for (imlp = &inet->mc_list;     (iml = rtnl_dereference( imlp)) != NULL;     imlp = &iml->next_rcu) {if (iml->multi.imr_multiaddr.s_addr != group)continue;if (ifindex) {if (iml->multi.imr_ifindex != ifindex)continue;} else if (imr->imr_address.s_addr && imr->imr_address.s_addr !=iml->multi.imr_address.s_addr)continue;(void) ip_mc_leave_src(sk, iml, in_dev); imlp = iml->next_rcu;if (in_dev)ip_mc_dec_group(in_dev, group);  decrease mem now to avoid the memleak warning ", "rcu_read_unlock();it->cache = &mrt->mfc_unres_queue;spin_lock_bh(it->lock);if (!list_empty(it->cache))return list_first_entry(it->cache, struct mr_mfc, list);end_of_list:spin_unlock_bh(it->lock);it->cache = NULL;return NULL;}EXPORT_SYMBOL(mr_mfc_seq_next": "mr_mfc_seq_next(struct seq_file  seq, void  v,      loff_t  pos){struct mr_mfc_iter  it = seq->private;struct net  net = seq_file_net(seq);struct mr_table  mrt = it->mrt;struct mr_mfc  c = v;++ pos;if (v == SEQ_START_TOKEN)return mr_mfc_seq_idx(net, seq->private, 0);if (c->list.next != it->cache)return list_entry(c->list.next, struct mr_mfc, list);if (it->cache == &mrt->mfc_unres_queue)goto end_of_list;  exhausted cache_array, show unresolved ", "if (c->mfc_parent >= MAXVIFS) ": "mr_fill_mroute(struct mr_table  mrt, struct sk_buff  skb,   struct mr_mfc  c, struct rtmsg  rtm){struct net_device  vif_dev;struct rta_mfc_stats mfcs;struct nlattr  mp_attr;struct rtnexthop  nhp;unsigned long lastuse;int ct;  If cache is unresolved, don't try to parse IIF and OIF ", "if (filter->filter_set) ": "mr_rtm_dumproute(struct sk_buff  skb, struct netlink_callback  cb,     struct mr_table  ( iter)(struct net  net,      struct mr_table  mrt),     int ( fill)(struct mr_table  mrt, struct sk_buff  skb, u32 portid, u32 seq, struct mr_mfc  c, int cmd, int flags),     spinlock_t  lock, struct fib_dump_filter  filter){unsigned int t = 0, s_t = cb->args[0];struct net  net = sock_net(skb->sk);struct mr_table  mrt;int err;  multicast does not track protocol or have route type other   than RTN_MULTICAST ", "rcu_read_lock();for (vifi = 0; vifi < mrt->maxvif; vifi++, v++) ": "mr_dump(struct net  net, struct notifier_block  nb, unsigned short family,    int ( rules_dump)(struct net  net,      struct notifier_block  nb,      struct netlink_ext_ack  extack),    struct mr_table  ( mr_iter)(struct net  net,struct mr_table  mrt),    struct netlink_ext_ack  extack){struct mr_table  mrt;int err;err = rules_dump(net, nb, extack);if (err)return err;for (mrt = mr_iter(net, NULL); mrt; mrt = mr_iter(net, mrt)) {struct vif_device  v = &mrt->vif_table[0];struct net_device  vif_dev;struct mr_mfc  mfc;int vifi;  Notifiy on table VIF entries ", "if (greh->flags == 0 && tpi->proto == htons(ETH_P_WCCP)) ": "gre_parse_header(struct sk_buff  skb, struct tnl_ptk_info  tpi,     bool  csum_err, __be16 proto, int nhs){const struct gre_base_hdr  greh;__be32  options;int hdr_len;if (unlikely(!pskb_may_pull(skb, nhs + sizeof(struct gre_base_hdr))))return -EINVAL;greh = (struct gre_base_hdr  )(skb->data + nhs);if (unlikely(greh->flags & (GRE_VERSION | GRE_ROUTING)))return -EINVAL;tpi->flags = gre_flags_to_tnl_flags(greh->flags);hdr_len = gre_calc_hlen(tpi->flags);if (!pskb_may_pull(skb, nhs + hdr_len))return -EINVAL;greh = (struct gre_base_hdr  )(skb->data + nhs);tpi->proto = greh->protocol;options = (__be32  )(greh + 1);if (greh->flags & GRE_CSUM) {if (!skb_checksum_simple_validate(skb)) {skb_checksum_try_convert(skb, IPPROTO_GRE, null_compute_pseudo);} else if (csum_err) { csum_err = true;return -EINVAL;}options++;}if (greh->flags & GRE_KEY) {tpi->key =  options;options++;} else {tpi->key = 0;}if (unlikely(greh->flags & GRE_SEQ)) {tpi->seq =  options;options++;} else {tpi->seq = 0;}  WCCP version 1 and 2 protocol decoding.   - Change protocol to IPv4IPv6   - When dealing with WCCPv2, Skip extra 4 bytes in GRE header ", "EXPORT_SYMBOL(tcp_memory_allocated": "tcp_memory_allocated ____cacheline_aligned_in_smp;  Current allocated memory. ", "tcp_snd_cwnd_set(tp, TCP_INIT_CWND);/* There's a bubble in the pipe until at least the first ACK. ": "tcp_init_sock(struct sock  sk){struct inet_connection_sock  icsk = inet_csk(sk);struct tcp_sock  tp = tcp_sk(sk);tp->out_of_order_queue = RB_ROOT;sk->tcp_rtx_queue = RB_ROOT;tcp_init_xmit_timers(sk);INIT_LIST_HEAD(&tp->tsq_node);INIT_LIST_HEAD(&tp->tsorted_sent_queue);icsk->icsk_rto = TCP_TIMEOUT_INIT;icsk->icsk_rto_min = TCP_RTO_MIN;icsk->icsk_delack_max = TCP_DELACK_MAX;tp->mdev_us = jiffies_to_usecs(TCP_TIMEOUT_INIT);minmax_reset(&tp->rtt_min, tcp_jiffies32, ~0U);  So many TCP implementations out there (incorrectly) count the   initial SYN frame in their delayed-ACK and congestion control   algorithms that we must have the following bandaid to talk   efficiently to them.  -DaveM ", "mask = 0;/* * EPOLLHUP is certainly not done right. But poll() doesn't * have a notion of HUP in just one direction, and for a * socket the read side is more interesting. * * Some poll() documentation says that EPOLLHUP is incompatible * with the EPOLLOUT/POLLWR flags, so somebody should check this * all. But careful, it tends to be safer to return too many * bits than too few, and you can easily break real applications * if you don't tell them that something has hung up! * * Check-me. * * Check number 1. EPOLLHUP is _UNMASKABLE_ event (see UNIX98 and * our fs/select.c). It means that after we received EOF, * poll always returns immediately, making impossible poll() on write() * in state CLOSE_WAIT. One solution is evident --- to set EPOLLHUP * if and only if shutdown has been made in both directions. * Actually, it is interesting to look how Solaris and DUX * solve this dilemma. I would prefer, if EPOLLHUP were maskable, * then we could set it on SND_SHUTDOWN. BTW examples given * in Stevens' books assume exactly this behaviour, it explains * why EPOLLHUP is incompatible with EPOLLOUT.--ANK * * NOTE. Check for TCP_CLOSE is added. The goal is to prevent * blocking on fresh not-connected or disconnected socket. --ANK ": "tcp_poll(struct file  file, struct socket  sock, poll_table  wait){__poll_t mask;struct sock  sk = sock->sk;const struct tcp_sock  tp = tcp_sk(sk);u8 shutdown;int state;sock_poll_wait(file, sock, wait);state = inet_sk_state_load(sk);if (state == TCP_LISTEN)return inet_csk_listen_poll(sk);  Socket is not locked. We are protected from async events   by poll logic and correct handling of state changes   made by other threads is impossible in any case. ", "read_descriptor_t rd_desc = ": "tcp_splice_read(struct sock  sk, struct tcp_splice_state  tss){  Store TCP splice context information in read_descriptor_t. ", "tp->fastopen_req = kzalloc(sizeof(struct tcp_fastopen_request),   sk->sk_allocation);if (unlikely(!tp->fastopen_req))return -ENOBUFS;tp->fastopen_req->data = msg;tp->fastopen_req->size = size;tp->fastopen_req->uarg = uarg;if (inet->defer_connect) ": "tcp_sendmsg_fastopen(struct sock  sk, struct msghdr  msg, int  copied, size_t size, struct ubuf_info  uarg){struct tcp_sock  tp = tcp_sk(sk);struct inet_sock  inet = inet_sk(sk);struct sockaddr  uaddr = msg->msg_name;int err, flags;if (!(READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_fastopen) &      TFO_CLIENT_ENABLE) ||    (uaddr && msg->msg_namelen >= sizeof(uaddr->sa_family) &&     uaddr->sa_family == AF_UNSPEC))return -EOPNOTSUPP;if (tp->fastopen_req)return -EALREADY;   Another Fast Open is in progress ", "tcp_eat_recv_skb(sk, skb);}return NULL;}EXPORT_SYMBOL(tcp_recv_skb": "tcp_recv_skb(struct sock  sk, u32 seq, u32  off){struct sk_buff  skb;u32 offset;while ((skb = skb_peek(&sk->sk_receive_queue)) != NULL) {offset = seq - TCP_SKB_CB(skb)->seq;if (unlikely(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)) {pr_err_once(\"%s: found a SYN, please report !\\n\", __func__);offset--;}if (offset < skb->len || (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)) { off = offset;return skb;}  This looks weird, but this can happen if TCP collapsing   splitted a fat GRO packet, while we released socket lock   in skb_splice_bits() ", "ssize_t tcp_splice_read(struct socket *sock, loff_t *ppos,struct pipe_inode_info *pipe, size_t len,unsigned int flags)": "tcp_read_sock(sk, &rd_desc, tcp_splice_data_recv);}      tcp_splice_read - splice data from TCP socket to a pipe   @sock:socket to splice from   @ppos:position (not valid)   @pipe:pipe to splice to   @len:number of bytes to splice   @flags:splice modifier flags     Description:      Will read pages from given socket and fill them into a pipe.    ", "if (left != len)tcp_cleanup_rbuf(sk, len - left);}EXPORT_SYMBOL(tcp_read_done": "tcp_read_done(struct sock  sk, size_t len){struct tcp_sock  tp = tcp_sk(sk);u32 seq = tp->copied_seq;struct sk_buff  skb;size_t left;u32 offset;if (sk->sk_state == TCP_LISTEN)return;left = len;while (left && (skb = tcp_recv_skb(sk, seq, &offset)) != NULL) {int used;used = min_t(size_t, skb->len - offset, left);seq += used;left -= used;if (skb->len > offset + used)break;if (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN) {tcp_eat_recv_skb(sk, skb);++seq;break;}tcp_eat_recv_skb(sk, skb);}WRITE_ONCE(tp->copied_seq, seq);tcp_rcv_space_adjust(sk);  Clean up data we have read: This will do ACK frames. ", "tcp_data_ready(sk);if (sk->sk_userlocks & SOCK_RCVBUF_LOCK)return 0;val <<= 1;if (val > sk->sk_rcvbuf) ": "tcp_set_rcvlowat(struct sock  sk, int val){int cap;if (sk->sk_userlocks & SOCK_RCVBUF_LOCK)cap = sk->sk_rcvbuf >> 1;elsecap = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_rmem[2]) >> 1;val = min(val, cap);WRITE_ONCE(sk->sk_rcvlowat, val ? : 1);  Check if we need to signal EPOLLIN right now ", "vm_flags_set(vma, VM_MIXEDMAP);vma->vm_ops = &tcp_vm_ops;return 0;}EXPORT_SYMBOL(tcp_mmap": "tcp_mmap(struct file  file, struct socket  sock,     struct vm_area_struct  vma){if (vma->vm_flags & (VM_WRITE | VM_EXEC))return -EPERM;vm_flags_clear(vma, VM_MAYWRITE | VM_MAYEXEC);  Instruct vm_insert_page() to not mmap_read_lock(mm) ", "void __tcp_cleanup_rbuf(struct sock *sk, int copied)": "tcp_recvmsg has given to the user so far, it speeds up the   calculation of whether or not we must ACK for the sake of   a window update. ", "if (!(how & SEND_SHUTDOWN))return;/* If we've already sent a FIN, or it's a closed state, skip this. ": "tcp_shutdown(struct sock  sk, int how){ We need to grab some memory, and put together a FIN,  and then put it into the queue to be sent.  Tim MacKenzie(tym@dibbler.cs.monash.edu.au) 4 Dec '92. ", "void tcp_shutdown(struct sock *sk, int how)": "tcp_close_state(struct sock  sk){int next = (int)new_state[sk->sk_state];int ns = next & TCP_STATE_MASK;tcp_set_state(sk, ns);return next & TCP_ACTION_FIN;}   Shutdown the sending side of a connection. Much like close except  that we don't receive shut down or sock_set_flag(sk, SOCK_DEAD). ", "if (sk->sk_wait_pending)return -EBUSY;if (old_state != TCP_CLOSE)tcp_set_state(sk, TCP_CLOSE);/* ABORT function of RFC793 ": "tcp_disconnect(struct sock  sk, int flags){struct inet_sock  inet = inet_sk(sk);struct inet_connection_sock  icsk = inet_csk(sk);struct tcp_sock  tp = tcp_sk(sk);int old_state = sk->sk_state;u32 seq;  Deny disconnect if other threads are blocked in sk_wait_event()   or inet_wait_for_connect(). ", "WRITE_ONCE(tp->keepalive_time, val * HZ);if (sock_flag(sk, SOCK_KEEPOPEN) &&    !((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN))) ": "tcp_sock_set_keepidle_locked(struct sock  sk, int val){struct tcp_sock  tp = tcp_sk(sk);if (val < 1 || val > MAX_TCP_KEEPIDLE)return -EINVAL;  Paired with WRITE_ONCE() in keepalive_time_when() ", "WRITE_ONCE(tcp_sk(sk)->keepalive_probes, val);release_sock(sk);return 0;}EXPORT_SYMBOL(tcp_sock_set_keepcnt": "tcp_sock_set_keepcnt(struct sock  sk, int val){if (val < 1 || val > MAX_TCP_KEEPCNT)return -EINVAL;lock_sock(sk);  Paired with READ_ONCE() in keepalive_probes() ", "switch (optname) ": "tcp_setsockopt(struct sock  sk, int level, int optname,      sockptr_t optval, unsigned int optlen){struct tcp_sock  tp = tcp_sk(sk);struct inet_connection_sock  icsk = inet_csk(sk);struct net  net = sock_net(sk);int val;int err = 0;  These are datastring values, all the others are ints ", "if (level == SOL_TCP && optname == TCP_ZEROCOPY_RECEIVE)return true;return false;}EXPORT_SYMBOL(tcp_bpf_bypass_getsockopt": "tcp_bpf_bypass_getsockopt(int level, int optname){  TCP do_tcp_getsockopt has optimized getsockopt implementation   to avoid extra socket lock for TCP_ZEROCOPY_RECEIVE. ", "if (level == SOL_TCP && optname == TCP_ZEROCOPY_RECEIVE)return true;return false;}EXPORT_SYMBOL(tcp_bpf_bypass_getsockopt);int tcp_getsockopt(struct sock *sk, int level, int optname, char __user *optval,   int __user *optlen)": "tcp_getsockopt(struct sock  sk, int level,      int optname, sockptr_t optval, sockptr_t optlen){struct inet_connection_sock  icsk = inet_csk(sk);struct tcp_sock  tp = tcp_sk(sk);struct net  net = sock_net(sk);int val, len;if (copy_from_sockptr(&len, optlen, sizeof(int)))return -EFAULT;len = min_t(unsigned int, len, sizeof(int));if (len < 0)return -EINVAL;switch (optname) {case TCP_MAXSEG:val = tp->mss_cache;if (tp->rx_opt.user_mss &&    ((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN)))val = tp->rx_opt.user_mss;if (tp->repair)val = tp->rx_opt.mss_clamp;break;case TCP_NODELAY:val = !!(tp->nonagle&TCP_NAGLE_OFF);break;case TCP_CORK:val = !!(tp->nonagle&TCP_NAGLE_CORK);break;case TCP_KEEPIDLE:val = keepalive_time_when(tp)  HZ;break;case TCP_KEEPINTVL:val = keepalive_intvl_when(tp)  HZ;break;case TCP_KEEPCNT:val = keepalive_probes(tp);break;case TCP_SYNCNT:val = READ_ONCE(icsk->icsk_syn_retries) ? :READ_ONCE(net->ipv4.sysctl_tcp_syn_retries);break;case TCP_LINGER2:val = READ_ONCE(tp->linger2);if (val >= 0)val = (val ? : READ_ONCE(net->ipv4.sysctl_tcp_fin_timeout))  HZ;break;case TCP_DEFER_ACCEPT:val = READ_ONCE(icsk->icsk_accept_queue.rskq_defer_accept);val = retrans_to_secs(val, TCP_TIMEOUT_INIT  HZ,      TCP_RTO_MAX  HZ);break;case TCP_WINDOW_CLAMP:val = tp->window_clamp;break;case TCP_INFO: {struct tcp_info info;if (copy_from_sockptr(&len, optlen, sizeof(int)))return -EFAULT;tcp_get_info(sk, &info);len = min_t(unsigned int, len, sizeof(info));if (copy_to_sockptr(optlen, &len, sizeof(int)))return -EFAULT;if (copy_to_sockptr(optval, &info, len))return -EFAULT;return 0;}case TCP_CC_INFO: {const struct tcp_congestion_ops  ca_ops;union tcp_cc_info info;size_t sz = 0;int attr;if (copy_from_sockptr(&len, optlen, sizeof(int)))return -EFAULT;ca_ops = icsk->icsk_ca_ops;if (ca_ops && ca_ops->get_info)sz = ca_ops->get_info(sk, ~0U, &attr, &info);len = min_t(unsigned int, len, sz);if (copy_to_sockptr(optlen, &len, sizeof(int)))return -EFAULT;if (copy_to_sockptr(optval, &info, len))return -EFAULT;return 0;}case TCP_QUICKACK:val = !inet_csk_in_pingpong_mode(sk);break;case TCP_CONGESTION:if (copy_from_sockptr(&len, optlen, sizeof(int)))return -EFAULT;len = min_t(unsigned int, len, TCP_CA_NAME_MAX);if (copy_to_sockptr(optlen, &len, sizeof(int)))return -EFAULT;if (copy_to_sockptr(optval, icsk->icsk_ca_ops->name, len))return -EFAULT;return 0;case TCP_ULP:if (copy_from_sockptr(&len, optlen, sizeof(int)))return -EFAULT;len = min_t(unsigned int, len, TCP_ULP_NAME_MAX);if (!icsk->icsk_ulp_ops) {len = 0;if (copy_to_sockptr(optlen, &len, sizeof(int)))return -EFAULT;return 0;}if (copy_to_sockptr(optlen, &len, sizeof(int)))return -EFAULT;if (copy_to_sockptr(optval, icsk->icsk_ulp_ops->name, len))return -EFAULT;return 0;case TCP_FASTOPEN_KEY: {u64 key[TCP_FASTOPEN_KEY_BUF_LENGTH  sizeof(u64)];unsigned int key_len;if (copy_from_sockptr(&len, optlen, sizeof(int)))return -EFAULT;key_len = tcp_fastopen_get_cipher(net, icsk, key)  TCP_FASTOPEN_KEY_LENGTH;len = min_t(unsigned int, len, key_len);if (copy_to_sockptr(optlen, &len, sizeof(int)))return -EFAULT;if (copy_to_sockptr(optval, key, len))return -EFAULT;return 0;}case TCP_THIN_LINEAR_TIMEOUTS:val = tp->thin_lto;break;case TCP_THIN_DUPACK:val = 0;break;case TCP_REPAIR:val = tp->repair;break;case TCP_REPAIR_QUEUE:if (tp->repair)val = tp->repair_queue;elsereturn -EINVAL;break;case TCP_REPAIR_WINDOW: {struct tcp_repair_window opt;if (copy_from_sockptr(&len, optlen, sizeof(int)))return -EFAULT;if (len != sizeof(opt))return -EINVAL;if (!tp->repair)return -EPERM;opt.snd_wl1= tp->snd_wl1;opt.snd_wnd= tp->snd_wnd;opt.max_window= tp->max_window;opt.rcv_wnd= tp->rcv_wnd;opt.rcv_wup= tp->rcv_wup;if (copy_to_sockptr(optval, &opt, len))return -EFAULT;return 0;}case TCP_QUEUE_SEQ:if (tp->repair_queue == TCP_SEND_QUEUE)val = tp->write_seq;else if (tp->repair_queue == TCP_RECV_QUEUE)val = tp->rcv_nxt;elsereturn -EINVAL;break;case TCP_USER_TIMEOUT:val = READ_ONCE(icsk->icsk_user_timeout);break;case TCP_FASTOPEN:val = READ_ONCE(icsk->icsk_accept_queue.fastopenq.max_qlen);break;case TCP_FASTOPEN_CONNECT:val = tp->fastopen_connect;break;case TCP_FASTOPEN_NO_COOKIE:val = tp->fastopen_no_cookie;break;case TCP_TX_DELAY:val = READ_ONCE(tp->tcp_tx_delay);break;case TCP_TIMESTAMP:val = tcp_time_stamp_raw() + READ_ONCE(tp->tsoffset);break;case TCP_NOTSENT_LOWAT:val = READ_ONCE(tp->notsent_lowat);break;case TCP_INQ:val = tp->recvmsg_inq;break;case TCP_SAVE_SYN:val = tp->save_syn;break;case TCP_SAVED_SYN: {if (copy_from_sockptr(&len, optlen, sizeof(int)))return -EFAULT;sockopt_lock_sock(sk);if (tp->saved_syn) {if (len < tcp_saved_syn_len(tp->saved_syn)) {len = tcp_saved_syn_len(tp->saved_syn);if (copy_to_sockptr(optlen, &len, sizeof(int))) {sockopt_release_sock(sk);return -EFAULT;}sockopt_release_sock(sk);return -EINVAL;}len = tcp_saved_syn_len(tp->saved_syn);if (copy_to_sockptr(optlen, &len, sizeof(int))) {sockopt_release_sock(sk);return -EFAULT;}if (copy_to_sockptr(optval, tp->saved_syn->data, len)) {sockopt_release_sock(sk);return -EFAULT;}tcp_saved_syn_free(tp);sockopt_release_sock(sk);} else {sockopt_release_sock(sk);len = 0;if (copy_to_sockptr(optlen, &len, sizeof(int)))return -EFAULT;}return 0;}#ifdef CONFIG_MMUcase TCP_ZEROCOPY_RECEIVE: {struct scm_timestamping_internal tss;struct tcp_zerocopy_receive zc = {};int err;if (copy_from_sockptr(&len, optlen, sizeof(int)))return -EFAULT;if (len < 0 ||    len < offsetofend(struct tcp_zerocopy_receive, length))return -EINVAL;if (unlikely(len > sizeof(zc))) {err = check_zeroed_sockptr(optval, sizeof(zc),   len - sizeof(zc));if (err < 1)return err == 0 ? -EINVAL : err;len = sizeof(zc);if (copy_to_sockptr(optlen, &len, sizeof(int)))return -EFAULT;}if (copy_from_sockptr(&zc, optval, len))return -EFAULT;if (zc.reserved)return -EINVAL;if (zc.msg_flags &  ~(TCP_VALID_ZC_MSG_FLAGS))return -EINVAL;sockopt_lock_sock(sk);err = tcp_zerocopy_receive(sk, &zc, &tss);err = BPF_CGROUP_RUN_PROG_GETSOCKOPT_KERN(sk, level, optname,  &zc, &len, err);sockopt_release_sock(sk);if (len >= offsetofend(struct tcp_zerocopy_receive, msg_flags))goto zerocopy_rcv_cmsg;switch (len) {case offsetofend(struct tcp_zerocopy_receive, msg_flags):goto zerocopy_rcv_cmsg;case offsetofend(struct tcp_zerocopy_receive, msg_controllen):case offsetofend(struct tcp_zerocopy_receive, msg_control):case offsetofend(struct tcp_zerocopy_receive, flags):case offsetofend(struct tcp_zerocopy_receive, copybuf_len):case offsetofend(struct tcp_zerocopy_receive, copybuf_address):case offsetofend(struct tcp_zerocopy_receive, err):goto zerocopy_rcv_sk_err;case offsetofend(struct tcp_zerocopy_receive, inq):goto zerocopy_rcv_inq;case offsetofend(struct tcp_zerocopy_receive, length):default:goto zerocopy_rcv_out;}zerocopy_rcv_cmsg:if (zc.msg_flags & TCP_CMSG_TS)tcp_zc_finalize_rx_tstamp(sk, &zc, &tss);elsezc.msg_flags = 0;zerocopy_rcv_sk_err:if (!err)zc.err = sock_error(sk);zerocopy_rcv_inq:zc.inq = tcp_inq_hint(sk);zerocopy_rcv_out:if (!err && copy_to_sockptr(optval, &zc, len))err = -EFAULT;return err;}#endifdefault:return -ENOPROTOOPT;}if (copy_to_sockptr(optlen, &len, sizeof(int)))return -EFAULT;if (copy_to_sockptr(optval, &val, len))return -EFAULT;return 0;}bool tcp_bpf_bypass_getsockopt(int level, int optname){  TCP do_tcp_getsockopt has optimized getsockopt implementation   to avoid extra socket lock for TCP_ZEROCOPY_RECEIVE. ", "smp_wmb();/* Paired with READ_ONCE() from tcp_alloc_md5sig_pool() * and tcp_get_md5sig_pool().": "tcp_get_md5sig_pool() ", "struct scatterlist sg;sg_init_one(&sg, key->key, keylen);ahash_request_set_crypt(hp->md5_req, &sg, NULL, keylen);/* We use data_race() because tcp_md5_do_add() might change key->key under us ": "tcp_md5_hash_key(struct tcp_md5sig_pool  hp, const struct tcp_md5sig_key  key){u8 keylen = READ_ONCE(key->keylen);   paired with WRITE_ONCE() in tcp_md5_do_add ", "const __u8 *hash_location = NULL;struct tcp_md5sig_key *hash_expected;const struct tcphdr *th = tcp_hdr(skb);const struct tcp_sock *tp = tcp_sk(sk);int genhash, l3index;u8 newhash[16];/* sdif set, means packet ingressed via a device * in an L3 domain and dif is set to the l3mdev ": "tcp_inbound_md5_hash(const struct sock  sk, const struct sk_buff  skb,     const void  saddr, const void  daddr,     int family, int dif, int sdif){    This gets called for each TCP segment that arrives   so we want to be efficient.   We have 3 drop cases:   o No MD5 hash and one expected.   o MD5 hash and we're not expecting one.   o MD5 hash and its wrong. ", "#include <linux/cache.h>#include <linux/module.h>#include <linux/netdevice.h>#include <linux/spinlock.h>#include <net/protocol.h>struct net_protocol __rcu *inet_protos[MAX_INET_PROTOS] __read_mostly;EXPORT_SYMBOL(inet_protos);const struct net_offload __rcu *inet_offloads[MAX_INET_PROTOS] __read_mostly;EXPORT_SYMBOL(inet_offloads);int inet_add_protocol(const struct net_protocol *prot, unsigned char protocol)": "inet_del_protocol() to correctly    maintain copy bit. ", "if (skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL)partial = (__force __wsum)uh->len;elsepartial = (__force __wsum)htonl(skb->len);partial = csum_sub(csum_unfold(uh->check), partial);/* setup inner skb. ": "skb_udp_tunnel_segment(struct sk_buff  skb,netdev_features_t features,struct sk_buff  ( gso_inner_segment)(struct sk_buff  skb,     netdev_features_t features),__be16 new_protocol, bool is_ipv6){int tnl_hlen = skb_inner_mac_header(skb) - skb_transport_header(skb);bool remcsum, need_csum, offload_csum, gso_partial;struct sk_buff  segs = ERR_PTR(-EINVAL);struct udphdr  uh = udp_hdr(skb);u16 mac_offset = skb->mac_header;__be16 protocol = skb->protocol;u16 mac_len = skb->mac_len;int udp_offset, outer_hlen;__wsum partial;bool need_ipsec;if (unlikely(!pskb_may_pull(skb, tnl_hlen)))goto out;  Adjust partial header checksum to negate old length.   We cannot rely on the value contained in uh->len as it is   possible that the actual value exceeds the boundaries of the   16 bit length field due to the header being added outside of an   IP or IPv6 frame that was already limited to 64K - 1. ", "if (!uh->check) ": "udp_gro_receive_segment(struct list_head  head,       struct sk_buff  skb){struct udphdr  uh = udp_gro_udphdr(skb);struct sk_buff  pp = NULL;struct udphdr  uh2;struct sk_buff  p;unsigned int ulen;int ret = 0;  requires non zero csum, for symmetry with GSO ", "NAPI_GRO_CB(skb)->encap_mark = 0;/* Set encapsulation before calling into inner gro_complete() * functions to make them set up the inner offsets. ": "udp_gro_complete_segment(struct sk_buff  skb){struct udphdr  uh = udp_hdr(skb);skb->csum_start = (unsigned char  )uh - skb->head;skb->csum_offset = offsetof(struct udphdr, check);skb->ip_summed = CHECKSUM_PARTIAL;skb_shinfo(skb)->gso_segs = NAPI_GRO_CB(skb)->count;skb_shinfo(skb)->gso_type |= SKB_GSO_UDP_L4;if (skb->encapsulation)skb->inner_transport_header = skb->transport_header;return 0;}int udp_gro_complete(struct sk_buff  skb, int nhoff,     udp_lookup_t lookup){__be16 newlen = htons(skb->len - nhoff);struct udphdr  uh = (struct udphdr  )(skb->data + nhoff);struct sock  sk;int err;uh->len = newlen;sk = INDIRECT_CALL_INET(lookup, udp6_lib_lookup_skb,udp4_lib_lookup_skb, skb, uh->source, uh->dest);if (sk && udp_sk(sk)->gro_complete) {skb_shinfo(skb)->gso_type = uh->check ? SKB_GSO_UDP_TUNNEL_CSUM: SKB_GSO_UDP_TUNNEL;  clear the encap mark, so that inner frag_list gro_complete   can take place ", "if (addr_len < sizeof(struct sockaddr_in))return -EINVAL;sock_owned_by_me(sk);return BPF_CGROUP_RUN_PROG_INET4_CONNECT(sk, uaddr);}/* This will initiate an outgoing connection. ": "tcp_v4_connect() and intended to   prevent BPF program called below from accessing bytes that are out   of the bound specified by user in addr_len. ", "if (mtu < dst_mtu(dst) && ip_dont_fragment(sk, dst))WRITE_ONCE(sk->sk_err_soft, EMSGSIZE);mtu = dst_mtu(dst);if (inet->pmtudisc != IP_PMTUDISC_DONT &&    ip_sk_accept_pmtu(sk) &&    inet_csk(sk)->icsk_pmtu_cookie > mtu) ": "tcp_v4_mtu_reduced(struct sock  sk){struct inet_sock  inet = inet_sk(sk);struct dst_entry  dst;u32 mtu;if ((1 << sk->sk_state) & (TCPF_LISTEN | TCPF_CLOSE))return;mtu = READ_ONCE(tcp_sk(sk)->mtu_info);dst = inet_csk_update_pmtu(sk, mtu);if (!dst)return;  Something is about to be wrong... Remember soft error   for the case, if this connection will not able to recover. ", "if (seq != tcp_rsk(req)->snt_isn) ": "tcp_req_err(struct sock  sk, u32 seq, bool abort){struct request_sock  req = inet_reqsk(sk);struct net  net = sock_net(sk);  ICMPs are not backlogged, hence we cannot get   an established socket here. ", "tcp_retransmit_timer(sk);}}EXPORT_SYMBOL(tcp_ld_RTO_revert": "tcp_ld_RTO_revert(struct sock  sk, u32 seq){struct inet_connection_sock  icsk = inet_csk(sk);struct tcp_sock  tp = tcp_sk(sk);struct sk_buff  skb;s32 remaining;u32 delta_us;if (sock_owned_by_user(sk))return;if (seq != tp->snd_una  || !icsk->icsk_retransmits ||    !icsk->icsk_backoff)return;skb = tcp_rtx_queue_head(sk);if (WARN_ON_ONCE(!skb))return;icsk->icsk_backoff--;icsk->icsk_rto = tp->srtt_us ? __tcp_set_rto(tp) : TCP_TIMEOUT_INIT;icsk->icsk_rto = inet_csk_rto_backoff(icsk, TCP_RTO_MAX);tcp_mstamp_refresh(tp);delta_us = (u32)(tp->tcp_mstamp - tcp_skb_timestamp_us(skb));remaining = icsk->icsk_rto - usecs_to_jiffies(delta_us);if (remaining > 0) {inet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,  remaining, TCP_RTO_MAX);} else {  RTO revert clocked out retransmission.   Will retransmit now. ", "void tcp_v4_send_check(struct sock *sk, struct sk_buff *skb)": "tcp_v4_send_check(struct sk_buff  skb, __be32 saddr, __be32 daddr){struct tcphdr  th = tcp_hdr(skb);th->check = ~tcp_v4_check(skb->len, saddr, daddr, 0);skb->csum_start = skb_transport_header(skb) - skb->head;skb->csum_offset = offsetof(struct tcphdr, check);}  This routine computes an IPv4 TCP checksum. ", "md5sig = rcu_dereference_check(tp->md5sig_info,       lockdep_sock_is_held(sk));if (!md5sig)return NULL;hlist_for_each_entry_rcu(key, &md5sig->head, node, lockdep_sock_is_held(sk)) ": "__tcp_md5_do_lookup(const struct sock  sk, int l3index,   const union tcp_md5_addr  addr,   int family){const struct tcp_sock  tp = tcp_sk(sk);struct tcp_md5sig_key  key;const struct tcp_md5sig_info  md5sig;__be32 mask;struct tcp_md5sig_key  best_match = NULL;bool match;  caller either holds rcu_read_lock() or socket lock ", "struct tcp_md5sig_key *key;struct tcp_sock *tp = tcp_sk(sk);struct tcp_md5sig_info *md5sig;key = tcp_md5_do_lookup_exact(sk, addr, family, prefixlen, l3index, flags);if (key) ": "tcp_md5_do_add(struct sock  sk, const union tcp_md5_addr  addr,    int family, u8 prefixlen, int l3index, u8 flags,    const u8  newkey, u8 newkeylen, gfp_t gfp){  Add Key to the list ", "arg.iov[0].iov_len += TCPOLEN_MD5SIG_ALIGNED;rep.th.doff = arg.iov[0].iov_len / 4;tcp_v4_md5_hash_hdr((__u8 *) &rep.opt[1],     key, ip_hdr(skb)->saddr,     ip_hdr(skb)->daddr, &rep.th);}#endif/* Can't co-exist with TCPMD5, hence check rep.opt[0] ": "tcp_v4_md5_hash_skb(newhash, key, NULL, skb);if (genhash || memcmp(hash_location, newhash, 16) != 0)goto out;}if (key) {rep.opt[0] = htonl((TCPOPT_NOP << 24) |   (TCPOPT_NOP << 16) |   (TCPOPT_MD5SIG << 8) |   TCPOLEN_MD5SIG);  Update length and the length the header thinks exists ", "if (skb_rtable(skb)->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST))goto drop;return tcp_conn_request(&tcp_request_sock_ops,&tcp_request_sock_ipv4_ops, sk, skb);drop:tcp_listendrop(sk);return 0;}EXPORT_SYMBOL(tcp_v4_conn_request": "tcp_v4_conn_request(struct sock  sk, struct sk_buff  skb){  Never answer to SYNs send to broadcast or multicast ", "if (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_reflect_tos))newinet->tos = tcp_rsk(req)->syn_tos & ~INET_ECN_MASK;if (!dst) ": "inet_sk_rx_dst_set(newsk, skb);newtp      = tcp_sk(newsk);newinet      = inet_sk(newsk);ireq      = inet_rsk(req);sk_daddr_set(newsk, ireq->ir_rmt_addr);sk_rcv_saddr_set(newsk, ireq->ir_loc_addr);newsk->sk_bound_dev_if = ireq->ir_iif;newinet->inet_saddr   = ireq->ir_loc_addr;inet_opt      = rcu_dereference(ireq->ireq_opt);RCU_INIT_POINTER(newinet->inet_opt, inet_opt);newinet->mc_index     = inet_iif(skb);newinet->mc_ttl      = ip_hdr(skb)->ttl;newinet->rcv_tos      = ip_hdr(skb)->tos;inet_csk(newsk)->icsk_ext_hdr_len = 0;if (inet_opt)inet_csk(newsk)->icsk_ext_hdr_len = inet_opt->opt.optlen;newinet->inet_id = get_random_u16();  Set ToS of the new socket based upon the value of incoming SYN.   ECT bits are set later in tcp_init_transfer(). ", "struct dst_entry *dst;dst = rcu_dereference_protected(sk->sk_rx_dst,lockdep_sock_is_held(sk));sock_rps_save_rxhash(sk, skb);sk_mark_napi_id(sk, skb);if (dst) ": "tcp_v4_do_rcv(struct sock  sk, struct sk_buff  skb){enum skb_drop_reason reason;struct sock  rsk;if (sk->sk_state == TCP_ESTABLISHED) {   Fast path ", "skb_condense(skb);skb_dst_drop(skb);if (unlikely(tcp_checksum_complete(skb))) ": "tcp_add_backlog(struct sock  sk, struct sk_buff  skb,     enum skb_drop_reason  reason){u32 limit, tail_gso_size, tail_gso_segs;struct skb_shared_info  shinfo;const struct tcphdr  th;struct tcphdr  thtail;struct sk_buff  tail;unsigned int hdrlen;bool fragstolen;u32 gso_segs;u32 gso_size;int delta;  In case all data was pulled from skb frags (in __pskb_pull_tail()),   we can fix skb->truesize to its real value to avoid future drops.   This is valid because skb is not yet charged to the socket.   It has been noticed pure SACK packets were sometimes dropped   (if cooked by drivers without copybreak feature). ", "tcp_write_queue_purge(sk);/* Check if we want to disable active TFO ": "tcp_v4_destroy_sock(struct sock  sk){struct tcp_sock  tp = tcp_sk(sk);trace_tcp_destroy_sock(sk);tcp_clear_xmit_timers(sk);tcp_cleanup_congestion_control(sk);tcp_cleanup_ulp(sk);  Cleanup up the write buffer. ", "if (tp->syn_data && sk->sk_state == TCP_SYN_SENT)mss = -1;elsemss = tcp_current_mss(sk);skb_rbtree_walk(skb, &sk->tcp_rtx_queue) ": "tcp_simple_retransmit(struct sock  sk){const struct inet_connection_sock  icsk = inet_csk(sk);struct tcp_sock  tp = tcp_sk(sk);struct sk_buff  skb;int mss;  A fastopen SYN request is stored as two separate packets within   the retransmit queue, this is done by tcp_send_syn_data().   As a result simply checking the MSS of the frames in the queue   will not work for the SYN packet.     Us being here is an indication of a path MTU issue so we can   assume that the fastopen SYN was lost and just mark all the   frames in the retransmit queue as lost. We will use an MSS of   -1 to mark all frames as lost, otherwise compute the current MSS. ", "length--;continue;default:if (length < 2)return;opsize = *ptr++;if (opsize < 2) /* \"silly options\" ": "tcp_parse_options(const struct net  net,       const struct sk_buff  skb,       struct tcp_options_received  opt_rx, int estab,       struct tcp_fastopen_cookie  foc){const unsigned char  ptr;const struct tcphdr  th = tcp_hdr(skb);int length = (th->doff   4) - sizeof(struct tcphdr);ptr = (const unsigned char  )(th + 1);opt_rx->saw_tstamp = 0;opt_rx->saw_unknown = 0;while (length > 0) {int opcode =  ptr++;int opsize;switch (opcode) {case TCPOPT_EOL:return;case TCPOPT_NOP:  Ref: RFC 793 section 3.1 ", "while (length >= TCPOLEN_MD5SIG) ": "tcp_parse_md5sig_option(const struct tcphdr  th){int length = (th->doff << 2) - sizeof( th);const u8  ptr = (const u8  )(th + 1);  If not enough data remaining, we can short cut ", "trace_tcp_probe(sk, skb);tcp_mstamp_refresh(tp);if (unlikely(!rcu_access_pointer(sk->sk_rx_dst)))inet_csk(sk)->icsk_af_ops->sk_rx_dst_set(sk, skb);/* *Header prediction. *The code loosely follows the one in the famous *\"30 instruction TCP receive\" Van Jacobson mail. * *Van's trick is to deposit buffers into socket queue *on a device interrupt, to call tcp_recv function *on the receive process context and checksum and copy *the buffer to user space. smart... * *Our current scheme is not silly either but we take the *extra cost of the net_bh soft interrupt processing... *We do checksum and copy also but from device to kernel. ": "tcp_rcv_established(struct sock  sk, struct sk_buff  skb){enum skb_drop_reason reason = SKB_DROP_REASON_NOT_SPECIFIED;const struct tcphdr  th = (const struct tcphdr  )skb->data;struct tcp_sock  tp = tcp_sk(sk);unsigned int len = skb->len;  TCP congestion window tracking ", "return -1;#elsegoto consume;#endif}/* \"fifth, if neither of the SYN or RST bits is set then * drop the segment and return.\" ": "tcp_rcv_state_process   is not flawless. So, discard packet for sanity.   Uncomment this return to process the data. ", "if ((syncookies == 2 || inet_csk_reqsk_queue_is_full(sk)) && !isn) ": "tcp_conn_request(struct request_sock_ops  rsk_ops,     const struct tcp_request_sock_ops  af_ops,     struct sock  sk, struct sk_buff  skb){struct tcp_fastopen_cookie foc = { .len = -1 };__u32 isn = TCP_SKB_CB(skb)->tcp_tw_isn;struct tcp_options_received tmp_opt;struct tcp_sock  tp = tcp_sk(sk);struct net  net = sock_net(sk);struct sock  fastopen_sk = NULL;struct request_sock  req;bool want_cookie = false;struct dst_entry  dst;struct flowi fl;u8 syncookies;syncookies = READ_ONCE(net->ipv4.sysctl_tcp_syncookies);  TW buckets are converted to open requests without   limitations, they conserve resources and peer is   evidently real one. ", "int skb_tunnel_check_pmtu(struct sk_buff *skb, struct dst_entry *encap_dst,  int headroom, bool reply)": "skb_tunnel_check_pmtu() - Check, update PMTU and trigger ICMP reply as needed   @skb:Buffer being sent by encapsulation, L2 headers expected   @encap_dst:Destination for tunnel encapsulation (outer IP)   @headroom:Encapsulation header size, bytes   @reply:Build matching ICMP or ICMPv6 message as a result     L2 tunnel implementations that can carry IP and can be directly bridged   (currently UDP tunnels) can't always rely on IP forwarding paths to handle   PMTU discovery. In the bridged case, ICMP or ICMPv6 messages need to be built   based on payload and sent back by the encapsulation itself.     For routable interfaces, we just need to update the PMTU for the destination.     Return: 0 if ICMP error not needed, length if built, negative value on error ", "if (!template_ops)return 0;num_ops = hweight32(table->valid_hooks);if (num_ops == 0) ": "ipt_register_table(struct net  net, const struct xt_table  table,       const struct ipt_replace  repl,       const struct nf_hook_ops  template_ops){struct nf_hook_ops  ops;unsigned int num_ops;int ret, i;struct xt_table_info  newinfo;struct xt_table_info bootstrap = {0};void  loc_cpu_entry;struct xt_table  new_table;newinfo = xt_alloc_table_info(repl->size);if (!newinfo)return -ENOMEM;loc_cpu_entry = newinfo->entries;memcpy(loc_cpu_entry, repl->entries, repl->size);ret = translate_table(net, newinfo, loc_cpu_entry, repl);if (ret != 0) {xt_free_table_info(newinfo);return ret;}new_table = xt_register_table(net, table, &bootstrap, newinfo);if (IS_ERR(new_table)) {struct ipt_entry  iter;xt_entry_foreach(iter, loc_cpu_entry, newinfo->size)cleanup_entry(iter, net);xt_free_table_info(newinfo);return PTR_ERR(new_table);}  No template? No need to do anything. This is used by 'nat' table, it registers   with the nat core instead of the netfilter core. ", "ret = xt_register_targets(ipt_builtin_tg, ARRAY_SIZE(ipt_builtin_tg));if (ret < 0)goto err2;/* Register setsockopt ": "ipt_unregister_table_exit(struct net  net, const char  name){struct xt_table  table = xt_find_table(net, NFPROTO_IPV4, name);if (table)__ipt_unregister_table(net, table);}static struct xt_target ipt_builtin_tg[] __read_mostly = {{.name             = XT_STANDARD_TARGET,.targetsize       = sizeof(int),.family           = NFPROTO_IPV4,#ifdef CONFIG_NETFILTER_XTABLES_COMPAT.compatsize       = sizeof(compat_int_t),.compat_from_user = compat_standard_from_user,.compat_to_user   = compat_standard_to_user,#endif},{.name             = XT_ERROR_TARGET,.target           = ipt_error,.targetsize       = XT_FUNCTION_MAXNAMELEN,.family           = NFPROTO_IPV4,},};static struct nf_sockopt_ops ipt_sockopts = {.pf= PF_INET,.set_optmin= IPT_BASE_CTL,.set_optmax= IPT_SO_SET_MAX+1,.set= do_ipt_set_ctl,.get_optmin= IPT_BASE_CTL,.get_optmax= IPT_SO_GET_MAX+1,.get= do_ipt_get_ctl,.owner= THIS_MODULE,};static int __net_init ip_tables_net_init(struct net  net){return xt_proto_init(net, NFPROTO_IPV4);}static void __net_exit ip_tables_net_exit(struct net  net){xt_proto_fini(net, NFPROTO_IPV4);}static struct pernet_operations ip_tables_net_ops = {.init = ip_tables_net_init,.exit = ip_tables_net_exit,};static int __init ip_tables_init(void){int ret;ret = register_pernet_subsys(&ip_tables_net_ops);if (ret < 0)goto err1;  No one else will be downing sem now, so we won't sleep ", "unsigned int verdict = NF_DROP;const char *indev, *outdev;const void *table_base;struct ipt_entry *e, **jumpstack;unsigned int stackidx, cpu;const struct xt_table_info *private;struct xt_action_param acpar;unsigned int addend;/* Initialization ": "ipt_do_table(void  priv,     struct sk_buff  skb,     const struct nf_hook_state  state){const struct xt_table  table = priv;unsigned int hook = state->hook;static const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));const struct iphdr  ip;  Initializing verdict to NF_DROP keeps gcc happy. ", "static struct xt_target arpt_builtin_tg[] __read_mostly = ": "arpt_register_table(struct net  net,const struct xt_table  table,const struct arpt_replace  repl,const struct nf_hook_ops  template_ops){struct nf_hook_ops  ops;unsigned int num_ops;int ret, i;struct xt_table_info  newinfo;struct xt_table_info bootstrap = {0};void  loc_cpu_entry;struct xt_table  new_table;newinfo = xt_alloc_table_info(repl->size);if (!newinfo)return -ENOMEM;loc_cpu_entry = newinfo->entries;memcpy(loc_cpu_entry, repl->entries, repl->size);ret = translate_table(net, newinfo, loc_cpu_entry, repl);if (ret != 0) {xt_free_table_info(newinfo);return ret;}new_table = xt_register_table(net, table, &bootstrap, newinfo);if (IS_ERR(new_table)) {struct arpt_entry  iter;xt_entry_foreach(iter, loc_cpu_entry, newinfo->size)cleanup_entry(iter, net);xt_free_table_info(newinfo);return PTR_ERR(new_table);}num_ops = hweight32(table->valid_hooks);if (num_ops == 0) {ret = -EINVAL;goto out_free;}ops = kmemdup(template_ops, sizeof( ops)   num_ops, GFP_KERNEL);if (!ops) {ret = -ENOMEM;goto out_free;}for (i = 0; i < num_ops; i++)ops[i].priv = new_table;new_table->ops = ops;ret = nf_register_net_hooks(net, ops, num_ops);if (ret != 0)goto out_free;return ret;out_free:__arpt_unregister_table(net, new_table);return ret;}void arpt_unregister_table_pre_exit(struct net  net, const char  name){struct xt_table  table = xt_find_table(net, NFPROTO_ARP, name);if (table)nf_unregister_net_hooks(net, table->ops, hweight32(table->valid_hooks));}EXPORT_SYMBOL(arpt_unregister_table_pre_exit);void arpt_unregister_table(struct net  net, const char  name){struct xt_table  table = xt_find_table(net, NFPROTO_ARP, name);if (table)__arpt_unregister_table(net, table);}  The built-in targets: standard (NULL) and error. ", "loc_cpu_entry = private->entries;xt_entry_foreach(iter, loc_cpu_entry, private->size)cleanup_entry(iter, net);if (private->number > private->initial_entries)module_put(table_owner);xt_free_table_info(private);}int arpt_register_table(struct net *net,const struct xt_table *table,const struct arpt_replace *repl,const struct nf_hook_ops *template_ops)": "arpt_unregister_table(struct net  net, struct xt_table  table){struct xt_table_info  private;void  loc_cpu_entry;struct module  table_owner = table->me;struct arpt_entry  iter;private = xt_unregister_table(table);  Decrease module usage counts and free resources ", "cpu     = smp_processor_id();table_base = private->entries;jumpstack  = (struct arpt_entry **)private->jumpstack[cpu];/* No TEE support for arptables, so no need to switch to alternate * stack.  All targets that reenter must return absolute verdicts. ": "arpt_do_table(void  priv,   struct sk_buff  skb,   const struct nf_hook_state  state){const struct xt_table  table = priv;unsigned int hook = state->hook;static const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));unsigned int verdict = NF_DROP;const struct arphdr  arp;struct arpt_entry  e,   jumpstack;const char  indev,  outdev;const void  table_base;unsigned int cpu, stackidx = 0;const struct xt_table_info  private;struct xt_action_param acpar;unsigned int addend;if (!pskb_may_pull(skb, arp_hdr_len(skb->dev)))return NF_DROP;indev = state->in ? state->in->name : nulldevname;outdev = state->out ? state->out->name : nulldevname;local_bh_disable();addend = xt_write_recseq_begin();private = READ_ONCE(table->private);   Address dependency. ", "tcph->psh = 0;tcph->fin = 0;tcph->rst = 0;}} else ": "tso_build_hdr(const struct sk_buff  skb, char  hdr, struct tso_t  tso,   int size, bool is_last){int hdr_len = skb_transport_offset(skb) + tso->tlen;int mac_hdr_len = skb_network_offset(skb);memcpy(hdr, skb->data, hdr_len);if (!tso->ipv6) {struct iphdr  iph = (void  )(hdr + mac_hdr_len);iph->id = htons(tso->ip_id);iph->tot_len = htons(size + hdr_len - mac_hdr_len);tso->ip_id++;} else {struct ipv6hdr  iph = (void  )(hdr + mac_hdr_len);iph->payload_len = htons(size + tso->tlen);}hdr += skb_transport_offset(skb);if (tso->tlen != sizeof(struct udphdr)) {struct tcphdr  tcph = (struct tcphdr  )hdr;put_unaligned_be32(tso->tcp_seq, &tcph->seq);if (!is_last) {  Clear all special flags for not last packet ", "tso->size -= size;tso->data += size;if ((tso->size == 0) &&    (tso->next_frag_idx < skb_shinfo(skb)->nr_frags)) ": "tso_build_data(const struct sk_buff  skb, struct tso_t  tso, int size){tso->tcp_seq += size;   not worth avoiding this operation for UDP ", "tso->size = skb_headlen(skb) - hdr_len;tso->data = skb->data + hdr_len;if ((tso->size == 0) &&    (tso->next_frag_idx < skb_shinfo(skb)->nr_frags)) ": "tso_start(struct sk_buff  skb, struct tso_t  tso){int tlen = skb_is_gso_tcp(skb) ? tcp_hdrlen(skb) : sizeof(struct udphdr);int hdr_len = skb_transport_offset(skb) + tlen;tso->tlen = tlen;tso->ip_id = ntohs(ip_hdr(skb)->id);tso->tcp_seq = (tlen != sizeof(struct udphdr)) ? ntohl(tcp_hdr(skb)->seq) : 0;tso->next_frag_idx = 0;tso->ipv6 = vlan_get_protocol(skb) == htons(ETH_P_IPV6);  Build first data ", "stats->alloc_stats.fast += pool->alloc_stats.fast;stats->alloc_stats.slow += pool->alloc_stats.slow;stats->alloc_stats.slow_high_order += pool->alloc_stats.slow_high_order;stats->alloc_stats.empty += pool->alloc_stats.empty;stats->alloc_stats.refill += pool->alloc_stats.refill;stats->alloc_stats.waive += pool->alloc_stats.waive;for_each_possible_cpu(cpu) ": "page_pool_get_stats(struct page_pool  pool, struct page_pool_stats  stats){int cpu = 0;if (!stats)return false;  The caller is responsible to initialize stats. ", "refcount_set(&pool->user_cnt, 1);if (pool->p.flags & PP_FLAG_DMA_MAP)get_device(pool->p.dev);return 0;}struct page_pool *page_pool_create(const struct page_pool_params *params)": "page_pool_destroy() ", "if (unlikely(pp_order))return __page_pool_alloc_page_order(pool, gfp);/* Unnecessary as alloc cache is empty, but guarantees zero count ": "page_pool_alloc_pages_slow(struct page_pool  pool, gfp_t gfp){const int bulk = PP_ALLOC_CACHE_REFILL;unsigned int pp_flags = pool->p.flags;unsigned int pp_order = pool->p.order;struct page  page;int i, nr_pages;  Don't support bulk alloc for high-order pages ", "goto skip_dma_unmap;dma = page_pool_get_dma_addr(page);/* When page is unmapped, it cannot be returned to our pool ": "page_pool_release_page(struct page_pool  pool, struct page  page){dma_addr_t dma;int count;if (!(pool->p.flags & PP_FLAG_DMA_MAP))  Always account for inflight pages, even if we didn't   map them ", "recycle_stat_inc(pool, ring_full);page_pool_return_page(pool, page);}}EXPORT_SYMBOL(page_pool_put_defragged_page": "page_pool_put_defragged_page(struct page_pool  pool, struct page  page,  unsigned int dma_sync_size, bool allow_direct){page = __page_pool_put_page(pool, page, dma_sync_size, allow_direct);if (page && !page_pool_recycle_in_ring(pool, page)) {  Cache full, fallback to free pages ", "if (!page_pool_is_last_frag(pool, page))continue;page = __page_pool_put_page(pool, page, -1, false);/* Approved for bulk recycling in ptr_ring cache ": "page_pool_put_page_bulk(struct page_pool  pool, void   data,     int count){int i, bulk_len = 0;bool in_softirq;for (i = 0; i < count; i++) {struct page  page = virt_to_head_page(data[i]);  It is not the last user for the page frag case ", "WARN_ON(!test_bit(NAPI_STATE_SCHED, &pool->p.napi->state) ||READ_ONCE(pool->p.napi->list_owner) != -1);WRITE_ONCE(pool->p.napi, NULL);}EXPORT_SYMBOL(page_pool_unlink_napi": "page_pool_unlink_napi(struct page_pool  pool){if (!pool->p.napi)return;  To avoid races with recycling and additional barriers make sure   pool and NAPI are unlinked when NAPI is disabled. ", "while (pool->alloc.count) ": "page_pool_update_nid(struct page_pool  pool, int new_nid){struct page  page;trace_page_pool_update_nid(pool, new_nid);pool->p.nid = new_nid;  Flush pool alloc cache, as refill will check NUMA node ", "if (unlikely((page->pp_magic & ~0x3UL) != PP_SIGNATURE))return false;pp = page->pp;/* Allow direct recycle if we have reasons to believe that we are * in the same context as the consumer would run, so there's * no possible race. ": "page_pool_return_skb_page(struct page  page, bool napi_safe){struct napi_struct  napi;struct page_pool  pp;bool allow_direct;page = compound_head(page);  page->pp_magic is OR'ed with PP_SIGNATURE after the allocation   in order to preserve any existing bits, such as bit 0 for the   head page of compound page and bit 1 for pfmemalloc page, so   mask those bits for freeing side when doing below checking,   and page_is_pfmemalloc() is checked in __page_pool_put_page()   to avoid recycling the pfmemalloc page. ", "void dev_add_offload(struct packet_offload *po)": "dev_add_offload - register offload handlers  @po: protocol offload declaration    Add protocol offload handlers to the networking stack. The passed  &proto_offload is linked into kernel lists and may not be freed until  it has been removed from the kernel lists.    This call does not sleep therefore it can not  guarantee all CPU's that are in middle of receiving packets  will see the new offload handlers (until the next received packet). ", "static void __dev_remove_offload(struct packet_offload *po)": "dev_remove_offload - remove offload handler  @po: packet offload declaration    Remove a protocol offload handler that was previously added to the  kernel offload handlers by dev_add_offload(). The passed &offload_type  is removed from the kernel lists and can be freed or reused once this  function returns.          The packet type might still be in use by receivers  and must not be freed until after all the CPU's have gone  through a quiescent state. ", "void napi_gro_flush(struct napi_struct *napi, bool flush_old)": "napi_gro_flush_chain(struct napi_struct  napi, u32 index,   bool flush_old){struct list_head  head = &napi->gro_hash[index].list;struct sk_buff  skb,  p;list_for_each_entry_safe_reverse(skb, p, head, list) {if (flush_old && NAPI_GRO_CB(skb)->age == jiffies)return;skb_list_del_init(skb);napi_gro_complete(napi, skb);napi->gro_hash[index].count--;}if (!napi->gro_hash[index].count)__clear_bit(index, &napi->gro_bitmask);}  napi->gro_hash[].list contains packets ordered by age.   youngest packets at the head of it.   Complete skbs in reverse order to reduce latencies. ", "static struct sk_buff *napi_frags_skb(struct napi_struct *napi)": "napi_gro_frags() and napi_gro_receive()   We copy ethernet header into skb->data to have a common layout. ", "sum = csum_fold(csum_add(NAPI_GRO_CB(skb)->csum, wsum));/* See comments in __skb_checksum_complete(). ": "__skb_gro_checksum_complete(struct sk_buff  skb){__wsum wsum;__sum16 sum;wsum = skb_checksum(skb, skb_gro_offset(skb), skb_gro_len(skb), 0);  NAPI_GRO_CB(skb)->csum holds pseudo checksum ", "error = sock_error(sk);if (error)goto out_err;if (READ_ONCE(queue->prev) != skb)goto out;/* Socket shut down? ": "__skb_wait_for_more_packets(struct sock  sk, struct sk_buff_head  queue,int  err, long  timeo_p,const struct sk_buff  skb){int error;DEFINE_WAIT_FUNC(wait, receiver_wake_function);prepare_to_wait_exclusive(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);  Socket errors? ", "struct sk_buff *__skb_try_recv_datagram(struct sock *sk,struct sk_buff_head *queue,unsigned int flags, int *off, int *err,struct sk_buff **last)": "skb_free_datagram). Returns NULL with @err set to  -EAGAIN if no data was available or to some other value if an  error was detected.      It does not lock socket since today. This function is    free of race conditions. This measure shouldcan improve    significantly datagram socket latencies at high loads,    when data copying to user space takes lots of time.    (BTW I've just killed the last cli() in IPIPv6corenetlinkpacket     8) Great win.)                       --ANK (980729)    The order of the tests when we find no data waiting are specified  quite explicitly by POSIX 1003.1g, don't change them without having  the standard around please. ", "int error = sock_error(sk);if (error)goto no_packet;do ": "skb_recv_datagram() ", "__kfree_skb(skb);}EXPORT_SYMBOL(__skb_free_datagram_locked": "__skb_free_datagram_locked(struct sock  sk, struct sk_buff  skb, int len){bool slow;if (!skb_unref(skb)) {sk_peek_offset_bwd(sk, len);return;}slow = lock_sock_fast(sk);sk_peek_offset_bwd(sk, len);skb_orphan(skb);unlock_sock_fast(sk, slow);  skb is now orphaned, can be freed outside of locked section ", "int skb_kill_datagram(struct sock *sk, struct sk_buff *skb, unsigned int flags)": "skb_kill_datagram - Free a datagram skbuff forcibly  @sk: socket  @skb: datagram skbuff  @flags: MSG\\_ flags    This function frees a datagram skbuff that was received by  skb_recv_datagram.  The flags argument must match the one  used for skb_recv_datagram.    If the MSG_PEEK flag is set, and the packet is still on the  receive queue of the socket, it will be taken off the queue  before it is freed.    This function currently only disables BH when acquiring the  sk_receive_queue lock.  Therefore it must not be used in a  context where that lock is acquired in an IRQ context.    It returns 0 if the packet was removed by us. ", "int skb_copy_and_hash_datagram_iter(const struct sk_buff *skb, int offset,   struct iov_iter *to, int len,   struct ahash_request *hash)": "skb_copy_and_hash_datagram_iter - Copy datagram to an iovec iterator            and update a hash.  @skb: buffer to copy  @offset: offset in the buffer to start copying from  @to: iovec iterator to copy to  @len: amount of data to copy from buffer to iovec        @hash: hash request to update ", "int skb_copy_datagram_iter(const struct sk_buff *skb, int offset,   struct iov_iter *to, int len)": "skb_copy_datagram_iter - Copy a datagram to an iovec iterator.  @skb: buffer to copy  @offset: offset in the buffer to start copying from  @to: iovec iterator to copy to  @len: amount of data to copy from buffer to iovec ", "int skb_copy_datagram_from_iter(struct sk_buff *skb, int offset, struct iov_iter *from, int len)": "skb_copy_datagram_from_iter - Copy a datagram from an iov_iter.  @skb: buffer to copy  @offset: offset in the buffer to start copying to  @from: the copy source  @len: amount of data to copy to buffer from iovec    Returns 0 or -EFAULT. ", "last_head = head;refs++;continue;}}if (refs) ": "zerocopy_sg_from_iter(struct msghdr  msg, struct sock  sk,    struct sk_buff  skb, struct iov_iter  from,    size_t length){int frag;if (msg && msg->msg_ubuf && msg->sg_from_iter)return msg->sg_from_iter(sk, skb, from, length);frag = skb_shinfo(skb)->nr_frags;while (length && iov_iter_count(from)) {struct page  head,  last_head = NULL;struct page  pages[MAX_SKB_FRAGS];int refs, order, n = 0;size_t start;ssize_t copied;unsigned long truesize;if (frag == MAX_SKB_FRAGS)return -EMSGSIZE;copied = iov_iter_get_pages2(from, pages, length,    MAX_SKB_FRAGS - frag, &start);if (copied < 0)return -EFAULT;length -= copied;truesize = PAGE_ALIGN(copied + start);skb->data_len += copied;skb->len += copied;skb->truesize += truesize;if (sk && sk->sk_type == SOCK_STREAM) {sk_wmem_queued_add(sk, truesize);if (!skb_zcopy_pure(skb))sk_mem_charge(sk, truesize);} else {refcount_add(truesize, &skb->sk->sk_wmem_alloc);}head = compound_head(pages[n]);order = compound_order(head);for (refs = 0; copied != 0; start = 0) {int size = min_t(int, copied, PAGE_SIZE - start);if (pages[n] - head > (1UL << order) - 1) {head = compound_head(pages[n]);order = compound_order(head);}start += (pages[n] - head) << PAGE_SHIFT;copied -= size;n++;if (frag) {skb_frag_t  last = &skb_shinfo(skb)->frags[frag - 1];if (head == skb_frag_page(last) &&    start == skb_frag_off(last) + skb_frag_size(last)) {skb_frag_size_add(last, size);  We combined this page, we need to release   a reference. Since compound pages refcount   is shared among many pages, batch the refcount   adjustments to limit false sharing. ", "int skb_copy_and_csum_datagram_msg(struct sk_buff *skb,   int hlen, struct msghdr *msg)": "skb_copy_and_csum_datagram_msg - Copy and checksum skb to user iovec.  @skb: skbuff  @hlen: hardware length  @msg: destination    Caller _must_ check that skb will fit to this iovec.    Returns: 0       - success.   -EINVAL - checksum failure.   -EFAULT - fault during copy. ", "#include <linux/module.h>#include <linux/types.h>#include <linux/kernel.h>#include <linux/uaccess.h>#include <linux/mm.h>#include <linux/interrupt.h>#include <linux/errno.h>#include <linux/sched.h>#include <linux/inet.h>#include <linux/netdevice.h>#include <linux/rtnetlink.h>#include <linux/poll.h>#include <linux/highmem.h>#include <linux/spinlock.h>#include <linux/slab.h>#include <linux/pagemap.h>#include <linux/uio.h>#include <linux/indirect_call_wrapper.h>#include <net/protocol.h>#include <linux/skbuff.h>#include <net/checksum.h>#include <net/sock.h>#include <net/tcp_states.h>#include <trace/events/skb.h>#include <net/busy_poll.h>/* *Is a socket 'connection oriented' ? ": "datagram_poll() from old       udp.c code)    Fixes:  Alan Cox:NULL return from skb_peek_copy()  understood  Alan Cox:Rewrote skb_read_datagram to avoid the  skb_peek_copy stuff.  Alan Cox:Added support for SOCK_SEQPACKET.  IPX can no longer use the SO_TYPE hack  but AX.25 now works right, and SPX is  feasible.  Alan Cox:Fixed write poll of non IP protocol  crash.  Florian  La Roche:Changed for my new skbuff handling.  Darryl Miles:Fixed non-blocking SOCK_SEQPACKET.  Linus Torvalds:BSD semantic fixes.  Alan Cox:Datagram iovec handling  Darryl Miles:Fixed non-blocking SOCK_STREAM.  Alan Cox:POSIXisms  Pete Wyckoff    :       Unconnected accept() fix.   ", ".refcnt = REFCOUNT_INIT(1),};EXPORT_SYMBOL(dst_default_metrics": "dst_default_metrics = {  This initializer is needed to force linker to place this variable   into const section. Otherwise it might end into bss section.   We really want to avoid false sharing on this variable, and catch   any writes on it. ", "void dst_dev_put(struct dst_entry *dst)": "dst_release_immediate(dst);return NULL;}EXPORT_SYMBOL(dst_destroy);static void dst_destroy_rcu(struct rcu_head  head){struct dst_entry  dst = container_of(head, struct dst_entry, rcu_head);dst = dst_destroy(dst);}  Operations to mark dst as DEAD and clean up the net device referenced   by dst:   1. put the dst under blackhole interface and discard all txrx packets      on this route.   2. release the net_device   This function should be called when removing routes from the fib tree   in preparation for a NETDEV_DOWNNETDEV_UNREGISTER event and also to   make the next dst_ops->check() fail. ", "list_add_tail(&r->list, &ops->rules_list);return 0;}EXPORT_SYMBOL(fib_default_rule_add": "fib_default_rule_add(struct fib_rules_ops  ops, u32 pref, u32 table, u32 flags){struct fib_rule  r;r = kzalloc(ops->rule_size, GFP_KERNEL_ACCOUNT);if (r == NULL)return -ENOMEM;refcount_set(&r->refcnt, 1);r->action = FR_ACT_TO_TBL;r->pref = pref;r->table = table;r->flags = flags;r->proto = RTPROT_KERNEL;r->fr_net = ops->fro_net;r->uid_range = fib_kuid_range_unset;r->suppress_prefixlen = -1;r->suppress_ifgroup = -1;  The lock is not required here, the list in unreacheable   at the moment this function is called ", "BUG_ON(key->offset > USHRT_MAX);BUG_ON(dissector_uses_key(flow_dissector,  key->key_id));dissector_set_key(flow_dissector, key->key_id);flow_dissector->offset[key->key_id] = key->offset;}/* Ensure that the dissector always includes control and basic key. * That way we are able to avoid handling lack of these in fast path. ": "skb_flow_dissector_init(struct flow_dissector  flow_dissector,     const struct flow_dissector_key  key,     unsigned int key_count){unsigned int i;memset(flow_dissector, 0, sizeof( flow_dissector));for (i = 0; i < key_count; i++, key++) {  User should make sure that every key target offset is within   boundaries of unsigned short. ", "__be32 __skb_flow_get_ports(const struct sk_buff *skb, int thoff, u8 ip_proto,    const void *data, int hlen)": "__skb_flow_get_ports - extract the upper layer ports and return them   @skb: sk_buff to extract the ports from   @thoff: transport header offset   @ip_proto: protocol for which to get port offset   @data: raw buffer pointer to the packet, if NULL use skb->data   @hlen: packet header length, if @data is NULL use skb_headlen(skb)     The function will try to retrieve the ports at offset thoff + poff where poff   is the protocol port offset returned from proto_ports_offset ", "void skb_flow_get_icmp_tci(const struct sk_buff *skb,   struct flow_dissector_key_icmp *key_icmp,   const void *data, int thoff, int hlen)": "skb_flow_get_icmp_tci - extract ICMP(6) Type, Code and Identifier fields   @skb: sk_buff to extract from   @key_icmp: struct flow_dissector_key_icmp to fill   @data: raw buffer pointer to the packet   @thoff: offset to extract at   @hlen: packet header length ", "}EXPORT_SYMBOL(skb_flow_dissect_ct": "skb_flow_dissect_ct(const struct sk_buff  skb,    struct flow_dissector  flow_dissector,    void  target_container, u16  ctinfo_map,    size_t mapsize, bool post_ct, u16 zone){#if IS_ENABLED(CONFIG_NF_CONNTRACK)struct flow_dissector_key_ct  key;enum ip_conntrack_info ctinfo;struct nf_conn_labels  cl;struct nf_conn  ct;if (!dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_CT))return;ct = nf_ct_get(skb, &ctinfo);if (!ct && !post_ct)return;key = skb_flow_dissector_target(flow_dissector,FLOW_DISSECTOR_KEY_CT,target_container);if (!ct) {key->ct_state = TCA_FLOWER_KEY_CT_FLAGS_TRACKED |TCA_FLOWER_KEY_CT_FLAGS_INVALID;key->ct_zone = zone;return;}if (ctinfo < mapsize)key->ct_state = ctinfo_map[ctinfo];#if IS_ENABLED(CONFIG_NF_CONNTRACK_ZONES)key->ct_zone = ct->zone.id;#endif#if IS_ENABLED(CONFIG_NF_CONNTRACK_MARK)key->ct_mark = READ_ONCE(ct->mark);#endifcl = nf_ct_labels_find(ct);if (cl)memcpy(key->ct_labels, cl->bits, sizeof(key->ct_labels));#endif   CONFIG_NF_CONNTRACK ", "if (!dissector_uses_key(flow_dissector,FLOW_DISSECTOR_KEY_ENC_KEYID) &&    !dissector_uses_key(flow_dissector,FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS) &&    !dissector_uses_key(flow_dissector,FLOW_DISSECTOR_KEY_ENC_IPV6_ADDRS) &&    !dissector_uses_key(flow_dissector,FLOW_DISSECTOR_KEY_ENC_CONTROL) &&    !dissector_uses_key(flow_dissector,FLOW_DISSECTOR_KEY_ENC_PORTS) &&    !dissector_uses_key(flow_dissector,FLOW_DISSECTOR_KEY_ENC_IP) &&    !dissector_uses_key(flow_dissector,FLOW_DISSECTOR_KEY_ENC_OPTS))return;info = skb_tunnel_info(skb);if (!info)return;key = &info->key;switch (ip_tunnel_info_af(info)) ": "skb_flow_dissect_tunnel_info(const struct sk_buff  skb,     struct flow_dissector  flow_dissector,     void  target_container){struct ip_tunnel_info  info;struct ip_tunnel_key  key;  A quick check to see if there might be something to do. ", "}EXPORT_SYMBOL(skb_flow_dissect_ct);voidskb_flow_dissect_tunnel_info(const struct sk_buff *skb,     struct flow_dissector *flow_dissector,     void *target_container)": "__skb_flow_dissect_icmp(const struct sk_buff  skb,    struct flow_dissector  flow_dissector,    void  target_container, const void  data,    int thoff, int hlen){struct flow_dissector_key_icmp  key_icmp;if (!dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_ICMP))return;key_icmp = skb_flow_dissector_target(flow_dissector,     FLOW_DISSECTOR_KEY_ICMP,     target_container);skb_flow_get_icmp_tci(skb, key_icmp, data, thoff, hlen);}static void __skb_flow_dissect_l2tpv3(const struct sk_buff  skb,      struct flow_dissector  flow_dissector,      void  target_container, const void  data,      int nhoff, int hlen){struct flow_dissector_key_l2tpv3  key_l2tpv3;struct {__be32 session_id;}  hdr, _hdr;if (!dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_L2TPV3))return;hdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data, hlen, &_hdr);if (!hdr)return;key_l2tpv3 = skb_flow_dissector_target(flow_dissector,       FLOW_DISSECTOR_KEY_L2TPV3,       target_container);key_l2tpv3->session_id = hdr->session_id;}void skb_flow_dissect_meta(const struct sk_buff  skb,   struct flow_dissector  flow_dissector,   void  target_container){struct flow_dissector_key_meta  meta;if (!dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_META))return;meta = skb_flow_dissector_target(flow_dissector, FLOW_DISSECTOR_KEY_META, target_container);meta->ingress_ifindex = skb->skb_iif;#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)if (tc_skb_ext_tc_enabled()) {struct tc_skb_ext  ext;ext = skb_ext_find(skb, TC_SKB_EXT);if (ext)meta->l2_miss = ext->l2_miss;}#endif}EXPORT_SYMBOL(skb_flow_dissect_meta);static voidskb_flow_dissect_set_enc_addr_type(enum flow_dissector_key_id type,   struct flow_dissector  flow_dissector,   void  target_container){struct flow_dissector_key_control  ctrl;if (!dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_ENC_CONTROL))return;ctrl = skb_flow_dissector_target(flow_dissector, FLOW_DISSECTOR_KEY_ENC_CONTROL, target_container);ctrl->addr_type = type;}voidskb_flow_dissect_ct(const struct sk_buff  skb,    struct flow_dissector  flow_dissector,    void  target_container, u16  ctinfo_map,    size_t mapsize, bool post_ct, u16 zone){#if IS_ENABLED(CONFIG_NF_CONNTRACK)struct flow_dissector_key_ct  key;enum ip_conntrack_info ctinfo;struct nf_conn_labels  cl;struct nf_conn  ct;if (!dissector_uses_key(flow_dissector, FLOW_DISSECTOR_KEY_CT))return;ct = nf_ct_get(skb, &ctinfo);if (!ct && !post_ct)return;key = skb_flow_dissector_target(flow_dissector,FLOW_DISSECTOR_KEY_CT,target_container);if (!ct) {key->ct_state = TCA_FLOWER_KEY_CT_FLAGS_TRACKED |TCA_FLOWER_KEY_CT_FLAGS_INVALID;key->ct_zone = zone;return;}if (ctinfo < mapsize)key->ct_state = ctinfo_map[ctinfo];#if IS_ENABLED(CONFIG_NF_CONNTRACK_ZONES)key->ct_zone = ct->zone.id;#endif#if IS_ENABLED(CONFIG_NF_CONNTRACK_MARK)key->ct_mark = READ_ONCE(ct->mark);#endifcl = nf_ct_labels_find(ct);if (cl)memcpy(key->ct_labels, cl->bits, sizeof(key->ct_labels));#endif   CONFIG_NF_CONNTRACK ", "void __skb_get_hash(struct sk_buff *skb)": "flow_keys_dissector_symmetric __read_mostly;u32 __skb_get_hash_symmetric(const struct sk_buff  skb){struct flow_keys keys;__flow_hash_secret_init();memset(&keys, 0, sizeof(keys));__skb_flow_dissect(NULL, skb, &flow_keys_dissector_symmetric,   &keys, NULL, 0, 0, 0,   FLOW_DISSECTOR_F_STOP_AT_FLOW_LABEL);return __flow_hash_from_keys(&keys, &hashrnd);}EXPORT_SYMBOL_GPL(__skb_get_hash_symmetric);     __skb_get_hash: calculate a flow hash   @skb: sk_buff to calculate flow hash from     This function calculates a flow hash based on srcdst addresses   and srcdst port numbers.  Sets hash in skb to non-zero hash value   on success, zero indicates no valid hash.  Also, sets l4_hash in skb   if hash is a canonical 4-tuple hash over transport ports. ", "int sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap)": "sk_filter_trim_cap - run a packet through a socket filter  @sk: sock associated with &sk_buff  @skb: buffer to filter  @cap: limit on how short the eBPF program may trim the packet     Run the eBPF program and then cut skb->data to correct size returned by   the program. If pkt_len is 0 we toss packet. If skb->len is smaller   than pkt_len we keep whole skb->data. This is the socket level   wrapper to bpf_prog_run. It returns 0 if the packet should   be accepted or -EPERM if the packet should be tossed.   ", "if (id == NET_ID_ZERO)return 0;if (id > 0)return id;return NETNSA_NSID_NOT_ASSIGNED;}static void rtnl_net_notifyid(struct net *net, int cmd, int id, u32 portid,      struct nlmsghdr *nlh, gfp_t gfp);/* This function returns the id of a peer netns. If no id is assigned, one will * be allocated and returned. ": "peernet2id(const struct net  net, struct net  peer){int id = idr_for_each(&net->netns_ids, net_eq_idr, peer);  Magic value for id 0. ", "void net_ns_barrier(void)": "net_ns_barrier - wait until concurrent net_cleanup_work is done     cleanup_net runs from work queue and will first remove namespaces   from the global list, then run net exit functions.     Call this in module exit path to make sure that all netns   ->exit ops have been invoked before the function is removed. ", "bool sk_ns_capable(const struct sock *sk,   struct user_namespace *user_ns, int cap)": "sk_ns_capable - General socket capability test   @sk: Socket to use a capability on or through   @user_ns: The user namespace of the capability to use   @cap: The capability to use     Test to see if the opener of the socket had when the socket was   created and the current process has the capability @cap in the user   namespace @user_ns. ", "bool sk_capable(const struct sock *sk, int cap)": "sk_capable - Socket global capability test   @sk: Socket to use a capability on or through   @cap: The global capability to use     Test to see if the opener of the socket had when the socket was   created and the current process has the capability @cap in all user   namespaces. ", "bool sk_net_capable(const struct sock *sk, int cap)": "sk_net_capable - Network namespace socket capability test   @sk: Socket to use a capability on or through   @cap: The capability to use     Test to see if the opener of the socket had when the socket was created   and the current process has the capability @cap over the network namespace   the socket is a member of. ", "BUG_ON(!sock_flag(sk, SOCK_MEMALLOC));noreclaim_flag = memalloc_noreclaim_save();ret = INDIRECT_CALL_INET(sk->sk_backlog_rcv, tcp_v6_do_rcv, tcp_v4_do_rcv, sk, skb);memalloc_noreclaim_restore(noreclaim_flag);return ret;}EXPORT_SYMBOL(__sk_backlog_rcv": "__sk_backlog_rcv(struct sock  sk, struct sk_buff  skb){int ret;unsigned int noreclaim_flag;  these should have been dropped before queueing ", "skb_dst_force(skb);spin_lock_irqsave(&list->lock, flags);sock_skb_set_dropcount(sk, skb);__skb_queue_tail(list, skb);spin_unlock_irqrestore(&list->lock, flags);if (!sock_flag(sk, SOCK_DEAD))sk->sk_data_ready(sk);return 0;}EXPORT_SYMBOL(__sock_queue_rcv_skb": "__sock_queue_rcv_skb(struct sock  sk, struct sk_buff  skb){unsigned long flags;struct sk_buff_head  list = &sk->sk_receive_queue;if (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf) {atomic_inc(&sk->sk_drops);trace_sock_rcvqueue_full(sk, skb);return -ENOMEM;}if (!sk_rmem_schedule(sk, skb, skb->truesize)) {atomic_inc(&sk->sk_drops);return -ENOBUFS;}skb->dev = NULL;skb_set_owner_r(skb, sk);  we escape from rcu protected region, make sure we dont leak   a norefcounted dst ", "mutex_acquire(&sk->sk_lock.dep_map, 0, 1, _RET_IP_);rc = sk_backlog_rcv(sk, skb);mutex_release(&sk->sk_lock.dep_map, _RET_IP_);} else if (sk_add_backlog(sk, skb, READ_ONCE(sk->sk_rcvbuf))) ": "lock_sock_nested(sk);elsebh_lock_sock(sk);if (!sock_owned_by_user(sk)) {    trylock + unlock semantics: ", "ret = -EPERM;if (sk->sk_bound_dev_if && !ns_capable(net->user_ns, CAP_NET_RAW))goto out;ret = -EINVAL;if (ifindex < 0)goto out;/* Paired with all READ_ONCE() done locklessly. ": "sock_bindtoindex_locked(struct sock  sk, int ifindex){int ret = -ENOPROTOOPT;#ifdef CONFIG_NETDEVICESstruct net  net = sock_net(sk);  Sorry... ", "val = min_t(int, val, INT_MAX / 2);sk->sk_userlocks |= SOCK_RCVBUF_LOCK;/* We double it on the way in to account for \"struct sk_buff\" etc. * overhead.   Applications assume that the SO_RCVBUF setting they make * will allow that much actual data to be received on that socket. * * Applications are unaware that \"struct sk_buff\" and other overheads * allocate from the receive buffer during socket buffer allocation. * * And after considering the possible alternatives, returning the value * we actually used in getsockopt is the most desirable behavior. ": "sockopt_release_sock(sk);out:#endifreturn ret;}static int sock_getbindtodevice(struct sock  sk, sockptr_t optval,sockptr_t optlen, int len){int ret = -ENOPROTOOPT;#ifdef CONFIG_NETDEVICESint bound_dev_if = READ_ONCE(sk->sk_bound_dev_if);struct net  net = sock_net(sk);char devname[IFNAMSIZ];if (bound_dev_if == 0) {len = 0;goto zero;}ret = -EINVAL;if (len < IFNAMSIZ)goto out;ret = netdev_get_name(net, devname, bound_dev_if);if (ret)goto out;len = strlen(devname) + 1;ret = -EFAULT;if (copy_to_sockptr(optval, devname, len))goto out;zero:ret = -EFAULT;if (copy_to_sockptr(optlen, &len, sizeof(int)))goto out;ret = 0;out:#endifreturn ret;}bool sk_mc_loop(struct sock  sk){if (dev_recursion_level())return false;if (!sk)return true;switch (sk->sk_family) {case AF_INET:return inet_sk(sk)->mc_loop;#if IS_ENABLED(CONFIG_IPV6)case AF_INET6:return inet6_sk(sk)->mc_loop;#endif}WARN_ON_ONCE(1);return true;}EXPORT_SYMBOL(sk_mc_loop);void sock_set_reuseaddr(struct sock  sk){lock_sock(sk);sk->sk_reuse = SK_CAN_REUSE;release_sock(sk);}EXPORT_SYMBOL(sock_set_reuseaddr);void sock_set_reuseport(struct sock  sk){lock_sock(sk);sk->sk_reuseport = true;release_sock(sk);}EXPORT_SYMBOL(sock_set_reuseport);void sock_no_linger(struct sock  sk){lock_sock(sk);sk->sk_lingertime = 0;sock_set_flag(sk, SOCK_LINGER);release_sock(sk);}EXPORT_SYMBOL(sock_no_linger);void sock_set_priority(struct sock  sk, u32 priority){lock_sock(sk);WRITE_ONCE(sk->sk_priority, priority);release_sock(sk);}EXPORT_SYMBOL(sock_set_priority);void sock_set_sndtimeo(struct sock  sk, s64 secs){lock_sock(sk);if (secs && secs < MAX_SCHEDULE_TIMEOUT  HZ - 1)WRITE_ONCE(sk->sk_sndtimeo, secs   HZ);elseWRITE_ONCE(sk->sk_sndtimeo, MAX_SCHEDULE_TIMEOUT);release_sock(sk);}EXPORT_SYMBOL(sock_set_sndtimeo);static void __sock_set_timestamps(struct sock  sk, bool val, bool new, bool ns){if (val)  {sock_valbool_flag(sk, SOCK_TSTAMP_NEW, new);sock_valbool_flag(sk, SOCK_RCVTSTAMPNS, ns);sock_set_flag(sk, SOCK_RCVTSTAMP);sock_enable_timestamp(sk, SOCK_TIMESTAMP);} else {sock_reset_flag(sk, SOCK_RCVTSTAMP);sock_reset_flag(sk, SOCK_RCVTSTAMPNS);}}void sock_enable_timestamps(struct sock  sk){lock_sock(sk);__sock_set_timestamps(sk, true, false, true);release_sock(sk);}EXPORT_SYMBOL(sock_enable_timestamps);void sock_set_timestamp(struct sock  sk, int optname, bool valbool){switch (optname) {case SO_TIMESTAMP_OLD:__sock_set_timestamps(sk, valbool, false, false);break;case SO_TIMESTAMP_NEW:__sock_set_timestamps(sk, valbool, true, false);break;case SO_TIMESTAMPNS_OLD:__sock_set_timestamps(sk, valbool, false, true);break;case SO_TIMESTAMPNS_NEW:__sock_set_timestamps(sk, valbool, true, true);break;}}static int sock_timestamping_bind_phc(struct sock  sk, int phc_index){struct net  net = sock_net(sk);struct net_device  dev = NULL;bool match = false;int  vclock_index;int i, num;if (sk->sk_bound_dev_if)dev = dev_get_by_index(net, sk->sk_bound_dev_if);if (!dev) {pr_err(\"%s: sock not bind to device\\n\", __func__);return -EOPNOTSUPP;}num = ethtool_get_phc_vclocks(dev, &vclock_index);dev_put(dev);for (i = 0; i < num; i++) {if ( (vclock_index + i) == phc_index) {match = true;break;}}if (num > 0)kfree(vclock_index);if (!match)return -EINVAL;sk->sk_bind_phc = phc_index;return 0;}int sock_set_timestamping(struct sock  sk, int optname,  struct so_timestamping timestamping){int val = timestamping.flags;int ret;if (val & ~SOF_TIMESTAMPING_MASK)return -EINVAL;if (val & SOF_TIMESTAMPING_OPT_ID_TCP &&    !(val & SOF_TIMESTAMPING_OPT_ID))return -EINVAL;if (val & SOF_TIMESTAMPING_OPT_ID &&    !(sk->sk_tsflags & SOF_TIMESTAMPING_OPT_ID)) {if (sk_is_tcp(sk)) {if ((1 << sk->sk_state) &    (TCPF_CLOSE | TCPF_LISTEN))return -EINVAL;if (val & SOF_TIMESTAMPING_OPT_ID_TCP)atomic_set(&sk->sk_tskey, tcp_sk(sk)->write_seq);elseatomic_set(&sk->sk_tskey, tcp_sk(sk)->snd_una);} else {atomic_set(&sk->sk_tskey, 0);}}if (val & SOF_TIMESTAMPING_OPT_STATS &&    !(val & SOF_TIMESTAMPING_OPT_TSONLY))return -EINVAL;if (val & SOF_TIMESTAMPING_BIND_PHC) {ret = sock_timestamping_bind_phc(sk, timestamping.bind_phc);if (ret)return ret;}sk->sk_tsflags = val;sock_valbool_flag(sk, SOCK_TSTAMP_NEW, optname == SO_TIMESTAMPING_NEW);if (val & SOF_TIMESTAMPING_RX_SOFTWARE)sock_enable_timestamp(sk,      SOCK_TIMESTAMPING_RX_SOFTWARE);elsesock_disable_timestamp(sk,       (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE));return 0;}void sock_set_keepalive(struct sock  sk){lock_sock(sk);if (sk->sk_prot->keepalive)sk->sk_prot->keepalive(sk, true);sock_valbool_flag(sk, SOCK_KEEPOPEN, true);release_sock(sk);}EXPORT_SYMBOL(sock_set_keepalive);static void __sock_set_rcvbuf(struct sock  sk, int val){  Ensure val   2 fits into an int, to prevent max_t() from treating it   as a negative value. ", "sk_mem_reclaim(sk);}EXPORT_SYMBOL_GPL(sk_clear_memalloc);int __sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)": "sk_allocation |= __GFP_MEMALLOC;static_branch_inc(&memalloc_socks_key);}EXPORT_SYMBOL_GPL(sk_set_memalloc);void sk_clear_memalloc(struct sock  sk){sock_reset_flag(sk, SOCK_MEMALLOC);sk->sk_allocation &= ~__GFP_MEMALLOC;static_branch_dec(&memalloc_socks_key);    SOCK_MEMALLOC is allowed to ignore rmem limits to ensure forward   progress of swapping. SOCK_MEMALLOC may be cleared while   it has rmem allocations due to the last swapfile being deactivated   but there is a risk that the socket is unusable due to exceeding   the rmem limits. Reclaim the reserves and obey rmem limits again. ", "struct sock *sk_alloc(struct net *net, int family, gfp_t priority,      struct proto *prot, int kern)": "sk_free(sk);out_free:if (slab != NULL)kmem_cache_free(slab, sk);elsekfree(sk);return NULL;}static void sk_prot_free(struct proto  prot, struct sock  sk){struct kmem_cache  slab;struct module  owner;owner = prot->owner;slab = prot->slab;cgroup_sk_free(&sk->sk_cgrp_data);mem_cgroup_sk_free(sk);security_sk_free(sk);if (slab != NULL)kmem_cache_free(slab, sk);elsekfree(sk);module_put(owner);}    sk_alloc - All socket objects are allocated here  @net: the applicable net namespace  @family: protocol family  @priority: for allocation (%GFP_KERNEL, %GFP_ATOMIC, etc)  @prot: struct proto associated with this new sock instance  @kern: is this to be a kernel socket? ", "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt#include <asm/unaligned.h>#include <linux/capability.h>#include <linux/errno.h>#include <linux/errqueue.h>#include <linux/types.h>#include <linux/socket.h>#include <linux/in.h>#include <linux/kernel.h>#include <linux/module.h>#include <linux/proc_fs.h>#include <linux/seq_file.h>#include <linux/sched.h>#include <linux/sched/mm.h>#include <linux/timer.h>#include <linux/string.h>#include <linux/sockios.h>#include <linux/net.h>#include <linux/mm.h>#include <linux/slab.h>#include <linux/interrupt.h>#include <linux/poll.h>#include <linux/tcp.h>#include <linux/init.h>#include <linux/highmem.h>#include <linux/user_namespace.h>#include <linux/static_key.h>#include <linux/memcontrol.h>#include <linux/prefetch.h>#include <linux/compat.h>#include <linux/mroute.h>#include <linux/mroute6.h>#include <linux/icmpv6.h>#include <linux/uaccess.h>#include <linux/netdevice.h>#include <net/protocol.h>#include <linux/skbuff.h>#include <net/net_namespace.h>#include <net/request_sock.h>#include <net/sock.h>#include <linux/net_tstamp.h>#include <net/xfrm.h>#include <linux/ipsec.h>#include <net/cls_cgroup.h>#include <net/netprio_cgroup.h>#include <linux/sock_diag.h>#include <linux/filter.h>#include <net/sock_reuseport.h>#include <net/bpf_sk_storage.h>#include <trace/events/sock.h>#include <net/tcp.h>#include <net/busy_poll.h>#include <net/phonet/phonet.h>#include <linux/ethtool.h>#include \"dev.h\"static DEFINE_MUTEX(proto_list_mutex);static LIST_HEAD(proto_list);static void sock_def_write_space_wfree(struct sock *sk);static void sock_def_write_space(struct sock *sk);/** * sk_ns_capable - General socket capability test * @sk: Socket to use a capability on or through * @user_ns: The user namespace of the capability to use * @cap: The capability to use * * Test to see if the opener of the socket had when the socket was * created and the current process has the capability @cap in the user * namespace @user_ns. ": "sock_kfree_s()  Andi Kleen:Fix write_space callback  Chris Evans:Security fixes - signedness again  Arnaldo C. Melo :       cleanups, use skb_queue_purge     To Fix: ", "refcount_add(skb->truesize, &sk->sk_wmem_alloc);}EXPORT_SYMBOL(skb_set_owner_w": "skb_set_owner_w(struct sk_buff  skb, struct sock  sk){skb_orphan(skb);skb->sk = sk;#ifdef CONFIG_INETif (unlikely(!sk_fullsock(sk))) {skb->destructor = sock_edemux;sock_hold(sk);return;}#endifskb->destructor = sock_wfree;skb_set_hash_from_sk(skb, sk);    We used to take a refcount on sk, but following operation   is enough to guarantee sk_free() wont free this sock until   all in-flight packets are completed ", "if (skb->decrypted)return false;#endifreturn (skb->destructor == sock_wfree ||(IS_ENABLED(CONFIG_INET) && skb->destructor == tcp_wfree));}/* This helper is used by netem, as it can hold packets in its * delay queue. We want to allow the owner socket to send more * packets, as if they were already TX completed by a typical driver. * But we also want to keep skb->sk set because some packet schedulers * rely on it (sch_fq for example). ": "skb_orphan_partial(const struct sk_buff  skb){#ifdef CONFIG_TLS_DEVICE  Drivers depend on in-order delivery for crypto offload,   partial orphan breaks out-of-order-OK logic. ", "case SCM_RIGHTS:case SCM_CREDENTIALS:break;default:return -EINVAL;}return 0;}EXPORT_SYMBOL(__sock_cmsg_send": "__sock_cmsg_send(struct sock  sk, struct cmsghdr  cmsg,     struct sockcm_cookie  sockc){u32 tsflags;switch (cmsg->cmsg_type) {case SO_MARK:if (!ns_capable(sock_net(sk)->user_ns, CAP_NET_RAW) &&    !ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN))return -EPERM;if (cmsg->cmsg_len != CMSG_LEN(sizeof(u32)))return -EINVAL;sockc->mark =  (u32  )CMSG_DATA(cmsg);break;case SO_TIMESTAMPING_OLD:if (cmsg->cmsg_len != CMSG_LEN(sizeof(u32)))return -EINVAL;tsflags =  (u32  )CMSG_DATA(cmsg);if (tsflags & ~SOF_TIMESTAMPING_TX_RECORD_MASK)return -EINVAL;sockc->tsflags &= ~SOF_TIMESTAMPING_TX_RECORD_MASK;sockc->tsflags |= tsflags;break;case SCM_TXTIME:if (!sock_flag(sk, SOCK_TXTIME))return -EINVAL;if (cmsg->cmsg_len != CMSG_LEN(sizeof(u64)))return -EINVAL;sockc->transmit_time = get_unaligned((u64  )CMSG_DATA(cmsg));break;  SCM_RIGHTS and SCM_CREDENTIALS are semantically in SOL_UNIX. ", "case SCM_RIGHTS:case SCM_CREDENTIALS:break;default:return -EINVAL;}return 0;}EXPORT_SYMBOL(__sock_cmsg_send);int sock_cmsg_send(struct sock *sk, struct msghdr *msg,   struct sockcm_cookie *sockc)": "sock_cmsg_send(struct sock  sk, struct cmsghdr  cmsg,     struct sockcm_cookie  sockc){u32 tsflags;switch (cmsg->cmsg_type) {case SO_MARK:if (!ns_capable(sock_net(sk)->user_ns, CAP_NET_RAW) &&    !ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN))return -EPERM;if (cmsg->cmsg_len != CMSG_LEN(sizeof(u32)))return -EINVAL;sockc->mark =  (u32  )CMSG_DATA(cmsg);break;case SO_TIMESTAMPING_OLD:if (cmsg->cmsg_len != CMSG_LEN(sizeof(u32)))return -EINVAL;tsflags =  (u32  )CMSG_DATA(cmsg);if (tsflags & ~SOF_TIMESTAMPING_TX_RECORD_MASK)return -EINVAL;sockc->tsflags &= ~SOF_TIMESTAMPING_TX_RECORD_MASK;sockc->tsflags |= tsflags;break;case SCM_TXTIME:if (!sock_flag(sk, SOCK_TXTIME))return -EINVAL;if (cmsg->cmsg_len != CMSG_LEN(sizeof(u64)))return -EINVAL;sockc->transmit_time = get_unaligned((u64  )CMSG_DATA(cmsg));break;  SCM_RIGHTS and SCM_CREDENTIALS are semantically in SOL_UNIX. ", "bool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t gfp)": "skb_page_frag_refill - check that a page_frag contains enough room   @sz: minimum size of the fragment we want to get   @pfrag: pointer to page_frag   @gfp: priority for memory allocation     Note: While this allocator tries to use high order pages, there is   no guarantee that allocations succeed. Therefore, @sz MUST be   less or equal than PAGE_SIZE. ", "int sk_wait_data(struct sock *sk, long *timeo, const struct sk_buff *skb)": "sk_wait_data - wait for data to arrive at sk_receive_queue   @sk:    sock to wait on   @timeo: for how long   @skb:   last skb seen on sk_receive_queue     Now socket state including sk->sk_err is changed only under lock,   hence we may omit checks after joining wait queue.   We check receive queue before schedule() only as optimization;   it is very likely that release_sock() added new data. ", "int __sk_mem_raise_allocated(struct sock *sk, int size, int amt, int kind)": "__sk_mem_schedule(), but does not update sk_forward_alloc ", "void __sk_mem_reduce_allocated(struct sock *sk, int amount)": "__sk_mem_reclaim(), but does not update sk_forward_alloc ", "return -ENODEV;}EXPORT_SYMBOL(sock_no_mmap": "sock_no_mmap(struct file  file, struct socket  sock, struct vm_area_struct  vma){  Mirror missing mmap method error code ", "smp_wmb();refcount_set(&sk->sk_refcnt, 1);atomic_set(&sk->sk_drops, 0);}EXPORT_SYMBOL(sock_init_data_uid": "sock_init_data_uid(struct socket  sock, struct sock  sk, kuid_t uid){sk_init_common(sk);sk->sk_send_head=NULL;timer_setup(&sk->sk_timer, NULL, 0);sk->sk_allocation=GFP_KERNEL;sk->sk_rcvbuf=READ_ONCE(sysctl_rmem_default);sk->sk_sndbuf=READ_ONCE(sysctl_wmem_default);sk->sk_state=TCP_CLOSE;sk->sk_use_task_frag=true;sk_set_socket(sk, sock);sock_set_flag(sk, SOCK_ZAPPED);if (sock) {sk->sk_type=sock->type;RCU_INIT_POINTER(sk->sk_wq, &sock->wq);sock->sk=sk;} else {RCU_INIT_POINTER(sk->sk_wq, NULL);}sk->sk_uid=uid;rwlock_init(&sk->sk_callback_lock);if (sk->sk_kern_sock)lockdep_set_class_and_name(&sk->sk_callback_lock,af_kern_callback_keys + sk->sk_family,af_family_kern_clock_key_strings[sk->sk_family]);elselockdep_set_class_and_name(&sk->sk_callback_lock,af_callback_keys + sk->sk_family,af_family_clock_key_strings[sk->sk_family]);sk->sk_state_change=sock_def_wakeup;sk->sk_data_ready=sock_def_readable;sk->sk_write_space=sock_def_write_space;sk->sk_error_report=sock_def_error_report;sk->sk_destruct=sock_def_destruct;sk->sk_frag.page=NULL;sk->sk_frag.offset=0;sk->sk_peek_off=-1;sk->sk_peer_pid =NULL;sk->sk_peer_cred=NULL;spin_lock_init(&sk->sk_peer_lock);sk->sk_write_pending=0;sk->sk_rcvlowat=1;sk->sk_rcvtimeo=MAX_SCHEDULE_TIMEOUT;sk->sk_sndtimeo=MAX_SCHEDULE_TIMEOUT;sk->sk_stamp = SK_DEFAULT_STAMP;#if BITS_PER_LONG==32seqlock_init(&sk->sk_stamp_seq);#endifatomic_set(&sk->sk_zckey, 0);#ifdef CONFIG_NET_RX_BUSY_POLLsk->sk_napi_id=0;sk->sk_ll_usec=READ_ONCE(sysctl_net_busy_read);#endifsk->sk_max_pacing_rate = ~0UL;sk->sk_pacing_rate = ~0UL;WRITE_ONCE(sk->sk_pacing_shift, 10);sk->sk_incoming_cpu = -1;sk_rx_queue_clear(sk);    Before updating sk_refcnt, we must commit prior changes to memory   (DocumentationRCUrculist_nulls.rst for details) ", "smp_wmb();refcount_set(&sk->sk_refcnt, 1);atomic_set(&sk->sk_drops, 0);}EXPORT_SYMBOL(sock_init_data": "sock_init_data_uid(struct socket  sock, struct sock  sk, kuid_t uid){sk_init_common(sk);sk->sk_send_head=NULL;timer_setup(&sk->sk_timer, NULL, 0);sk->sk_allocation=GFP_KERNEL;sk->sk_rcvbuf=READ_ONCE(sysctl_rmem_default);sk->sk_sndbuf=READ_ONCE(sysctl_wmem_default);sk->sk_state=TCP_CLOSE;sk->sk_use_task_frag=true;sk_set_socket(sk, sock);sock_set_flag(sk, SOCK_ZAPPED);if (sock) {sk->sk_type=sock->type;RCU_INIT_POINTER(sk->sk_wq, &sock->wq);sock->sk=sk;} else {RCU_INIT_POINTER(sk->sk_wq, NULL);}sk->sk_uid=uid;rwlock_init(&sk->sk_callback_lock);if (sk->sk_kern_sock)lockdep_set_class_and_name(&sk->sk_callback_lock,af_kern_callback_keys + sk->sk_family,af_family_kern_clock_key_strings[sk->sk_family]);elselockdep_set_class_and_name(&sk->sk_callback_lock,af_callback_keys + sk->sk_family,af_family_clock_key_strings[sk->sk_family]);sk->sk_state_change=sock_def_wakeup;sk->sk_data_ready=sock_def_readable;sk->sk_write_space=sock_def_write_space;sk->sk_error_report=sock_def_error_report;sk->sk_destruct=sock_def_destruct;sk->sk_frag.page=NULL;sk->sk_frag.offset=0;sk->sk_peek_off=-1;sk->sk_peer_pid =NULL;sk->sk_peer_cred=NULL;spin_lock_init(&sk->sk_peer_lock);sk->sk_write_pending=0;sk->sk_rcvlowat=1;sk->sk_rcvtimeo=MAX_SCHEDULE_TIMEOUT;sk->sk_sndtimeo=MAX_SCHEDULE_TIMEOUT;sk->sk_stamp = SK_DEFAULT_STAMP;#if BITS_PER_LONG==32seqlock_init(&sk->sk_stamp_seq);#endifatomic_set(&sk->sk_zckey, 0);#ifdef CONFIG_NET_RX_BUSY_POLLsk->sk_napi_id=0;sk->sk_ll_usec=READ_ONCE(sysctl_net_busy_read);#endifsk->sk_max_pacing_rate = ~0UL;sk->sk_pacing_rate = ~0UL;WRITE_ONCE(sk->sk_pacing_shift, 10);sk->sk_incoming_cpu = -1;sk_rx_queue_clear(sk);    Before updating sk_refcnt, we must commit prior changes to memory   (DocumentationRCUrculist_nulls.rst for details) ", "if (optlen > IFNAMSIZ - 1)optlen = IFNAMSIZ - 1;memset(devname, 0, sizeof(devname));ret = -EFAULT;if (copy_from_sockptr(devname, optval, optlen))goto out;index = 0;if (devname[0] != '\\0') ": "release_sock(sk);return ret;}EXPORT_SYMBOL(sock_bindtoindex);static int sock_setbindtodevice(struct sock  sk, sockptr_t optval, int optlen){int ret = -ENOPROTOOPT;#ifdef CONFIG_NETDEVICESstruct net  net = sock_net(sk);char devname[IFNAMSIZ];int index;ret = -EINVAL;if (optlen < 0)goto out;  Bind this socket to a particular device like \"eth0\",   as specified in the passed interface name. If the   name is \"\" or the option length is zero the socket   is not bound. ", "return false;}__lock_sock(sk);sk->sk_lock.owned = 1;__acquire(&sk->sk_lock.slock);spin_unlock_bh(&sk->sk_lock.slock);return true;}EXPORT_SYMBOL(__lock_sock_fast": "__lock_sock_fast(struct sock  sk) __acquires(&sk->sk_lock.slock){might_sleep();spin_lock_bh(&sk->sk_lock.slock);if (!sock_owned_by_user_nocheck(sk)) {    Fast path return with bottom halves disabled and   sock::sk_lock.slock held.     The 'mutex' is not contended and holding   sock::sk_lock.slock prevents all other lockers to   proceed so the corresponding unlock_sock_fast() can   avoid the slow path of release_sock() completely and   just release slock.     From a semantical POV this is equivalent to 'acquiring'   the 'mutex', hence the corresponding lockdep   mutex_release() has to happen in the fast path of   unlock_sock_fast(). ", "if (timeval && !in_compat_syscall()) ": "sock_gettstamp(struct socket  sock, void __user  userstamp,   bool timeval, bool time32){struct sock  sk = sock->sk;struct timespec64 ts;sock_enable_timestamp(sk, SOCK_TIMESTAMP);ts = ktime_to_timespec64(sock_read_timestamp(sk));if (ts.tv_sec == -1)return -ENOENT;if (ts.tv_sec == 0) {ktime_t kt = ktime_get_real();sock_write_timestamp(sk, kt);ts = ktime_to_timespec64(kt);}if (timeval)ts.tv_nsec = 1000;#ifdef CONFIG_COMPAT_32BIT_TIMEif (time32)return put_old_timespec32(&ts, userstamp);#endif#ifdef CONFIG_SPARC64  beware of padding in sparc64 timeval ", "return READ_ONCE(sk->sk_prot)->getsockopt(sk, level, optname, optval, optlen);}EXPORT_SYMBOL(sock_common_getsockopt": "sock_common_getsockopt(struct socket  sock, int level, int optname,   char __user  optval, int __user  optlen){struct sock  sk = sock->sk;  IPV6_ADDRFORM can change sk->sk_prot under us. ", "return READ_ONCE(sk->sk_prot)->setsockopt(sk, level, optname, optval, optlen);}EXPORT_SYMBOL(sock_common_setsockopt": "sock_common_setsockopt(struct socket  sock, int level, int optname,   sockptr_t optval, unsigned int optlen){struct sock  sk = sock->sk;  IPV6_ADDRFORM can change sk->sk_prot under us. ", "sk->sk_prot->unhash(sk);/* * In this point socket cannot receive new packets, but it is possible * that some packets are in flight because some CPU runs receiver and * did hash table lookup before we unhashed socket. They will achieve * receive queue and will be purged by socket destructor. * * Also we still have packets pending on receive queue and probably, * our own packets waiting in device queues. sock_destroy will drain * receive queue, but transmitted packets will delay socket destruction * until the last reference will be released. ": "sk_common_release(struct sock  sk){if (sk->sk_prot->destroy)sk->sk_prot->destroy(sk);    Observation: when sk_common_release is called, processes have   no access to socket. But net still has.   Step one, detach it from networking:     A. Remove from hash tables. ", "if (rc <= 0)return rc;/* Otherwise call the default handler ": "sk_ioctl(struct sock  sk, unsigned int cmd, void __user  arg){int rc = 1;if (sk->sk_type == SOCK_RAW && sk->sk_family == AF_INET)rc = ipmr_sk_ioctl(sk, cmd, arg);else if (sk->sk_type == SOCK_RAW && sk->sk_family == AF_INET6)rc = ip6mr_sk_ioctl(sk, cmd, arg);else if (sk_is_phonet(sk))rc = phonet_sk_ioctl(sk, cmd, arg);  If ioctl was processed, returns its value ", "#include <linux/module.h>#include <linux/jiffies.h>#include <linux/kernel.h>#include <linux/ctype.h>#include <linux/inet.h>#include <linux/mm.h>#include <linux/net.h>#include <linux/string.h>#include <linux/types.h>#include <linux/percpu.h>#include <linux/init.h>#include <linux/ratelimit.h>#include <linux/socket.h>#include <net/sock.h>#include <net/net_ratelimit.h>#include <net/ipv6.h>#include <asm/byteorder.h>#include <linux/uaccess.h>DEFINE_RATELIMIT_STATE(net_ratelimit_state, 5 * HZ, 10);/* * All net warning printk()s should be guarded by this function. ": "net_ratelimit Andi Kleen  in{4,6}_pton YOSHIFUJI Hideaki, Copyright (C)2006 USAGIWIDE Project    Created by Alexey Kuznetsov <kuznet@ms2.inr.ac.ru> ", "int in4_pton(const char *src, int srclen,     u8 *dst,     int delim, const char **end)": "in4_pton - convert an IPv4 address from literal to binary representation   @src: the start of the IPv4 address string   @srclen: the length of the string, -1 means strlen(src)   @dst: the binary (u8[4] array) representation of the IPv4 address   @delim: the delimiter of the IPv4 address in @src, -1 means no delimiter   @end: A pointer to the end of the parsed string will be placed here     Return one on success, return zero when any error occurs   and @end will point to the end of the parsed string.   ", "int in6_pton(const char *src, int srclen,     u8 *dst,     int delim, const char **end)": "in6_pton - convert an IPv6 address from literal to binary representation   @src: the start of the IPv6 address string   @srclen: the length of the string, -1 means strlen(src)   @dst: the binary (u8[16] array) representation of the IPv6 address   @delim: the delimiter of the IPv6 address in @src, -1 means no delimiter   @end: A pointer to the end of the parsed string will be placed here     Return one on success, return zero when any error occurs   and @end will point to the end of the parsed string.   ", "int inet_pton_with_scope(struct net *net, __kernel_sa_family_t af,const char *src, const char *port, struct sockaddr_storage *addr)": "inet_pton_with_scope - convert an IPv4IPv6 and port to socket address   @net: net namespace (used for scope handling)   @af: address family, AF_INET, AF_INET6 or AF_UNSPEC for either   @src: the start of the address string   @port: the start of the port string (or NULL for none)   @addr: output socket address     Return zero on success, return errno when any error occurs. ", "void inet_proto_csum_replace16(__sum16 *sum, struct sk_buff *skb,       const __be32 *from, const __be32 *to,       bool pseudohdr)": "inet_proto_csum_replace16 - update layer 4 header checksum field   @sum: Layer 4 header checksum field   @skb: sk_buff for the packet   @from: old IPv6 address   @to: new IPv6 address   @pseudohdr: True if layer 4 header checksum includes pseudoheader     Update layer 4 header as per the update in IPv6 srcdst address.     There is no need to update skb->csum in this function, because update in two   fields a.) IPv6 srcdst address and b.) L4 header checksum cancels each other   for skb->csum calculation. Whereas inet_proto_csum_replace4 function needs to   update skb->csum, because update in 3 fields a.) IPv4 srcdst address,   b.) IPv4 Header checksum and c.) L4 header checksum results in same diff as   L4 Header checksum for skb->csum calculation. ", "struct net_device *of_find_net_device_by_node(struct device_node *np)": "of_find_net_device_by_node - lookup the net device for the device node   @np: OF device node     Looks up the net_device structure corresponding with the device node.   If successful, returns a pointer to the net_device with the embedded   struct device refcount incremented by one, or NULL on failure. The   refcount must be dropped when done with the net_device. ", "static int neigh_blackhole(struct neighbour *neigh, struct sk_buff *skb)": "neigh_update_notify(struct neighbour  neigh, u32 nlmsg_pid);static int pneigh_ifdown_and_unlock(struct neigh_table  tbl,    struct net_device  dev);#ifdef CONFIG_PROC_FSstatic const struct seq_operations neigh_stat_seq_ops;#endif    Neighbour hash table buckets are protected with rwlock tbl->lock.   - All the scansupdates to hash buckets MUST be made under this lock.   - NOTHING clever should be made under this lock: no callbacks     to protocol backends, no attempts to send something to network.     It will result in deadlocks, if backenddriver wants to use neighbour     cache.   - If the entry requires some non-trivial actions, increase     its reference count and release table lock.   Neighbour entries are protected:   - with reference count.   - with rwlock neigh->lock   Reference count prevents destruction.   neigh->lock mainly serializes ll address data and its validity state.   However, the same lock is used to protect another entry fields:    - timer    - resolution queue   Again, nothing clever shall be made under neigh->lock,   the most complicated procedure, which we allow is dev->hard_header.   It is supposed, that dev->hard_header is simplistic and does   not make callbacks to neighbour tables. ", "if (tbl->constructor &&(error = tbl->constructor(n)) < 0) ": "__neigh_create(struct neigh_table  tbl, const void  pkey,struct net_device  dev, u32 flags,bool exempt_from_gc, bool want_ref){u32 hash_val, key_len = tbl->key_len;struct neighbour  n1,  rc,  n;struct neigh_hash_table  nht;int error;n = neigh_alloc(tbl, dev, flags, exempt_from_gc);trace_neigh_create(tbl, dev, pkey, n, exempt_from_gc);if (!n) {rc = ERR_PTR(-ENOBUFS);goto out;}memcpy(n->primary_key, pkey, key_len);n->dev = dev;netdev_hold(dev, &n->dev_tracker, GFP_ATOMIC);  Protocol specific setup. ", "cancel_delayed_work_sync(&tbl->managed_work);cancel_delayed_work_sync(&tbl->gc_work);del_timer_sync(&tbl->proxy_timer);pneigh_queue_purge(&tbl->proxy_queue, NULL, tbl->family);neigh_ifdown(tbl, NULL);if (atomic_read(&tbl->entries))pr_crit(\"neighbour leakage\\n\");call_rcu(&rcu_dereference_protected(tbl->nht, 1)->rcu, neigh_hash_free_rcu);tbl->nht = NULL;kfree(tbl->phash_buckets);tbl->phash_buckets = NULL;remove_proc_entry(tbl->id, init_net.proc_net_stat);free_percpu(tbl->stats);tbl->stats = NULL;return 0;}EXPORT_SYMBOL(neigh_table_clear": "neigh_table_clear(int index, struct neigh_table  tbl){neigh_tables[index] = NULL;  It is not clean... Fix it to unload IPv6 module safely ", "for (chain = 0; chain < (1 << nht->hash_shift); chain++) ": "neigh_for_each(struct neigh_table  tbl, void ( cb)(struct neighbour  , void  ), void  cookie){int chain;struct neigh_hash_table  nht;rcu_read_lock();nht = rcu_dereference(tbl->nht);read_lock_bh(&tbl->lock);   avoid resizes ", "memset(&t->neigh_vars[NEIGH_VAR_GC_INTERVAL], 0,       sizeof(t->neigh_vars[NEIGH_VAR_GC_INTERVAL]));} else ": "neigh_sysctl_register(struct net_device  dev, struct neigh_parms  p,  proc_handler  handler){int i;struct neigh_sysctl_table  t;const char  dev_name_source;char neigh_path[ sizeof(\"netneigh\") + IFNAMSIZ + IFNAMSIZ ];char  p_name;t = kmemdup(&neigh_sysctl_template, sizeof( t), GFP_KERNEL_ACCOUNT);if (!t)goto err;for (i = 0; i < NEIGH_VAR_GC_INTERVAL; i++) {t->neigh_vars[i].data += (long) p;t->neigh_vars[i].extra1 = dev;t->neigh_vars[i].extra2 = p;}if (dev) {dev_name_source = dev->name;  Terminate the table early ", "WARN_ON(!list_empty(&net_todo_list));mutex_unlock(&rtnl_mutex);while (head) ": "rtnl_unlock(void){struct sk_buff  head = defer_kfree_skb_list;defer_kfree_skb_list = NULL;  Ensure that we didn't actually add any TODO item when __rtnl_unlock()   is used. In some places, e.g. in cfg80211, we have code that will do   something like     rtnl_lock()     wiphy_lock()     ...     rtnl_unlock()     and because netdev_run_todo() acquires the RTNL for items on the list   we could cause a situation such as this:   Thread 1Thread 2    rtnl_lock()    unregister_netdevice()    __rtnl_unlock()   rtnl_lock()   wiphy_lock()   rtnl_unlock()     netdev_run_todo()       __rtnl_unlock()          list not empty now        because of thread 2    rtnl_lock()       while (!list_empty(...))         rtnl_lock()    wiphy_lock()        DEADLOCK          However, usage of __rtnl_unlock() is rare, and so we can ensure that   it's not used in cases where something is added to do the list. ", "if (metrics == dst_default_metrics.metrics)return 0;mx = nla_nest_start_noflag(skb, RTA_METRICS);if (mx == NULL)return -ENOBUFS;for (i = 0; i < RTAX_MAX; i++) ": "rtnetlink_put_metrics(struct sk_buff  skb, u32  metrics){struct nlattr  mx;int i, valid = 0;  nothing is dumped for dst_default_metrics, so just skip the loop ", "if (tb[IFLA_NET_NS_PID])net = get_net_ns_by_pid(nla_get_u32(tb[IFLA_NET_NS_PID]));else if (tb[IFLA_NET_NS_FD])net = get_net_ns_by_fd(nla_get_u32(tb[IFLA_NET_NS_FD]));elsenet = get_net(src_net);return net;}EXPORT_SYMBOL(rtnl_link_get_net": "rtnl_link_get_net(struct net  src_net, struct nlattr  tb[]){struct net  net;  Examine the link attributes and figure out which   network namespace we are talking about. ", "int ndo_dflt_fdb_add(struct ndmsg *ndm,     struct nlattr *tb[],     struct net_device *dev,     const unsigned char *addr, u16 vid,     u16 flags)": "ndo_dflt_fdb_add - default netdevice operation to add an FDB entry ", "int ndo_dflt_fdb_del(struct ndmsg *ndm,     struct nlattr *tb[],     struct net_device *dev,     const unsigned char *addr, u16 vid)": "ndo_dflt_fdb_del - default netdevice operation to delete an FDB entry ", "int ndo_dflt_fdb_dump(struct sk_buff *skb,      struct netlink_callback *cb,      struct net_device *dev,      struct net_device *filter_dev,      int *idx)": "ndo_dflt_fdb_dump - default netdevice operation to dump an FDB table.   @skb: socket buffer to store message in   @cb: netlink callback   @dev: netdevice   @filter_dev: ignored   @idx: the number of FDB table entries dumped is added to  @idx     Default netdevice operation to dump the existing unicast address list.   Returns number of addresses from list put in skb. ", "__hw_addr_del_entry(from_list, ha, false, false);}static int __hw_addr_sync_multiple(struct netdev_hw_addr_list *to_list,   struct netdev_hw_addr_list *from_list,   int addr_len)": "__hw_addr_unsync_one(struct netdev_hw_addr_list  to_list, struct netdev_hw_addr_list  from_list, struct netdev_hw_addr  ha, int addr_len){int err;err = __hw_addr_del_ex(to_list, ha->addr, addr_len, ha->type,       false, true);if (err)return;ha->sync_cnt--;  address on from list is not marked synced ", "int __hw_addr_sync_dev(struct netdev_hw_addr_list *list,       struct net_device *dev,       int (*sync)(struct net_device *, const unsigned char *),       int (*unsync)(struct net_device *,     const unsigned char *))": "__hw_addr_sync_dev - Synchonize device's multicast list    @list: address list to syncronize    @dev:  device to sync    @sync: function to call if address should be added    @unsync: function to call if address should be removed      This function is intended to be called from the ndo_set_rx_mode    function of devices that require explicit address addremove    notifications.  The unsync function may be NULL in which case    the addresses requiring removal will simply be removed without    any notification to the device.  ", "int __hw_addr_ref_sync_dev(struct netdev_hw_addr_list *list,   struct net_device *dev,   int (*sync)(struct net_device *,       const unsigned char *, int),   int (*unsync)(struct net_device *, const unsigned char *, int))": "__hw_addr_ref_sync_dev - Synchronize device's multicast address list taking    into account references    @list: address list to synchronize    @dev:  device to sync    @sync: function to call if address or reference on it should be added    @unsync: function to call if address or some reference on it should removed      This function is intended to be called from the ndo_set_rx_mode    function of devices that require explicit address or references on it    addremove notifications. The unsync function may be NULL in which case    the addresses or references on it requiring removal will simply be    removed without any notification to the device. That is responsibility of    the driver to identify and distribute address or references on it between    internal address tables.  ", "void __hw_addr_ref_unsync_dev(struct netdev_hw_addr_list *list,      struct net_device *dev,      int (*unsync)(struct net_device *,    const unsigned char *, int))": "__hw_addr_ref_unsync_dev - Remove synchronized addresses and references on    it from device    @list: address list to remove synchronized addresses (references on it) from    @dev:  device to sync    @unsync: function to call if address and references on it should be removed      Remove all addresses that were added to the device by    __hw_addr_ref_sync_dev(). This function is intended to be called from the    ndo_stop or ndo_open functions on devices that require explicit address (or    references on it) addremove notifications. If the unsync function pointer    is NULL then this function can be used to just reset the sync_cnt for the    addresses in the list.  ", "void __hw_addr_unsync_dev(struct netdev_hw_addr_list *list,  struct net_device *dev,  int (*unsync)(struct net_device *,const unsigned char *))": "__hw_addr_unsync_dev - Remove synchronized addresses from device    @list: address list to remove synchronized addresses from    @dev:  device to sync    @unsync: function to call if address should be removed      Remove all addresses that were added to the device by __hw_addr_sync_dev().    This function is intended to be called from the ndo_stop or ndo_open    functions on devices that require explicit address addremove    notifications.  If the unsync function pointer is NULL then this function    can be used to just reset the sync_cnt for the addresses in the list.  ", "int dev_addr_add(struct net_device *dev, const unsigned char *addr, unsigned char addr_type)": "dev_addr_add - Add a device address  @dev: device  @addr: address to add  @addr_type: address type    Add a device address to the device or increase the reference count if  it already exists.    The caller must hold the rtnl_mutex. ", "int dev_addr_del(struct net_device *dev, const unsigned char *addr, unsigned char addr_type)": "dev_addr_del - Release a device address.  @dev: device  @addr: address to delete  @addr_type: address type    Release reference to a device address and remove it from the device  if the reference count drops to zero.    The caller must hold the rtnl_mutex. ", "int dev_uc_add_excl(struct net_device *dev, const unsigned char *addr)": "dev_uc_add_excl - Add a global secondary unicast address  @dev: device  @addr: address to add ", "int dev_uc_del(struct net_device *dev, const unsigned char *addr)": "dev_uc_del - Release secondary unicast address.  @dev: device  @addr: address to delete    Release reference to a secondary unicast address and remove it  from the device if the reference count drops to zero. ", "int dev_uc_sync(struct net_device *to, struct net_device *from)": "dev_uc_sync - Synchronize device's unicast list to another device  @to: destination device  @from: source device    Add newly added addresses to the destination device and release  addresses that have no users left. The source device must be  locked by netif_addr_lock_bh.    This function is intended to be called from the dev->set_rx_mode  function of layered software devices.  This function assumes that  addresses will only ever be synced to the @to devices and no other. ", "int dev_uc_sync_multiple(struct net_device *to, struct net_device *from)": "dev_uc_sync_multiple - Synchronize device's unicast list to another  device, but allow for multiple calls to sync to multiple devices.  @to: destination device  @from: source device    Add newly added addresses to the destination device and release  addresses that have been deleted from the source. The source device  must be locked by netif_addr_lock_bh.    This function is intended to be called from the dev->set_rx_mode  function of layered software devices.  It allows for a single source  device to be synced to multiple destination devices. ", "void dev_uc_unsync(struct net_device *to, struct net_device *from)": "dev_uc_unsync - Remove synchronized addresses from the destination device  @to: destination device  @from: source device    Remove all addresses that were added to the destination device by  dev_uc_sync(). This function is intended to be called from the  dev->stop function of layered software devices. ", "void dev_uc_flush(struct net_device *dev)": "dev_uc_flush - Flush unicast addresses  @dev: device    Flush unicast addresses. ", "void dev_uc_init(struct net_device *dev)": "dev_uc_init - Init unicast address list  @dev: device    Init unicast address list. ", "int dev_mc_add_excl(struct net_device *dev, const unsigned char *addr)": "dev_mc_add_excl - Add a global secondary multicast address  @dev: device  @addr: address to add ", "int dev_mc_add_global(struct net_device *dev, const unsigned char *addr)": "dev_mc_add_global - Add a global multicast address  @dev: device  @addr: address to add    Add a global multicast address to the device. ", "int dev_mc_del(struct net_device *dev, const unsigned char *addr)": "dev_mc_del(struct net_device  dev, const unsigned char  addr,bool global){int err;netif_addr_lock_bh(dev);err = __hw_addr_del_ex(&dev->mc, addr, dev->addr_len,       NETDEV_HW_ADDR_T_MULTICAST, global, false);if (!err)__dev_set_rx_mode(dev);netif_addr_unlock_bh(dev);return err;}    dev_mc_del - Delete a multicast address.  @dev: device  @addr: address to delete    Release reference to a multicast address and remove it  from the device if the reference count drops to zero. ", "int dev_mc_del_global(struct net_device *dev, const unsigned char *addr)": "dev_mc_del_global - Delete a global multicast address.  @dev: device  @addr: address to delete    Release reference to a multicast address and remove it  from the device if the reference count drops to zero. ", "int dev_mc_sync(struct net_device *to, struct net_device *from)": "dev_mc_sync - Synchronize device's multicast list to another device  @to: destination device  @from: source device    Add newly added addresses to the destination device and release  addresses that have no users left. The source device must be  locked by netif_addr_lock_bh.    This function is intended to be called from the ndo_set_rx_mode  function of layered software devices. ", "int dev_mc_sync_multiple(struct net_device *to, struct net_device *from)": "dev_mc_sync_multiple - Synchronize device's multicast list to another  device, but allow for multiple calls to sync to multiple devices.  @to: destination device  @from: source device    Add newly added addresses to the destination device and release  addresses that have no users left. The source device must be  locked by netif_addr_lock_bh.    This function is intended to be called from the ndo_set_rx_mode  function of layered software devices.  It allows for a single  source device to be synced to multiple destination devices. ", "void dev_mc_unsync(struct net_device *to, struct net_device *from)": "dev_mc_unsync - Remove synchronized addresses from the destination device  @to: destination device  @from: source device    Remove all addresses that were added to the destination device by  dev_mc_sync(). This function is intended to be called from the  dev->stop function of layered software devices. ", "void dev_mc_flush(struct net_device *dev)": "dev_mc_flush - Flush multicast addresses  @dev: device    Flush multicast addresses. ", "void dev_mc_init(struct net_device *dev)": "dev_mc_init - Init multicast address list  @dev: device    Init multicast address list. ", "intgnet_stats_start_copy_compat(struct sk_buff *skb, int type, int tc_stats_type,     int xstats_type, spinlock_t *lock,     struct gnet_dump *d, int padattr)__acquires(lock)": "gnet_stats_start_copy_compat - start dumping procedure in compatibility mode   @skb: socket buffer to put statistics TLVs into   @type: TLV type for top level statistic TLV   @tc_stats_type: TLV type for backward compatibility struct tc_stats TLV   @xstats_type: TLV type for backward compatibility xstats TLV   @lock: statistics lock   @d: dumping handle   @padattr: padding attribute     Initializes the dumping handle, grabs the statistic lock and appends   an empty TLV header to the socket buffer for use a container for all   other statistic TLVS.     The dumping handle is marked to be in backward compatibility mode telling   all gnet_stats_copy_XXX() functions to fill a local copy of struct tc_stats.     Returns 0 on success or -1 if the room in the socket buffer was not sufficient. ", "return gnet_stats_copy(d, TCA_STATS_PKT64, &bstats_packets,       sizeof(bstats_packets), TCA_STATS_PAD);}return 0;}/** * gnet_stats_copy_basic - copy basic statistics into statistic TLV * @d: dumping handle * @cpu: copy statistic per cpu * @b: basic statistics * @running: true if @b represents a running qdisc, thus @b's *           internal values might change during basic reads. *           Only used if @cpu is NULL * * Context: task; must not be run from IRQ or BH contexts * * Appends the basic statistics to the top level TLV created by * gnet_stats_start_copy(). * * Returns 0 on success or -1 with the statistic lock released * if the room in the socket buffer was not sufficient. ": "gnet_stats_copy_basic(struct gnet_dump  d, struct gnet_stats_basic_sync __percpu  cpu, struct gnet_stats_basic_sync  b, int type, bool running){u64 bstats_bytes, bstats_packets;gnet_stats_read_basic(&bstats_bytes, &bstats_packets, cpu, b, running);if (d->compat_tc_stats && type == TCA_STATS_BASIC) {d->tc_stats.bytes = bstats_bytes;d->tc_stats.packets = bstats_packets;}if (d->tail) {struct gnet_stats_basic sb;int res;memset(&sb, 0, sizeof(sb));sb.bytes = bstats_bytes;sb.packets = bstats_packets;res = gnet_stats_copy(d, type, &sb, sizeof(sb), TCA_STATS_PAD);if (res < 0 || sb.packets == bstats_packets)return res;  emit 64bit stats only if needed ", "intgnet_stats_copy_basic_hw(struct gnet_dump *d, struct gnet_stats_basic_sync __percpu *cpu, struct gnet_stats_basic_sync *b, bool running)": "gnet_stats_copy_basic_hw - copy basic hw statistics into statistic TLV   @d: dumping handle   @cpu: copy statistic per cpu   @b: basic statistics   @running: true if @b represents a running qdisc, thus @b's             internal values might change during basic reads.             Only used if @cpu is NULL     Context: task; must not be run from IRQ or BH contexts     Appends the basic statistics to the top level TLV created by   gnet_stats_start_copy().     Returns 0 on success or -1 with the statistic lock released   if the room in the socket buffer was not sufficient. ", "intgnet_stats_copy_rate_est(struct gnet_dump *d, struct net_rate_estimator __rcu **rate_est)": "gnet_stats_copy_rate_est - copy rate estimator statistics into statistics TLV   @d: dumping handle   @rate_est: rate estimator     Appends the rate estimator statistics to the top level TLV created by   gnet_stats_start_copy().     Returns 0 on success or -1 with the statistic lock released   if the room in the socket buffer was not sufficient. ", "intgnet_stats_copy_queue(struct gnet_dump *d,      struct gnet_stats_queue __percpu *cpu_q,      struct gnet_stats_queue *q, __u32 qlen)": "gnet_stats_copy_queue - copy queue statistics into statistics TLV   @d: dumping handle   @cpu_q: per cpu queue statistics   @q: queue statistics   @qlen: queue length statistics     Appends the queue statistics to the top level TLV created by   gnet_stats_start_copy(). Using per cpu queue statistics if   they are available.     Returns 0 on success or -1 with the statistic lock released   if the room in the socket buffer was not sufficient. ", "intgnet_stats_copy_app(struct gnet_dump *d, void *st, int len)": "gnet_stats_copy_app - copy application specific statistics into statistics TLV   @d: dumping handle   @st: application specific statistics data   @len: length of data     Appends the application specific statistics to the top level TLV created by   gnet_stats_start_copy() and remembers the data for XSTATS if the dumping   handle is in backward compatibility mode.     Returns 0 on success or -1 with the statistic lock released   if the room in the socket buffer was not sufficient. ", "if (ret == 0 && d->tail->nla_type == padattr)d->tail = (struct nlattr *)((char *)d->tail +    NLA_ALIGN(d->tail->nla_len));return ret;}return 0;}EXPORT_SYMBOL(gnet_stats_start_copy_compat);/** * gnet_stats_start_copy - start dumping procedure in compatibility mode * @skb: socket buffer to put statistics TLVs into * @type: TLV type for top level statistic TLV * @lock: statistics lock * @d: dumping handle * @padattr: padding attribute * * Initializes the dumping handle, grabs the statistic lock and appends * an empty TLV header to the socket buffer for use a container for all * other statistic TLVS. * * Returns 0 on success or -1 if the room in the socket buffer was not sufficient. ": "gnet_stats_finish_copy() adjusts the length of   the right attribute. ", "void dev_load(struct net *net, const char *name)": "dev_load - load a network module  @net: the applicable net namespace  @name: name of interface    If a network interface is not present and the process has suitable  privileges this function loads the module. If module loading is not  available in this kernel then it becomes a nop. ", "for (i = 0; i < num_actions; i++)rule->action.entries[i].hw_stats = FLOW_ACTION_HW_STATS_DONT_CARE;return rule;}EXPORT_SYMBOL(flow_rule_alloc": "flow_rule_alloc(unsigned int num_actions){struct flow_rule  rule;int i;rule = kzalloc(struct_size(rule, action.entries, num_actions),       GFP_KERNEL);if (!rule)return NULL;rule->action.num_entries = num_actions;  Pre-fill each action hw_stats with DONT_CARE.   Caller can override this if it wants stats for a given action. ", "*size = ksize(data);/* krealloc() will immediately return \"data\" when * \"ksize(data)\" is requested: it is the existing upper * bounds. As a result, GFP_ATOMIC will be ignored. Note * that this \"new\" pointer needs to be passed back to the * caller for use so the __alloc_size hinting will be * tracked correctly. ": "build_skb(struct sk_buff  skb, void  data,     unsigned int  size){void  resized;  Must find the allocation size (and grow it to match). ", "if (WARN_ONCE(size == 0, \"Use slab_build_skb() instead\"))data = __slab_build_skb(skb, data, &size);__finalize_skb_around(skb, data, size);}/** * __build_skb - build a network buffer * @data: data buffer provided by caller * @frag_size: size of data (must not be 0) * * Allocate a new &sk_buff. Caller provides space holding head and * skb_shared_info. @data must have been allocated from the page * allocator or vmalloc(). (A @frag_size of 0 to indicate a kmalloc() * allocation is deprecated, and callers should use slab_build_skb() * instead.) * The return is the new skb buffer. * On a failure the return is %NULL, and @data is not freed. * Notes : *  Before IO, driver allocates only data buffer where NIC put incoming frame *  Driver should add room at head (NET_SKB_PAD) and *  MUST add room at tail (SKB_DATA_ALIGN(skb_shared_info)) *  After IO, driver calls build_skb(), to allocate sk_buff and populate it *  before giving packet to stack. *  RX rings only contains data buffers, not full skbs. ": "build_skb_around(struct sk_buff  skb, void  data,       unsigned int frag_size){unsigned int size = frag_size;  frag_size == 0 is considered deprecated now. Callers   using slab buffer should use slab_build_skb() instead. ", "static struct sk_buff *__napi_build_skb(void *data, unsigned int frag_size)": "napi_build_skb - build a network buffer   @data: data buffer provided by caller   @frag_size: size of data     Version of __build_skb() that uses NAPI percpu caches to obtain   skbuff_head instead of inplace allocation.     Returns a new &sk_buff on success, %NULL on allocation failure. ", "struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,    int flags, int node)": "__alloc_skb-allocate a network buffer  @size: size to allocate  @gfp_mask: allocation mask  @flags: If SKB_ALLOC_FCLONE is set, allocate from fclone cache  instead of head cache and allocate a cloned (child) skb.  If SKB_ALLOC_RX is set, __GFP_MEMALLOC will be used for  allocations in case the data is required for writeback  @node: numa node to allocate memory on    Allocate a new &sk_buff. The returned buffer has no headroom and a  tail room of at least size bytes. The object has a reference count  of one. The return is the buffer. On a failure the return is %NULL.    Buffers may only be allocated from interrupts using a @gfp_mask of  %GFP_ATOMIC. ", "struct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int len,   gfp_t gfp_mask)": "__netdev_alloc_skb - allocate an skbuff for rx on a specific device  @dev: network device to receive on  @len: length to allocate  @gfp_mask: get_free_pages mask, passed to alloc_skb    Allocate a new &sk_buff and assign it a usage count of one. The  buffer has NET_SKB_PAD headroom built in. Users should allocate  the headroom they think they need without accounting for the  built in space. The built in space is used for optimisations.    %NULL is returned if there is no free memory. ", "struct sk_buff *__napi_alloc_skb(struct napi_struct *napi, unsigned int len, gfp_t gfp_mask)": "__napi_alloc_skb - allocate skbuff for rx in a specific NAPI instance  @napi: napi instance this buffer was allocated for  @len: length to allocate  @gfp_mask: get_free_pages mask, passed to alloc_skb and alloc_pages    Allocate a new sk_buff for use in NAPI receive.  This buffer will  attempt to allocate the head from a special reserved region used  only for NAPI Rx allocation.  By doing this we can save several  CPU cycles by avoiding having to disable and re-enable IRQs.    %NULL is returned if there is no free memory. ", "void __kfree_skb(struct sk_buff *skb)": "__kfree_skb - private function  @skb: buffer    Free an sk_buff. Release anything attached to the buffer.  Clean the state. This is an internal helper function. Users should  always call kfree_skb ", "void __fix_addresskfree_skb_reason(struct sk_buff *skb, enum skb_drop_reason reason)": "consume_skb(skb, __builtin_return_address(0));elsetrace_kfree_skb(skb, __builtin_return_address(0), reason);return true;}    kfree_skb_reason - free an sk_buff with special reason  @skb: buffer to free  @reason: reason why this skb is dropped    Drop a reference to the buffer and free it if the usage count has  hit zero. Meanwhile, pass the drop reason to 'kfree_skb'  tracepoint. ", "skb->pp_recycle = 0;}/* *Free an skbuff by memory without cleaning the state. ": "kfree_skb_list_reason(shinfo->frag_list, reason);skb_free_head(skb, napi_safe);exit:  When we clone an SKB we copy the reycling bit. The pp_recycle   bit is only set on the head though, so in order to avoid races   while trying to recycle fragments on __skb_frag_unref() we need   to make one SKB responsible for triggering the recycle path.   So disable the recycling bit if an SKB is cloned and we have   additional references to the fragmented part of the SKB.   Eventually the last SKB will have the recycling bit set and it's   dataref set to 0, which will trigger the recycling ", "void skb_tx_error(struct sk_buff *skb)": "skb_tx_error - report an sk_buff xmit error  @skb: buffer that triggered an error    Report xmit error if a device callback is tracking this skb.  skb must be freed afterwards. ", "if (unlikely(!budget)) ": "napi_consume_skb(struct sk_buff  skb, int budget){  Zero budget indicate non-NAPI context called us, like netpoll ", "/* *The functions in this file will not compile correctly with gcc 2.4.x ": "skb_copy.  Alan Cox:Added all the changed routines Linus  only put in the headers  Ray VanTassle:Fixed --skb->lock in free  Alan Cox:skb_copy copy arp field  Andi Kleen:slabified it.  Robert Olsson:Removed skb_head_pool    NOTE:  The __skb_ routines should be called with interrupts  disabled, or you better be  real  sure that the operation is atomic  with respect to whatever list is being frobbed (e.g. via lock_sock()  or via disabling bottom half handlers, etc). ", "if (skb->ip_summed == CHECKSUM_PARTIAL)skb->csum_start += off;/* ": "skb_headers_offset_update(struct sk_buff  skb, int off){  Only adjust this if it actually is csum_start rather than csum ", "struct sk_buff *__pskb_copy_fclone(struct sk_buff *skb, int headroom,   gfp_t gfp_mask, bool fclone)": "skb_copy_bits(skb, -headerlen, n->head, headerlen + skb->len));skb_copy_header(n, skb);return n;}EXPORT_SYMBOL(skb_copy);    __pskb_copy_fclone-  create copy of an sk_buff with private head.  @skb: buffer to copy  @headroom: headroom of new skb  @gfp_mask: allocation priority  @fclone: if true allocate the copy of the skb from the fclone  cache instead of the head cache; it is recommended to set this  to true for the cases where the copy will likely be cloned    Make a copy of both an &sk_buff and part of its data, located  in header. Fragmented data remain shared. This is used when  the caller wishes to modify only header of &sk_buff and needs  private copy of the header to alter. Returns %NULL on failure  or the pointer to the buffer on success.  The returned buffer has a reference count of 1. ", "int pskb_expand_head(struct sk_buff *skb, int nhead, int ntail,     gfp_t gfp_mask)": "skb_expand_head - reallocate header of &sk_buff  @skb: buffer to reallocate  @nhead: room to add at head  @ntail: room to add at tail  @gfp_mask: allocation priority    Expands (or creates identical copy, if @nhead and @ntail are zero)  header of @skb. &sk_buff itself is not changed. &sk_buff MUST have  reference count of 1. Returns zero in the case of success or error,  if expansion failed. In the last case, &sk_buff is not changed.    All the pointers pointing into skb header may change and must be  reloaded after call to this function. ", "struct sk_buff *skb_copy_expand(const struct sk_buff *skb,int newheadroom, int newtailroom,gfp_t gfp_mask)": "skb_copy_expand-copy and expand sk_buff  @skb: buffer to copy  @newheadroom: new free bytes at head  @newtailroom: new free bytes at tail  @gfp_mask: allocation priority    Make a copy of both an &sk_buff and its data and while doing so  allocate additional space.    This is used when the caller wishes to modify the data and needs a  private copy of the data to alter as well as more space for new fields.  Returns %NULL on failure or the pointer to the buffer  on success. The returned buffer has a reference count of 1.    You must pass %GFP_ATOMIC as the allocation priority if this function  is called from an interrupt. ", "int __skb_pad(struct sk_buff *skb, int pad, bool free_on_error)": "__skb_pad-zero pad the tail of an skb  @skb: buffer to pad  @pad: space to pad  @free_on_error: free buffer on error    Ensure that a buffer is followed by a padding area that is zero  filled. Used by network drivers which may DMA or transfer data  beyond the buffer end onto the wire.    May return error in out of memory cases. The skb is freed on error  if @free_on_error is true. ", "static void skb_panic(struct sk_buff *skb, unsigned int sz, void *addr,      const char msg[])": "skb_push().  Called via the wrapper skb_over_panic() or skb_under_panic().  Keep out of line to prevent kernel bloat.  __builtin_return_address is not used because it is not always reliable. ", "void *skb_pull(struct sk_buff *skb, unsigned int len)": "skb_pull - remove data from the start of a buffer  @skb: buffer to use  @len: amount of data to remove    This function removes data from the start of a buffer, returning  the memory to the headroom. A pointer to the next data in the buffer  is returned. Once the data has been pulled future pushes will overwrite  the old data. ", "void *skb_pull_data(struct sk_buff *skb, size_t len)": "skb_pull_data - remove data from the start of a buffer returning its  original position.  @skb: buffer to use  @len: amount of data to remove    This function removes data from the start of a buffer, returning  the memory to the headroom. A pointer to the original data in the buffer  is returned after checking if there is enough data to pull. Once the  data has been pulled future pushes will overwrite the old data. ", "if (!gfp_mask) ": "___pskb_trim(skb, orig_len);skb->sk = save_sk;return err;}skb_zcopy_set(skb, uarg, NULL);return skb->len - orig_len;}EXPORT_SYMBOL_GPL(skb_zerocopy_iter_stream);void __skb_zcopy_downgrade_managed(struct sk_buff  skb){int i;skb_shinfo(skb)->flags &= ~SKBFL_MANAGED_FRAG_REFS;for (i = 0; i < skb_shinfo(skb)->nr_frags; i++)skb_frag_ref(skb, i);}EXPORT_SYMBOL_GPL(__skb_zcopy_downgrade_managed);static int skb_zerocopy_clone(struct sk_buff  nskb, struct sk_buff  orig,      gfp_t gfp_mask){if (skb_zcopy(orig)) {if (skb_zcopy(nskb)) {  !gfp_mask callers are verified to !skb_zcopy(nskb) ", "/* Moves tail of skb head forward, copying data from fragmented part, * when it is necessary. * 1. It may fail due to malloc failure. * 2. It may change skb pointers. * * It is pretty complicated. Luckily, it is called only in exceptional cases. ": "skb_checksum(skb, len, delta, 0),   len);} else if (skb->ip_summed == CHECKSUM_PARTIAL) {int hdlen = (len > skb_headlen(skb)) ? skb_headlen(skb) : len;int offset = skb_checksum_start_offset(skb) + skb->csum_offset;if (offset + sizeof(__sum16) > hdlen)return -EINVAL;}return __pskb_trim(skb, len);}EXPORT_SYMBOL(pskb_trim_rcsum_slow);    __pskb_pull_tail - advance tail of skb header  @skb: buffer to reallocate  @delta: number of bytes to advance tail    The function makes a sense only on a fragmented &sk_buff,  it expands header moving its tail forward and copying necessary  data from fragmented part.    &sk_buff MUST have reference count of 1.    Returns %NULL (and &sk_buff does not change) if pull failed  or value of new tail of skb in the case of success.    All the pointers pointing into skb header may change and must be  reloaded after call to this function. ", "int skb_store_bits(struct sk_buff *skb, int offset, const void *from, int len)": "skb_store_bits - store bits from kernel buffer to skb  @skb: destination buffer  @offset: offset in destination  @from: source buffer  @len: number of bytes to copy    Copy the specified number of bytes from the source buffer to the  destination skb.  This function handles all the messy bits of  traversing fragment lists and such. ", "if (copy > 0) ": "skb_copy_and_csum_bits(const struct sk_buff  skb, int offset,    u8  to, int len){int start = skb_headlen(skb);int i, copy = start - offset;struct sk_buff  frag_iter;int pos = 0;__wsum csum = 0;  Copy header. ", "if (likely(!sum)) ": "__skb_checksum_complete_head(struct sk_buff  skb, int len){__sum16 sum;sum = csum_fold(skb_checksum(skb, 0, len, skb->csum));  See comments in __skb_checksum_complete(). ", "struct sk_buff *skb_dequeue(struct sk_buff_head *list)": "skb_dequeue - remove from the head of the queue  @list: list to dequeue from    Remove the head of the list. The list lock is taken so the function  may be used safely with other locking list functions. The head item is  returned or %NULL if the list is empty. ", "struct sk_buff *skb_dequeue_tail(struct sk_buff_head *list)": "skb_dequeue_tail - remove from the tail of the queue  @list: list to dequeue from    Remove the tail of the list. The list lock is taken so the function  may be used safely with other locking list functions. The tail item is  returned or %NULL if the list is empty. ", "void skb_queue_purge(struct sk_buff_head *list)": "skb_queue_purge - empty a list  @list: list to empty    Delete all buffers on an &sk_buff list. Each buffer is removed from  the list and one reference dropped. This function takes the list  lock and is atomic with respect to other list locking functions. ", "void skb_queue_head(struct sk_buff_head *list, struct sk_buff *newsk)": "skb_queue_head - queue a buffer at the list head  @list: list to use  @newsk: buffer to queue    Queue a buffer at the start of the list. This function takes the  list lock and can be used safely with other locking &sk_buff functions  safely.    A buffer cannot be placed on two lists at the same time. ", "if (orig_uarg && uarg != orig_uarg)return -EEXIST;err = __zerocopy_sg_from_iter(msg, sk, skb, &msg->msg_iter, len);if (err == -EFAULT || (err == -EMSGSIZE && skb->len == orig_len)) ": "skb_queue_tail(q, skb);skb = NULL;}spin_unlock_irqrestore(&q->lock, flags);sk_error_report(sk);release:consume_skb(skb);sock_put(sk);}void msg_zerocopy_callback(struct sk_buff  skb, struct ubuf_info  uarg,   bool success){struct ubuf_info_msgzc  uarg_zc = uarg_to_msgzc(uarg);uarg_zc->zerocopy = uarg_zc->zerocopy & success;if (refcount_dec_and_test(&uarg->refcnt))__msg_zerocopy_callback(uarg_zc);}EXPORT_SYMBOL_GPL(msg_zerocopy_callback);void msg_zerocopy_put_abort(struct ubuf_info  uarg, bool have_uref){struct sock  sk = skb_from_uarg(uarg_to_msgzc(uarg))->sk;atomic_dec(&sk->sk_zckey);uarg_to_msgzc(uarg)->len--;if (have_uref)msg_zerocopy_callback(NULL, uarg, true);}EXPORT_SYMBOL_GPL(msg_zerocopy_put_abort);int skb_zerocopy_iter_stream(struct sock  sk, struct sk_buff  skb,     struct msghdr  msg, int len,     struct ubuf_info  uarg){struct ubuf_info  orig_uarg = skb_zcopy(skb);int err, orig_len = skb->len;  An skb can only point to one uarg. This edge case happens when   TCP appends to an skb, but zerocopy_realloc triggered a new alloc. ", "void skb_unlink(struct sk_buff *skb, struct sk_buff_head *list)": "skb_unlink-remove a buffer from a list  @skb: buffer to remove  @list: list to use    Remove a packet from a list. The list locks are taken and this  function is atomic with respect to other list locked calls    You must know what list the SKB is on. ", "void skb_append(struct sk_buff *old, struct sk_buff *newsk, struct sk_buff_head *list)": "skb_append-append a buffer  @old: buffer to insert after  @newsk: buffer to insert  @list: list to use    Place a packet after a given packet in a list. The list locks are taken  and this function is atomic with respect to other list locked calls.  A buffer cannot be placed on two lists at the same time. ", "for (i = 0; i < skb_shinfo(skb)->nr_frags; i++)skb_shinfo(skb1)->frags[i] = skb_shinfo(skb)->frags[i];skb_shinfo(skb1)->nr_frags = skb_shinfo(skb)->nr_frags;skb_shinfo(skb)->nr_frags  = 0;skb1->data_len   = skb->data_len;skb1->len   += skb1->data_len;skb->data_len   = 0;skb->len   = len;skb_set_tail_pointer(skb, len);}static inline void skb_split_no_header(struct sk_buff *skb,       struct sk_buff* skb1,       const u32 len, int pos)": "skb_split_inside_header(struct sk_buff  skb,   struct sk_buff  skb1,   const u32 len, const int pos){int i;skb_copy_from_linear_data_offset(skb, len, skb_put(skb1, pos - len), pos - len);  And move data appendix as is. ", "void skb_prepare_seq_read(struct sk_buff *skb, unsigned int from,  unsigned int to, struct skb_seq_state *st)": "skb_seq_read() for the first time. ", "void skb_abort_seq_read(struct skb_seq_state *st)": "skb_abort_seq_read - Abort a sequential read of skb data   @st: state variable     Must be called if skb_seq_read() was not called until it   returned 0. ", "unsigned int skb_find_text(struct sk_buff *skb, unsigned int from,   unsigned int to, struct ts_config *config)": "skb_find_text - Find a text pattern in skb data   @skb: the buffer to look in   @from: search offset   @to: search limit   @config: textsearch configuration     Finds a pattern in the skb data according to the specified   textsearch configuration. Use textsearch_next() to retrieve   subsequent occurrences of the pattern. Returns the offset   to the first occurrence or UINT_MAX if no match was found. ", "skb_dst_force(skb);skb_queue_tail(&sk->sk_error_queue, skb);if (!sock_flag(sk, SOCK_DEAD))sk_error_report(sk);return 0;}EXPORT_SYMBOL(sock_queue_err_skb": "sock_queue_err_skb(struct sock  sk, struct sk_buff  skb){if (atomic_read(&sk->sk_rmem_alloc) + skb->truesize >=    (unsigned int)READ_ONCE(sk->sk_rcvbuf))return -ENOMEM;skb_orphan(skb);skb->sk = sk;skb->destructor = sock_rmem_free;atomic_add(skb->truesize, &sk->sk_rmem_alloc);skb_set_err_queue(skb);  before exiting rcu section, make sure dst is refcounted ", "struct sk_buff *skb_clone_sk(struct sk_buff *skb)": "skb_clone_sk - create clone of skb, and take reference to socket   @skb: the skb to clone     This function creates a clone of a buffer that holds a reference on   sk_refcnt.  Buffers created via this function are meant to be   returned using sock_queue_err_skb, or free via kfree_skb.     When passing buffers allocated with this function to sock_queue_err_skb   it is necessary to wrap the call with sock_holdsock_put in order to   prevent the socket from being released prior to being enqueued on   the sk_error_queue. ", "#define MAX_IP_HDR_LEN 128static int skb_checksum_setup_ipv4(struct sk_buff *skb, bool recalculate)": "skb_checksum_setup_ip(struct sk_buff  skb,      typeof(IPPROTO_IP) proto,      unsigned int off){int err;switch (proto) {case IPPROTO_TCP:err = skb_maybe_pull_tail(skb, off + sizeof(struct tcphdr),  off + MAX_TCP_HDR_LEN);if (!err && !skb_partial_csum_set(skb, off,  offsetof(struct tcphdr,   check)))err = -EPROTO;return err ? ERR_PTR(err) : &tcp_hdr(skb)->check;case IPPROTO_UDP:err = skb_maybe_pull_tail(skb, off + sizeof(struct udphdr),  off + sizeof(struct udphdr));if (!err && !skb_partial_csum_set(skb, off,  offsetof(struct udphdr,   check)))err = -EPROTO;return err ? ERR_PTR(err) : &udp_hdr(skb)->check;}return ERR_PTR(-EPROTO);}  This value should be large enough to cover a tagged ethernet header plus   maximally sized IP and TCP or UDP headers. ", "struct sk_buff *skb_checksum_trimmed(struct sk_buff *skb,     unsigned int transport_len,     __sum16(*skb_chkf)(struct sk_buff *skb))": "skb_checksum_trimmed - validate checksum of an skb   @skb: the skb to check   @transport_len: the data length beyond the network header   @skb_chkf: checksum function to use     Applies the given checksum function skb_chkf to the provided skb.   Returns a checked and maybe trimmed skb. Returns NULL on error.     If the skb has data beyond the given transport length, then a   trimmed & cloned skb is checked and returned.     Caller needs to set the skb transport header and free any returned skb if it   differs from the provided skb. ", "bool skb_try_coalesce(struct sk_buff *to, struct sk_buff *from,      bool *fragstolen, int *delta_truesize)": "skb_try_coalesce - try to merge skb to prior one   @to: prior buffer   @from: buffer to add   @fragstolen: pointer to boolean   @delta_truesize: how much more was allocated than was requested ", "return skb;}skb = skb_share_check(skb, GFP_ATOMIC);if (unlikely(!skb))goto err_free;/* We may access the two bytes after vlan_hdr in vlan_set_encap_proto(). ": "skb_vlan_untag(struct sk_buff  skb){struct vlan_hdr  vhdr;u16 vlan_tci;if (unlikely(skb_vlan_tag_present(skb))) {  vlan_tci is already set-up so leave this for another time ", "int skb_vlan_pop(struct sk_buff *skb)": "skb_vlan_pop(struct sk_buff  skb, u16  vlan_tci){int offset = skb->data - skb_mac_header(skb);int err;if (WARN_ONCE(offset,      \"__skb_vlan_pop got skb with skb->data not at mac header (offset %d)\\n\",      offset)) {return -EINVAL;}err = skb_ensure_writable(skb, VLAN_ETH_HLEN);if (unlikely(err))return err;skb_postpull_rcsum(skb, skb->data + (2   ETH_ALEN), VLAN_HLEN);vlan_remove_tag(skb, vlan_tci);skb->mac_header += VLAN_HLEN;if (skb_network_offset(skb) < ETH_HLEN)skb_set_network_header(skb, ETH_HLEN);skb_reset_mac_len(skb);return err;}EXPORT_SYMBOL(__skb_vlan_pop);  Pop a vlan tag either from hwaccel or from payload.   Expects skb->data at mac header. ", "int skb_eth_pop(struct sk_buff *skb)": "skb_eth_pop() - Drop the Ethernet header at the head of a packet     @skb: Socket buffer to modify     Drop the Ethernet header of @skb.     Expects that skb->data points to the mac header and that no VLAN tags are   present.     Returns 0 on success, -errno otherwise. ", "int skb_eth_push(struct sk_buff *skb, const unsigned char *dst, const unsigned char *src)": "skb_eth_push() - Add a new Ethernet header at the head of a packet     @skb: Socket buffer to modify   @dst: Destination MAC address of the new header   @src: Source MAC address of the new header     Prepend @skb with a new Ethernet header.     Expects that skb->data points to the mac header, which must be empty.     Returns 0 on success, -errno otherwise. ", "struct sk_buff *alloc_skb_with_frags(unsigned long header_len,     unsigned long data_len,     int max_page_order,     int *errcode,     gfp_t gfp_mask)": "alloc_skb_with_frags - allocate skb with page frags     @header_len: size of linear part   @data_len: needed length in frags   @max_page_order: max page order desired.   @errcode: pointer to error code if any   @gfp_mask: allocation mask     This can be used to allocate a paged skb, given a maximal order for frags. ", "int pskb_trim_rcsum_slow(struct sk_buff *skb, unsigned int len)": "skb_condense(skb);return 0;}EXPORT_SYMBOL(___pskb_trim);  Note : use pskb_trim_rcsum() instead of calling this directly ", "void *skb_ext_add(struct sk_buff *skb, enum skb_ext_id id)": "skb_ext_add - allocate space for given extension, COW if needed   @skb: buffer   @id: extension to allocate space for     Allocates enough space for the given extension.   If the extension is already present, a pointer to that extension   is returned.     If the skb was cloned, COW applies and the returned memory can be   modified without changing the extension space of clones buffers.     Returns pointer to the extension or NULL on allocation failure. ", "void *__skb_ext_set(struct sk_buff *skb, enum skb_ext_id id,    struct skb_ext *ext)": "__skb_ext_put(old);return new;}     __skb_ext_set - attach the specified extension storage to this skb   @skb: buffer   @id: extension id   @ext: extension storage previously allocated via __skb_ext_alloc()     Existing extensions, if any, are cleared.     Returns the pointer to the extension. ", "ssize_t skb_splice_from_iter(struct sk_buff *skb, struct iov_iter *iter,     ssize_t maxsize, gfp_t gfp)": "skb_splice_from_iter - Splice (or copy) pages to skbuff   @skb: The buffer to add pages to   @iter: Iterator representing the pages to be added   @maxsize: Maximum amount of pages to be added   @gfp: Allocation flags     This is a common helper function for supporting MSG_SPLICE_PAGES.  It   extracts pages from an iterator and adds them to the socket buffer if   possible, copying them to fragments if not possible (such as if they're slab   pages).     Returns the amount of data splicedcopied or -EMSGSIZE if there's   insufficient space in the buffer to transfer anything. ", "int sk_stream_wait_connect(struct sock *sk, long *timeo_p)": "sk_stream_wait_connect - Wait for a socket to get into the connected state   @sk: sock to wait on   @timeo_p: for how long to wait     Must be called with the socket locked. ", "int sk_stream_wait_memory(struct sock *sk, long *timeo_p)": "sk_stream_wait_memory - Wait for more memory for a socket   @sk: socket to wait for memory   @timeo_p: for how long ", "__skb_queue_purge(&sk->sk_receive_queue);/* Next, the error queue. * We need to use queue lock, because other threads might * add packets to the queue without socket lock being held. ": "sk_stream_kill_queues(struct sock  sk){  First the read buffer. ", "/* The first check was omitted in <= 2.2.5. The reasoning was   that parser checks cmsg_len in any case, so that   additional check would be work duplication.   But if cmsg_level is not SOL_SOCKET, we do not check   for too short ancillary data object at all! Oops.   OK, let's add it... ": "__scm_send(struct socket  sock, struct msghdr  msg, struct scm_cookie  p){struct cmsghdr  cmsg;int err;for_each_cmsghdr(cmsg, msg) {err = -EINVAL;  Verify that cmsg_len is at least sizeof(struct cmsghdr) ", "}if (msg->msg_controllen < cmlen) ": "put_cmsg(struct msghdr   msg, int level, int type, int len, void  data){int cmlen = CMSG_LEN(len);if (msg->msg_flags & MSG_CMSG_COMPAT)return put_cmsg_compat(msg, level, type, len, data);if (!msg->msg_control || msg->msg_controllen < sizeof(struct cmsghdr)) {msg->msg_flags |= MSG_CTRUNC;return 0;   XXX: return error? check spec. ", "if (WARN_ON_ONCE(!msg->msg_control_is_user))return;if (msg->msg_flags & MSG_CMSG_COMPAT) ": "scm_detach_fds(struct msghdr  msg, struct scm_cookie  scm){struct cmsghdr __user  cm =(__force struct cmsghdr __user  )msg->msg_control_user;unsigned int o_flags = (msg->msg_flags & MSG_CMSG_CLOEXEC) ? O_CLOEXEC : 0;int fdmax = min_t(int, scm_max_fds(msg), scm->fp->count);int __user  cmsg_data = CMSG_USER_DATA(cm);int err = 0, i;  no use for FD passing from kernel space callers ", "int gen_new_estimator(struct gnet_stats_basic_sync *bstats,      struct gnet_stats_basic_sync __percpu *cpu_bstats,      struct net_rate_estimator __rcu **rate_est,      spinlock_t *lock,      bool running,      struct nlattr *opt)": "gen_new_estimator - create a new rate estimator   @bstats: basic statistics   @cpu_bstats: bstats per cpu   @rate_est: rate estimator statistics   @lock: lock for statistics and control path   @running: true if @bstats represents a running qdisc, thus @bstats'             internal values might change during basic reads. Only used             if @bstats_cpu is NULL   @opt: rate estimator configuration TLV     Creates a new rate estimator with &bstats as source and &rate_est   as destination. A new timer with the interval specified in the   configuration TLV is created. Upon each interval, the latest statistics   will be read from &bstats and the estimated rate will be stored in   &rate_est with the statistics lock grabbed during this period.     Returns 0 on success or a negative error code.   ", "void gen_kill_estimator(struct net_rate_estimator __rcu **rate_est)": "gen_kill_estimator - remove a rate estimator   @rate_est: rate estimator     Removes the rate estimator.   ", "int gen_replace_estimator(struct gnet_stats_basic_sync *bstats,  struct gnet_stats_basic_sync __percpu *cpu_bstats,  struct net_rate_estimator __rcu **rate_est,  spinlock_t *lock,  bool running, struct nlattr *opt)": "gen_replace_estimator - replace rate estimator configuration   @bstats: basic statistics   @cpu_bstats: bstats per cpu   @rate_est: rate estimator statistics   @lock: lock for statistics and control path   @running: true if @bstats represents a running qdisc, thus @bstats'             internal values might change during basic reads. Only used             if @cpu_bstats is NULL   @opt: rate estimator configuration TLV     Replaces the configuration of a rate estimator by calling   gen_kill_estimator() and gen_new_estimator().     Returns 0 on success or a negative error code. ", "bool gen_estimator_active(struct net_rate_estimator __rcu **rate_est)": "gen_estimator_active - test if estimator is currently in use   @rate_est: rate estimator     Returns true if estimator is active, and false if not. ", "if (pdev) ": "of_get_mac_address_nvmem(struct device_node  np, u8  addr){struct platform_device  pdev = of_find_device_by_node(np);struct nvmem_cell  cell;const void  mac;size_t len;int ret;  Try lookup by device first, there might be a nvmem_cell_lookup   associated with a given device. ", "int of_get_ethdev_address(struct device_node *np, struct net_device *dev)": "of_get_ethdev_address()   @np:Caller's Device Node   @dev:Pointer to netdevice which address will be updated     Search the device tree for the best MAC address to use.   If found set @dev->dev_addr to that address.     See documentation of of_get_mac_address() for more information on how   the best address is determined.     Return: 0 on success and errno in case of error. ", "struct sk_buff *skb_eth_gso_segment(struct sk_buff *skb,    netdev_features_t features, __be16 type)": "skb_eth_gso_segment - segmentation handler for ethernet protocols.  @skb: buffer to segment  @features: features for the output path (see dev->features)  @type: Ethernet Protocol ID ", "struct sk_buff *skb_mac_gso_segment(struct sk_buff *skb,    netdev_features_t features)": "skb_mac_gso_segment - mac layer segmentation handler.  @skb: buffer to segment  @features: features for the output path (see dev->features) ", "struct sk_buff *__skb_gso_segment(struct sk_buff *skb,  netdev_features_t features, bool tx_path)": "__skb_gso_segment - Perform segmentation on skb.  @skb: buffer to segment  @features: features for the output path (see dev->features)  @tx_path: whether it is called in TX path    This function segments the given skb and returns a list of segments.    It may return NULL if the skb requires no segmentation.  This is  only possible when GSO is used for verifying header integrity.    Segmentation preserves SKB_GSO_CB_OFFSET bytes of previous skb cb. ", "DEFINE_RWLOCK(dev_base_lock);EXPORT_SYMBOL(dev_base_lock": "dev_base_lock and the rtnl   semaphore.     Pure readers hold dev_base_lock for reading, or rcu_read_lock()     Writers must hold the rtnl semaphore while they loop through the   dev_base_head list, and hold dev_base_lock for writing when they do the   actual updates.  This allows pure readers to access the list even   while a writer is preparing to update it.     To put it another way, dev_base_lock is held for writing only to   protect against pure readers; the rtnl semaphore provides the   protection against other writers.     See, for example usages, register_netdevice() and   unregister_netdevice(), which must be called with the rtnl   semaphore held. ", "void dev_add_pack(struct packet_type *pt)": "dev_add_pack - add packet handler  @pt: packet type declaration    Add a protocol handler to the networking stack. The passed &packet_type  is linked into kernel lists and may not be freed until it has been  removed from the kernel lists.    This call does not sleep therefore it can not  guarantee all CPU's that are in middle of receiving packets  will see the new packet type (until the next received packet). ", "void __dev_remove_pack(struct packet_type *pt)": "dev_remove_pack - remove packet handler  @pt: packet type declaration    Remove a protocol handler that was previously added to the kernel  protocol handlers by dev_add_pack(). The passed &packet_type is removed  from the kernel lists and can be freed or reused once this function  returns.          The packet type might still be in use by receivers  and must not be freed until after all the CPU's have gone  through a quiescent state. ", "int dev_get_iflink(const struct net_device *dev)": "dev_get_iflink- get 'iflink' value of a interface  @dev: targeted interface    Indicates the ifindex the interface is linked to.  Physical interfaces have the same 'ifindex' and 'iflink' values. ", "struct net_device *__dev_get_by_name(struct net *net, const char *name)": "dev_get_by_name- find a device by its name  @net: the applicable net namespace  @name: name to find    Find an interface by name. Must be called under RTNL semaphore  or @dev_base_lock. If the name is found a pointer to the device  is returned. If the name is not found then %NULL is returned. The  reference counters are not incremented so the caller must be  careful with locks. ", "struct net_device *dev_get_by_name_rcu(struct net *net, const char *name)": "dev_get_by_name_rcu- find a device by its name   @net: the applicable net namespace   @name: name to find     Find an interface by name.   If the name is found a pointer to the device is returned.   If the name is not found then %NULL is returned.   The reference counters are not incremented so the caller must be   careful with locks. The caller must hold RCU lock. ", "struct net_device *dev_get_by_name(struct net *net, const char *name)": "netdev_get_by_name() instead ", "struct net_device *__dev_get_by_index(struct net *net, int ifindex)": "dev_get_by_index - find a device by its ifindex  @net: the applicable net namespace  @ifindex: index of device    Search for an interface by index. Returns %NULL if the device  is not found or a pointer to the device. The device has not  had its reference counter increased so the caller must be careful  about locking. The caller must hold either the RTNL semaphore  or @dev_base_lock. ", "struct net_device *dev_get_by_index_rcu(struct net *net, int ifindex)": "dev_get_by_index_rcu - find a device by its ifindex  @net: the applicable net namespace  @ifindex: index of device    Search for an interface by index. Returns %NULL if the device  is not found or a pointer to the device. The device has not  had its reference counter increased so the caller must be careful  about locking. The caller must hold RCU lock. ", "struct net_device *dev_get_by_index(struct net *net, int ifindex)": "netdev_get_by_index() instead ", "struct net_device *dev_get_by_napi_id(unsigned int napi_id)": "dev_get_by_napi_id - find a device by napi_id  @napi_id: ID of the NAPI struct    Search for an interface by NAPI ID. Returns %NULL if the device  is not found or a pointer to the device. The device has not had  its reference counter increased so the caller must be careful  about locking. The caller must hold RCU lock. ", "struct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,       const char *ha)": "dev_getbyhwaddr_rcu - find a device by its hardware address  @net: the applicable net namespace  @type: media type of device  @ha: hardware address    Search for an interface by MAC address. Returns NULL if the device  is not found or a pointer to the device.  The caller must hold RCU or RTNL.  The returned device has not had its ref count increased  and the caller must therefore be careful about locking   ", "struct net_device *__dev_get_by_flags(struct net *net, unsigned short if_flags,      unsigned short mask)": "__dev_get_by_flags - find any device with given flags  @net: the applicable net namespace  @if_flags: IFF_  values  @mask: bitmask of bits in if_flags to check    Search for any interface with the given flags. Returns NULL if a device  is not found or a pointer to the device. Must be called inside  rtnl_lock(), and result refcount is unchanged. ", "bool dev_valid_name(const char *name)": "dev_valid_name - check if name is okay for network device  @name: name string    Network device names need to be valid file names to  allow sysfs to work.  We also disallow any kind of  whitespace. ", "static int __dev_alloc_name(struct net *net, const char *name, char *buf)": "dev_alloc_name - allocate a name for a device  @net: network namespace to allocate the device name in  @name: name format string  @buf:  scratch buffer and result name string    Passed a format string - eg \"lt%d\" it will try and find a suitable  id. It scans list of devices to build up a free map, then chooses  the first empty slot. The caller must hold the dev_base or rtnl lock  while allocating the name and adding the device in order to avoid  duplicates.  Limited to bits_per_byte   page size devices (ie 32K on most platforms).  Returns the number of the unit assigned or a negative errno code. ", "int dev_set_alias(struct net_device *dev, const char *alias, size_t len)": "dev_set_alias - change ifalias of a device  @dev: device  @alias: name up to IFALIASZ  @len: limit of bytes to copy from info    Set ifalias for a device, ", "void netdev_features_change(struct net_device *dev)": "netdev_features_change - device changes features  @dev: device to cause notification    Called to indicate a device has changed features. ", "void netdev_state_change(struct net_device *dev)": "netdev_state_change - device changes state  @dev: device to cause notification    Called to indicate a device has changed state. This function calls  the notifier chains for netdev_chain and sends a NEWLINK message  to the routing socket. ", "void __netdev_notify_peers(struct net_device *dev)": "netdev_notify_peers - notify network peers about existence of @dev,   to be called when rtnl lock is already held.   @dev: network device     Generate traffic such that interested network peers are aware of   @dev, such as by generating a gratuitous ARP. This may be used when   a device wants to inform the rest of the network about some sort of   reconfiguration such as a failover event or virtual machine   migration. ", "#include <linux/uaccess.h>#include <linux/bitops.h>#include <linux/capability.h>#include <linux/cpu.h>#include <linux/types.h>#include <linux/kernel.h>#include <linux/hash.h>#include <linux/slab.h>#include <linux/sched.h>#include <linux/sched/mm.h>#include <linux/mutex.h>#include <linux/rwsem.h>#include <linux/string.h>#include <linux/mm.h>#include <linux/socket.h>#include <linux/sockios.h>#include <linux/errno.h>#include <linux/interrupt.h>#include <linux/if_ether.h>#include <linux/netdevice.h>#include <linux/etherdevice.h>#include <linux/ethtool.h>#include <linux/skbuff.h>#include <linux/kthread.h>#include <linux/bpf.h>#include <linux/bpf_trace.h>#include <net/net_namespace.h>#include <net/sock.h>#include <net/busy_poll.h>#include <linux/rtnetlink.h>#include <linux/stat.h>#include <net/dsa.h>#include <net/dst.h>#include <net/dst_metadata.h>#include <net/gro.h>#include <net/pkt_sched.h>#include <net/pkt_cls.h>#include <net/checksum.h>#include <net/xfrm.h>#include <linux/highmem.h>#include <linux/init.h>#include <linux/module.h>#include <linux/netpoll.h>#include <linux/rcupdate.h>#include <linux/delay.h>#include <net/iw_handler.h>#include <asm/current.h>#include <linux/audit.h>#include <linux/dmaengine.h>#include <linux/err.h>#include <linux/ctype.h>#include <linux/if_arp.h>#include <linux/if_vlan.h>#include <linux/ip.h>#include <net/ip.h>#include <net/mpls.h>#include <linux/ipv6.h>#include <linux/in.h>#include <linux/jhash.h>#include <linux/random.h>#include <trace/events/napi.h>#include <trace/events/net.h>#include <trace/events/skb.h>#include <trace/events/qdisc.h>#include <linux/inetdevice.h>#include <linux/cpu_rmap.h>#include <linux/static_key.h>#include <linux/hashtable.h>#include <linux/vmalloc.h>#include <linux/if_macvlan.h>#include <linux/errqueue.h>#include <linux/hrtimer.h>#include <linux/netfilter_netdev.h>#include <linux/crash_dump.h>#include <linux/sctp.h>#include <net/udp_tunnel.h>#include <linux/net_namespace.h>#include <linux/indirect_call_wrapper.h>#include <net/devlink.h>#include <linux/pm_runtime.h>#include <linux/prandom.h>#include <linux/once_lite.h>#include \"dev.h\"#include \"net-sysfs.h\"static DEFINE_SPINLOCK(ptype_lock);struct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;struct list_head ptype_all __read_mostly;/* Taps ": "function  call a packet.  Alan Cox:Hashed net_bh()  Richard Kooijman:Timestamp fixes.  Alan Cox:Wrong field in SIOCGIFDSTADDR  Alan Cox:Device lock protection.                Alan Cox        :       Fixed nasty side effect of device close  changes.  Rudi Cilibrasi:Pass the right thing to  set_mac_address()  Dave Miller:32bit quantity for the device lock to  make it work out on a Sparc.  Bjorn Ekwall:Added KERNELD hack.  Alan Cox:Cleaned up the backlog initialise.  Craig Metz:SIOCGIFCONF fix if space for under  1 device.      Thomas Bogendoerfer :Return ENODEV for dev_open, if there  is no device open function.  Andi Kleen:Fix error reporting for SIOCGIFCONF      Michael Chastain:Fix signedunsigned for SIOCGIFCONF  Cyrus Durgin:Cleaned for KMOD  Adam Sulmicki   :Bug Fix : Network Device Unload  A network device unload needs to purge  the backlog queue.  Paul Rusty Russell:SIOCSIFNAME                Pekka Riikonen  :Netdev boot-time settings code                Andrew Morton   :       Make unregister_netdevice wait                                        indefinitely on dev->refcnt                J Hadi Salim    :       - Backlog queue sampling          - netif_rx() feedback ", "netpoll_poll_disable(dev);call_netdevice_notifiers(NETDEV_GOING_DOWN, dev);clear_bit(__LINK_STATE_START, &dev->state);/* Synchronize to scheduled poll. We cannot touch poll list, it * can be even on different cpu. So just clear netif_running(). * * dev->stop() will invoke napi_disable() on all of it's * napi_struct instances on this device. ": "dev_close_many(struct list_head  head){struct net_device  dev;ASSERT_RTNL();might_sleep();list_for_each_entry(dev, head, close_list) {  Temporarily disable netpoll until the interface is down ", "void dev_disable_lro(struct net_device *dev)": "dev_disable_lro - disable Large Receive Offload on a device  @dev: device    Disable Large Receive Offload (LRO) on a net device.  Must be  called under RTNL.  This is needed if received packets may be  forwarded to another interface. ", "int register_netdevice_notifier(struct notifier_block *nb)": "register_netdevice_notifier - register a network notifier block   @nb: notifier     Register a notifier to be called when network device events occur.   The notifier passed is linked into the kernel structures and must   not be reused until it has been unregistered. A negative errno code   is returned on a failure.     When registered all registration and up events are replayed   to the new notifier to allow device to have a race free   view of the network device list. ", "int unregister_netdevice_notifier(struct notifier_block *nb)": "unregister_netdevice_notifier - unregister a network notifier block   @nb: notifier     Unregister a notifier previously registered by   register_netdevice_notifier(). The notifier is unlinked into the   kernel structures and may then be reused. A negative errno code   is returned on a failure.     After unregistering unregister and down device events are synthesized   for all devices on the device list to the removed notifier to remove   the need for special case cleanup code. ", "int register_netdevice_notifier_net(struct net *net, struct notifier_block *nb)": "unregister_netdevice_notifier_net(struct net  net,       struct notifier_block  nb){int err;err = raw_notifier_chain_unregister(&net->netdev_chain, nb);if (err)return err;call_netdevice_unregister_net_notifiers(nb, net);return 0;}     register_netdevice_notifier_net - register a per-netns network notifier block   @net: network namespace   @nb: notifier     Register a notifier to be called when network device events occur.   The notifier passed is linked into the kernel structures and must   not be reused until it has been unregistered. A negative errno code   is returned on a failure.     When registered all registration and up events are replayed   to the new notifier to allow device to have a race free   view of the network device list. ", "DEFINE_RWLOCK(dev_base_lock);EXPORT_SYMBOL(dev_base_lock);static DEFINE_MUTEX(ifalias_mutex);/* protects napi_hash addition/deletion and napi_gen_id ": "call_netdevice_notifiers_extack(unsigned long val,   struct net_device  dev,   struct netlink_ext_ack  extack);static struct napi_struct  napi_by_id(unsigned int napi_id);    The @dev_base_head list is protected by @dev_base_lock and the rtnl   semaphore.     Pure readers hold dev_base_lock for reading, or rcu_read_lock()     Writers must hold the rtnl semaphore while they loop through the   dev_base_head list, and hold dev_base_lock for writing when they do the   actual updates.  This allows pure readers to access the list even   while a writer is preparing to update it.     To put it another way, dev_base_lock is held for writing only to   protect against pure readers; the rtnl semaphore provides the   protection against other writers.     See, for example usages, register_netdevice() and   unregister_netdevice(), which must be called with the rtnl   semaphore held. ", "for (i = 0; i < TC_MAX_QUEUE; i++, tc++) ": "netdev_txq_to_tc(struct net_device  dev, unsigned int txq){if (dev->num_tc) {struct netdev_tc_txq  tc = &dev->tc_to_txq[0];int i;  walk through the TCs and see if it falls into any of them ", "num_tc = dev->num_tc;if (num_tc < 0)return -EINVAL;/* If queue belongs to subordinate dev use its map ": "netif_set_xps_queue(struct net_device  dev, const unsigned long  mask,  u16 index, enum xps_map_type type){struct xps_dev_maps  dev_maps,  new_dev_maps = NULL,  old_dev_maps = NULL;const unsigned long  online_mask = NULL;bool active = false, copy = false;int i, j, tci, numa_node_id = -2;int maps_sz, num_tc = 1, tc = 0;struct xps_map  map,  new_map;unsigned int nr_ids;WARN_ON_ONCE(index >= dev->num_tx_queues);if (dev->num_tc) {  Do not allow XPS on subordinate device directly ", "dev->num_tc = 0;memset(dev->tc_to_txq, 0, sizeof(dev->tc_to_txq));memset(dev->prio_tc_map, 0, sizeof(dev->prio_tc_map));}EXPORT_SYMBOL(netdev_reset_tc": "netdev_reset_tc(struct net_device  dev){#ifdef CONFIG_XPSnetif_reset_xps_queues_gt(dev, 0);#endifnetdev_unbind_all_sb_channels(dev);  Reset TC configuration of device ", "dev->num_tc = 0;memset(dev->tc_to_txq, 0, sizeof(dev->tc_to_txq));memset(dev->prio_tc_map, 0, sizeof(dev->prio_tc_map));}EXPORT_SYMBOL(netdev_reset_tc);int netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset)": "netdev_unbind_sb_channel(dev, txq->sb_dev);}}void netdev_reset_tc(struct net_device  dev){#ifdef CONFIG_XPSnetif_reset_xps_queues_gt(dev, 0);#endifnetdev_unbind_all_sb_channels(dev);  Reset TC configuration of device ", "if (sb_dev->num_tc >= 0 || tc >= dev->num_tc)return -EINVAL;/* We cannot hand out queues we don't have ": "netdev_bind_sb_channel_queue(struct net_device  dev, struct net_device  sb_dev, u8 tc, u16 count, u16 offset){  Make certain the sb_dev and dev are already configured ", "if (netif_is_multiqueue(dev))return -ENODEV;/* We allow channels 1 - 32767 to be used for subordinate channels. * Channel 0 is meant to be \"native\" mode and used only to represent * the main root device. We allow writing 0 to reset the device back * to normal mode after being used as a subordinate channel. ": "netdev_set_sb_channel(struct net_device  dev, u16 channel){  Do not use a multiqueue device to represent a subordinate channel ", "static void netif_setup_tc(struct net_device *dev, unsigned int txq)": "netif_set_real_num_tx_queues. ", "int netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq)": "netif_set_real_num_rx_queues - set actual number of RX queues used  @dev: Network device  @rxq: Actual number of RX queues    This must be called either with the rtnl_lock held or before  registration of the net device.  Returns 0 on success, or a  negative error code.  If called before registration, it always  succeeds. ", "int netif_set_real_num_queues(struct net_device *dev,      unsigned int txq, unsigned int rxq)": "netif_set_real_num_queues - set actual number of RX and TX queues used  @dev: Network device  @txq: Actual number of TX queues  @rxq: Actual number of RX queues    Set the real number of both TX and RX queues.  Does nothing if the number of queues is already correct. ", "void netif_set_tso_max_size(struct net_device *dev, unsigned int size)": "netif_set_tso_max_size() - set the max size of TSO frames supported   @dev:netdev to update   @size:max skb->len of a TSO frame     Set the limit on the size of TSO super-frames the device can handle.   Unless explicitly set the stack will assume the value of   %GSO_LEGACY_MAX_SIZE. ", "void netif_set_tso_max_segs(struct net_device *dev, unsigned int segs)": "netif_set_tso_max_segs() - set the max number of segs supported for TSO   @dev:netdev to update   @segs:max number of TCP segments     Set the limit on the number of TCP segments the device can generate from   a single TSO super-frame.   Unless explicitly set the stack will assume the value of %GSO_MAX_SEGS. ", "void netif_inherit_tso_max(struct net_device *to, const struct net_device *from)": "netif_inherit_tso_max() - copy all TSO limits from a lower device to an upper   @to:netdev to update   @from:netdev from which to copy the limits ", "int netif_get_num_default_rss_queues(void)": "netif_get_num_default_rss_queues - default number of RSS queues     Default value is the number of physical cores if there are only 1 or 2, or   divided by 2 if there are more. ", "void netif_device_detach(struct net_device *dev)": "netif_device_detach - mark device as removed   @dev: network device     Mark device as removed from system and therefore no longer available. ", "void netif_device_attach(struct net_device *dev)": "netif_tx_stop_all_queues(dev);}}EXPORT_SYMBOL(netif_device_detach);     netif_device_attach - mark device as attached   @dev: network device     Mark device as attached from system and restart if needed. ", "if (skb_has_shared_frag(skb)) ": "skb_checksum_help(struct sk_buff  skb){__wsum csum;int ret = 0, offset;if (skb->ip_summed == CHECKSUM_COMPLETE)goto out_set_summed;if (unlikely(skb_is_gso(skb))) {skb_warn_bad_offload(skb);return -EINVAL;}  Before computing a checksum, we should make sure no frag could   be modified by an external entity : checksum could be wrong. ", "if (skb->encapsulation)features &= dev->hw_enc_features;if (skb_vlan_tagged(skb))features = netdev_intersect_features(features,     dev->vlan_features |     NETIF_F_HW_VLAN_CTAG_TX |     NETIF_F_HW_VLAN_STAG_TX);if (dev->netdev_ops->ndo_features_check)features &= dev->netdev_ops->ndo_features_check(skb, dev,features);elsefeatures &= dflt_features_check(skb, dev, features);return harmonize_features(skb, features);}EXPORT_SYMBOL(netif_skb_features": "netif_skb_features(struct sk_buff  skb){struct net_device  dev = skb->dev;netdev_features_t features = dev->features;if (skb_is_gso(skb))features = gso_features_check(skb, dev, features);  If encapsulation offload request, verify we are testing   hardware encapsulation features instead of standard   features for the netdev ", "int dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *skb)": "dev_loopback_xmit - loop back @skb  @net: network namespace this loopback is happening in  @sk:  sk needed to be a netfilter okfn  @skb: buffer to transmit ", "int __dev_queue_xmit(struct sk_buff *skb, struct net_device *sb_dev)": "__dev_queue_xmit() - transmit a buffer   @skb:buffer to transmit   @sb_dev:suboordinate device used for L2 forwarding offload     Queue a buffer for transmission to a network device. The caller must   have set the device and priority and built the buffer before calling   this function. The function can be called from an interrupt.     When calling this method, interrupts MUST be enabled. This is because   the BH enable code must have IRQs enabled so that it will not deadlock.     Regardless of the return value, the skb is consumed, so it is currently   difficult to retry a send to this method. (You can bump the ref count   before sending to hold a reference for retry if you are careful.)     Return:     0- buffer successfully transmitted     positive qdisc return code- NET_XMIT_DROP etc.     negative errno- other errors ", "bool rps_may_expire_flow(struct net_device *dev, u16 rxq_index, u32 flow_id, u16 filter_id)": "rps_may_expire_flow - check whether an RFS hardware filter may be removed   @dev: Device on which the filter was set   @rxq_index: RX queue index   @flow_id: Flow ID passed to ndo_rx_flow_steer()   @filter_id: Filter ID returned by ndo_rx_flow_steer()     Drivers that implement ndo_rx_flow_steer() should periodically call   this function for each installed filter and remove the filters for   which it returns %true. ", "int __netif_rx(struct sk_buff *skb)": "__netif_rx-Slightly optimized version of netif_rx  @skb: buffer to post    This behaves as netif_rx except that it does not disable bottom halves.  As a result this function may only be invoked from the interrupt context  (either hard or soft interrupt). ", "skb->pkt_type = PACKET_OTHERHOST;} else if (eth_type_vlan(skb->protocol)) ": "netif_receive_skb_core(struct sk_buff   pskb, bool pfmemalloc,    struct packet_type   ppt_prev){struct packet_type  ptype,  pt_prev;rx_handler_func_t  rx_handler;struct sk_buff  skb =  pskb;struct net_device  orig_dev;bool deliver_exact = false;int ret = NET_RX_DROP;__be16 type;net_timestamp_check(!READ_ONCE(netdev_tstamp_prequeue), skb);trace_netif_receive_skb(skb);orig_dev = skb->dev;skb_reset_network_header(skb);if (!skb_transport_header_was_set(skb))skb_reset_transport_header(skb);skb_reset_mac_len(skb);pt_prev = NULL;another_round:skb->skb_iif = skb->dev->ifindex;__this_cpu_inc(softnet_data.processed);if (static_branch_unlikely(&generic_xdp_needed_key)) {int ret2;migrate_disable();ret2 = do_xdp_generic(rcu_dereference(skb->dev->xdp_prog), skb);migrate_enable();if (ret2 != XDP_PASS) {ret = NET_RX_DROP;goto out;}}if (eth_type_vlan(skb->protocol)) {skb = skb_vlan_untag(skb);if (unlikely(!skb))goto out;}if (skb_skip_tc_classify(skb))goto skip_classify;if (pfmemalloc)goto skip_taps;list_for_each_entry_rcu(ptype, &ptype_all, list) {if (pt_prev)ret = deliver_skb(skb, pt_prev, orig_dev);pt_prev = ptype;}list_for_each_entry_rcu(ptype, &skb->dev->ptype_all, list) {if (pt_prev)ret = deliver_skb(skb, pt_prev, orig_dev);pt_prev = ptype;}skip_taps:#ifdef CONFIG_NET_INGRESSif (static_branch_unlikely(&ingress_needed_key)) {bool another = false;nf_skip_egress(skb, true);skb = sch_handle_ingress(skb, &pt_prev, &ret, orig_dev, &another);if (another)goto another_round;if (!skb)goto out;nf_skip_egress(skb, false);if (nf_ingress(skb, &pt_prev, &ret, orig_dev) < 0)goto out;}#endifskb_reset_redirect(skb);skip_classify:if (pfmemalloc && !skb_pfmemalloc_protocol(skb))goto drop;if (skb_vlan_tag_present(skb)) {if (pt_prev) {ret = deliver_skb(skb, pt_prev, orig_dev);pt_prev = NULL;}if (vlan_do_receive(&skb))goto another_round;else if (unlikely(!skb))goto out;}rx_handler = rcu_dereference(skb->dev->rx_handler);if (rx_handler) {if (pt_prev) {ret = deliver_skb(skb, pt_prev, orig_dev);pt_prev = NULL;}switch (rx_handler(&skb)) {case RX_HANDLER_CONSUMED:ret = NET_RX_SUCCESS;goto out;case RX_HANDLER_ANOTHER:goto another_round;case RX_HANDLER_EXACT:deliver_exact = true;break;case RX_HANDLER_PASS:break;default:BUG();}}if (unlikely(skb_vlan_tag_present(skb)) && !netdev_uses_dsa(skb->dev)) {check_vlan_id:if (skb_vlan_tag_get_id(skb)) {  Vlan id is non 0 and vlan_do_receive() above couldn't   find vlan device. ", "static int get_rps_cpu(struct net_device *dev, struct sk_buff *skb,       struct rps_dev_flow **rflowp)": "netif_receive_skb and returns the target   CPU from the RPS map of the receiving queue for a given skb.   rcu_read_lock must be held on entry. ", "/* Current (common) ptype of sublist ": "netif_receive_skb_list_ptype(struct list_head  head,  struct packet_type  pt_prev,  struct net_device  orig_dev){struct sk_buff  skb,  next;if (!pt_prev)return;if (list_empty(head))return;if (pt_prev->list_func != NULL)INDIRECT_CALL_INET(pt_prev->list_func, ipv6_list_rcv,   ip_list_rcv, head, pt_prev, orig_dev);elselist_for_each_entry_safe(skb, next, head, list) {skb_list_del_init(skb);pt_prev->func(skb, skb->dev, pt_prev, orig_dev);}}static void __netif_receive_skb_list_core(struct list_head  head, bool pfmemalloc){  Fast-path assumptions:   - There is no RX handler.   - Only one packet_type matches.   If either of these fails, we will end up doing some per-packet   processing in-line, then handling the 'last ptype' for the whole   sublist.  This can't cause out-of-order delivery to any single ptype,   because the 'last ptype' must be constant across the sublist, and all   other ptypes are handled per-packet. ", "thread = READ_ONCE(napi->thread);if (thread) ": "napi_enable()dev_set_threaded().   Use READ_ONCE() to guarantee a complete   read on napi->thread. Only call   wake_up_process() when it's not NULL. ", "bool napi_schedule_prep(struct napi_struct *n)": "napi_schedule_prep - check if napi can be scheduled  @n: napi context     Test if NAPI routine is already running, and if not mark   it as running.  This is used as a condition variable to   insure only one NAPI poll instance runs.  We also make   sure there is no pending NAPI disable. ", "static int enqueue_to_backlog(struct sk_buff *skb, int cpu,      unsigned int *qtail)": "__napi_schedule_irqoff(&mysd->backlog);}#ifdef CONFIG_NET_FLOW_LIMITint netdev_flow_limit_table_len __read_mostly = (1 << 12);#endifstatic bool skb_flow_limit(struct sk_buff  skb, unsigned int qlen){#ifdef CONFIG_NET_FLOW_LIMITstruct sd_flow_limit  fl;struct softnet_data  sd;unsigned int old_flow, new_flow;if (qlen < (READ_ONCE(netdev_max_backlog) >> 1))return false;sd = this_cpu_ptr(&softnet_data);rcu_read_lock();fl = rcu_dereference(sd->flow_limit);if (fl) {new_flow = skb_get_hash(skb) & (fl->num_buckets - 1);old_flow = fl->history[fl->history_head];fl->history[fl->history_head] = new_flow;fl->history_head++;fl->history_head &= FLOW_LIMIT_HISTORY - 1;if (likely(fl->buckets[old_flow]))fl->buckets[old_flow]--;if (++fl->buckets[new_flow] > (FLOW_LIMIT_HISTORY >> 1)) {fl->count++;rcu_read_unlock();return true;}}rcu_read_unlock();#endifreturn false;}    enqueue_to_backlog is called to queue an skb to a per CPU backlog   queue (may be a remote CPU queue). ", "if (unlikely(n->state & (NAPIF_STATE_NPSVC | NAPIF_STATE_IN_BUSY_POLL)))return false;if (work_done) ": "napi_complete_done(struct napi_struct  n, int work_done){unsigned long flags, val, new, timeout = 0;bool ret = true;    1) Don't let napi dequeue from the cpu poll list      just in case its running on a different cpu.   2) If we are busy polling, do nothing here, we have      the guarantee we will be called later. ", "if (val & (NAPIF_STATE_DISABLE | NAPIF_STATE_SCHED |   NAPIF_STATE_IN_BUSY_POLL)) ": "napi_busy_loop(unsigned int napi_id,    bool ( loop_end)(void  , unsigned long),    void  loop_end_arg, bool prefer_busy_poll, u16 budget){unsigned long start_time = loop_end ? busy_loop_current_time() : 0;int ( napi_poll)(struct napi_struct  napi, int budget);void  have_poll_lock = NULL;struct napi_struct  napi;restart:napi_poll = NULL;rcu_read_lock();napi = napi_by_id(napi_id);if (!napi)goto out;if (!IS_ENABLED(CONFIG_PREEMPT_RT))preempt_disable();for (;;) {int work = 0;local_bh_disable();if (!napi_poll) {unsigned long val = READ_ONCE(napi->state);  If multiple threads are competing for this napi,   we avoid dirtying napi->state as much as we can. ", "if (dev->threaded && napi_kthread_create(napi))dev->threaded = 0;}EXPORT_SYMBOL(netif_napi_add_weight": "netif_napi_add_weight(struct net_device  dev, struct napi_struct  napi,   int ( poll)(struct napi_struct  , int), int weight){if (WARN_ON(test_and_set_bit(NAPI_STATE_LISTED, &napi->state)))return;INIT_LIST_HEAD(&napi->poll_list);INIT_HLIST_NODE(&napi->napi_hash_node);hrtimer_init(&napi->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_PINNED);napi->timer.function = napi_watchdog;init_gro_hash(napi);napi->skb = NULL;INIT_LIST_HEAD(&napi->rx_list);napi->rx_count = 0;napi->poll = poll;if (weight > NAPI_POLL_WEIGHT)netdev_err_once(dev, \"%s() called with weight %d\\n\", __func__,weight);napi->weight = weight;napi->dev = dev;#ifdef CONFIG_NETPOLLnapi->poll_owner = -1;#endifnapi->list_owner = -1;set_bit(NAPI_STATE_SCHED, &napi->state);set_bit(NAPI_STATE_NPSVC, &napi->state);list_add_rcu(&napi->dev_list, &dev->napi_list);napi_hash_add(napi);napi_get_frags_check(napi);  Create kthread for this napi if dev->threaded is set.   Clear dev->threaded if kthread creation failed so that   threaded mode will not be enabled in napi_enable(). ", "smp_mb__after_atomic(); /* Commit netif_running(). ": "napi_disable() on all of it's   napi_struct instances on this device. ", "bool netdev_has_upper_dev(struct net_device *dev,  struct net_device *upper_dev)": "netdev_has_upper_dev(struct net_device  upper_dev,    struct netdev_nested_priv  priv){struct net_device  dev = (struct net_device  )priv->data;return upper_dev == dev;}     netdev_has_upper_dev - Check if device is linked to an upper device   @dev: device   @upper_dev: upper device to check     Find out if a device is linked to specified upper device and return true   in case it is. Note that this checks only immediate upper device,   not through a complete stack of devices. The caller must hold the RTNL lock. ", "bool netdev_has_upper_dev_all_rcu(struct net_device *dev,  struct net_device *upper_dev)": "netdev_has_upper_dev_all_rcu - Check if device is linked to an upper device   @dev: device   @upper_dev: upper device to check     Find out if a device is linked to specified upper device and return true   in case it is. Note that this checks the entire upper device chain.   The caller must hold rcu lock. ", "bool netdev_has_any_upper_dev(struct net_device *dev)": "netdev_has_any_upper_dev - Check if device is linked to some device   @dev: device     Find out if a device is linked to an upper device and return true in case   it is. The caller must hold the RTNL lock. ", "struct net_device *netdev_master_upper_dev_get(struct net_device *dev)": "netdev_master_upper_dev_get - Get master upper device   @dev: device     Find a master upper device and return pointer to it or NULL in case   it's not there. The caller must hold the RTNL lock. ", "struct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev, struct list_head **iter)": "netdev_upper_get_next_dev_rcu - Get the next dev from upper list   @dev: device   @iter: list_head    of the current position     Gets the next device from the dev's upper list, starting from iter   position. The caller must hold RCU read lock. ", "void *netdev_lower_get_next_private(struct net_device *dev,    struct list_head **iter)": "netdev_lower_get_next_private - Get the next ->private from the     lower neighbour list   @dev: device   @iter: list_head    of the current position     Gets the next netdev_adjacent->private from the dev's lower neighbour   list, starting from iter position. The caller must hold either hold the   RTNL lock or its own locking that guarantees that the neighbour lower   list will remain unchanged. ", "void *netdev_lower_get_next_private_rcu(struct net_device *dev,struct list_head **iter)": "netdev_lower_get_next_private_rcu - Get the next ->private from the         lower neighbour list, RCU         variant   @dev: device   @iter: list_head    of the current position     Gets the next netdev_adjacent->private from the dev's lower neighbour   list, starting from iter position. The caller must hold RCU read lock. ", "void *netdev_lower_get_first_private_rcu(struct net_device *dev)": "netdev_lower_get_first_private_rcu - Get the first ->private from the         lower neighbour list, RCU         variant   @dev: device     Gets the first netdev_adjacent->private from the dev's lower neighbour   list. The caller must hold RCU read lock. ", "struct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev)": "netdev_master_upper_dev_get_rcu - Get master upper device   @dev: device     Find a master upper device and return pointer to it or NULL in case   it's not there. The caller must hold the RCU read lock. ", "if (__netdev_has_upper_dev(upper_dev, dev))return -EBUSY;if ((dev->lower_level + upper_dev->upper_level) > MAX_NEST_DEV)return -EMLINK;if (!master) ": "netdev_upper_dev_link(struct net_device  dev,   struct net_device  upper_dev, bool master,   void  upper_priv, void  upper_info,   struct netdev_nested_priv  priv,   struct netlink_ext_ack  extack){struct netdev_notifier_changeupper_info changeupper_info = {.info = {.dev = dev,.extack = extack,},.upper_dev = upper_dev,.master = master,.linking = true,.upper_info = upper_info,};struct net_device  master_dev;int ret = 0;ASSERT_RTNL();if (dev == upper_dev)return -EBUSY;  To prevent loops, check if dev is not upper device to upper_dev. ", "int netdev_master_upper_dev_link(struct net_device *dev, struct net_device *upper_dev, void *upper_priv, void *upper_info, struct netlink_ext_ack *extack)": "netdev_master_upper_dev_link - Add a master link to the upper device   @dev: device   @upper_dev: new upper device   @upper_priv: upper device private   @upper_info: upper info to be passed down via notifier   @extack: netlink extended ack     Adds a link to device which is upper to this one. In this case, only   one master upper device can be linked, although other non-master devices   might be linked as well. The caller must hold the RTNL lock.   On a failure a negative errno code is returned. On success the reference   counts are adjusted and the function returns zero. ", "void netdev_upper_dev_unlink(struct net_device *dev,     struct net_device *upper_dev)": "netdev_upper_dev_unlink(struct net_device  dev,      struct net_device  upper_dev,      struct netdev_nested_priv  priv){struct netdev_notifier_changeupper_info changeupper_info = {.info = {.dev = dev,},.upper_dev = upper_dev,.linking = false,};ASSERT_RTNL();changeupper_info.master = netdev_master_upper_dev_get(dev) == upper_dev;call_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER,      &changeupper_info.info);__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);call_netdevice_notifiers_info(NETDEV_CHANGEUPPER,      &changeupper_info.info);__netdev_update_upper_level(dev, NULL);__netdev_walk_all_lower_dev(dev, __netdev_update_upper_level, NULL);__netdev_update_lower_level(upper_dev, priv);__netdev_walk_all_upper_dev(upper_dev, __netdev_update_lower_level,    priv);}     netdev_upper_dev_unlink - Removes a link to upper device   @dev: device   @upper_dev: new upper device     Removes a link to device which is upper to this one. The caller must hold   the RTNL lock. ", "void netdev_bonding_info_change(struct net_device *dev,struct netdev_bonding_info *bonding_info)": "netdev_bonding_info_change - Dispatch event about slave change   @dev: device   @bonding_info: info to dispatch     Send NETDEV_BONDING_INFO to netdev notifiers with info.   The caller must hold the RTNL lock. ", "netdev_hw_stats64_add(stats, &report_delta.stats);if (p_stats)*p_stats = *stats;*p_used = report_delta.used;return notifier_to_errno(rc);}int netdev_offload_xstats_get(struct net_device *dev,      enum netdev_offload_xstats_type type,      struct rtnl_hw_stats64 *p_stats, bool *p_used,      struct netlink_ext_ack *extack)": "netdev_offload_xstats_get_ptr(const struct net_device  dev,      enum netdev_offload_xstats_type type){switch (type) {case NETDEV_OFFLOAD_XSTATS_TYPE_L3:return dev->offload_xstats_l3;}WARN_ON(1);return NULL;}bool netdev_offload_xstats_enabled(const struct net_device  dev,   enum netdev_offload_xstats_type type){ASSERT_RTNL();return netdev_offload_xstats_get_ptr(dev, type);}EXPORT_SYMBOL(netdev_offload_xstats_enabled);struct netdev_notifier_offload_xstats_ru {bool used;};struct netdev_notifier_offload_xstats_rd {struct rtnl_hw_stats64 stats;bool used;};static void netdev_hw_stats64_add(struct rtnl_hw_stats64  dest,  const struct rtnl_hw_stats64  src){dest->rx_packets  += src->rx_packets;dest->tx_packets  += src->tx_packets;dest->rx_bytes  += src->rx_bytes;dest->tx_bytes  += src->tx_bytes;dest->rx_errors  += src->rx_errors;dest->tx_errors  += src->tx_errors;dest->rx_dropped  += src->rx_dropped;dest->tx_dropped  += src->tx_dropped;dest->multicast  += src->multicast;}static int netdev_offload_xstats_get_used(struct net_device  dev,  enum netdev_offload_xstats_type type,  bool  p_used,  struct netlink_ext_ack  extack){struct netdev_notifier_offload_xstats_ru report_used = {};struct netdev_notifier_offload_xstats_info info = {.info.dev = dev,.info.extack = extack,.type = type,.report_used = &report_used,};int rc;WARN_ON(!netdev_offload_xstats_enabled(dev, type));rc = call_netdevice_notifiers_info(NETDEV_OFFLOAD_XSTATS_REPORT_USED,   &info.info); p_used = report_used.used;return notifier_to_errno(rc);}static int netdev_offload_xstats_get_stats(struct net_device  dev,   enum netdev_offload_xstats_type type,   struct rtnl_hw_stats64  p_stats,   bool  p_used,   struct netlink_ext_ack  extack){struct netdev_notifier_offload_xstats_rd report_delta = {};struct netdev_notifier_offload_xstats_info info = {.info.dev = dev,.info.extack = extack,.type = type,.report_delta = &report_delta,};struct rtnl_hw_stats64  stats;int rc;stats = netdev_offload_xstats_get_ptr(dev, type);if (WARN_ON(!stats))return -EINVAL;rc = call_netdevice_notifiers_info(NETDEV_OFFLOAD_XSTATS_REPORT_DELTA,   &info.info);  Cache whatever we got, even if there was an error, otherwise the   successful stats retrievals would get lost. ", "struct net_device *netdev_get_xmit_slave(struct net_device *dev, struct sk_buff *skb, bool all_slaves)": "netdev_get_xmit_slave - Get the xmit slave of master device   @dev: device   @skb: The packet   @all_slaves: assume all the slaves are active     The reference counters are not incremented so the caller must be   careful with locks. The caller must hold RCU lock.   %NULL is returned if no slave is found. ", "struct net_device *netdev_sk_get_lowest_dev(struct net_device *dev,    struct sock *sk)": "netdev_sk_get_lowest_dev - Get the lowest device in chain given device and socket   @dev: device   @sk: the socket     %NULL is returned if no lower device is found. ", "void netdev_lower_state_changed(struct net_device *lower_dev,void *lower_state_info)": "netdev_lower_state_changed - Dispatch event about lower device state change   @lower_dev: device   @lower_state_info: state to dispatch     Send NETDEV_CHANGELOWERSTATE to netdev notifiers with info.   The caller must hold the RTNL lock. ", "if (inc < 0)dev->flags &= ~IFF_PROMISC;else ": "dev_set_promiscuity(struct net_device  dev, int inc, bool notify){unsigned int old_flags = dev->flags;kuid_t uid;kgid_t gid;ASSERT_RTNL();dev->flags |= IFF_PROMISC;dev->promiscuity += inc;if (dev->promiscuity == 0) {    Avoid overflow.   If inc causes overflow, untouch promisc and return error. ", "if (inc < 0)dev->flags &= ~IFF_ALLMULTI;else ": "dev_set_allmulti(struct net_device  dev, int inc, bool notify){unsigned int old_flags = dev->flags, old_gflags = dev->gflags;ASSERT_RTNL();dev->flags |= IFF_ALLMULTI;dev->allmulti += inc;if (dev->allmulti == 0) {    Avoid overflow.   If inc causes overflow, untouch allmulti and return error. ", "unsigned int dev_get_flags(const struct net_device *dev)": "dev_get_flags - get flags reported to userspace  @dev: device    Get the combination of flag bits exported through APIs to userspace. ", "dev->flags = (flags & (IFF_DEBUG | IFF_NOTRAILERS | IFF_NOARP |       IFF_DYNAMIC | IFF_MULTICAST | IFF_PORTSEL |       IFF_AUTOMEDIA)) |     (dev->flags & (IFF_UP | IFF_VOLATILE | IFF_PROMISC |    IFF_ALLMULTI));/* *Load in the correct multicast list now the flags have changed. ": "dev_change_flags(struct net_device  dev, unsigned int flags,       struct netlink_ext_ack  extack){unsigned int old_flags = dev->flags;int ret;ASSERT_RTNL();   Set the flags on our device. ", "WRITE_ONCE(dev->mtu, new_mtu);return 0;}EXPORT_SYMBOL(__dev_set_mtu": "__dev_set_mtu(struct net_device  dev, int new_mtu){const struct net_device_ops  ops = dev->netdev_ops;if (ops->ndo_change_mtu)return ops->ndo_change_mtu(dev, new_mtu);  Pairs with all the lockless reads of dev->mtu in the stack ", "WRITE_ONCE(dev->mtu, new_mtu);return 0;}EXPORT_SYMBOL(__dev_set_mtu);int dev_validate_mtu(struct net_device *dev, int new_mtu,     struct netlink_ext_ack *extack)": "dev_set_mtu(struct net_device  dev, int new_mtu){const struct net_device_ops  ops = dev->netdev_ops;if (ops->ndo_change_mtu)return ops->ndo_change_mtu(dev, new_mtu);  Pairs with all the lockless reads of dev->mtu in the stack ", "int dev_pre_changeaddr_notify(struct net_device *dev, const char *addr,      struct netlink_ext_ack *extack)": "dev_pre_changeaddr_notify - Call NETDEV_PRE_CHANGEADDR.  @dev: device  @addr: new address  @extack: netlink extended ack ", "int dev_set_mac_address(struct net_device *dev, struct sockaddr *sa,struct netlink_ext_ack *extack)": "dev_set_mac_address - Change Media Access Control Address  @dev: device  @sa: new address  @extack: netlink extended ack    Change the hardware (MAC) address of the device ", "int dev_get_port_parent_id(struct net_device *dev,   struct netdev_phys_item_id *ppid,   bool recurse)": "dev_get_port_parent_id - Get the device's port parent identifier  @dev: network device  @ppid: pointer to a storage for the port's parent identifier  @recurse: allowdisallow recursion to lower devices    Get the devices's port parent identifier ", "bool netdev_port_same_parent_id(struct net_device *a, struct net_device *b)": "netdev_port_same_parent_id - Indicate if two network devices have  the same port parent identifier  @a: first network device  @b: second network device ", "static void dev_disable_gro_hw(struct net_device *dev)": "netdev_update_features(dev);if (unlikely(dev->features & NETIF_F_LRO))netdev_WARN(dev, \"failed to disable LRO!\\n\");netdev_for_each_lower_dev(dev, lower_dev, iter)dev_disable_lro(lower_dev);}EXPORT_SYMBOL(dev_disable_lro);    dev_disable_gro_hw - disable HW Generic Receive Offload on a device  @dev: device    Disable HW Generic Receive Offload (GRO_HW) on a net device.  Must be  called under RTNL.  This is needed if Generic XDP is installed on  the device. ", "void netdev_change_features(struct net_device *dev)": "netdev_change_features - recalculate device features  @dev: the device to check    Recalculate dev->features set and send notifications even  if they have not changed. Should be called instead of  netdev_update_features() if also dev->vlan_features might  have changed to allow the changes to be propagated to stacked  VLAN devices. ", "void netif_stacked_transfer_operstate(const struct net_device *rootdev,struct net_device *dev)": "netif_stacked_transfer_operstate -transfer operstate  @rootdev: the root or lower level device to transfer state from  @dev: the device to transfer operstate to    Transfer operational state from root to device. This is normally  called when a stacking relationship exists between the root  device and the device(a leaf device). ", "memset((char *)stats64 + n * sizeof(u64), 0,       sizeof(*stats64) - n * sizeof(u64));}EXPORT_SYMBOL(netdev_stats_to_stats64": "netdev_stats_to_stats64(struct rtnl_link_stats64  stats64,     const struct net_device_stats  netdev_stats){size_t i, n = sizeof( netdev_stats)  sizeof(atomic_long_t);const atomic_long_t  src = (atomic_long_t  )netdev_stats;u64  dst = (u64  )stats64;BUILD_BUG_ON(n > sizeof( stats64)  sizeof(u64));for (i = 0; i < n; i++)dst[i] = (unsigned long)atomic_long_read(&src[i]);  zero out counters that only exist in rtnl_link_stats64 ", "return READ_ONCE(dev->core_stats);}EXPORT_SYMBOL(netdev_core_stats_alloc": "netdev_core_stats_alloc(struct net_device  dev){struct net_device_core_stats __percpu  p;p = alloc_percpu_gfp(struct net_device_core_stats,     GFP_ATOMIC | __GFP_NOWARN);if (p && cmpxchg(&dev->core_stats, NULL, p))free_percpu(p);  This READ_ONCE() pairs with the cmpxchg() above ", "struct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,struct rtnl_link_stats64 *storage)": "dev_get_stats- get network device statistics  @dev: device to get statistics from  @storage: place to store stats    Get network statistics from device. Return @storage.  The device driver may provide its own method by setting  dev->netdev_ops->get_stats64 or dev->netdev_ops->get_stats;  otherwise the internal statistics structure is used. ", "struct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,unsigned char name_assign_type,void (*setup)(struct net_device *),unsigned int txqs, unsigned int rxqs)": "alloc_netdev_mqs - allocate network device   @sizeof_priv: size of private data to allocate space for   @name: device name format string   @name_assign_type: origin of device name   @setup: callback to initialize device   @txqs: the number of TX subqueues to allocate   @rxqs: the number of RX subqueues to allocate     Allocates a struct net_device with private data area for driver use   and performs basic initialization.  Also allocates subqueue structs   for each queue on the device. ", "dev->needs_free_netdev = false;unregister_netdevice_queue(dev, NULL);goto out;}/* *Prevent userspace races by waiting until the network *device is fully setup before sending notifications. ": "free_netdev() on failure ", "/** *dev_get_iflink- get 'iflink' value of a interface *@dev: targeted interface * *Indicates the ifindex the interface is linked to. *Physical interfaces have the same 'ifindex' and 'iflink' values. ": "synchronize_net();}EXPORT_SYMBOL(dev_remove_pack);                                                                                       Device Interface Subroutines                                                                                 ", "if (!dev->rtnl_link_ops ||    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)rtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL, 0, NULL);out:return ret;err_uninit_notify:call_netdevice_notifiers(NETDEV_PRE_UNINIT, dev);err_uninit:if (dev->netdev_ops->ndo_uninit)dev->netdev_ops->ndo_uninit(dev);if (dev->priv_destructor)dev->priv_destructor(dev);err_free_name:netdev_name_node_free(dev->name_node);goto out;}EXPORT_SYMBOL(register_netdevice);/** *init_dummy_netdev- init a dummy network device for NAPI *@dev: device to init * *This takes a network device structure and initialize the minimum *amount of fields so it can be used to schedule NAPI polls without *registering a full blown interface. This is to be used by drivers *that need to tie several hardware interfaces to a single NAPI *poll scheduler due to HW limitations. ": "unregister_netdevice_queue(dev, NULL);goto out;}   Prevent userspace races by waiting until the network  device is fully setup before sending notifications. ", "for_each_cpu(cpu, &flush_cpus)flush_work(per_cpu_ptr(&flush_works, cpu));cpus_read_unlock();}static void net_rps_send_ipi(struct softnet_data *remsd)": "unregister_netdevice_many() will take care of   them ", "netdev_features_t netdev_increment_features(netdev_features_t all,netdev_features_t one, netdev_features_t mask)": "netdev_increment_features - increment feature set by one  @all: current feature set  @one: new feature set  @mask: mask feature set    Computes a new feature set after adding a device with feature set  @one to the master device with current feature set @all.  Will not  enable anything that is off in @mask. Returns the new feature set. ", "defer = kmalloc(sizeof(*defer), GFP_KERNEL | __GFP_NOWARN);if (likely(defer)) ": "gro_cells_destroy(struct gro_cells  gcells){struct percpu_free_defer  defer;int i;if (!gcells->cells)return;for_each_possible_cpu(i) {struct gro_cell  cell = per_cpu_ptr(gcells->cells, i);napi_disable(&cell->napi);__netif_napi_del(&cell->napi);__skb_queue_purge(&cell->napi_skbs);}  We need to observe an rcu grace period before freeing ->cells,   because netpoll could access dev->napi_list under rcu protection.   Try hard using call_rcu() instead of synchronize_rcu(),   because we might be called from cleanup_net(), and we   definitely do not want to block this critical task. ", "if (!ni || down_trylock(&ni->dev_lock))return;/* Some drivers will take the same locks in poll and xmit, * we can't poll if local CPU is already in xmit. ": "netpoll_poll_dev(struct net_device  dev){struct netpoll_info  ni = rcu_dereference_bh(dev->npinfo);const struct net_device_ops  ops;  Don't do any rx activity if the dev_lock mutex is held   the dev_openclose paths use this to block netpoll activity   while changing device state ", "struct netpoll_info *npinfo;lockdep_assert_irqs_disabled();dev = np->dev;npinfo = rcu_dereference_bh(dev->npinfo);if (!npinfo || !netif_running(dev) || !netif_device_present(dev)) ": "netpoll_send_skb(struct netpoll  np, struct sk_buff  skb){netdev_tx_t status = NETDEV_TX_BUSY;struct net_device  dev;unsigned long tries;  It is up to the caller to keep npinfo alive. ", "*(unsigned char *)ip6h = 0x60;ip6h->flow_lbl[0] = 0;ip6h->flow_lbl[1] = 0;ip6h->flow_lbl[2] = 0;ip6h->payload_len = htons(sizeof(struct udphdr) + len);ip6h->nexthdr = IPPROTO_UDP;ip6h->hop_limit = 32;ip6h->saddr = np->local_ip.in6;ip6h->daddr = np->remote_ip.in6;eth = skb_push(skb, ETH_HLEN);skb_reset_mac_header(skb);skb->protocol = eth->h_proto = htons(ETH_P_IPV6);} else ": "netpoll_send_udp(struct netpoll  np, const char  msg, int len){int total_len, ip_len, udp_len;struct sk_buff  skb;struct udphdr  udph;struct iphdr  iph;struct ethhdr  eth;static atomic_t ip_ident;struct ipv6hdr  ip6h;if (!IS_ENABLED(CONFIG_PREEMPT_RT))WARN_ON_ONCE(!irqs_disabled());udp_len = len + sizeof( udph);if (np->ipv6)ip_len = udp_len + sizeof( ip6h);elseip_len = udp_len + sizeof( iph);total_len = ip_len + LL_RESERVED_SPACE(np->dev);skb = find_skb(np, total_len + np->dev->needed_tailroom,       total_len - len);if (!skb)return;skb_copy_to_linear_data(skb, msg, len);skb_put(skb, len);skb_push(skb, sizeof( udph));skb_reset_transport_header(skb);udph = udp_hdr(skb);udph->source = htons(np->local_port);udph->dest = htons(np->remote_port);udph->len = htons(udp_len);if (np->ipv6) {udph->check = 0;udph->check = csum_ipv6_magic(&np->local_ip.in6,      &np->remote_ip.in6,      udp_len, IPPROTO_UDP,      csum_partial(udph, udp_len, 0));if (udph->check == 0)udph->check = CSUM_MANGLED_0;skb_push(skb, sizeof( ip6h));skb_reset_network_header(skb);ip6h = ipv6_hdr(skb);  ip6h->version = 6; ip6h->priority = 0; ", "if ((delim = strchr(cur, ',')) == NULL)goto parse_failed;*delim = 0;strscpy(np->dev_name, cur, sizeof(np->dev_name));cur = delim;}cur++;if (*cur != '@') ": "netpoll_parse_options(struct netpoll  np, char  opt){char  cur=opt,  delim;int ipv6;bool ipversion_set = false;if ( cur != '@') {if ((delim = strchr(cur, '@')) == NULL)goto parse_failed; delim = 0;if (kstrtou16(cur, 10, &np->local_port))goto parse_failed;cur = delim;}cur++;if ( cur != '') {ipversion_set = true;if ((delim = strchr(cur, '')) == NULL)goto parse_failed; delim = 0;ipv6 = netpoll_parse_ip_addr(cur, &np->local_ip);if (ipv6 < 0)goto parse_failed;elsenp->ipv6 = (bool)ipv6;cur = delim;}cur++;if ( cur != ',') {  parse out dev name ", "rcu_assign_pointer(ndev->npinfo, npinfo);return 0;free_npinfo:kfree(npinfo);out:return err;}EXPORT_SYMBOL_GPL(__netpoll_setup);int netpoll_setup(struct netpoll *np)": "netpoll_setup(struct netpoll  np, struct net_device  ndev){struct netpoll_info  npinfo;const struct net_device_ops  ops;int err;np->dev = ndev;strscpy(np->dev_name, ndev->name, IFNAMSIZ);if (ndev->priv_flags & IFF_DISABLE_NETPOLL) {np_err(np, \"%s doesn't support polling, aborting\\n\",       np->dev_name);err = -ENOTSUPP;goto out;}if (!ndev->npinfo) {npinfo = kmalloc(sizeof( npinfo), GFP_KERNEL);if (!npinfo) {err = -ENOMEM;goto out;}sema_init(&npinfo->dev_lock, 1);skb_queue_head_init(&npinfo->txq);INIT_DELAYED_WORK(&npinfo->tx_work, queue_process);refcount_set(&npinfo->refcnt, 1);ops = np->dev->netdev_ops;if (ops->ndo_netpoll_setup) {err = ops->ndo_netpoll_setup(ndev, npinfo);if (err)goto free_npinfo;}} else {npinfo = rtnl_dereference(ndev->npinfo);refcount_inc(&npinfo->refcnt);}npinfo->netpoll = np;  last thing to do is link it to the net device structure ", "synchronize_rcu();__netpoll_cleanup(np);kfree(np);}EXPORT_SYMBOL_GPL(__netpoll_free);void netpoll_cleanup(struct netpoll *np)": "netpoll_cleanup(struct netpoll  np){struct netpoll_info  npinfo;npinfo = rtnl_dereference(np->dev->npinfo);if (!npinfo)return;synchronize_srcu(&netpoll_srcu);if (refcount_dec_and_test(&npinfo->refcnt)) {const struct net_device_ops  ops;ops = np->dev->netdev_ops;if (ops->ndo_netpoll_cleanup)ops->ndo_netpoll_cleanup(np->dev);RCU_INIT_POINTER(np->dev->npinfo, NULL);call_rcu(&npinfo->rcu, rcu_cleanup_netpoll_info);} elseRCU_INIT_POINTER(np->dev->npinfo, NULL);}EXPORT_SYMBOL_GPL(__netpoll_cleanup);void __netpoll_free(struct netpoll  np){ASSERT_RTNL();  Wait for transmitting packets to finish before freeing. ", "spin_lock_bh(&reuseport_lock);/* Allocation attempts can occur concurrently via the setsockopt path * and the bind/hash path.  Nothing to do when we lose the race. ": "reuseport_alloc(unsigned int max_socks){unsigned int size = sizeof(struct sock_reuseport) +      sizeof(struct sock  )   max_socks;struct sock_reuseport  reuse = kzalloc(size, GFP_ATOMIC);if (!reuse)return NULL;reuse->max_socks = max_socks;RCU_INIT_POINTER(reuse->prog, NULL);return reuse;}int reuseport_alloc(struct sock  sk, bool bind_inany){struct sock_reuseport  reuse;int id, ret = 0;  bh lock used since this function call may precede hlist lock in   soft irq of receive path or setsockopt from process context ", "smp_wmb();reuse->num_socks++;reuseport_get_incoming_cpu(sk, reuse);}static bool __reuseport_detach_sock(struct sock *sk,    struct sock_reuseport *reuse)": "reuseport_add_sock(struct sock  sk, struct sock_reuseport  reuse){reuse->socks[reuse->num_socks] = sk;  paired with smp_rmb() in reuseport_(select|migrate)_sock() ", "WRITE_ONCE(reuse->num_closed_socks, reuse->num_closed_socks + 1);reuseport_get_incoming_cpu(sk, reuse);}static bool __reuseport_detach_closed_sock(struct sock *sk,   struct sock_reuseport *reuse)": "reuseport_detach_sock(struct sock  sk,    struct sock_reuseport  reuse){int i = reuseport_sock_index(sk, reuse, false);if (i == -1)return false;reuse->socks[i] = reuse->socks[reuse->num_socks - 1];reuse->num_socks--;reuseport_put_incoming_cpu(sk, reuse);return true;}static void __reuseport_add_closed_sock(struct sock  sk,struct sock_reuseport  reuse){reuse->socks[reuse->max_socks - reuse->num_closed_socks - 1] = sk;  paired with READ_ONCE() in inet_csk_bind_conflict() ", "bpf_sk_reuseport_detach(sk);__reuseport_detach_sock(sk, reuse);__reuseport_add_closed_sock(sk, reuse);spin_unlock_bh(&reuseport_lock);return;}spin_unlock_bh(&reuseport_lock);}/* Not capable to do migration, detach immediately ": "reuseport_stop_listen_sock(struct sock  sk){if (sk->sk_protocol == IPPROTO_TCP) {struct sock_reuseport  reuse;struct bpf_prog  prog;spin_lock_bh(&reuseport_lock);reuse = rcu_dereference_protected(sk->sk_reuseport_cb,  lockdep_is_held(&reuseport_lock));prog = rcu_dereference_protected(reuse->prog, lockdep_is_held(&reuseport_lock));if (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_migrate_req) ||    (prog && prog->expected_attach_type == BPF_SK_REUSEPORT_SELECT_OR_MIGRATE)) {  Migration capable, move sk from the listening section   to the closed section. ", "WRITE_ONCE(reuse->incoming_cpu, reuse->incoming_cpu + 1);}static void __reuseport_put_incoming_cpu(struct sock_reuseport *reuse)": "reuseport_select_sock_by_hash(). ", "struct sock *reuseport_migrate_sock(struct sock *sk,    struct sock *migrating_sk,    struct sk_buff *skb)": "reuseport_migrate_sock - Select a socket from an SO_REUSEPORT group.    @sk: close()ed or shutdown()ed socket in the group.    @migrating_sk: ESTABLISHEDSYN_RECV full socket in the accept queue or      NEW_SYN_RECV request socket during 3WHS.    @skb: skb to run through BPF filter.    Returns a socket (with sk_refcnt +1) that should accept the child socket    (or NULL on error). ", "return -EINVAL;}spin_lock_bh(&reuseport_lock);reuse = rcu_dereference_protected(sk->sk_reuseport_cb,  lockdep_is_held(&reuseport_lock));old_prog = rcu_dereference_protected(reuse->prog,     lockdep_is_held(&reuseport_lock));rcu_assign_pointer(reuse->prog, prog);spin_unlock_bh(&reuseport_lock);sk_reuseport_prog_free(old_prog);return 0;}EXPORT_SYMBOL(reuseport_attach_prog": "reuseport_attach_prog(struct sock  sk, struct bpf_prog  prog){struct sock_reuseport  reuse;struct bpf_prog  old_prog;if (sk_unhashed(sk)) {int err;if (!sk->sk_reuseport)return -EINVAL;err = reuseport_alloc(sk, false);if (err)return err;} else if (!rcu_access_pointer(sk->sk_reuseport_cb)) {  The socket wasn't bound with SO_REUSEPORT ", "if (!reuse) ": "reuseport_detach_prog(struct sock  sk){struct sock_reuseport  reuse;struct bpf_prog  old_prog;old_prog = NULL;spin_lock_bh(&reuseport_lock);reuse = rcu_dereference_protected(sk->sk_reuseport_cb,  lockdep_is_held(&reuseport_lock));  reuse must be checked after acquiring the reuseport_lock   because reuseport_grow() can detach a closed sk. ", "info = list_first_entry_or_null(&context->records_list,struct tls_record_info, list);if (!info)return NULL;/* send the start_marker record if seq number is before the * tls offload start marker sequence number. This record is * required to handle TCP packets which are before TLS offload * started. *  And if it's not start marker, look if this seq number * belongs to the list. ": "tls_get_record(struct tls_offload_context_tx  context,       u32 seq, u64  p_record_sn){u64 record_sn = context->hint_record_sn;struct tls_record_info  info,  last;info = context->retransmit_hint;if (!info ||    before(seq, info->end_seq - info->len)) {  if retransmit_hint is irrelevant start   from the beginning of the list ", "int dns_query(struct net *net,      const char *type, const char *name, size_t namelen,      const char *options, char **_result, time64_t *_expiry,      bool invalidate)": "dns_query - Query the DNS   @net: The network namespace to operate in.   @type: Query type (or NULL for straight host->IP lookup)   @name: Name to look up   @namelen: Length of name   @options: Request options (or NULL if no options)   @_result: Where to place the returned data (or NULL)   @_expiry: Where to store the result expiry time (or NULL)   @invalidate: Always invalidate the key after use     The data will be returned in the pointer at  result, if provided, and the   caller is responsible for freeing it.     The description should be of the form \"[<query_type>:]<domain_name>\", and   the options need to be appropriate for the query type requested.  If no   query_type is given, then the query is a straight hostname to IP address   lookup.     The DNS resolution lookup is performed by upcalling to userspace by way of   requesting a key of type dns_resolver.     Returns the size of the result on success, -ve error code otherwise. ", "struct sock *sk = pn_find_sock_by_res(net, res);if (sk)": "pn_skb_send(struct sock  sk, struct sk_buff  skb,const struct sockaddr_pn  target){struct net  net = sock_net(sk);struct net_device  dev;struct pn_sock  pn = pn_sk(sk);int err;u16 src, dst;u8 daddr, saddr, res;src = pn->sobject;if (target != NULL) {dst = pn_sockaddr_get_object(target);res = pn_sockaddr_get_resource(target);} else {dst = pn->dobject;res = pn->resource;}daddr = pn_addr(dst);err = -EHOSTUNREACH;if (sk->sk_bound_dev_if)dev = dev_get_by_index(net, sk->sk_bound_dev_if);else if (phonet_address_lookup(net, daddr) == 0) {dev = phonet_device_get(net);skb->pkt_type = PACKET_LOOPBACK;} else if (dst == 0) {  Resource routing (small race until phonet_rcv()) ", "int port, pmin, pmax;phonet_get_local_port_range(&pmin, &pmax);for (port = pmin; port <= pmax; port++) ": "pn_sock_get_port(struct sock  sk, unsigned short sport){static int port_cur;struct net  net = sock_net(sk);struct pn_sock  pn = pn_sk(sk);struct sockaddr_pn try_sa;struct sock  tmpsk;memset(&try_sa, 0, sizeof(struct sockaddr_pn));try_sa.spn_family = AF_PHONET;WARN_ON(!mutex_is_locked(&port_mutex));if (!sport) {  search free port ", "int p9_error_init(void)": "p9_error_init - preload mappings into hash list   ", "int p9_errstr2errno(char *errstr, int len)": "p9_errstr2errno - convert error string to error number   @errstr: error string   @len: length of error string   ", "break;}if (clnt->trans_mod->show_options)return clnt->trans_mod->show_options(m, clnt);return 0;}EXPORT_SYMBOL(p9_show_client_options": "p9_show_client_options(struct seq_file  m, struct p9_client  clnt){if (clnt->msize != DEFAULT_MSIZE)seq_printf(m, \",msize=%u\", clnt->msize);seq_printf(m, \",trans=%s\", clnt->trans_mod->name);switch (clnt->proto_version) {case p9_proto_legacy:seq_puts(m, \",noextend\");break;case p9_proto_2000u:seq_puts(m, \",version=9p2000.u\");break;case p9_proto_2000L:  Default ", "if (unlikely(!fc->sdata))return;if (fc->cache)kmem_cache_free(fc->cache, fc->sdata);elsekfree(fc->sdata);}EXPORT_SYMBOL(p9_fcall_fini": "p9_fcall_fini(struct p9_fcall  fc){  sdata can be NULL for interrupted requests in trans_rdma,   and kmem_cache_free does not do NULL-check for us ", "refcount_set(&req->refcount, 0);init_waitqueue_head(&req->wq);INIT_LIST_HEAD(&req->req_list);idr_preload(GFP_NOFS);spin_lock_irq(&c->lock);if (type == P9_TVERSION)tag = idr_alloc(&c->reqs, req, P9_NOTAG, P9_NOTAG + 1,GFP_NOWAIT);elsetag = idr_alloc(&c->reqs, req, 0, P9_NOTAG, GFP_NOWAIT);req->tc.tag = tag;spin_unlock_irq(&c->lock);idr_preload_end();if (tag < 0)goto free;/* Init ref to two because in the general case there is one ref * that is put asynchronously by a writer thread, one ref * temporarily given by p9_tag_lookup and put by p9_client_cb * in the recv thread, and one ref put by p9_req_put in the * main thread. The only exception is virtio that does not use * p9_tag_lookup but does not have a writer thread either * (the write happens synchronously in the request/zc_request * callback), so p9_client_cb eats the second ref there * as the pointer is duplicated directly by virtqueue_add_sgs() ": "p9_tag_lookup does not accept a request that is not fully   initialized. refcount_set to 2 below will mark request ready. ", "refcount_set(&req->refcount, 2);return req;free:p9_fcall_fini(&req->tc);p9_fcall_fini(&req->rc);free_req:kmem_cache_free(p9_req_cache, req);return ERR_PTR(-ENOMEM);}/** * p9_tag_lookup - Look up a request by tag. * @c: Client session. * @tag: Transaction ID. * * Context: Any context. * Return: A request, or %NULL if there is no request with that tag. ": "p9_client_cb   in the recv thread, and one ref put by p9_req_put in the   main thread. The only exception is virtio that does not use   p9_tag_lookup but does not have a writer thread either   (the write happens synchronously in the requestzc_request   callback), so p9_client_cb eats the second ref there   as the pointer is duplicated directly by virtqueue_add_sgs() ", "intp9_parse_header(struct p9_fcall *pdu, int32_t *size, int8_t *type,int16_t *tag, int rewind)": "p9_parse_header - parse header arguments out of a packet   @pdu: packet to parse   @size: size of packet   @type: type of request   @tag: tag of packet   @rewind: set if we need to rewind offset afterwards ", "clnt->fcall_cache =kmem_cache_create_usercopy(\"9p-fcall-cache\", clnt->msize,   0, 0, P9_HDRSZ + 4,   clnt->msize - (P9_HDRSZ + 4),   NULL);return clnt;close_trans:clnt->trans_mod->close(clnt);put_trans:v9fs_put_trans(clnt->trans_mod);free_client:kfree(clnt);return ERR_PTR(err);}EXPORT_SYMBOL(p9_client_create": "p9_client_create(const char  dev_name, char  options){int err;struct p9_client  clnt;char  client_id;clnt = kmalloc(sizeof( clnt), GFP_KERNEL);if (!clnt)return ERR_PTR(-ENOMEM);clnt->trans_mod = NULL;clnt->trans = NULL;clnt->fcall_cache = NULL;client_id = utsname()->nodename;memcpy(clnt->name, client_id, strlen(client_id) + 1);spin_lock_init(&clnt->lock);idr_init(&clnt->fids);idr_init(&clnt->reqs);err = parse_opts(options, clnt);if (err < 0)goto free_client;if (!clnt->trans_mod)clnt->trans_mod = v9fs_get_default_trans();if (!clnt->trans_mod) {err = -EPROTONOSUPPORT;p9_debug(P9_DEBUG_ERROR, \"No transport defined or default transport\\n\");goto free_client;}p9_debug(P9_DEBUG_MUX, \"clnt %p trans %p msize %d protocol %d\\n\", clnt, clnt->trans_mod, clnt->msize, clnt->proto_version);err = clnt->trans_mod->create(clnt, dev_name, options);if (err)goto put_trans;if (clnt->msize > clnt->trans_mod->maxsize) {clnt->msize = clnt->trans_mod->maxsize;pr_info(\"Limiting 'msize' to %d as this is the maximum \"\"supported by transport %s\\n\",clnt->msize, clnt->trans_mod->name);}if (clnt->msize < 4096) {p9_debug(P9_DEBUG_ERROR, \"Please specify a msize of at least 4k\\n\");err = -EINVAL;goto close_trans;}err = p9_client_version(clnt);if (err)goto close_trans;  P9_HDRSZ + 4 is the smallest packet header we can have that is   followed by data accessed from userspace by read ", "if (err == -ERESTARTSYS) ": "p9_client_clunk(struct p9_fid  fid){int err = 0;struct p9_client  clnt;struct p9_req_t  req;int retries = 0;again:p9_debug(P9_DEBUG_9P, \">>> TCLUNK fid %d (try %d)\\n\", fid->fid, retries);clnt = fid->clnt;req = p9_client_rpc(clnt, P9_TCLUNK, \"d\", fid->fid);if (IS_ERR(req)) {err = PTR_ERR(req);goto error;}p9_debug(P9_DEBUG_9P, \"<<< RCLUNK fid %d\\n\", fid->fid);p9_req_put(clnt, req);error:  Fid is not valid even after a failed clunk   If interrupted, retry once then give up and   leak fid until umount. ", "if (clnt->trans_mod->zc_request && rsize > 1024) ": "p9_client_readdir(struct p9_fid  fid, char  data, u32 count, u64 offset){int err, rsize, non_zc = 0;struct p9_client  clnt;struct p9_req_t  req;char  dataptr;struct kvec kv = {.iov_base = data, .iov_len = count};struct iov_iter to;iov_iter_kvec(&to, ITER_DEST, &kv, 1, count);p9_debug(P9_DEBUG_9P, \">>> TREADDIR fid %d offset %llu count %d\\n\", fid->fid, offset, count);clnt = fid->clnt;rsize = fid->iounit;if (!rsize || rsize > clnt->msize - P9_READDIRHDRSZ)rsize = clnt->msize - P9_READDIRHDRSZ;if (count < rsize)rsize = count;  Don't bother zerocopy for small IO (< 1024) ", "void p9_release_pages(struct page **pages, int nr_pages)": "p9_release_pages - Release pages after the transaction.   @pages: array of pages to be put   @nr_pages: size of array ", "EXPORT_SYMBOL(p9_debug_level": "p9_debug_level;  feature-rific global debug level  ", "void v9fs_register_trans(struct p9_trans_module *m)": "v9fs_register_trans - register a new transport with 9p   @m: structure describing the transport module and entry points   ", "void v9fs_unregister_trans(struct p9_trans_module *m)": "v9fs_unregister_trans - unregister a 9p transport   @m: the transport to remove   ", "struct p9_trans_module *v9fs_get_trans_by_name(const char *s)": "v9fs_get_trans_by_name - get transport with the matching name   @s: string identifying transport   ", "struct p9_trans_module *v9fs_get_default_trans(void)": "v9fs_get_default_trans - get the default transport   ", "h = req->request->front.iov_base;h->monhdr.have_version = 0;h->monhdr.session_mon = cpu_to_le16(-1);h->monhdr.session_mon_tid = 0;h->fsid = monc->monmap->fsid;h->contains_data_pool = (data_pool != CEPH_NOPOOL);h->data_pool = cpu_to_le64(data_pool);send_generic_request(monc, req);mutex_unlock(&monc->mutex);ret = wait_generic_request(req);out:put_generic_request(req);return ret;}EXPORT_SYMBOL(ceph_monc_do_statfs": "ceph_monc_do_statfs(struct ceph_mon_client  monc, u64 data_pool,struct ceph_statfs  buf){struct ceph_mon_generic_request  req;struct ceph_mon_statfs  h;int ret = -ENOMEM;req = alloc_generic_request(monc, GFP_NOFS);if (!req)goto out;req->request = ceph_msg_new(CEPH_MSG_STATFS, sizeof( h), GFP_NOFS,    true);if (!req->request)goto out;req->reply = ceph_msg_new(CEPH_MSG_STATFS_REPLY, 64, GFP_NOFS, true);if (!req->reply)goto out;req->u.st = buf;req->request->hdr.version = cpu_to_le16(2);mutex_lock(&monc->mutex);register_generic_request(req);  fill out request ", "ceph_encode_string(&p, end, what, strlen(what));WARN_ON(p != end);}send_generic_request(monc, req);mutex_unlock(&monc->mutex);return req;err_put_req:put_generic_request(req);return ERR_PTR(-ENOMEM);}/* * Send MMonGetVersion and wait for the reply. * * @what: one of \"mdsmap\", \"osdmap\" or \"monmap\" ": "ceph_monc_get_version(struct ceph_mon_client  monc, const char  what,ceph_monc_callback_t cb, u64 private_data){struct ceph_mon_generic_request  req;req = alloc_generic_request(monc, GFP_NOIO);if (!req)goto err_put_req;req->request = ceph_msg_new(CEPH_MSG_MON_GET_VERSION,    sizeof(u64) + sizeof(u32) + strlen(what),    GFP_NOIO, true);if (!req->request)goto err_put_req;req->reply = ceph_msg_new(CEPH_MSG_MON_GET_VERSION_REPLY, 32, GFP_NOIO,  true);if (!req->reply)goto err_put_req;req->complete_cb = cb;req->private_data = private_data;mutex_lock(&monc->mutex);register_generic_request(req);{void  p = req->request->front.iov_base;void  const end = p + req->request->front_alloc_len;ceph_encode_64(&p, req->tid);   handle ", "ret = do_mon_command(monc,     \"": "ceph_monc_blocklist_add(struct ceph_mon_client  monc,    struct ceph_entity_addr  client_addr){int ret;ret = do_mon_command(monc,     \"{ \\\"prefix\\\": \\\"osd blocklist\\\", \\\\\"blocklistop\\\": \\\"add\\\", \\\\\"addr\\\": \\\"%pISpc%u\\\" }\",     &client_addr->in_addr,     le32_to_cpu(client_addr->nonce));if (ret == -EINVAL) {    The monitor returns EINVAL on an unrecognized command.   Try the legacy command -- it is exactly the same except   for the name. ", "/* authentication ": "ceph_monc_init(struct ceph_mon_client  monc, struct ceph_client  cl){int err;dout(\"init\\n\");memset(monc, 0, sizeof( monc));monc->client = cl;mutex_init(&monc->mutex);err = build_initial_monmap(monc);if (err)goto out;  connection ", "ceph_msgr_flush();ceph_auth_destroy(monc->auth);WARN_ON(!RB_EMPTY_ROOT(&monc->generic_request_tree));ceph_msg_put(monc->m_auth);ceph_msg_put(monc->m_auth_reply);ceph_msg_put(monc->m_subscribe);ceph_msg_put(monc->m_subscribe_ack);kfree(monc->monmap);}EXPORT_SYMBOL(ceph_monc_stop": "ceph_monc_stop(struct ceph_mon_client  monc){dout(\"stop\\n\");cancel_delayed_work_sync(&monc->delayed_work);mutex_lock(&monc->mutex);__close_session(monc);monc->cur_mon = -1;mutex_unlock(&monc->mutex);    flush msgr queue before we destroy ourselves to ensure that:    - any work that references our embedded con is finished.    - any osd_client or other work that may reference an authorizer      finishes before we shut down the auth subsystem. ", "struct sockaddr_in *in4 = (struct sockaddr_in *)&ss;struct sockaddr_in6 *in6 = (struct sockaddr_in6 *)&ss;i = atomic_inc_return(&addr_str_seq) & ADDR_STR_COUNT_MASK;s = addr_str[i];switch (ss.ss_family) ": "ceph_pr_addr(const struct ceph_entity_addr  addr){int i;char  s;struct sockaddr_storage ss = addr->in_addr;   align ", "ceph_con_flag_clear(con, CEPH_CON_F_SOCK_CLOSED);con_sock_state_closed(con);return rc;}static void ceph_con_reset_protocol(struct ceph_connection *con)": "ceph_con_close_socket(struct ceph_connection  con){int rc = 0;dout(\"%s con %p sock %p\\n\", __func__, con, con->sock);if (con->sock) {rc = con->sock->ops->shutdown(con->sock, SHUT_RDWR);sock_release(con->sock);con->sock = NULL;}    Forcibly clear the SOCK_CLOSED flag.  It gets set   independent of the connection mutex, and we could have   received a socket close event before we had the chance to   shut the socket down. ", "mutex_unlock(&con->mutex);queue_con(con);}EXPORT_SYMBOL(ceph_con_open": "ceph_con_open(struct ceph_connection  con,   __u8 entity_type, __u64 entity_num,   struct ceph_entity_addr  addr){mutex_lock(&con->mutex);dout(\"con_open %p %s\\n\", con, ceph_pr_addr(addr));WARN_ON(con->state != CEPH_CON_S_CLOSED);con->state = CEPH_CON_S_PREOPEN;con->peer_name.type = (__u8) entity_type;con->peer_name.num = cpu_to_le64(entity_num);memcpy(&con->peer_addr, addr, sizeof( addr));con->delay = 0;        reset backoff memory ", "struct in_addr *addr4 = &((struct sockaddr_in *)&ss)->sin_addr;struct in6_addr *addr6 = &((struct sockaddr_in6 *)&ss)->sin6_addr;switch (ss.ss_family) ": "ceph_addr_is_blank(const struct ceph_entity_addr  addr){struct sockaddr_storage ss = addr->in_addr;   align ", "msg->hdr.src = con->msgr->inst.name;BUG_ON(msg->front.iov_len != le32_to_cpu(msg->hdr.front_len));msg->needs_out_seq = true;mutex_lock(&con->mutex);if (con->state == CEPH_CON_S_CLOSED) ": "ceph_con_send(struct ceph_connection  con, struct ceph_msg  msg){  set src+dst ", "if (front_len) ": "ceph_msg_new2(int type, int front_len, int max_data_items,       gfp_t flags, bool can_fail){struct ceph_msg  m;m = kmem_cache_zalloc(ceph_msg_cache, flags);if (m == NULL)goto out;m->hdr.type = cpu_to_le16(type);m->hdr.priority = cpu_to_le16(CEPH_MSG_PRIO_DEFAULT);m->hdr.front_len = cpu_to_le32(front_len);INIT_LIST_HEAD(&m->list_head);kref_init(&m->kref);  front ", "static void ceph_msg_free(struct ceph_msg *m)": "ceph_msg_get(msg);}    Free a generically kmalloc'd message. ", "static void ceph_msg_remove(struct ceph_msg *msg)": "ceph_msg_put(con->in_msg);con->in_msg = NULL;}if (con->out_msg) {WARN_ON(con->out_msg->con != con);ceph_msg_put(con->out_msg);con->out_msg = NULL;}if (con->bounce_page) {__free_page(con->bounce_page);con->bounce_page = NULL;}if (ceph_msgr2(from_msgr(con->msgr)))ceph_con_v2_reset_protocol(con);elseceph_con_v1_reset_protocol(con);}    Reset a connection.  Discard all incoming and outgoing messages   and clear  _seq state. ", "*p = struct_end;ret = 0;bad:return ret;}static intceph_decode_entity_addr_legacy(void **p, void *end,       struct ceph_entity_addr *addr)": "ceph_decode_entity_addr_versioned(void   p, void  end,  struct ceph_entity_addr  addr){int ret;u8 struct_v;u32 struct_len, addr_len;void  struct_end;ret = ceph_start_decoding(p, end, 1, \"entity_addr_t\", &struct_v,  &struct_len);if (ret)goto bad;ret = -EINVAL;struct_end =  p + struct_len;ceph_decode_copy_safe(p, end, &addr->type, sizeof(addr->type), bad);ceph_decode_copy_safe(p, end, &addr->nonce, sizeof(addr->nonce), bad);ceph_decode_32_safe(p, end, addr_len, bad);if (addr_len > sizeof(addr->in_addr))goto bad;memset(&addr->in_addr, 0, sizeof(addr->in_addr));if (addr_len) {ceph_decode_copy_safe(p, end, &addr->in_addr, addr_len, bad);addr->in_addr.ss_family =le16_to_cpu((__force __le16)addr->in_addr.ss_family);}  Advance past anything the client doesn't yet understand ", "if (addr_cnt == 1 && !memchr_inv(&tmp_addr, 0, sizeof(tmp_addr)))return 0;  /* weird but effectively the same as !addr_cnt ": "ceph_decode_entity_addrvec(void   p, void  end, bool msgr2,       struct ceph_entity_addr  addr){__le32 my_type = msgr2 ? CEPH_ENTITY_ADDR_TYPE_MSGR2 : CEPH_ENTITY_ADDR_TYPE_LEGACY;struct ceph_entity_addr tmp_addr;int addr_cnt;bool found;u8 marker;int ret;int i;ceph_decode_8_safe(p, end, marker, e_inval);if (marker != 2) {pr_err(\"bad addrvec marker %d\\n\", marker);return -EINVAL;}ceph_decode_32_safe(p, end, addr_cnt, e_inval);dout(\"%s addr_cnt %d\\n\", __func__, addr_cnt);found = false;for (i = 0; i < addr_cnt; i++) {ret = ceph_decode_entity_addr(p, end, &tmp_addr);if (ret)return ret;dout(\"%s i %d addr %s\\n\", __func__, i, ceph_pr_addr(&tmp_addr));if (tmp_addr.type == my_type) {if (found) {pr_err(\"another match of type %d in addrvec\\n\",       le32_to_cpu(my_type));return -EINVAL;}memcpy(addr, &tmp_addr, sizeof( addr));found = true;}}if (found)return 0;if (!addr_cnt)return 0;    normal -- e.g. unused OSD idslot ", "dest->name = kmalloc(src->name_len + 1,     GFP_NOIO | __GFP_NOFAIL);} else ": "ceph_oid_destroy(dest);if (src->name != src->inline_name) {  very rare, see ceph_object_id definition ", "int ceph_cls_lock(struct ceph_osd_client *osdc,  struct ceph_object_id *oid,  struct ceph_object_locator *oloc,  char *lock_name, u8 type, char *cookie,  char *tag, char *desc, u8 flags)": "ceph_cls_lock - grab rados lock for object   @osdc: OSD client instance   @oid: object to lock   @oloc: object to lock   @lock_name: the name of the lock   @type: lock type (CEPH_CLS_LOCK_EXCLUSIVE or CEPH_CLS_LOCK_SHARED)   @cookie: user-defined identifier for this instance of the lock   @tag: user-defined tag   @desc: user-defined lock description   @flags: lock flags     All operations on the same lock should use the same tag. ", "int ceph_cls_unlock(struct ceph_osd_client *osdc,    struct ceph_object_id *oid,    struct ceph_object_locator *oloc,    char *lock_name, char *cookie)": "ceph_cls_unlock - release rados lock for object   @osdc: OSD client instance   @oid: object to lock   @oloc: object to lock   @lock_name: the name of the lock   @cookie: user-defined identifier for this instance of the lock ", "int ceph_cls_break_lock(struct ceph_osd_client *osdc,struct ceph_object_id *oid,struct ceph_object_locator *oloc,char *lock_name, char *cookie,struct ceph_entity_name *locker)": "ceph_cls_break_lock - release rados lock for object for specified client   @osdc: OSD client instance   @oid: object to lock   @oloc: object to lock   @lock_name: the name of the lock   @cookie: user-defined identifier for this instance of the lock   @locker: current lock owner ", "ceph_start_encoding(&p, 1, 1,    cookie_op_buf_size - CEPH_ENCODING_START_BLK_LEN);ceph_encode_string(&p, end, lock_name, name_len);ceph_encode_8(&p, type);ceph_encode_string(&p, end, old_cookie, old_cookie_len);ceph_encode_string(&p, end, tag, tag_len);ceph_encode_string(&p, end, new_cookie, new_cookie_len);dout(\"%s lock_name %s type %d old_cookie %s tag %s new_cookie %s\\n\",     __func__, lock_name, type, old_cookie, tag, new_cookie);ret = ceph_osdc_call(osdc, oid, oloc, \"lock\", \"set_cookie\",     CEPH_OSD_FLAG_WRITE, cookie_op_page,     cookie_op_buf_size, NULL, NULL);dout(\"%s: status %d\\n\", __func__, ret);__free_page(cookie_op_page);return ret;}EXPORT_SYMBOL(ceph_cls_set_cookie": "ceph_cls_set_cookie(struct ceph_osd_client  osdc,struct ceph_object_id  oid,struct ceph_object_locator  oloc,char  lock_name, u8 type, char  old_cookie,char  tag, char  new_cookie){int cookie_op_buf_size;int name_len = strlen(lock_name);int old_cookie_len = strlen(old_cookie);int tag_len = strlen(tag);int new_cookie_len = strlen(new_cookie);void  p,  end;struct page  cookie_op_page;int ret;cookie_op_buf_size = name_len + sizeof(__le32) +     old_cookie_len + sizeof(__le32) +     tag_len + sizeof(__le32) +     new_cookie_len + sizeof(__le32) +     sizeof(u8) + CEPH_ENCODING_START_BLK_LEN;if (cookie_op_buf_size > PAGE_SIZE)return -E2BIG;cookie_op_page = alloc_page(GFP_NOIO);if (!cookie_op_page)return -ENOMEM;p = page_address(cookie_op_page);end = p + cookie_op_buf_size;  encode cls_lock_set_cookie_op struct ", "ceph_start_encoding(&p, 1, 1,    get_info_op_buf_size - CEPH_ENCODING_START_BLK_LEN);ceph_encode_string(&p, end, lock_name, name_len);dout(\"%s lock_name %s\\n\", __func__, lock_name);ret = ceph_osdc_call(osdc, oid, oloc, \"lock\", \"get_info\",     CEPH_OSD_FLAG_READ, get_info_op_page,     get_info_op_buf_size, &reply_page, &reply_len);dout(\"%s: status %d\\n\", __func__, ret);if (ret >= 0) ": "ceph_cls_lock_info(struct ceph_osd_client  osdc,       struct ceph_object_id  oid,       struct ceph_object_locator  oloc,       char  lock_name, u8  type, char   tag,       struct ceph_locker   lockers, u32  num_lockers){int get_info_op_buf_size;int name_len = strlen(lock_name);struct page  get_info_op_page,  reply_page;size_t reply_len = PAGE_SIZE;void  p,  end;int ret;get_info_op_buf_size = name_len + sizeof(__le32) +       CEPH_ENCODING_START_BLK_LEN;if (get_info_op_buf_size > PAGE_SIZE)return -E2BIG;get_info_op_page = alloc_page(GFP_NOIO);if (!get_info_op_page)return -ENOMEM;reply_page = alloc_page(GFP_NOIO);if (!reply_page) {__free_page(get_info_op_page);return -ENOMEM;}p = page_address(get_info_op_page);end = p + get_info_op_buf_size;  encode cls_lock_get_info_op struct ", "ceph_start_encoding(&p, 1, 1,    assert_op_buf_size - CEPH_ENCODING_START_BLK_LEN);ceph_encode_string(&p, end, lock_name, name_len);ceph_encode_8(&p, type);ceph_encode_string(&p, end, cookie, cookie_len);ceph_encode_string(&p, end, tag, tag_len);WARN_ON(p != end);osd_req_op_cls_request_data_pages(req, which, pages, assert_op_buf_size,  0, false, true);return 0;}EXPORT_SYMBOL(ceph_cls_assert_locked": "ceph_cls_assert_locked(struct ceph_osd_request  req, int which,   char  lock_name, u8 type, char  cookie, char  tag){int assert_op_buf_size;int name_len = strlen(lock_name);int cookie_len = strlen(cookie);int tag_len = strlen(tag);struct page   pages;void  p,  end;int ret;assert_op_buf_size = name_len + sizeof(__le32) +     cookie_len + sizeof(__le32) +     tag_len + sizeof(__le32) +     sizeof(u8) + CEPH_ENCODING_START_BLK_LEN;if (assert_op_buf_size > PAGE_SIZE)return -E2BIG;ret = osd_req_op_cls_init(req, which, \"lock\", \"assert_locked\");if (ret)return ret;pages = ceph_alloc_page_vector(1, GFP_NOIO);if (IS_ERR(pages))return PTR_ERR(pages);p = page_address(pages[0]);end = p + assert_op_buf_size;  encode cls_lock_assert_op struct ", "while (space > pl->num_pages_free) ": "ceph_pagelist_reserve(struct ceph_pagelist  pl, size_t space){if (space <= pl->room)return 0;space -= pl->room;space = (space + PAGE_SIZE - 1) >> PAGE_SHIFT;     conv to num pages ", "int ceph_pagelist_reserve(struct ceph_pagelist *pl, size_t space)": "ceph_pagelist_free_reserve(pl);kfree(pl);}EXPORT_SYMBOL(ceph_pagelist_release);static int ceph_pagelist_addpage(struct ceph_pagelist  pl){struct page  page;if (!pl->num_pages_free) {page = __page_cache_alloc(GFP_NOFS);} else {page = list_first_entry(&pl->free_list, struct page, lru);list_del(&page->lru);--pl->num_pages_free;}if (!page)return -ENOMEM;pl->room += PAGE_SIZE;ceph_pagelist_unmap_tail(pl);list_add_tail(&page->lru, &pl->head);pl->mapped_tail = kmap(page);return 0;}int ceph_pagelist_append(struct ceph_pagelist  pl, const void  buf, size_t len){while (pl->room < len) {size_t bit = pl->room;int ret;memcpy(pl->mapped_tail + (pl->length & ~PAGE_MASK),       buf, bit);pl->length += bit;pl->room -= bit;buf += bit;len -= bit;ret = ceph_pagelist_addpage(pl);if (ret)return ret;}memcpy(pl->mapped_tail + (pl->length & ~PAGE_MASK), buf, len);pl->length += len;pl->room -= len;return 0;}EXPORT_SYMBOL(ceph_pagelist_append);  Allocate enough pages for a pagelist to append the given amount   of data without allocating.   Returns: 0 on success, -ENOMEM on error. ", "list_move_tail(&page->lru, &pl->free_list);++pl->num_pages_free;}pl->room = c->room;if (!list_empty(&pl->head)) ": "ceph_pagelist_truncate(struct ceph_pagelist  pl,   struct ceph_pagelist_cursor  c){struct page  page;if (pl != c->pl)return -EINVAL;ceph_pagelist_unmap_tail(pl);while (pl->head.prev != c->page_lru) {page = list_entry(pl->head.prev, struct page, lru);  move from pagelist to reserve ", "__u32 len;      /* how many key bytes still need mixing ": "ceph_str_hash_rjenkins(const char  str, unsigned int length){const unsigned char  k = (const unsigned char  )str;__u32 a, b, c;    the internal state ", "static bool contains(const int *arr, int cnt, int val)": "ceph_auth_get_authorizer(struct ceph_auth_client  ac,       struct ceph_auth_handshake  auth,       int peer_type, bool force_new,       int  proto, int  pref_mode, int  fallb_mode){int ret;mutex_lock(&ac->mutex);if (force_new && auth->authorizer) {ceph_auth_destroy_authorizer(auth->authorizer);auth->authorizer = NULL;}if (!auth->authorizer)ret = ac->ops->create_authorizer(ac, peer_type, auth);else if (ac->ops->update_authorizer)ret = ac->ops->update_authorizer(ac, peer_type, auth);elseret = 0;if (ret)goto out; proto = ac->protocol;if (pref_mode && fallb_mode) { pref_mode = ac->preferred_mode; fallb_mode = ac->fallback_mode;}out:mutex_unlock(&ac->mutex);return ret;}EXPORT_SYMBOL(__ceph_auth_get_authorizer);void ceph_auth_destroy_authorizer(struct ceph_authorizer  a){a->destroy(a);}EXPORT_SYMBOL(ceph_auth_destroy_authorizer);int ceph_auth_add_authorizer_challenge(struct ceph_auth_client  ac,       struct ceph_authorizer  a,       void  challenge_buf,       int challenge_buf_len){int ret = 0;mutex_lock(&ac->mutex);if (ac->ops && ac->ops->add_authorizer_challenge)ret = ac->ops->add_authorizer_challenge(ac, a, challenge_buf,challenge_buf_len);mutex_unlock(&ac->mutex);return ret;}EXPORT_SYMBOL(ceph_auth_add_authorizer_challenge);int ceph_auth_verify_authorizer_reply(struct ceph_auth_client  ac,      struct ceph_authorizer  a,      void  reply, int reply_len,      u8  session_key, int  session_key_len,      u8  con_secret, int  con_secret_len){int ret = 0;mutex_lock(&ac->mutex);if (ac->ops && ac->ops->verify_authorizer_reply)ret = ac->ops->verify_authorizer_reply(ac, a,reply, reply_len, session_key, session_key_len,con_secret, con_secret_len);mutex_unlock(&ac->mutex);return ret;}EXPORT_SYMBOL(ceph_auth_verify_authorizer_reply);void ceph_auth_invalidate_authorizer(struct ceph_auth_client  ac, int peer_type){mutex_lock(&ac->mutex);if (ac->ops && ac->ops->invalidate_authorizer)ac->ops->invalidate_authorizer(ac, peer_type);mutex_unlock(&ac->mutex);}EXPORT_SYMBOL(ceph_auth_invalidate_authorizer);    msgr2 authentication ", "u32 blockoff;/* offset into su ": "ceph_extent_to_file(struct ceph_file_layout  l,u64 objno, u64 objoff, u64 objlen,struct ceph_file_extent   file_extents,u32  num_file_extents){u32 stripes_per_object = l->object_size  l->stripe_unit;u64 blockno;  which su ", "if (!net_eq(current->nsproxy->net_ns, read_pnet(&client->msgr.net)))return -1;ret = memcmp(opt1, opt2, ofs);if (ret)return ret;ret = strcmp_null(opt1->name, opt2->name);if (ret)return ret;if (opt1->key && !opt2->key)return -1;if (!opt1->key && opt2->key)return 1;if (opt1->key && opt2->key) ": "ceph_compare_options(struct ceph_options  new_opt, struct ceph_client  client){struct ceph_options  opt1 = new_opt;struct ceph_options  opt2 = client->options;int ofs = offsetof(struct ceph_options, mon_addr);int i;int ret;    Don't bother comparing options if network namespaces don't   match. ", "ret = ceph_parse_ips(buf, buf + len, opt->mon_addr, CEPH_MAX_MON,     &opt->num_mon, delim);if (ret) ": "ceph_parse_mon_ips(const char  buf, size_t len, struct ceph_options  opt,       struct fc_log  l, char delim){struct p_log log = {.prefix = \"libceph\", .log = l};int ret;  ip1[:port1][<delim>ip2[:port2]...] ", "if (result.uint_32 < 1 || result.uint_32 > INT_MAX / 1000)goto out_of_range;opt->osd_keepalive_timeout =    msecs_to_jiffies(result.uint_32 * 1000);break;case Opt_osd_idle_ttl:/* 0 isn't well defined right now, reject it ": "ceph_parse_param(struct fs_parameter  param, struct ceph_options  opt,     struct fc_log  l){struct fs_parse_result result;int token, err;struct p_log log = {.prefix = \"libceph\", .log = l};token = __fs_parse(&log, ceph_parameters, param, &result);dout(\"%s fs_parse '%s' token %d\\n\", __func__, param->key, token);if (token < 0)return token;switch (token) {case Opt_ip:err = ceph_parse_ips(param->string,     param->string + param->size,     &opt->my_addr, 1, NULL, ',');if (err) {error_plog(&log, \"Failed to parse ip: %d\", err);return err;}opt->flags |= CEPH_OPT_MYIP;break;case Opt_fsid:err = ceph_parse_fsid(param->string, &opt->fsid);if (err) {error_plog(&log, \"Failed to parse fsid: %d\", err);return err;}opt->flags |= CEPH_OPT_FSID;break;case Opt_name:kfree(opt->name);opt->name = param->string;param->string = NULL;break;case Opt_secret:ceph_crypto_key_destroy(opt->key);kfree(opt->key);opt->key = kzalloc(sizeof( opt->key), GFP_KERNEL);if (!opt->key)return -ENOMEM;err = ceph_crypto_key_unarmor(opt->key, param->string);if (err) {error_plog(&log, \"Failed to parse secret: %d\", err);return err;}break;case Opt_key:ceph_crypto_key_destroy(opt->key);kfree(opt->key);opt->key = kzalloc(sizeof( opt->key), GFP_KERNEL);if (!opt->key)return -ENOMEM;return get_secret(opt->key, param->string, &log);case Opt_crush_location:ceph_clear_crush_locs(&opt->crush_locs);err = ceph_parse_crush_location(param->string,&opt->crush_locs);if (err) {error_plog(&log, \"Failed to parse CRUSH location: %d\",   err);return err;}break;case Opt_read_from_replica:switch (result.uint_32) {case Opt_read_from_replica_no:opt->read_from_replica = 0;break;case Opt_read_from_replica_balance:opt->read_from_replica = CEPH_OSD_FLAG_BALANCE_READS;break;case Opt_read_from_replica_localize:opt->read_from_replica = CEPH_OSD_FLAG_LOCALIZE_READS;break;default:BUG();}break;case Opt_ms_mode:switch (result.uint_32) {case Opt_ms_mode_legacy:opt->con_modes[0] = CEPH_CON_MODE_UNKNOWN;opt->con_modes[1] = CEPH_CON_MODE_UNKNOWN;break;case Opt_ms_mode_crc:opt->con_modes[0] = CEPH_CON_MODE_CRC;opt->con_modes[1] = CEPH_CON_MODE_UNKNOWN;break;case Opt_ms_mode_secure:opt->con_modes[0] = CEPH_CON_MODE_SECURE;opt->con_modes[1] = CEPH_CON_MODE_UNKNOWN;break;case Opt_ms_mode_prefer_crc:opt->con_modes[0] = CEPH_CON_MODE_CRC;opt->con_modes[1] = CEPH_CON_MODE_SECURE;break;case Opt_ms_mode_prefer_secure:opt->con_modes[0] = CEPH_CON_MODE_SECURE;opt->con_modes[1] = CEPH_CON_MODE_CRC;break;default:BUG();}break;case Opt_osdkeepalivetimeout:  0 isn't well defined right now, reject it ", "if (m->count != pos)m->count--;return 0;}EXPORT_SYMBOL(ceph_print_client_options": "ceph_print_client_options(struct seq_file  m, struct ceph_client  client,      bool show_all){struct ceph_options  opt = client->options;size_t pos = m->count;struct rb_node  n;if (opt->name) {seq_puts(m, \"name=\");seq_escape(m, opt->name, \", \\t\\n\\\\\");seq_putc(m, ',');}if (opt->key)seq_puts(m, \"secret=<hidden>,\");if (!RB_EMPTY_ROOT(&opt->crush_locs)) {seq_puts(m, \"crush_location=\");for (n = rb_first(&opt->crush_locs); ; ) {struct crush_loc_node  loc =    rb_entry(n, struct crush_loc_node, cl_node);seq_printf(m, \"%s:%s\", loc->cl_loc.cl_type_name,   loc->cl_loc.cl_name);n = rb_next(n);if (!n)break;seq_putc(m, '|');}seq_putc(m, ',');}if (opt->read_from_replica == CEPH_OSD_FLAG_BALANCE_READS) {seq_puts(m, \"read_from_replica=balance,\");} else if (opt->read_from_replica == CEPH_OSD_FLAG_LOCALIZE_READS) {seq_puts(m, \"read_from_replica=localize,\");}if (opt->con_modes[0] != CEPH_CON_MODE_UNKNOWN) {if (opt->con_modes[0] == CEPH_CON_MODE_CRC &&    opt->con_modes[1] == CEPH_CON_MODE_UNKNOWN) {seq_puts(m, \"ms_mode=crc,\");} else if (opt->con_modes[0] == CEPH_CON_MODE_SECURE &&   opt->con_modes[1] == CEPH_CON_MODE_UNKNOWN) {seq_puts(m, \"ms_mode=secure,\");} else if (opt->con_modes[0] == CEPH_CON_MODE_CRC &&   opt->con_modes[1] == CEPH_CON_MODE_SECURE) {seq_puts(m, \"ms_mode=prefer-crc,\");} else if (opt->con_modes[0] == CEPH_CON_MODE_SECURE &&   opt->con_modes[1] == CEPH_CON_MODE_CRC) {seq_puts(m, \"ms_mode=prefer-secure,\");}}if (opt->flags & CEPH_OPT_FSID)seq_printf(m, \"fsid=%pU,\", &opt->fsid);if (opt->flags & CEPH_OPT_NOSHARE)seq_puts(m, \"noshare,\");if (opt->flags & CEPH_OPT_NOCRC)seq_puts(m, \"nocrc,\");if (opt->flags & CEPH_OPT_NOMSGSIGN)seq_puts(m, \"nocephx_sign_messages,\");if ((opt->flags & CEPH_OPT_TCP_NODELAY) == 0)seq_puts(m, \"notcp_nodelay,\");if (show_all && (opt->flags & CEPH_OPT_ABORT_ON_FULL))seq_puts(m, \"abort_on_full,\");if (opt->flags & CEPH_OPT_RXBOUNCE)seq_puts(m, \"rxbounce,\");if (opt->mount_timeout != CEPH_MOUNT_TIMEOUT_DEFAULT)seq_printf(m, \"mount_timeout=%d,\",   jiffies_to_msecs(opt->mount_timeout)  1000);if (opt->osd_idle_ttl != CEPH_OSD_IDLE_TTL_DEFAULT)seq_printf(m, \"osd_idle_ttl=%d,\",   jiffies_to_msecs(opt->osd_idle_ttl)  1000);if (opt->osd_keepalive_timeout != CEPH_OSD_KEEPALIVE_DEFAULT)seq_printf(m, \"osdkeepalivetimeout=%d,\",    jiffies_to_msecs(opt->osd_keepalive_timeout)  1000);if (opt->osd_request_timeout != CEPH_OSD_REQUEST_TIMEOUT_DEFAULT)seq_printf(m, \"osd_request_timeout=%d,\",   jiffies_to_msecs(opt->osd_request_timeout)  1000);  drop redundant comma ", "if (ceph_test_opt(client, MYIP))myaddr = &client->options->my_addr;ceph_messenger_init(&client->msgr, myaddr);/* subsystems ": "ceph_create_client(struct ceph_options  opt, void  private){struct ceph_client  client;struct ceph_entity_addr  myaddr = NULL;int err;err = wait_for_random_bytes();if (err < 0)return ERR_PTR(err);client = kzalloc(sizeof( client), GFP_KERNEL);if (client == NULL)return ERR_PTR(-ENOMEM);client->private = private;client->options = opt;mutex_init(&client->mount_mutex);init_waitqueue_head(&client->auth_wq);client->auth_err = 0;client->extra_mon_dispatch = NULL;client->supported_features = CEPH_FEATURES_SUPPORTED_DEFAULT;client->required_features = CEPH_FEATURES_REQUIRED_DEFAULT;if (!ceph_test_opt(client, NOMSGSIGN))client->required_features |= CEPH_FEATURE_MSG_AUTH;  msgr ", "ceph_osdc_stop(&client->osdc);ceph_monc_stop(&client->monc);ceph_messenger_fini(&client->msgr);ceph_debugfs_client_cleanup(client);ceph_destroy_options(client->options);kfree(client);dout(\"destroy_client %p done\\n\", client);}EXPORT_SYMBOL(ceph_destroy_client": "ceph_destroy_client(struct ceph_client  client){dout(\"destroy_client %p\\n\", client);atomic_set(&client->msgr.stopping, 1);  unmount ", "err = ceph_monc_open_session(&client->monc);if (err < 0)return err;while (!have_mon_and_osd_map(client)) ": "ceph_open_session(struct ceph_client  client, unsigned long started){unsigned long timeout = client->options->mount_timeout;long err;  open session, and wait for mon and osd maps ", "msg_size = CEPH_ENCODING_START_BLK_LEN +CEPH_PGID_ENCODING_LEN + 1; /* spgid ": "ceph_osdc_alloc_messages(struct ceph_osd_request  req, gfp_t gfp,      int num_request_data_items,      int num_reply_data_items){struct ceph_osd_client  osdc = req->r_osdc;struct ceph_msg  msg;int msg_size;WARN_ON(req->r_request || req->r_reply);WARN_ON(ceph_oid_empty(&req->r_base_oid));WARN_ON(ceph_oloc_empty(&req->r_base_oloc));  create request message ", "memset(req, 0, sizeof(*req));kref_init(&req->r_kref);init_completion(&req->r_completion);RB_CLEAR_NODE(&req->r_node);RB_CLEAR_NODE(&req->r_mc_node);INIT_LIST_HEAD(&req->r_private_item);target_init(&req->r_t);}struct ceph_osd_request *ceph_osdc_alloc_request(struct ceph_osd_client *osdc,       struct ceph_snap_context *snapc,       unsigned int num_ops,       bool use_mempool,       gfp_t gfp_flags)": "osd_req_op_init() ", "BUG_ON(length > previous);op->extent.length = length;if (op->op == CEPH_OSD_OP_WRITE || op->op == CEPH_OSD_OP_WRITEFULL)op->indata_len -= previous - length;}EXPORT_SYMBOL(osd_req_op_extent_update": "osd_req_op_extent_update(struct ceph_osd_request  osd_req,unsigned int which, u64 length){struct ceph_osd_req_op  op;u64 previous;BUG_ON(which >= osd_req->r_num_ops);op = &osd_req->r_ops[which];previous = op->extent.length;if (length == previous)return;  Nothing to do ", "op->indata_len = prev_op->indata_len;op->outdata_len = prev_op->outdata_len;op->extent = prev_op->extent;/* adjust offset ": "osd_req_op_extent_dup_last(struct ceph_osd_request  osd_req,unsigned int which, u64 offset_inc){struct ceph_osd_req_op  op,  prev_op;BUG_ON(which + 1 >= osd_req->r_num_ops);prev_op = &osd_req->r_ops[which];op = osd_req_op_init(osd_req, which + 1, prev_op->op, prev_op->flags);  dup previous one ", "op->flags |= CEPH_OSD_OP_FLAG_FAILOK;}EXPORT_SYMBOL(osd_req_op_alloc_hint_init": "osd_req_op_alloc_hint_init(struct ceph_osd_request  osd_req,unsigned int which,u64 expected_object_size,u64 expected_write_size,u32 flags){struct ceph_osd_req_op  op;op = osd_req_op_init(osd_req, which, CEPH_OSD_OP_SETALLOCHINT, 0);op->alloc_hint.expected_object_size = expected_object_size;op->alloc_hint.expected_write_size = expected_write_size;op->alloc_hint.flags = flags;    CEPH_OSD_OP_SETALLOCHINT op is advisory and therefore deemed   not worth a feature bit.  Set FAILOK per-op flag to make   sure older osds don't trip over an unsupported opcode. ", "r = calc_layout(layout, off, plen, &objnum, &objoff, &objlen);if (r)goto fail;if (opcode == CEPH_OSD_OP_CREATE || opcode == CEPH_OSD_OP_DELETE) ": "ceph_osdc_new_request(struct ceph_osd_client  osdc,       struct ceph_file_layout  layout,       struct ceph_vino vino,       u64 off, u64  plen,       unsigned int which, int num_ops,       int opcode, int flags,       struct ceph_snap_context  snapc,       u32 truncate_seq,       u64 truncate_size,       bool use_mempool){struct ceph_osd_request  req;u64 objnum = 0;u64 objoff = 0;u64 objlen = 0;int r;BUG_ON(opcode != CEPH_OSD_OP_READ && opcode != CEPH_OSD_OP_WRITE &&       opcode != CEPH_OSD_OP_ZERO && opcode != CEPH_OSD_OP_TRUNCATE &&       opcode != CEPH_OSD_OP_CREATE && opcode != CEPH_OSD_OP_DELETE);req = ceph_osdc_alloc_request(osdc, snapc, num_ops, use_mempool,GFP_NOFS);if (!req) {r = -ENOMEM;goto fail;}  calculate max write size ", "if (off) ": "ceph_zero_page_vector_range(int off, int len, struct page   pages){int i = off >> PAGE_SHIFT;off &= ~PAGE_MASK;dout(\"zero_page_vector_page %u~%u\\n\", off, len);  leading partial page? ", "/* * Create a new ceph snapshot context large enough to hold the * indicated number of snapshot ids (which can be 0).  Caller has * to fill in snapc->seq and snapc->snaps[0..snap_count-1]. * * Returns a null pointer if an error occurs. ": "ceph_put_snap_context().  When the reference count reaches zero   the entire structure is freed. ", "if (!net_eq(current->nsproxy->net_ns, sock_net(sk)))return -EINVAL;if (!asoc)return -EINVAL;/* An association cannot be branched off from an already peeled-off * socket, nor is this supported for tcp style sockets. ": "sctp_do_peeloff(struct sock  sk, sctp_assoc_t id, struct socket   sockp){struct sctp_association  asoc = sctp_id2assoc(sk, id);struct sctp_sock  sp = sctp_sk(sk);struct socket  sock;int err = 0;  Do not peel off from one netns to another one. ", "int atm_pcr_goal(const struct atm_trafprm *tp)": "atm_pcr_goal returns the positive PCR if it should be rounded up, the   negative PCR if it should be rounded down, and zero if the maximum available   bandwidth should be used.     The rules are as follows (  = maximum, - = absent (0), x = value \"x\",   (x+ = x or next value above x, x- = x or next value below):    min max pcrresultmin max pcrresult  -   -   -  (UBR only)x   -   -x+  -   -     x   -       -   -   zz-x   -   zz-  -       - x       -x+  -         x           -       zz-x       zz-  -   y   -y-x   y   -x+  -   y    y-x   y    y-  -   y   zz-x   y   zz-     All non-error cases can be converted with the following simple set of rules:       if pcr == z then z-     else if min == x && pcr == - then x+       else if max == y then y-   else   ", "mutex_lock(&atm_dev_mutex);list_del(&dev->dev_list);mutex_unlock(&atm_dev_mutex);atm_dev_release_vccs(dev);atm_unregister_sysfs(dev);atm_proc_dev_deregister(dev);atm_dev_put(dev);}EXPORT_SYMBOL(atm_dev_deregister": "atm_dev_deregister(struct atm_dev  dev){BUG_ON(test_bit(ATM_DF_REMOVED, &dev->flags));set_bit(ATM_DF_REMOVED, &dev->flags);    if we remove current device from atm_devs list, new device   with same number can appear, such we need deregister proc,   release async all vccs and remove them from vccs list too ", "WARN_ON(signal < ATM_PHY_SIG_LOST || signal > ATM_PHY_SIG_FOUND);if (dev->signal == signal)return; /* no change ": "atm_dev_signal_change(struct atm_dev  dev, char signal){pr_debug(\"%s signal=%d dev=%p number=%d dev->signal=%d\\n\",__func__, signal, dev, dev->number, dev->signal);  atm driver sending invalid signal ", "if ((st & htonl(0xE0000000)) != htonl(0x00000000) &&    (st & htonl(0xE0000000)) != htonl(0xE0000000))return (IPV6_ADDR_UNICAST |IPV6_ADDR_SCOPE_TYPE(IPV6_ADDR_SCOPE_GLOBAL));if ((st & htonl(0xFF000000)) == htonl(0xFF000000)) ": "__ipv6_addr_type(const struct in6_addr  addr){__be32 st;st = addr->s6_addr32[0];  Consider all addresses with the first three bits different of   000 and 111 as unicasts. ", "void in6_dev_finish_destroy(struct inet6_dev *idev)": "in6_dev_finish_destroy_rcu(struct rcu_head  head){struct inet6_dev  idev = container_of(head, struct inet6_dev, rcu);snmp6_free_dev(idev);kfree(idev);}  Nobody refers to this device, we may destroy it. ", "/* saddr(16) + first_seg(1) + flags(1) + keyid(4) + seglist(16n) ": "seg6_hmac_compute(struct seg6_hmac_info  hinfo, struct ipv6_sr_hdr  hdr,      struct in6_addr  saddr, u8  output){__be32 hmackeyid = cpu_to_be32(hinfo->hmackeyid);u8 tmp_out[SEG6_HMAC_MAX_DIGESTSIZE];int plen, i, dgsize, wrsize;char  ring,  off;  a 160-byte buffer for digest output allows to store highest known   hash function (RadioGatun) with up to 1216 bits ", "if (idev->cnf.seg6_require_hmac > 0 && !tlv)return false;/* no check ": "seg6_hmac_validate_skb(struct sk_buff  skb){u8 hmac_output[SEG6_HMAC_FIELD_LEN];struct net  net = dev_net(skb->dev);struct seg6_hmac_info  hinfo;struct sr6_tlv_hmac  tlv;struct ipv6_sr_hdr  srh;struct inet6_dev  idev;idev = __in6_dev_get(skb->dev);srh = (struct ipv6_sr_hdr  )skb_transport_header(skb);tlv = seg6_get_tlv_hmac(srh);  mandatory check but no tlv ", "struct seg6_hmac_info *seg6_hmac_info_lookup(struct net *net, u32 key)": "seg6_hmac_info_lookup(net, be32_to_cpu(tlv->hmackeyid));if (!hinfo)return false;if (seg6_hmac_compute(hinfo, srh, &ipv6_hdr(skb)->saddr, hmac_output))return false;if (memcmp(hmac_output, tlv->hmac, SEG6_HMAC_FIELD_LEN) != 0)return false;return true;}EXPORT_SYMBOL(seg6_hmac_validate_skb);  called with rcu_read_lock() ", "if (!(ipv6_addr_type(&ipv6_hdr(skb)->saddr) & IPV6_ADDR_LINKLOCAL))return -EINVAL;/* MLDv1? ": "ipv6_mc_check_mld_reportv2(struct sk_buff  skb){unsigned int len = skb_transport_offset(skb);len += sizeof(struct mld2_report);return ipv6_mc_may_pull(skb, len) ? 0 : -EINVAL;}static int ipv6_mc_check_mld_query(struct sk_buff  skb){unsigned int transport_len = ipv6_transport_len(skb);struct mld_msg  mld;unsigned int len;  RFC2710+RFC3810 (MLDv1+MLDv2) require link-local source addresses ", "dst = daddr;src = (xfrm_address_t *)&in6addr_any;break;default:/* lookup state with wild-card addresses ": "xfrm6_input_addr(struct sk_buff  skb, xfrm_address_t  daddr,     xfrm_address_t  saddr, u8 proto){struct net  net = dev_net(skb->dev);struct xfrm_state  x = NULL;struct sec_path  sp;int i = 0;sp = secpath_set(skb);if (!sp) {XFRM_INC_STATS(net, LINUX_MIB_XFRMINERROR);goto drop;}if (1 + sp->len == XFRM_MAX_DEPTH) {XFRM_INC_STATS(net, LINUX_MIB_XFRMINBUFFERERROR);goto drop;}for (i = 0; i < 3; i++) {xfrm_address_t  dst,  src;switch (i) {case 0:dst = daddr;src = saddr;break;case 1:  lookup state with wild-card source address ", "if (unlikely(sk->sk_family != AF_INET6))goto unlock;switch (optname) ": "ipv6_setsockopt(struct sock  sk, int level, int optname,       sockptr_t optval, unsigned int optlen){struct ipv6_pinfo  np = inet6_sk(sk);struct net  net = sock_net(sk);int val, valbool;int retv = -ENOPROTOOPT;bool needs_rtnl = setsockopt_needs_rtnl(optname);if (sockptr_is_null(optval))val = 0;else {if (optlen >= sizeof(int)) {if (copy_from_sockptr(&val, optval, sizeof(val)))return -EFAULT;} elseval = 0;}valbool = (val != 0);if (ip6_mroute_opt(optname))return ip6_mroute_setsockopt(sk, optname, optval, optlen);if (needs_rtnl)rtnl_lock();sockopt_lock_sock(sk);  Another thread has converted the socket into IPv4 with   IPV6_ADDRFORM concurrently. ", "}if (!hdr)return 0;len = min_t(unsigned int, len, ipv6_optlen(hdr));if (copy_to_sockptr(optval, hdr, len))return -EFAULT;return len;}static int ipv6_get_msfilter(struct sock *sk, sockptr_t optval,     sockptr_t optlen, int len)": "ipv6_getsockopt_sticky(struct sock  sk, struct ipv6_txoptions  opt,  int optname, sockptr_t optval, int len){struct ipv6_opt_hdr  hdr;if (!opt)return 0;switch (optname) {case IPV6_HOPOPTS:hdr = opt->hopopt;break;case IPV6_RTHDRDSTOPTS:hdr = opt->dst0opt;break;case IPV6_RTHDR:hdr = (struct ipv6_opt_hdr  )opt->srcrt;break;case IPV6_DSTOPTS:hdr = opt->dst1opt;break;default:return -EINVAL;  should not happen ", "unsigned int hash = inet6_ehashfn(net, daddr, hnum, saddr, sport);unsigned int slot = hash & hashinfo->ehash_mask;struct inet_ehash_bucket *head = &hashinfo->ehash[slot];begin:sk_nulls_for_each_rcu(sk, node, &head->chain) ": "__inet6_lookup_established(struct net  net,struct inet_hashinfo  hashinfo,   const struct in6_addr  saddr,   const __be16 sport,   const struct in6_addr  daddr,   const u16 hnum,   const int dif, const int sdif){struct sock  sk;const struct hlist_nulls_node  node;const __portpair ports = INET_COMBINED_PORTS(sport, hnum);  Optimize here for direct hit, only listening connections can   have wildcards anyways. ", "__be32 ipv6_proxy_select_ident(struct net *net, struct sk_buff *skb)": "ipv6_select_ident(struct net  net,       const struct in6_addr  dst,       const struct in6_addr  src){return get_random_u32_above(0);}  This function exists only for tap drivers that must support broken   clients requesting UFO without specifying an IPv6 fragment ID.     This is similar to ipv6_select_ident() but we use an independent hash   seed to limit information leakage.     The network header must be set before calling this. ", "err = (__force int)skb_checksum_init_zero_check(skb, proto, uh->check,ip6_compute_pseudo);if (err)return err;if (skb->ip_summed == CHECKSUM_COMPLETE && !skb->csum_valid) ": "udp6_csum_init(struct sk_buff  skb, struct udphdr  uh, int proto){int err;UDP_SKB_CB(skb)->partial_cov = 0;UDP_SKB_CB(skb)->cscov = skb->len;if (proto == IPPROTO_UDPLITE) {err = udplite_checksum_init(skb, uh);if (err)return err;if (UDP_SKB_CB(skb)->partial_cov) {skb->csum = ip6_compute_pseudo(skb, proto);return 0;}}  To support RFC 6936 (allow zero checksum in UDPIPV6 for tunnels)   we accept a checksum of zero here. When we find the socket   for the UDP packet we'll check if that socket allows zero checksum   for IPv6 (set by socket option).     Note, we are only interested in != 0 or == 0, thus the   force to int. ", "if (sk->sk_state != TCP_CLOSE || inet->inet_num) ": "inet6_bind(struct sock  sk, struct sockaddr  uaddr, int addr_len,u32 flags){struct sockaddr_in6  addr = (struct sockaddr_in6  )uaddr;struct inet_sock  inet = inet_sk(sk);struct ipv6_pinfo  np = inet6_sk(sk);struct net  net = sock_net(sk);__be32 v4addr = 0;unsigned short snum;bool saved_ipv6only;int addr_type = 0;int err = 0;if (addr->sin6_family != AF_INET6)return -EAFNOSUPPORT;addr_type = ipv6_addr_type(&addr->sin6_addr);if ((addr_type & IPV6_ADDR_MULTICAST) && sk->sk_type == SOCK_STREAM)return -EINVAL;snum = ntohs(addr->sin6_port);if (!(flags & BIND_NO_CAP_NET_BIND_SERVICE) &&    snum && inet_port_requires_bind_service(net, snum) &&    !ns_capable(net->user_ns, CAP_NET_BIND_SERVICE))return -EACCES;if (flags & BIND_WITH_LOCK)lock_sock(sk);  Check these errors (active socket, double bind). ", "ipv6_sock_mc_close(sk);/* Free ac lists ": "inet6_release(struct socket  sock){struct sock  sk = sock->sk;if (!sk)return -EINVAL;  Free mc lists ", "prot = READ_ONCE(sk->sk_prot);if (!prot->ioctl)return -ENOIOCTLCMD;return sk_ioctl(sk, cmd, (void __user *)arg);}/*NOTREACHED": "inet6_ioctl(struct socket  sock, unsigned int cmd, unsigned long arg){void __user  argp = (void __user  )arg;struct sock  sk = sock->sk;struct net  net = sock_net(sk);const struct proto  prot;switch (cmd) {case SIOCADDRT:case SIOCDELRT: {struct in6_rtmsg rtmsg;if (copy_from_user(&rtmsg, argp, sizeof(rtmsg)))return -EFAULT;return ipv6_route_ioctl(net, cmd, &rtmsg);}case SIOCSIFADDR:return addrconf_add_ifaddr(net, argp);case SIOCDIFADDR:return addrconf_del_ifaddr(net, argp);case SIOCSIFDSTADDR:return addrconf_set_dstaddr(net, argp);default:  IPV6_ADDRFORM can change sk->sk_prot under us. ", "answer = NULL;ret = -EPERM;last_perm = &inetsw6[p->type];list_for_each(lh, &inetsw6[p->type]) ": "inet6_register_protosw(struct inet_protosw  p){struct list_head  lh;struct inet_protosw  answer;struct list_head  last_perm;int protocol = p->protocol;int ret;spin_lock_bh(&inetsw6_lock);ret = -EINVAL;if (p->type >= SOCK_MAX)goto out_illegal;  If we are trying to override a permanent protocol, bail. ", "case ARPHRD_FDDI:ipv6_eth_mc_map(addr, buf);return 0;case ARPHRD_ARCNET:ipv6_arcnet_mc_map(addr, buf);return 0;case ARPHRD_INFINIBAND:ipv6_ib_mc_map(addr, dev->broadcast, buf);return 0;case ARPHRD_IPGRE:return ipv6_ipgre_mc_map(addr, dev->broadcast, buf);default:if (dir) ": "ndisc_mc_map(const struct in6_addr  addr, char  buf, struct net_device  dev, int dir){switch (dev->type) {case ARPHRD_ETHER:case ARPHRD_IEEE802:  Not sure. Check it later. --ANK ", "skb_dst_drop(skb);skb_dst_set(skb, dst);#ifdef CONFIG_XFRMif (!(IP6CB(skb)->flags & IP6SKB_XFRM_TRANSFORMED) &&    xfrm_decode_session(skb, flowi6_to_flowi(&fl6), AF_INET6) == 0) ": "ip6_route_me_harder(struct net  net, struct sock  sk_partial, struct sk_buff  skb){const struct ipv6hdr  iph = ipv6_hdr(skb);struct sock  sk = sk_to_full_sk(sk_partial);struct net_device  dev = skb_dst(skb)->dev;struct flow_keys flkeys;unsigned int hh_len;struct dst_entry  dst;int strict = (ipv6_addr_type(&iph->daddr) &      (IPV6_ADDR_MULTICAST | IPV6_ADDR_LINKLOCAL));struct flowi6 fl6 = {.flowi6_l3mdev = l3mdev_master_ifindex(dev),.flowi6_mark = skb->mark,.flowi6_uid = sock_net_uid(net, sk),.daddr = iph->daddr,.saddr = iph->saddr,};int err;if (sk && sk->sk_bound_dev_if)fl6.flowi6_oif = sk->sk_bound_dev_if;else if (strict)fl6.flowi6_oif = dev->ifindex;fib6_rules_early_flow_dissect(net, skb, &fl6, &flkeys);dst = ip6_route_output(net, sk, &fl6);err = dst->error;if (err) {IP6_INC_STATS(net, ip6_dst_idev(dst), IPSTATS_MIB_OUTNOROUTES);net_dbg_ratelimited(\"ip6_route_me_harder: No more route\\n\");dst_release(dst);return err;}  Drop old route. ", "dst2 = dst;dst = xfrm_lookup(net, dst, flowi6_to_flowi(fl6), sk, 0);if (!IS_ERR(dst)) ": "icmp6_send: acast source\\n\");dst_release(dst);return ERR_PTR(-EINVAL);}  No need to clone since we're just using its address. ", "if (data_len < 128 || (data_len & 7) || skb->len < data_len)data_len = 0;skb2 = data_len ? skb_copy(skb, GFP_ATOMIC) : skb_clone(skb, GFP_ATOMIC);if (!skb2)return 1;skb_dst_drop(skb2);skb_pull(skb2, nhs);skb_reset_network_header(skb2);rt = rt6_lookup(dev_net(skb->dev), &ipv6_hdr(skb2)->saddr, NULL, 0,skb, 0);if (rt && rt->dst.dev)skb2->dev = rt->dst.dev;ipv6_addr_set_v4mapped(ip_hdr(skb)->saddr, &temp_saddr);if (data_len) ": "ip6_err_gen_icmpv6_unreach(struct sk_buff  skb, int nhs, int type,       unsigned int data_len){struct in6_addr temp_saddr;struct rt6_info  rt;struct sk_buff  skb2;u32 info = 0;if (!pskb_may_pull(skb, nhs + sizeof(struct ipv6hdr) + 8))return 1;  RFC 4884 (partial) support for ICMP extensions ", "if (net->ipv6.sysctl.fib_notify_on_flag_change == 2 &&    READ_ONCE(f6i->offload_failed) == offload_failed)return;WRITE_ONCE(f6i->offload_failed, offload_failed);if (!rcu_access_pointer(f6i->fib6_node))/* The route was removed from the tree, do not send * notification. ": "fib6_info_hw_flags_set(struct net  net, struct fib6_info  f6i,    bool offload, bool trap, bool offload_failed){struct sk_buff  skb;int err;if (READ_ONCE(f6i->offload) == offload &&    READ_ONCE(f6i->trap) == trap &&    READ_ONCE(f6i->offload_failed) == offload_failed)return;WRITE_ONCE(f6i->offload, offload);WRITE_ONCE(f6i->trap, trap);  2 means send notifications only if offload_failed was changed. ", "__u16 ip6_tnl_parse_tlv_enc_lim(struct sk_buff *skb, __u8 *raw)": "ip6_tnl_parse_tlv_enc_lim - handle encapsulation limit option     @skb: received socket buffer     @raw: the ICMPv6 error message data     Return:     0 if none was found,     else index to encapsulation limit  ", "int ip6_tnl_change_mtu(struct net_device *dev, int new_mtu)": "ip6_tnl_change_mtu - change mtu manually for tunnel device     @dev: virtual device associated with tunnel     @new_mtu: the new mtu     Return:     0 on success,     %-EINVAL if mtu too small  ", "if (sin6) ": "udpv6_sendmsg(struct sock  sk, struct msghdr  msg, size_t len){struct ipv6_txoptions opt_space;struct udp_sock  up = udp_sk(sk);struct inet_sock  inet = inet_sk(sk);struct ipv6_pinfo  np = inet6_sk(sk);DECLARE_SOCKADDR(struct sockaddr_in6  , sin6, msg->msg_name);struct in6_addr  daddr,  final_p, final;struct ipv6_txoptions  opt = NULL;struct ipv6_txoptions  opt_to_free = NULL;struct ip6_flowlabel  flowlabel = NULL;struct inet_cork_full cork;struct flowi6  fl6 = &cork.fl.u.ip6;struct dst_entry  dst;struct ipcm6_cookie ipc6;int addr_len = msg->msg_namelen;bool connected = false;int ulen = len;int corkreq = READ_ONCE(up->corkflag) || msg->msg_flags&MSG_MORE;int err;int is_udplite = IS_UDPLITE(sk);int ( getfrag)(void  , char  , int, int, int, struct sk_buff  );ipcm6_init(&ipc6);ipc6.gso_size = READ_ONCE(up->gso_size);ipc6.sockc.tsflags = sk->sk_tsflags;ipc6.sockc.mark = READ_ONCE(sk->sk_mark);  destination address check ", "return   (nexthdr == NEXTHDR_HOP)|| (nexthdr == NEXTHDR_ROUTING)|| (nexthdr == NEXTHDR_FRAGMENT)|| (nexthdr == NEXTHDR_AUTH)|| (nexthdr == NEXTHDR_NONE)|| (nexthdr == NEXTHDR_DEST);}EXPORT_SYMBOL(ipv6_ext_hdr": "ipv6_ext_hdr(u8 nexthdr){    find out if nexthdr is an extension header or a protocol ", "*flags |= IP6_FH_F_FRAG;fp = skb_header_pointer(skb,start+offsetof(struct frag_hdr,       frag_off),sizeof(_frag_off),&_frag_off);if (!fp)return -EBADMSG;_frag_off = ntohs(*fp) & ~0x7;if (_frag_off) ": "ipv6_find_hdr(const struct sk_buff  skb, unsigned int  offset,  int target, unsigned short  fragoff, int  flags){unsigned int start = skb_network_offset(skb) + sizeof(struct ipv6hdr);u8 nexthdr = ipv6_hdr(skb)->nexthdr;bool found;if (fragoff) fragoff = 0;if ( offset) {struct ipv6hdr _ip6,  ip6;ip6 = skb_header_pointer(skb,  offset, sizeof(_ip6), &_ip6);if (!ip6 || (ip6->version != 6))return -EBADMSG;start =  offset + sizeof(struct ipv6hdr);nexthdr = ip6->nexthdr;}do {struct ipv6_opt_hdr _hdr,  hp;unsigned int hdrlen;found = (nexthdr == target);if ((!ipv6_ext_hdr(nexthdr)) || nexthdr == NEXTHDR_NONE) {if (target < 0 || found)break;return -ENOENT;}hp = skb_header_pointer(skb, start, sizeof(_hdr), &_hdr);if (!hp)return -EBADMSG;if (nexthdr == NEXTHDR_ROUTING) {struct ipv6_rt_hdr _rh,  rh;rh = skb_header_pointer(skb, start, sizeof(_rh),&_rh);if (!rh)return -EBADMSG;if (flags && ( flags & IP6_FH_F_SKIP_RH) &&    rh->segments_left == 0)found = false;}if (nexthdr == NEXTHDR_FRAGMENT) {unsigned short _frag_off;__be16  fp;if (flags)  Indicate that this is a fragment ", "if (np)hlimit = np->hop_limit;if (hlimit < 0)hlimit = ip6_dst_hoplimit(dst);ip6_flow_hdr(hdr, tclass, ip6_make_flowlabel(net, skb, fl6->flowlabel,ip6_autoflowlabel(net, np), fl6));hdr->payload_len = htons(seg_len);hdr->nexthdr = proto;hdr->hop_limit = hlimit;hdr->saddr = fl6->saddr;hdr->daddr = *first_hop;skb->protocol = htons(ETH_P_IPV6);skb->priority = priority;skb->mark = mark;mtu = dst_mtu(dst);if ((skb->len <= mtu) || skb->ignore_df || skb_is_gso(skb)) ": "ip6_xmit(const struct sock  sk, struct sk_buff  skb, struct flowi6  fl6,     __u32 mark, struct ipv6_txoptions  opt, int tclass, u32 priority){struct net  net = sock_net(sk);const struct ipv6_pinfo  np = inet6_sk(sk);struct in6_addr  first_hop = &fl6->daddr;struct dst_entry  dst = skb_dst(skb);struct net_device  dev = dst->dev;struct inet6_dev  idev = ip6_dst_idev(dst);struct hop_jumbo_hdr  hop_jumbo;int hoplen = sizeof( hop_jumbo);unsigned int head_room;struct ipv6hdr  hdr;u8  proto = fl6->flowi6_proto;int seg_len = skb->len;int hlimit = -1;u32 mtu;head_room = sizeof(struct ipv6hdr) + hoplen + LL_RESERVED_SPACE(dev);if (opt)head_room += opt->opt_nflen + opt->opt_flen;if (unlikely(head_room > skb_headroom(skb))) {skb = skb_expand_head(skb, head_room);if (!skb) {IP6_INC_STATS(net, idev, IPSTATS_MIB_OUTDISCARDS);return -ENOBUFS;}}if (opt) {seg_len += opt->opt_nflen + opt->opt_flen;if (opt->opt_flen)ipv6_push_frag_opts(skb, opt, &proto);if (opt->opt_nflen)ipv6_push_nfrag_opts(skb, opt, &proto, &first_hop,     &fl6->saddr);}if (unlikely(seg_len > IPV6_MAXPLEN)) {hop_jumbo = skb_push(skb, hoplen);hop_jumbo->nexthdr = proto;hop_jumbo->hdrlen = 0;hop_jumbo->tlv_type = IPV6_TLV_JUMBO;hop_jumbo->tlv_len = 4;hop_jumbo->jumbo_payload_len = htonl(seg_len + hoplen);proto = IPPROTO_HOPOPTS;seg_len = 0;IP6CB(skb)->flags |= IP6SKB_FAKEJUMBO;}skb_push(skb, sizeof(struct ipv6hdr));skb_reset_network_header(skb);hdr = ipv6_hdr(skb);   Fill in the IPv6 header ", "*prevhdr = NEXTHDR_FRAGMENT;iter->tmp_hdr = kmemdup(skb_network_header(skb), hlen, GFP_ATOMIC);if (!iter->tmp_hdr)return -ENOMEM;iter->frag = skb_shinfo(skb)->frag_list;skb_frag_list_init(skb);iter->offset = 0;iter->hlen = hlen;iter->frag_id = frag_id;iter->nexthdr = nexthdr;__skb_pull(skb, hlen);fh = __skb_push(skb, sizeof(struct frag_hdr));__skb_push(skb, hlen);skb_reset_network_header(skb);memcpy(skb_network_header(skb), iter->tmp_hdr, hlen);fh->nexthdr = nexthdr;fh->reserved = 0;fh->frag_off = htons(IP6_MF);fh->identification = frag_id;first_len = skb_pagelen(skb);skb->data_len = first_len - skb_headlen(skb);skb->len = first_len;ipv6_hdr(skb)->payload_len = htons(first_len - sizeof(struct ipv6hdr));return 0;}EXPORT_SYMBOL(ip6_fraglist_init": "ip6_fraglist_init(struct sk_buff  skb, unsigned int hlen, u8  prevhdr,      u8 nexthdr, __be32 frag_id,      struct ip6_fraglist_iter  iter){unsigned int first_len;struct frag_hdr  fh;  BUILD HEADER ", "#include <linux/module.h>#include <linux/netdevice.h>#include <linux/spinlock.h>#include <net/protocol.h>#if IS_ENABLED(CONFIG_IPV6)struct inet6_protocol __rcu *inet6_protos[MAX_INET_PROTOS] __read_mostly;EXPORT_SYMBOL(inet6_protos);int inet6_add_protocol(const struct inet6_protocol *prot, unsigned char protocol)": "inet6_del_protocol() to correctly maintain copy bit. ", "if ((score->ifa->flags & IFA_F_TENTATIVE) &&    (!(score->ifa->flags & IFA_F_OPTIMISTIC)))continue;score->addr_type = __ipv6_addr_type(&score->ifa->addr);if (unlikely(score->addr_type == IPV6_ADDR_ANY ||     score->addr_type & IPV6_ADDR_MULTICAST)) ": "ipv6_dev_get_saddr(struct net  net,struct ipv6_saddr_dst  dst,struct inet6_dev  idev,struct ipv6_saddr_score  scores,int hiscore_idx){struct ipv6_saddr_score  score = &scores[1 - hiscore_idx],  hiscore = &scores[hiscore_idx];list_for_each_entry_rcu(score->ifa, &idev->addr_list, if_list) {int i;    - Tentative Address (RFC2462 section 5.4)    - A tentative address is not considered      \"assigned to an interface\" in the traditional      sense, unless it is also flagged as optimistic.   - Candidate Source Address (section 4)    - In any case, anycast addresses, multicast      addresses, and the unspecified address MUST      NOT be included in a candidate set. ", "static struct net_device *__ipv6_chk_addr_and_flags(struct net *net, const struct in6_addr *addr,  const struct net_device *dev, bool skip_dev_check,  int strict, u32 banned_flags)": "ipv6_chk_addr_and_flags(net, addr, dev, !dev,       strict, IFA_F_TENTATIVE);}EXPORT_SYMBOL(ipv6_chk_addr);  device argument is used to find the L3 domain of interest. If   skip_dev_check is set, then the ifp device is not checked against   the passed in dev argument. So the 2 cases for addresses checks are:     1. does the address exist in the L3 domain that dev is part of        (skip_dev_check = true), or       2. does the address exist on the specific device        (skip_dev_check = false) ", "struct net_device *ipv6_dev_find(struct net *net, const struct in6_addr *addr, struct net_device *dev)": "ipv6_dev_find - find the first device with a given source address.   @net: the net namespace   @addr: the source address   @dev: used to find the L3 domain of interest     The caller should be protected by RCU, or RTNL. ", "err = __ipv6_dev_mc_inc(dev, addr, mode);if (err) ": "ipv6_sock_mc_join(struct sock  sk, int ifindex,       const struct in6_addr  addr, unsigned int mode){struct net_device  dev = NULL;struct ipv6_mc_socklist  mc_lst;struct ipv6_pinfo  np = inet6_sk(sk);struct net  net = sock_net(sk);int err;ASSERT_RTNL();if (!ipv6_addr_is_multicast(addr))return -EINVAL;for_each_pmc_socklock(np, sk, mc_lst) {if ((ifindex == 0 || mc_lst->ifindex == ifindex) &&    ipv6_addr_equal(&mc_lst->addr, addr))return -EADDRINUSE;}mc_lst = sock_kmalloc(sk, sizeof(struct ipv6_mc_socklist), GFP_KERNEL);if (!mc_lst)return -ENOMEM;mc_lst->next = NULL;mc_lst->addr =  addr;if (ifindex == 0) {struct rt6_info  rt;rt = rt6_lookup(net, addr, NULL, 0, NULL, 0);if (rt) {dev = rt->dst.dev;ip6_rt_put(rt);}} elsedev = __dev_get_by_index(net, ifindex);if (!dev) {sock_kfree_s(sk, mc_lst, sizeof( mc_lst));return -ENODEV;}mc_lst->ifindex = dev->ifindex;mc_lst->sfmode = mode;RCU_INIT_POINTER(mc_lst->sflist, NULL);   now addincrease the group membership on the device ", "#define MLD_QI_DEFAULT(125 * HZ)/* RFC3810, 9.3. Query Response Interval ": "ipv6_dev_mc_inc(struct net_device  dev,     const struct in6_addr  addr, unsigned int mode);#define MLD_QRV_DEFAULT2  RFC3810, 9.2. Query Interval ", "err = -EINVAL;goto done;}/* if a source filter was set, must be the same mode as before ": "ipv6_dev_mc_dec(idev, &mc_lst->addr);} else {ip6_mc_leave_src(sk, mc_lst, NULL);}atomic_sub(sizeof( mc_lst), &sk->sk_omem_alloc);kfree_rcu(mc_lst, rcu);return 0;}}return -EADDRNOTAVAIL;}EXPORT_SYMBOL(ipv6_sock_mc_drop);static struct inet6_dev  ip6_mc_find_dev_rtnl(struct net  net,      const struct in6_addr  group,      int ifindex){struct net_device  dev = NULL;struct inet6_dev  idev = NULL;if (ifindex == 0) {struct rt6_info  rt = rt6_lookup(net, group, NULL, 0, NULL, 0);if (rt) {dev = rt->dst.dev;ip6_rt_put(rt);}} else {dev = __dev_get_by_index(net, ifindex);}if (!dev)return NULL;idev = __in6_dev_get(dev);if (!idev)return NULL;if (idev->dead)return NULL;return idev;}void __ipv6_sock_mc_close(struct sock  sk){struct ipv6_pinfo  np = inet6_sk(sk);struct ipv6_mc_socklist  mc_lst;struct net  net = sock_net(sk);ASSERT_RTNL();while ((mc_lst = sock_dereference(np->ipv6_mc_list, sk)) != NULL) {struct net_device  dev;np->ipv6_mc_list = mc_lst->next;dev = __dev_get_by_index(net, mc_lst->ifindex);if (dev) {struct inet6_dev  idev = __in6_dev_get(dev);ip6_mc_leave_src(sk, mc_lst, idev);if (idev)__ipv6_dev_mc_dec(idev, &mc_lst->addr);} else {ip6_mc_leave_src(sk, mc_lst, NULL);}atomic_sub(sizeof( mc_lst), &sk->sk_omem_alloc);kfree_rcu(mc_lst, rcu);}}void ipv6_sock_mc_close(struct sock  sk){struct ipv6_pinfo  np = inet6_sk(sk);if (!rcu_access_pointer(np->ipv6_mc_list))return;rtnl_lock();lock_sock(sk);__ipv6_sock_mc_close(sk);release_sock(sk);rtnl_unlock();}int ip6_mc_source(int add, int omode, struct sock  sk,struct group_source_req  pgsr){struct in6_addr  source,  group;struct ipv6_mc_socklist  pmc;struct inet6_dev  idev;struct ipv6_pinfo  inet6 = inet6_sk(sk);struct ip6_sf_socklist  psl;struct net  net = sock_net(sk);int i, j, rv;int leavegroup = 0;int err;source = &((struct sockaddr_in6  )&pgsr->gsr_source)->sin6_addr;group = &((struct sockaddr_in6  )&pgsr->gsr_group)->sin6_addr;if (!ipv6_addr_is_multicast(group))return -EINVAL;idev = ip6_mc_find_dev_rtnl(net, group, pgsr->gsr_interface);if (!idev)return -ENODEV;err = -EADDRNOTAVAIL;mutex_lock(&idev->mc_lock);for_each_pmc_socklock(inet6, sk, pmc) {if (pgsr->gsr_interface && pmc->ifindex != pgsr->gsr_interface)continue;if (ipv6_addr_equal(&pmc->addr, group))break;}if (!pmc) {  must have a prior join ", "static struct xt_target ip6t_builtin_tg[] __read_mostly = ": "ip6t_unregister_table_exit(struct net  net, const char  name){struct xt_table  table = xt_find_table(net, NFPROTO_IPV6, name);if (table)__ip6t_unregister_table(net, table);}  The built-in targets: standard (NULL) and error. ", "unsigned int verdict = NF_DROP;const char *indev, *outdev;const void *table_base;struct ip6t_entry *e, **jumpstack;unsigned int stackidx, cpu;const struct xt_table_info *private;struct xt_action_param acpar;unsigned int addend;/* Initialization ": "ip6t_do_table(void  priv, struct sk_buff  skb,      const struct nf_hook_state  state){const struct xt_table  table = priv;unsigned int hook = state->hook;static const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));  Initializing verdict to NF_DROP keeps gcc happy. ", "int tls_client_hello_anon(const struct tls_handshake_args *args, gfp_t flags)": "tls_client_hello_anon - request an anonymous TLS handshake on a socket   @args: socket and handshake parameters for this request   @flags: memory allocation control flags     Return values:     %0: Handshake request enqueue; ->done will be called when complete     %-ESRCH: No user agent is available     %-ENOMEM: Memory allocation failed ", "int tls_client_hello_x509(const struct tls_handshake_args *args, gfp_t flags)": "tls_client_hello_x509 - request an x.509-based TLS handshake on a socket   @args: socket and handshake parameters for this request   @flags: memory allocation control flags     Return values:     %0: Handshake request enqueue; ->done will be called when complete     %-ESRCH: No user agent is available     %-ENOMEM: Memory allocation failed ", "int tls_client_hello_psk(const struct tls_handshake_args *args, gfp_t flags)": "tls_client_hello_psk - request a PSK-based TLS handshake on a socket   @args: socket and handshake parameters for this request   @flags: memory allocation control flags     Return values:     %0: Handshake request enqueue; ->done will be called when complete     %-EINVAL: Wrong number of local peer IDs     %-ESRCH: No user agent is available     %-ENOMEM: Memory allocation failed ", "int tls_server_hello_x509(const struct tls_handshake_args *args, gfp_t flags)": "tls_server_hello_x509 - request a server TLS handshake on a socket   @args: socket and handshake parameters for this request   @flags: memory allocation control flags     Return values:     %0: Handshake request enqueue; ->done will be called when complete     %-ESRCH: No user agent is available     %-ENOMEM: Memory allocation failed ", "int tls_server_hello_psk(const struct tls_handshake_args *args, gfp_t flags)": "tls_server_hello_psk - request a server TLS handshake on a socket   @args: socket and handshake parameters for this request   @flags: memory allocation control flags     Return values:     %0: Handshake request enqueue; ->done will be called when complete     %-ESRCH: No user agent is available     %-ENOMEM: Memory allocation failed ", "bool tls_handshake_cancel(struct sock *sk)": "tls_handshake_cancel - cancel a pending handshake   @sk: socket on which there is an ongoing handshake     Request cancellation races with request completion. To determine   who won, callers examine the return value from this function.     Return values:     %true - Uncompleted handshake request was canceled     %false - Handshake request already completed or not found ", "struct nlmsghdr *handshake_genl_put(struct sk_buff *msg,    struct genl_info *info)": "handshake_genl_put - Create a generic netlink message header   @msg: buffer in which to create the header   @info: generic netlink message context     Returns a ready-to-use header, or NULL. ", "struct handshake_req *handshake_req_alloc(const struct handshake_proto *proto,  gfp_t flags)": "handshake_req_alloc - Allocate a handshake request   @proto: security protocol   @flags: memory allocation flags     Returns an initialized handshake_req or NULL. ", "void *handshake_req_private(struct handshake_req *req)": "handshake_req_private - Get per-handshake private data   @req: handshake arguments   ", "int handshake_req_submit(struct socket *sock, struct handshake_req *req, gfp_t flags)": "handshake_req_submit - Submit a handshake request   @sock: open socket on which to perform the handshake   @req: handshake arguments   @flags: memory allocation flags     Return values:     %0: Request queued     %-EINVAL: Invalid argument     %-EBUSY: A handshake is already under way for this socket     %-ESRCH: No handshake agent is available     %-EAGAIN: Too many pending handshake requests     %-ENOMEM: Failed to allocate memory     %-EMSGSIZE: Failed to construct notification message     %-EOPNOTSUPP: Handshake module not initialized     A zero return value from handshake_req_submit() means that   exactly one subsequent completion callback is guaranteed.     A negative return value from handshake_req_submit() means that   no completion callback will be done and that @req has been   destroyed. ", "bool handshake_req_cancel(struct sock *sk)": "handshake_req_cancel - Cancel an in-progress handshake   @sk: socket on which there is an ongoing handshake     Request cancellation races with request completion. To determine   who won, callers examine the return value from this function.     Return values:     %true - Uncompleted handshake request was canceled     %false - Handshake request already completed or not found ", "void xfrm_policy_destroy(struct xfrm_policy *policy)": "xfrm_policy_destroy_rcu(struct rcu_head  head){struct xfrm_policy  policy = container_of(head, struct xfrm_policy, rcu);security_xfrm_policy_free(policy->security);kfree(policy);}  Destroy xfrm_policy: descendant resources must be released to this moment. ", "static const struct xfrm_if_cb *xfrm_if_get_cb(void)": "xfrm_policy_insert_list(struct hlist_head  chain, struct xfrm_policy  policy,bool excl);static void xfrm_policy_insert_inexact_list(struct hlist_head  chain,    struct xfrm_policy  policy);static boolxfrm_policy_find_inexact_candidates(struct xfrm_pol_inexact_candidates  cand,    struct xfrm_pol_inexact_bin  b,    const xfrm_address_t  saddr,    const xfrm_address_t  daddr);static inline bool xfrm_pol_hold_rcu(struct xfrm_policy  policy){return refcount_inc_not_zero(&policy->refcnt);}static inline bool__xfrm4_selector_match(const struct xfrm_selector  sel, const struct flowi  fl){const struct flowi4  fl4 = &fl->u.ip4;return  addr4_match(fl4->daddr, sel->daddr.a4, sel->prefixlen_d) &&addr4_match(fl4->saddr, sel->saddr.a4, sel->prefixlen_s) &&!((xfrm_flowi_dport(fl, &fl4->uli) ^ sel->dport) & sel->dport_mask) &&!((xfrm_flowi_sport(fl, &fl4->uli) ^ sel->sport) & sel->sport_mask) &&(fl4->flowi4_proto == sel->proto || !sel->proto) &&(fl4->flowi4_oif == sel->ifindex || !sel->ifindex);}static inline bool__xfrm6_selector_match(const struct xfrm_selector  sel, const struct flowi  fl){const struct flowi6  fl6 = &fl->u.ip6;return  addr_match(&fl6->daddr, &sel->daddr, sel->prefixlen_d) &&addr_match(&fl6->saddr, &sel->saddr, sel->prefixlen_s) &&!((xfrm_flowi_dport(fl, &fl6->uli) ^ sel->dport) & sel->dport_mask) &&!((xfrm_flowi_sport(fl, &fl6->uli) ^ sel->sport) & sel->sport_mask) &&(fl6->flowi6_proto == sel->proto || !sel->proto) &&(fl6->flowi6_oif == sel->ifindex || !sel->ifindex);}bool xfrm_selector_match(const struct xfrm_selector  sel, const struct flowi  fl, unsigned short family){switch (family) {case AF_INET:return __xfrm4_selector_match(sel, fl);case AF_INET6:return __xfrm6_selector_match(sel, fl);}return false;}static const struct xfrm_policy_afinfo  xfrm_policy_get_afinfo(unsigned short family){const struct xfrm_policy_afinfo  afinfo;if (unlikely(family >= ARRAY_SIZE(xfrm_policy_afinfo)))return NULL;rcu_read_lock();afinfo = rcu_dereference(xfrm_policy_afinfo[family]);if (unlikely(!afinfo))rcu_read_unlock();return afinfo;}  Called with rcu_read_lock(). ", "list_del(&walk->walk.all);spin_unlock_bh(&net->xfrm.xfrm_policy_lock);}EXPORT_SYMBOL(xfrm_policy_walk_done": "xfrm_policy_walk_done(struct xfrm_policy_walk  walk, struct net  net){if (list_empty(&walk->walk.all))return;spin_lock_bh(&net->xfrm.xfrm_policy_lock);  FIXME where is net? ", "struct xfrm_policy *xfrm_policy_alloc(struct net *net, gfp_t gfp)": "xfrm_policy_delete(xp, dir))km_policy_expired(xp, dir, 1, 0);xfrm_pol_put(xp);}  Allocate xfrm_policy. Not used here, it is supposed to be used by pfkeyv2   SPD calls. ", "if (!if_id && ((dst_orig->flags & DST_NOXFRM) ||       !net->xfrm.policy_count[XFRM_POLICY_OUT]))goto nopol;xdst = xfrm_bundle_lookup(net, fl, family, dir, &xflo, if_id);if (xdst == NULL)goto nopol;if (IS_ERR(xdst)) ": "xfrm_lookup_with_ifid(struct net  net,struct dst_entry  dst_orig,const struct flowi  fl,const struct sock  sk,int flags, u32 if_id){struct xfrm_policy  pols[XFRM_POLICY_TYPE_MAX];struct xfrm_dst  xdst;struct dst_entry  dst,  route;u16 family = dst_orig->ops->family;u8 dir = XFRM_POLICY_OUT;int i, err, num_pols, num_xfrms = 0, drop_pols = 0;dst = NULL;xdst = NULL;route = NULL;sk = sk_const_to_full_sk(sk);if (sk && sk->sk_policy[XFRM_POLICY_OUT]) {num_pols = 1;pols[0] = xfrm_sk_policy_lookup(sk, XFRM_POLICY_OUT, fl, family,if_id);err = xfrm_expand_policies(fl, family, pols,   &num_pols, &num_xfrms);if (err < 0)goto dropdst;if (num_pols) {if (num_xfrms <= 0) {drop_pols = num_pols;goto no_transform;}xdst = xfrm_resolve_and_create_bundle(pols, num_pols, fl,family, dst_orig);if (IS_ERR(xdst)) {xfrm_pols_put(pols, num_pols);err = PTR_ERR(xdst);if (err == -EREMOTE)goto nopol;goto dropdst;} else if (xdst == NULL) {num_xfrms = 0;drop_pols = num_pols;goto no_transform;}route = xdst->route;}}if (xdst == NULL) {struct xfrm_flo xflo;xflo.dst_orig = dst_orig;xflo.flags = flags;  To accelerate a bit...  ", "skb_mark = skb->mark;skb->mark = pol->mark.v;xfrm_decode_session(skb, &fl, skb_dst(skb)->ops->family);skb->mark = skb_mark;dst_hold(xfrm_dst_path(skb_dst(skb)));dst = xfrm_lookup(net, xfrm_dst_path(skb_dst(skb)), &fl, skb->sk, 0);if (IS_ERR(dst)) ": "xfrm_lookup(net, xfrm_dst_path(dst), &fl, sk, XFRM_LOOKUP_QUEUE);if (IS_ERR(dst))goto purge_queue;if (dst->flags & DST_XFRM_QUEUE) {dst_release(dst);if (pq->timeout >= XFRM_QUEUE_TMO_MAX)goto purge_queue;pq->timeout = pq->timeout << 1;if (!mod_timer(&pq->hold_timer, jiffies + pq->timeout))xfrm_pol_hold(pol);goto out;}dst_release(dst);__skb_queue_head_init(&list);spin_lock(&pq->hold_queue.lock);pq->timeout = 0;skb_queue_splice_init(&pq->hold_queue, &list);spin_unlock(&pq->hold_queue.lock);while (!skb_queue_empty(&list)) {skb = __skb_dequeue(&list);  Fixup the mark to support VTI. ", "struct dst_entry *xfrm_lookup_route(struct net *net, struct dst_entry *dst_orig,    const struct flowi *fl,    const struct sock *sk, int flags)": "xfrm_lookup_route() must ensure a call to dst_output().   Otherwise we may send out blackholed packets. ", "sp = skb_sec_path(skb);if (sp) ": "__xfrm_policy_check(struct sock  sk, int dir, struct sk_buff  skb,unsigned short family){struct net  net = dev_net(skb->dev);struct xfrm_policy  pol;struct xfrm_policy  pols[XFRM_POLICY_TYPE_MAX];int npols = 0;int xfrm_nr;int pi;int reverse;struct flowi fl;int xerr_idx = -1;const struct xfrm_if_cb  ifcb;struct sec_path  sp;u32 if_id = 0;rcu_read_lock();ifcb = xfrm_if_get_cb();if (ifcb) {struct xfrm_if_decode_session_result r;if (ifcb->decode_session(skb, family, &r)) {if_id = r.if_id;net = r.net;}}rcu_read_unlock();reverse = dir & ~XFRM_POLICY_MASK;dir &= XFRM_POLICY_MASK;if (__xfrm_decode_session(skb, &fl, family, reverse) < 0) {XFRM_INC_STATS(net, LINUX_MIB_XFRMINHDRERROR);return 0;}nf_nat_decode_session(skb, &fl, family);  First, check used SA against their selectors. ", "match = 1;break;default:break;}}return match;}/* update endpoint address(es) of template(s) ": "xfrm_migrate_selector_match(const struct xfrm_selector  sel_cmp,const struct xfrm_selector  sel_tgt){if (sel_cmp->proto == IPSEC_ULPROTO_ANY) {if (sel_tgt->family == sel_cmp->family &&    xfrm_addr_equal(&sel_tgt->daddr, &sel_cmp->daddr,    sel_cmp->family) &&    xfrm_addr_equal(&sel_tgt->saddr, &sel_cmp->saddr,    sel_cmp->family) &&    sel_tgt->prefixlen_d == sel_cmp->prefixlen_d &&    sel_tgt->prefixlen_s == sel_cmp->prefixlen_s) {return true;}} else {if (memcmp(sel_tgt, sel_cmp, sizeof( sel_tgt)) == 0) {return true;}}return false;}static struct xfrm_policy  xfrm_migrate_policy_find(const struct xfrm_selector  sel,    u8 dir, u8 type, struct net  net, u32 if_id){struct xfrm_policy  pol,  ret = NULL;struct hlist_head  chain;u32 priority = ~0U;spin_lock_bh(&net->xfrm.xfrm_policy_lock);chain = policy_hash_direct(net, &sel->daddr, &sel->saddr, sel->family, dir);hlist_for_each_entry(pol, chain, bydst) {if ((if_id == 0 || pol->if_id == if_id) &&    xfrm_migrate_selector_match(sel, &pol->selector) &&    pol->type == type) {ret = pol;priority = ret->priority;break;}}chain = &net->xfrm.policy_inexact[dir];hlist_for_each_entry(pol, chain, bydst_inexact_list) {if ((pol->priority >= priority) && ret)break;if ((if_id == 0 || pol->if_id == if_id) &&    xfrm_migrate_selector_match(sel, &pol->selector) &&    pol->type == type) {ret = pol;break;}}xfrm_pol_hold(ret);spin_unlock_bh(&net->xfrm.xfrm_policy_lock);return ret;}static int migrate_tmpl_match(const struct xfrm_migrate  m, const struct xfrm_tmpl  t){int match = 0;if (t->mode == m->mode && t->id.proto == m->proto &&    (m->reqid == 0 || t->reqid == m->reqid)) {switch (t->mode) {case XFRM_MODE_TUNNEL:case XFRM_MODE_BEET:if (xfrm_addr_equal(&t->id.daddr, &m->old_daddr,    m->old_family) &&    xfrm_addr_equal(&t->saddr, &m->old_saddr,    m->old_family)) {match = 1;}break;case XFRM_MODE_TRANSPORT:  in case of transport mode, template does not store   any IP addresses, hence we just compare mode and   protocol ", "if (unlikely(seq < bottom))seq_hi++;} else ": "xfrm_replay_seqhi(struct xfrm_state  x, __be32 net_seq){u32 seq, seq_hi, bottom;struct xfrm_replay_state_esn  replay_esn = x->replay_esn;if (!(x->props.flags & XFRM_STATE_ESN))return 0;seq = ntohl(net_seq);seq_hi = replay_esn->seq_hi;bottom = replay_esn->seq - replay_esn->replay_window + 1;if (likely(replay_esn->seq >= replay_esn->replay_window - 1)) {  A. same subspace ", "return sp;/* allocated new secpath ": "secpath_set(struct sk_buff  skb){struct sec_path  sp,  tmp = skb_ext_find(skb, SKB_EXT_SEC_PATH);sp = skb_ext_add(skb, SKB_EXT_SEC_PATH);if (!sp)return NULL;if (tmp)   reused existing one (was COW'd if needed) ", "#include <linux/bottom_half.h>#include <linux/cache.h>#include <linux/interrupt.h>#include <linux/slab.h>#include <linux/module.h>#include <linux/netdevice.h>#include <linux/percpu.h>#include <net/dst.h>#include <net/ip.h>#include <net/xfrm.h>#include <net/ip_tunnels.h>#include <net/ip6_tunnel.h>#include <net/dst_metadata.h>#include \"xfrm_inout.h\"struct xfrm_trans_tasklet ": "xfrm_input.c     Changes:   YOSHIFUJI Hideaki @USAGI   Split up af-specific portion   ", "x->curlft.add_time = now - x->saved_tmo - 1;tmo = x->lft.hard_add_expires_seconds - x->saved_tmo;} elsegoto expired;}if (tmo < next)next = tmo;}if (x->lft.hard_use_expires_seconds) ": "km_query(struct xfrm_state  x, struct xfrm_tmpl  t, struct xfrm_policy  pol);static bool km_is_alive(const struct km_event  c);void km_state_expired(struct xfrm_state  x, int hard, u32 portid);int xfrm_register_type(const struct xfrm_type  type, unsigned short family){struct xfrm_state_afinfo  afinfo = xfrm_state_get_afinfo(family);int err = 0;if (!afinfo)return -EAFNOSUPPORT;#define X(afi, T, name) do {\\WARN_ON((afi)->type_ ## name);\\(afi)->type_ ## name = (T);\\} while (0)switch (type->proto) {case IPPROTO_COMP:X(afinfo, type, comp);break;case IPPROTO_AH:X(afinfo, type, ah);break;case IPPROTO_ESP:X(afinfo, type, esp);break;case IPPROTO_IPIP:X(afinfo, type, ipip);break;case IPPROTO_DSTOPTS:X(afinfo, type, dstopts);break;case IPPROTO_ROUTING:X(afinfo, type, routing);break;case IPPROTO_IPV6:X(afinfo, type, ipip6);break;default:WARN_ON(1);err = -EPROTONOSUPPORT;break;}#undef Xrcu_read_unlock();return err;}EXPORT_SYMBOL(xfrm_register_type);void xfrm_unregister_type(const struct xfrm_type  type, unsigned short family){struct xfrm_state_afinfo  afinfo = xfrm_state_get_afinfo(family);if (unlikely(afinfo == NULL))return;#define X(afi, T, name) do {\\WARN_ON((afi)->type_ ## name != (T));\\(afi)->type_ ## name = NULL;\\} while (0)switch (type->proto) {case IPPROTO_COMP:X(afinfo, type, comp);break;case IPPROTO_AH:X(afinfo, type, ah);break;case IPPROTO_ESP:X(afinfo, type, esp);break;case IPPROTO_IPIP:X(afinfo, type, ipip);break;case IPPROTO_DSTOPTS:X(afinfo, type, dstopts);break;case IPPROTO_ROUTING:X(afinfo, type, routing);break;case IPPROTO_IPV6:X(afinfo, type, ipip6);break;default:WARN_ON(1);break;}#undef Xrcu_read_unlock();}EXPORT_SYMBOL(xfrm_unregister_type);static const struct xfrm_type  xfrm_get_type(u8 proto, unsigned short family){const struct xfrm_type  type = NULL;struct xfrm_state_afinfo  afinfo;int modload_attempted = 0;retry:afinfo = xfrm_state_get_afinfo(family);if (unlikely(afinfo == NULL))return NULL;switch (proto) {case IPPROTO_COMP:type = afinfo->type_comp;break;case IPPROTO_AH:type = afinfo->type_ah;break;case IPPROTO_ESP:type = afinfo->type_esp;break;case IPPROTO_IPIP:type = afinfo->type_ipip;break;case IPPROTO_DSTOPTS:type = afinfo->type_dstopts;break;case IPPROTO_ROUTING:type = afinfo->type_routing;break;case IPPROTO_IPV6:type = afinfo->type_ipip6;break;default:break;}if (unlikely(type && !try_module_get(type->owner)))type = NULL;rcu_read_unlock();if (!type && !modload_attempted) {request_module(\"xfrm-type-%d-%d\", family, proto);modload_attempted = 1;goto retry;}return type;}static void xfrm_put_type(const struct xfrm_type  type){module_put(type->owner);}int xfrm_register_type_offload(const struct xfrm_type_offload  type,       unsigned short family){struct xfrm_state_afinfo  afinfo = xfrm_state_get_afinfo(family);int err = 0;if (unlikely(afinfo == NULL))return -EAFNOSUPPORT;switch (type->proto) {case IPPROTO_ESP:WARN_ON(afinfo->type_offload_esp);afinfo->type_offload_esp = type;break;default:WARN_ON(1);err = -EPROTONOSUPPORT;break;}rcu_read_unlock();return err;}EXPORT_SYMBOL(xfrm_register_type_offload);void xfrm_unregister_type_offload(const struct xfrm_type_offload  type,  unsigned short family){struct xfrm_state_afinfo  afinfo = xfrm_state_get_afinfo(family);if (unlikely(afinfo == NULL))return;switch (type->proto) {case IPPROTO_ESP:WARN_ON(afinfo->type_offload_esp != type);afinfo->type_offload_esp = NULL;break;default:WARN_ON(1);break;}rcu_read_unlock();}EXPORT_SYMBOL(xfrm_unregister_type_offload);static const struct xfrm_type_offload  xfrm_get_type_offload(u8 proto, unsigned short family, bool try_load){const struct xfrm_type_offload  type = NULL;struct xfrm_state_afinfo  afinfo;retry:afinfo = xfrm_state_get_afinfo(family);if (unlikely(afinfo == NULL))return NULL;switch (proto) {case IPPROTO_ESP:type = afinfo->type_offload_esp;break;default:break;}if ((type && !try_module_get(type->owner)))type = NULL;rcu_read_unlock();if (!type && try_load) {request_module(\"xfrm-offload-%d-%d\", family, proto);try_load = false;goto retry;}return type;}static void xfrm_put_type_offload(const struct xfrm_type_offload  type){module_put(type->owner);}static const struct xfrm_mode xfrm4_mode_map[XFRM_MODE_MAX] = {[XFRM_MODE_BEET] = {.encap = XFRM_MODE_BEET,.flags = XFRM_MODE_FLAG_TUNNEL,.family = AF_INET,},[XFRM_MODE_TRANSPORT] = {.encap = XFRM_MODE_TRANSPORT,.family = AF_INET,},[XFRM_MODE_TUNNEL] = {.encap = XFRM_MODE_TUNNEL,.flags = XFRM_MODE_FLAG_TUNNEL,.family = AF_INET,},};static const struct xfrm_mode xfrm6_mode_map[XFRM_MODE_MAX] = {[XFRM_MODE_BEET] = {.encap = XFRM_MODE_BEET,.flags = XFRM_MODE_FLAG_TUNNEL,.family = AF_INET6,},[XFRM_MODE_ROUTEOPTIMIZATION] = {.encap = XFRM_MODE_ROUTEOPTIMIZATION,.family = AF_INET6,},[XFRM_MODE_TRANSPORT] = {.encap = XFRM_MODE_TRANSPORT,.family = AF_INET6,},[XFRM_MODE_TUNNEL] = {.encap = XFRM_MODE_TUNNEL,.flags = XFRM_MODE_FLAG_TUNNEL,.family = AF_INET6,},};static const struct xfrm_mode  xfrm_get_mode(unsigned int encap, int family){const struct xfrm_mode  mode;if (unlikely(encap >= XFRM_MODE_MAX))return NULL;switch (family) {case AF_INET:mode = &xfrm4_mode_map[encap];if (mode->family == family)return mode;break;case AF_INET6:mode = &xfrm6_mode_map[encap];if (mode->family == family)return mode;break;default:break;}return NULL;}void xfrm_state_free(struct xfrm_state  x){kmem_cache_free(xfrm_state_cache, x);}EXPORT_SYMBOL(xfrm_state_free);static void ___xfrm_state_destroy(struct xfrm_state  x){hrtimer_cancel(&x->mtimer);del_timer_sync(&x->rtimer);kfree(x->aead);kfree(x->aalg);kfree(x->ealg);kfree(x->calg);kfree(x->encap);kfree(x->coaddr);kfree(x->replay_esn);kfree(x->preplay_esn);if (x->type_offload)xfrm_put_type_offload(x->type_offload);if (x->type) {x->type->destructor(x);xfrm_put_type(x->type);}if (x->xfrag.page)put_page(x->xfrag.page);xfrm_dev_state_free(x);security_xfrm_state_free(x);xfrm_state_free(x);}static void xfrm_state_gc_task(struct work_struct  work){struct xfrm_state  x;struct hlist_node  tmp;struct hlist_head gc_list;spin_lock_bh(&xfrm_state_gc_lock);hlist_move_list(&xfrm_state_gc_list, &gc_list);spin_unlock_bh(&xfrm_state_gc_lock);synchronize_rcu();hlist_for_each_entry_safe(x, tmp, &gc_list, gclist)___xfrm_state_destroy(x);}static enum hrtimer_restart xfrm_timer_handler(struct hrtimer  me){struct xfrm_state  x = container_of(me, struct xfrm_state, mtimer);enum hrtimer_restart ret = HRTIMER_NORESTART;time64_t now = ktime_get_real_seconds();time64_t next = TIME64_MAX;int warn = 0;int err = 0;spin_lock(&x->lock);xfrm_dev_state_update_curlft(x);if (x->km.state == XFRM_STATE_DEAD)goto out;if (x->km.state == XFRM_STATE_EXPIRED)goto expired;if (x->lft.hard_add_expires_seconds) {time64_t tmo = x->lft.hard_add_expires_seconds +x->curlft.add_time - now;if (tmo <= 0) {if (x->xflags & XFRM_SOFT_EXPIRE) {  enter hard expire without soft expire first?!   setting a new date could trigger this.   workaround: fix x->curflt.add_time by below: ", "static void __xfrm_state_bump_genids(struct xfrm_state *xnew)": "xfrm_state_walk  w;spin_lock_bh(&net->xfrm.xfrm_state_lock);list_for_each_entry(w, &net->xfrm.state_all, all) {x = container_of(w, struct xfrm_state, km);if (x->props.family != family ||x->id.spi != spi)continue;xfrm_state_hold(x);spin_unlock_bh(&net->xfrm.xfrm_state_lock);return x;}spin_unlock_bh(&net->xfrm.xfrm_state_lock);return NULL;}EXPORT_SYMBOL(xfrm_state_lookup_byspi);static void __xfrm_state_insert(struct xfrm_state  x){struct net  net = xs_net(x);unsigned int h;list_add(&x->km.all, &net->xfrm.state_all);h = xfrm_dst_hash(net, &x->id.daddr, &x->props.saddr,  x->props.reqid, x->props.family);XFRM_STATE_INSERT(bydst, &x->bydst, net->xfrm.state_bydst + h,  x->xso.type);h = xfrm_src_hash(net, &x->id.daddr, &x->props.saddr, x->props.family);XFRM_STATE_INSERT(bysrc, &x->bysrc, net->xfrm.state_bysrc + h,  x->xso.type);if (x->id.spi) {h = xfrm_spi_hash(net, &x->id.daddr, x->id.spi, x->id.proto,  x->props.family);XFRM_STATE_INSERT(byspi, &x->byspi, net->xfrm.state_byspi + h,  x->xso.type);}if (x->km.seq) {h = xfrm_seq_hash(net, x->km.seq);XFRM_STATE_INSERT(byseq, &x->byseq, net->xfrm.state_byseq + h,  x->xso.type);}hrtimer_start(&x->mtimer, ktime_set(1, 0), HRTIMER_MODE_REL_SOFT);if (x->replay_maxage)mod_timer(&x->rtimer, jiffies + x->replay_maxage);net->xfrm.state_num++;xfrm_hash_grow_check(net, x->bydst.next != NULL);}  net->xfrm.xfrm_state_lock is held ", "break;/* Packet offload: both policy and SA should * have same device. ": "xfrm_state_lookup_all(struct net  net, u32 mark,  const xfrm_address_t  daddr,  __be32 spi, u8 proto,  unsigned short family,  struct xfrm_dev_offload  xdo){unsigned int h = xfrm_spi_hash(net, daddr, spi, proto, family);struct xfrm_state  x;hlist_for_each_entry_rcu(x, net->xfrm.state_byspi + h, byspi) {#ifdef CONFIG_XFRM_OFFLOADif (xdo->type == XFRM_DEV_OFFLOAD_PACKET) {if (x->xso.type != XFRM_DEV_OFFLOAD_PACKET)  HW states are in the head of list, there is   no need to iterate further. ", "if (xfrm_addr_equal(&x->id.daddr, &m->new_daddr, m->new_family)) ": "xfrm_init_state(xc) < 0)goto error;memcpy(&xc->id.daddr, &m->new_daddr, sizeof(xc->id.daddr));memcpy(&xc->props.saddr, &m->new_saddr, sizeof(xc->props.saddr));  add state ", "if (x->km.state == XFRM_STATE_VALID) ": "xfrm_state_lookup_byaddr(struct net  net, u32 mark,     const xfrm_address_t  daddr,     const xfrm_address_t  saddr,     u8 proto, unsigned short family){unsigned int h = xfrm_src_hash(net, daddr, saddr, family);struct xfrm_state  x;hlist_for_each_entry_rcu(x, net->xfrm.state_bysrc + h, bysrc) {if (x->props.family != family ||    x->id.proto     != proto ||    !xfrm_addr_equal(&x->id.daddr, daddr, family) ||    !xfrm_addr_equal(&x->props.saddr, saddr, family))continue;if ((mark & x->mark.m) != x->mark.v)continue;if (!xfrm_state_hold_rcu(x))continue;return x;}return NULL;}static inline struct xfrm_state  __xfrm_state_locate(struct xfrm_state  x, int use_spi, int family){struct net  net = xs_net(x);u32 mark = x->mark.v & x->mark.m;if (use_spi)return __xfrm_state_lookup(net, mark, &x->id.daddr,   x->id.spi, x->id.proto, family);elsereturn __xfrm_state_lookup_byaddr(net, mark,  &x->id.daddr,  &x->props.saddr,  x->id.proto, family);}static void xfrm_hash_grow_check(struct net  net, int have_hash_collision){if (have_hash_collision &&    (net->xfrm.state_hmask + 1) < xfrm_state_hashmax &&    net->xfrm.state_num > net->xfrm.state_hmask)schedule_work(&net->xfrm.state_hash_work);}static void xfrm_state_look_at(struct xfrm_policy  pol, struct xfrm_state  x,       const struct flowi  fl, unsigned short family,       struct xfrm_state   best, int  acq_in_progress,       int  error){  Resolution logic:   1. There is a valid state with matching selector. Done.   2. Valid state with inappropriate selector. Skip.     Entering area of \"sysdeps\".     3. If state is not valid, selector is temporary, it selects      only session which triggered previous resolution. Key      manager will do something to install a state with proper      selector. ", "if (max >= 0x10000) ": "verify_spi_info(u8 proto, u32 min, u32 max, struct netlink_ext_ack  extack){switch (proto) {case IPPROTO_AH:case IPPROTO_ESP:break;case IPPROTO_COMP:  IPCOMP spi is 16-bits. ", "return 0;#endif /* CONFIG_CGROUP_DEVICE ": "devcgroup_check_permission(short type, u32 major, u32 minor, short access){int rc = BPF_CGROUP_RUN_PROG_DEVICE_CGROUP(type, major, minor, access);if (rc)return rc;#ifdef CONFIG_CGROUP_DEVICEreturn devcgroup_legacy_check_permission(type, major, minor, access);#else   CONFIG_CGROUP_DEVICE ", "void security_free_mnt_opts(void **mnt_opts)": "security_free_mnt_opts() - Free memory associated with mount options   @mnt_opts: LSM processed mount options     Free memory associated with @mnt_ops. ", "int security_sb_eat_lsm_opts(char *options, void **mnt_opts)": "security_sb_eat_lsm_opts() - Consume LSM mount options   @options: mount options   @mnt_opts: LSM processed mount options     Eat (scan @options) and save them in @mnt_opts.     Return: Returns 0 on success, negative values on failure. ", "int security_sb_mnt_opts_compat(struct super_block *sb,void *mnt_opts)": "security_sb_mnt_opts_compat() - Check if new mount options are allowed   @sb: filesystem superblock   @mnt_opts: new mount options     Determine if the new mount options in @mnt_opts are allowed given the   existing mounted filesystem at @sb.  @sb superblock being compared.     Return: Returns 0 if options are compatible. ", "int security_sb_remount(struct super_block *sb,void *mnt_opts)": "security_sb_remount() - Verify no incompatible mount changes during remount   @sb: filesystem superblock   @mnt_opts: (re)mount options     Extracts security system specific mount options and verifies no changes are   being made to those options.     Return: Returns 0 if permission is granted. ", "int security_sb_set_mnt_opts(struct super_block *sb,     void *mnt_opts,     unsigned long kern_flags,     unsigned long *set_kern_flags)": "security_sb_set_mnt_opts() - Set the mount options for a filesystem   @sb: filesystem superblock   @mnt_opts: binary mount options   @kern_flags: kernel flags (in)   @set_kern_flags: kernel flags (out)     Set the security relevant mount options used for a superblock.     Return: Returns 0 on success, error on failure. ", "int security_sb_clone_mnt_opts(const struct super_block *oldsb,       struct super_block *newsb,       unsigned long kern_flags,       unsigned long *set_kern_flags)": "security_sb_clone_mnt_opts() - Duplicate superblock mount options   @oldsb: source superblock   @newsb: destination superblock   @kern_flags: kernel flags (in)   @set_kern_flags: kernel flags (out)     Copy all security options from a given superblock to another.     Return: Returns 0 on success, error on failure. ", "int security_dentry_init_security(struct dentry *dentry, int mode,  const struct qstr *name,  const char **xattr_name, void **ctx,  u32 *ctxlen)": "security_dentry_init_security() - Perform dentry initialization   @dentry: the dentry to initialize   @mode: mode used to determine resource type   @name: name of the last path component   @xattr_name: name of the securityLSM xattr   @ctx: pointer to the resulting LSM context   @ctxlen: length of @ctx     Compute a context for a dentry as the inode is not yet available since NFSv4   has no label backed by an EA anyway.  It is important to note that   @xattr_name does not need to be free'd by the caller, it is a static string.     Return: Returns 0 on success, negative values on failure. ", "int security_dentry_create_files_as(struct dentry *dentry, int mode,    struct qstr *name,    const struct cred *old, struct cred *new)": "security_dentry_create_files_as() - Perform dentry initialization   @dentry: the dentry to initialize   @mode: mode used to determine resource type   @name: name of the last path component   @old: creds to use for LSM context calculations   @new: creds to modify     Compute a context for a dentry as the inode is not yet available and set   that context in passed in creds so that new files are created using that   context. Context is calculated using the passed in creds and not the creds   of the caller.     Return: Returns 0 on success, error on failure. ", "int security_inode_init_security(struct inode *inode, struct inode *dir, const struct qstr *qstr, const initxattrs initxattrs, void *fs_data)": "security_inode_init_security() - Initialize an inode's LSM context   @inode: the inode   @dir: parent directory   @qstr: last component of the pathname   @initxattrs: callback function to write xattrs   @fs_data: filesystem specific data     Obtain the security attribute name suffix and value to set on a newly   created inode and set up the incore security field for the new inode.  This   hook is called by the fs code as part of the inode creation transaction and   provides for atomic labeling of the inode, unlike the post_createmkdir...   hooks called by the VFS.  The hook function is expected to allocate the name   and value via kmalloc, with the caller being responsible for calling kfree   after using them.  If the security module does not use security attributes   or does not wish to put a security attribute on this particular inode, then   it should return -EOPNOTSUPP to skip this processing.     Return: Returns 0 on success, -EOPNOTSUPP if no security attribute is   needed, or -ENOMEM on memory allocation failure. ", "int security_path_mknod(const struct path *dir, struct dentry *dentry,umode_t mode, unsigned int dev)": "security_path_mknod() - Check if creating a special file is allowed   @dir: parent directory   @dentry: new file   @mode: new file mode   @dev: device number     Check permissions when creating a file. Note that this hook is called even   if mknod operation is being done for a regular file.     Return: Returns 0 if permission is granted. ", "int security_path_mkdir(const struct path *dir, struct dentry *dentry,umode_t mode)": "security_path_mkdir() - Check if creating a new directory is allowed   @dir: parent directory   @dentry: new directory   @mode: new directory mode     Check permissions to create a new directory in the existing directory.     Return: Returns 0 if permission is granted. ", "int security_path_unlink(const struct path *dir, struct dentry *dentry)": "security_path_unlink() - Check if removing a hard link is allowed   @dir: parent directory   @dentry: file     Check the permission to remove a hard link to a file.     Return: Returns 0 if permission is granted. ", "int security_path_rename(const struct path *old_dir, struct dentry *old_dentry, const struct path *new_dir, struct dentry *new_dentry, unsigned int flags)": "security_path_rename() - Check if renaming a file is allowed   @old_dir: parent directory of the old file   @old_dentry: the old file   @new_dir: parent directory of the new file   @new_dentry: the new file   @flags: flags     Check for permission to rename a file or directory.     Return: Returns 0 if permission is granted. ", "int security_inode_listsecurity(struct inode *inode,char *buffer, size_t buffer_size)": "security_inode_listsecurity() - List the xattr security label names   @inode: inode   @buffer: buffer   @buffer_size: size of buffer     Copy the extended attribute names for the security labels associated with   @inode into @buffer.  The maximum size of @buffer is specified by   @buffer_size.  @buffer may be NULL to request the size of the buffer   required.     Return: Returns number of bytes usedrequired on success. ", "int security_inode_copy_up(struct dentry *src, struct cred **new)": "security_inode_copy_up() - Create new creds for an overlayfs copy-up op   @src: union dentry of copy-up file   @new: newly created creds     A file is about to be copied up from lower layer to upper layer of overlay   filesystem. Security module can prepare a set of new creds and modify as   need be and return new creds. Caller will switch to new creds temporarily to   create new file and release newly allocated creds.     Return: Returns 0 on success or a negative error code on error. ", "int security_inode_copy_up_xattr(const char *name)": "security_inode_copy_up_xattr() - Filter xattrs in an overlayfs copy-up op   @name: xattr name     Filter the xattrs being copied up when a unioned file is copied up from a   lower layer to the unionoverlay layer.   The caller is responsible for   reading and writing the xattrs, this hook is merely a filter.     Return: Returns 0 to accept the xattr, 1 to discard the xattr, -EOPNOTSUPP           if the security module does not know about attribute, or a negative           error code to abort the copy up. ", "void security_cred_getsecid(const struct cred *c, u32 *secid)": "security_cred_getsecid() - Get the secid from a set of credentials   @c: credentials   @secid: secid value     Retrieve the security identifier of the cred structure @c.  In case of   failure, @secid will be set to zero. ", "void security_current_getsecid_subj(u32 *secid)": "security_current_getsecid_subj() - Get the current task's subjective secid   @secid: secid value     Retrieve the subjective security identifier of the current task and return   it in @secid.  In case of failure, @secid will be set to zero. ", "void security_task_getsecid_obj(struct task_struct *p, u32 *secid)": "security_task_getsecid_obj() - Get a task's objective secid   @p: target task   @secid: secid value     Retrieve the objective security identifier of the task_struct in @p and   return it in @secid. In case of failure, @secid will be set to zero. ", "void security_d_instantiate(struct dentry *dentry, struct inode *inode)": "security_d_instantiate() - Populate an inode's LSM state based on a dentry   @dentry: dentry   @inode: inode     Fill in @inode security information for a @dentry if allowed. ", "int security_ismaclabel(const char *name)": "security_ismaclabel() - Check is the named attribute is a MAC label   @name: full extended attribute name     Check if the extended attribute specified by @name represents a MAC label.     Return: Returns 1 if name is a MAC attribute otherwise returns 0. ", "int security_secid_to_secctx(u32 secid, char **secdata, u32 *seclen)": "security_secid_to_secctx() - Convert a secid to a secctx   @secid: secid   @secdata: secctx   @seclen: secctx length     Convert secid to security context.  If @secdata is NULL the length of the   result will be returned in @seclen, but no @secdata will be returned.  This   does mean that the length could change between calls to check the length and   the next call which actually allocates and returns the @secdata.     Return: Return 0 on success, error on failure. ", "int security_secctx_to_secid(const char *secdata, u32 seclen, u32 *secid)": "security_secctx_to_secid() - Convert a secctx to a secid   @secdata: secctx   @seclen: length of secctx   @secid: secid     Convert security context to secid.     Return: Returns 0 on success, error on failure. ", "void security_release_secctx(char *secdata, u32 seclen)": "security_release_secctx() - Free a secctx buffer   @secdata: secctx   @seclen: length of secctx     Release the security context. ", "void security_inode_invalidate_secctx(struct inode *inode)": "security_inode_invalidate_secctx() - Invalidate an inode's security label   @inode: inode     Notify the security module that it must revalidate the security context of   an inode. ", "int security_inode_notifysecctx(struct inode *inode, void *ctx, u32 ctxlen)": "security_inode_notifysecctx() - Nofify the LSM of an inode's security label   @inode: inode   @ctx: secctx   @ctxlen: length of secctx     Notify the security module of what the security context of an inode should   be.  Initializes the incore security context managed by the security module   for this inode.  Example usage: NFS client invokes this hook to initialize   the security context in its incore inode to the value provided by the server   for the file when the server returned the file's attributes to the client.   Must be called with inode->i_mutex locked.     Return: Returns 0 on success, error on failure. ", "int security_inode_setsecctx(struct dentry *dentry, void *ctx, u32 ctxlen)": "security_inode_setsecctx() - Change the security label of an inode   @dentry: inode   @ctx: secctx   @ctxlen: length of secctx     Change the security context of an inode.  Updates the incore security   context managed by the security module and invokes the fs code as needed   (via __vfs_setxattr_noperm) to update any backing xattrs that represent the   context.  Example usage: NFS server invokes this hook to change the security   context in its incore inode and on the backing filesystem to a value   provided by the client on a SETATTR operation.  Must be called with   inode->i_mutex locked.     Return: Returns 0 on success, error on failure. ", "int security_inode_getsecctx(struct inode *inode, void **ctx, u32 *ctxlen)": "security_inode_getsecctx() - Get the security label of an inode   @inode: inode   @ctx: secctx   @ctxlen: length of secctx     On success, returns 0 and fills out @ctx and @ctxlen with the security   context for the given @inode.     Return: Returns 0 on success, error on failure. ", "int security_unix_stream_connect(struct sock *sock, struct sock *other, struct sock *newsk)": "security_unix_stream_connect() - Check if a AF_UNIX stream is allowed   @sock: originating sock   @other: peer sock   @newsk: new sock     Check permissions before establishing a Unix domain stream connection   between @sock and @other.     The @unix_stream_connect and @unix_may_send hooks were necessary because   Linux provides an alternative to the conventional file name space for Unix   domain sockets.  Whereas binding and connecting to sockets in the file name   space is mediated by the typical file permissions (and caught by the mknod   and permission hooks in inode_security_ops), binding and connecting to   sockets in the abstract name space is completely unmediated.  Sufficient   control of Unix domain sockets in the abstract name space isn't possible   using only the socket layer hooks, since we need to know the actual target   socket, which is not looked up until we are inside the af_unix code.     Return: Returns 0 if permission is granted. ", "int security_unix_may_send(struct socket *sock,  struct socket *other)": "security_unix_may_send() - Check if AF_UNIX socket can send datagrams   @sock: originating sock   @other: peer sock     Check permissions before connecting or sending datagrams from @sock to   @other.     The @unix_stream_connect and @unix_may_send hooks were necessary because   Linux provides an alternative to the conventional file name space for Unix   domain sockets.  Whereas binding and connecting to sockets in the file name   space is mediated by the typical file permissions (and caught by the mknod   and permission hooks in inode_security_ops), binding and connecting to   sockets in the abstract name space is completely unmediated.  Sufficient   control of Unix domain sockets in the abstract name space isn't possible   using only the socket layer hooks, since we need to know the actual target   socket, which is not looked up until we are inside the af_unix code.     Return: Returns 0 if permission is granted. ", "int security_socket_socketpair(struct socket *socka, struct socket *sockb)": "security_socket_socketpair() - Check if creating a socketpair is allowed   @socka: first socket   @sockb: second socket     Check permissions before creating a fresh pair of sockets.     Return: Returns 0 if permission is granted and the connection was           established. ", "int security_sock_rcv_skb(struct sock *sk, struct sk_buff *skb)": "security_sock_rcv_skb() - Check if an incoming network packet is allowed   @sk: destination sock   @skb: incoming packet     Check permissions on incoming network packets.  This hook is distinct from   Netfilter's IP input hooks since it is the first time that the incoming   sk_buff @skb has been associated with a particular socket, @sk.  Must not   sleep inside this hook because some callers hold spinlocks.     Return: Returns 0 if permission is granted. ", "int security_socket_getpeersec_dgram(struct socket *sock,     struct sk_buff *skb, u32 *secid)": "security_socket_getpeersec_dgram() - Get the remote peer label   @sock: socket   @skb: datagram packet   @secid: remote peer label secid     This hook allows the security module to provide peer socket security state   for udp sockets on a per-packet basis to userspace via getsockopt   SO_GETPEERSEC. The application must first have indicated the IP_PASSSEC   option via getsockopt. It can then retrieve the security state returned by   this hook for a packet via the SCM_SECURITY ancillary message type.     Return: Returns 0 on success, error on failure. ", "void security_sk_clone(const struct sock *sk, struct sock *newsk)": "security_sk_clone() - Clone a sock's LSM state   @sk: original sock   @newsk: target sock     Clonecopy security structure. ", "void security_req_classify_flow(const struct request_sock *req,struct flowi_common *flic)": "security_req_classify_flow() - Set a flow's secid based on request_sock   @req: request_sock   @flic: target flow     Sets @flic's secid to @req's secid. ", "void security_sock_graft(struct sock *sk, struct socket *parent)": "security_sock_graft() - Reconcile LSM state when grafting a sock on a socket   @sk: sock being grafted   @parent: target parent socket     Sets @parent's inode secid to @sk's secid and update @sk with any necessary   LSM state from @parent. ", "int security_inet_conn_request(const struct sock *sk,       struct sk_buff *skb, struct request_sock *req)": "security_inet_conn_request() - Set request_sock state using incoming connect   @sk: parent listening sock   @skb: incoming connection   @req: new request_sock     Initialize the @req LSM state based on @sk and the incoming connect in @skb.     Return: Returns 0 if permission is granted. ", "void security_inet_conn_established(struct sock *sk,    struct sk_buff *skb)": "security_inet_conn_established() - Update sock's LSM state with connection   @sk: sock   @skb: connection packet     Update @sock's LSM state to represent a new connection from @skb. ", "int security_secmark_relabel_packet(u32 secid)": "security_secmark_relabel_packet() - Check if setting a secmark is allowed   @secid: new secmark value     Check if the process should be allowed to relabel packets to @secid.     Return: Returns 0 if permission is granted. ", "void security_secmark_refcount_inc(void)": "security_secmark_refcount_inc() - Increment the secmark labeling rule count     Tells the LSM to increment the number of secmark labeling rules loaded. ", "void security_secmark_refcount_dec(void)": "security_secmark_refcount_dec() - Decrement the secmark labeling rule count     Tells the LSM to decrement the number of secmark labeling rules loaded. ", "int security_tun_dev_alloc_security(void **security)": "security_tun_dev_alloc_security() - Allocate a LSM blob for a TUN device   @security: pointer to the LSM blob     This hook allows a module to allocate a security structure for a TUNdevice,   returning the pointer in @security.     Return: Returns a zero on success, negative values on failure. ", "void security_tun_dev_free_security(void *security)": "security_tun_dev_free_security() - Free a TUN device LSM blob   @security: LSM blob     This hook allows a module to free the security structure for a TUN device. ", "int security_tun_dev_create(void)": "security_tun_dev_create() - Check if creating a TUN device is allowed     Check permissions prior to creating a new TUN device.     Return: Returns 0 if permission is granted. ", "int security_tun_dev_attach_queue(void *security)": "security_tun_dev_attach_queue() - Check if attaching a TUN queue is allowed   @security: TUN device LSM blob     Check permissions prior to attaching to a TUN device queue.     Return: Returns 0 if permission is granted. ", "int security_tun_dev_open(void *security)": "security_tun_dev_open() - Update TUN device LSM state on open   @security: TUN device LSM blob     This hook can be used by the module to update any security state associated   with the TUN device's security structure.     Return: Returns 0 if permission is granted. ", "int security_sctp_assoc_request(struct sctp_association *asoc,struct sk_buff *skb)": "security_sctp_assoc_request() - Update the LSM on a SCTP association req   @asoc: SCTP association   @skb: packet requesting the association     Passes the @asoc and @chunk->skb of the association INIT packet to the LSM.     Return: Returns 0 on success, error on failure. ", "int security_sctp_bind_connect(struct sock *sk, int optname,       struct sockaddr *address, int addrlen)": "security_sctp_bind_connect() - Validate a list of addrs for a SCTP option   @sk: socket   @optname: SCTP option to validate   @address: list of IP addresses to validate   @addrlen: length of the address list     Validiate permissions required for each address associated with sock@sk.   Depending on @optname, the addresses will be treated as either a connect or   bind service. The @addrlen is calculated on each IPv4 and IPv6 address using   sizeof(struct sockaddr_in) or sizeof(struct sockaddr_in6).     Return: Returns 0 on success, error on failure. ", "void security_sctp_sk_clone(struct sctp_association *asoc, struct sock *sk,    struct sock *newsk)": "security_sctp_sk_clone() - Clone a SCTP sock's LSM state   @asoc: SCTP association   @sk: original sock   @newsk: target sock     Called whenever a new socket is created by accept(2) (i.e. a TCP style   socket) or when a socket is 'peeled off' e.g userspace calls   sctp_peeloff(3). ", "int security_sctp_assoc_established(struct sctp_association *asoc,    struct sk_buff *skb)": "security_sctp_assoc_established() - Update LSM state when assoc established   @asoc: SCTP association   @skb: packet establishing the association     Passes the @asoc and @chunk->skb of the association COOKIE_ACK packet to the   security module.     Return: Returns 0 if permission is granted. ", "int security_ib_pkey_access(void *sec, u64 subnet_prefix, u16 pkey)": "security_ib_pkey_access() - Check if access to an IB pkey is allowed   @sec: LSM blob   @subnet_prefix: subnet prefix of the port   @pkey: IB pkey     Check permission to access a pkey when modifying a QP.     Return: Returns 0 if permission is granted. ", "int security_ib_endport_manage_subnet(void *sec,      const char *dev_name, u8 port_num)": "security_ib_endport_manage_subnet() - Check if SMPs traffic is allowed   @sec: LSM blob   @dev_name: IB device name   @port_num: port number     Check permissions to send and receive SMPs on a end port.     Return: Returns 0 if permission is granted. ", "int security_ib_alloc_security(void **sec)": "security_ib_alloc_security() - Allocate an Infiniband LSM blob   @sec: LSM blob     Allocate a security structure for Infiniband objects.     Return: Returns 0 on success, non-zero on failure. ", "void security_ib_free_security(void *sec)": "security_ib_free_security() - Free an Infiniband LSM blob   @sec: LSM blob     Deallocate an Infiniband security structure. ", "int security_xfrm_policy_alloc(struct xfrm_sec_ctx **ctxp,       struct xfrm_user_sec_ctx *sec_ctx,       gfp_t gfp)": "security_xfrm_policy_alloc() - Allocate a xfrm policy LSM blob   @ctxp: xfrm security context being added to the SPD   @sec_ctx: security label provided by userspace   @gfp: gfp flags     Allocate a security structure to the xp->security field; the security field   is initialized to NULL when the xfrm_policy is allocated.     Return:  Return 0 if operation was successful. ", "void security_xfrm_policy_free(struct xfrm_sec_ctx *ctx)": "security_xfrm_policy_free() - Free a xfrm security context   @ctx: xfrm security context     Free LSM resources associated with @ctx. ", "int security_xfrm_state_alloc(struct xfrm_state *x,      struct xfrm_user_sec_ctx *sec_ctx)": "security_xfrm_state_alloc() - Allocate a xfrm state LSM blob   @x: xfrm state being added to the SAD   @sec_ctx: security label provided by userspace     Allocate a security structure to the @x->security field; the security field   is initialized to NULL when the xfrm_state is allocated. Set the context to   correspond to @sec_ctx.     Return: Return 0 if operation was successful. ", "int security_xfrm_state_delete(struct xfrm_state *x)": "security_xfrm_state_delete() - Check if deleting a xfrm state is allowed   @x: xfrm state     Authorize deletion of x->security.     Return: Returns 0 if permission is granted. ", "const char *const lockdown_reasons[LOCKDOWN_CONFIDENTIALITY_MAX + 1] = ": "security_locked_down() LSM hook. Placing this array here allows   all security modules to use the same descriptions for auditing   purposes. ", "struct key_type key_type_logon = ": "user_revoke,.destroy= user_destroy,.describe= user_describe,.read= user_read,};EXPORT_SYMBOL_GPL(key_type_user);    This key type is essentially the same as key_type_user, but it does   not define a .read op. This is suitable for storing username and   password pairs in the keyring that you do not want to be readable   from userspace. ", "key_ref_t lookup_user_key(key_serial_t id, unsigned long lflags,  enum key_need_perm need_perm)": "lookup_user_key_possessed(const struct key  key,       const struct key_match_data  match_data){return key == match_data->raw_data;}    Look up a key ID given us by userspace with a given permissions mask to get   the key it refers to.     Flags can be passed to request that special keyrings be created if referred   to directly, to permit partially constructed keys to be found and to skip   validity and permission checks on the found key.     Returns a pointer to the key with an incremented usage count if successful;   -EINVAL if the key ID is invalid; -ENOKEY if the key ID does not correspond   to a key or the best found key was a negative key; -EKEYREVOKED or   -EKEYEXPIRED if the best found key was revoked or expired; -EACCES if the   found key doesn't grant the requested permit or the LSM denied access to it;   or -ENOMEM if a special keyring couldn't be created.     In the case of a successful return, the possession attribute is set on the   returned key reference. ", "int key_task_permission(const key_ref_t key_ref, const struct cred *cred,enum key_need_perm need_perm)": "key_task_permission - Check a key can be used   @key_ref: The key to check.   @cred: The credentials to use.   @need_perm: The permission required.     Check to see whether permission is granted to use a key in the desired way,   but permit the security modules to override.     The caller must hold either a ref on cred or must hold the RCU readlock.     Returns 0 if successful, -EACCES if access is denied based on the   permissions bits or the LSM check. ", "int key_validate(const struct key *key)": "key_validate - Validate a key.   @key: The key to be validated.     Check that a key is valid, returning 0 if the key is okay, -ENOKEY if the   key is invalidated, -EKEYREVOKED if the key's type has been removed or if   the key has been revoked or -EKEYEXPIRED if the key has expired. ", "void key_free_user_ns(struct user_namespace *ns)": "key_type_keyring)return (void  )((unsigned long)key | KEYRING_PTR_SUBTYPE);return key;}static DEFINE_RWLOCK(keyring_name_lock);    Clean up the bits of user_namespace that belong to us. ", "if (key->type != ctx->index_key.type) ": "keyring_search_iterator(const void  object, void  iterator_data){struct keyring_search_context  ctx = iterator_data;const struct key  key = keyring_ptr_to_key(object);unsigned long kflags = READ_ONCE(key->flags);short state = READ_ONCE(key->state);kenter(\"{%d}\", key->serial);  ignore keys not of this type ", "static DECLARE_RWSEM(keyring_serialise_restrict_sem);/* * Check for restriction cycles that would prevent keyring garbage collection. * keyring_serialise_restrict_sem must be held. ": "keyring_restriction_alloc(key_restrict_link_func_t check){struct key_restriction  keyres =kzalloc(sizeof(struct key_restriction), GFP_KERNEL);if (!keyres)return ERR_PTR(-ENOMEM);keyres->check = check;return keyres;}    Semaphore to serialise restriction setup to prevent reference count   cycles through restriction key pointers. ", "if (index_key->type == &key_type_keyring)mutex_lock(&keyring_serialise_link_lock);return 0;}/* * Lock keyrings for move (link/unlink combination). ": "key_link_lock(struct key  keyring,    const struct keyring_index_key  index_key)__acquires(&keyring->sem)__acquires(&keyring_serialise_link_lock){if (keyring->type != &key_type_keyring)return -ENOTDIR;down_write(&keyring->sem);  Serialise linklink calls to prevent parallel calls causing a cycle   when linking two keyring in opposite orders. ", "static int __key_unlink_begin(struct key *keyring, struct key *key,      struct assoc_array_edit **_edit)": "key_unlink_lock(struct key  keyring)__acquires(&keyring->sem){if (keyring->type != &key_type_keyring)return -ENOTDIR;down_write(&keyring->sem);return 0;}    Begin the process of unlinking a key from a keyring. ", "if (l_keyring < u_keyring) ": "key_move_lock(struct key  l_keyring, struct key  u_keyring,    const struct keyring_index_key  index_key)__acquires(&l_keyring->sem)__acquires(&u_keyring->sem)__acquires(&keyring_serialise_link_lock){if (l_keyring->type != &key_type_keyring ||    u_keyring->type != &key_type_keyring)return -ENOTDIR;  We have to be very careful here to take the keyring locks in the   right order, lest we open ourselves to deadlocking against another   move operation. ", "int keyring_clear(struct key *keyring)": "keyring_clear - Clear a keyring   @keyring: The keyring to clear.     Clear the contents of the specified keyring.     Returns 0 if successful or -ENOTDIR if the keyring isn't a keyring. ", "int key_payload_reserve(struct key *key, size_t datalen)": "key_payload_reserve - Adjust data quota reservation for the key's payload   @key: The key to make the reservation for.   @datalen: The amount of data payload the caller now wants.     Adjust the amount of the owning user's key data quota that a key reserves.   If the amount is increased, then -EDQUOT may be returned if there isn't   enough free quota available.     If successful, 0 is returned. ", "if (key->state == KEY_IS_UNINSTANTIATED) ": "key_instantiate_and_link(struct key  key,      struct key_preparsed_payload  prep,      struct key  keyring,      struct key  authkey,      struct assoc_array_edit   _edit){int ret, awaken;key_check(key);key_check(keyring);awaken = 0;ret = -EBUSY;mutex_lock(&key_construction_mutex);  can't instantiate twice ", "int key_reject_and_link(struct key *key,unsigned timeout,unsigned error,struct key *keyring,struct key *authkey)": "key_reject_and_link - Negatively instantiate a key and link it into the keyring.   @key: The key to instantiate.   @timeout: The timeout on the negative key.   @error: The error to return when the key is hit.   @keyring: Keyring to create a link in on success (or NULL).   @authkey: The authorisation token permitting instantiation.     Negatively instantiate a key that's in the uninstantiated state and, if   successful, set its timeout and stored error and link it in to the   destination keyring if one is supplied.  The key and any links to the key   will be automatically garbage collected after the timeout expires.     Negative keys are used to rate limit repeated request_key() calls by causing   them to return the stored error code (typically ENOKEY) until the negative   key expires.     If successful, 0 is returned, the authorisation token is revoked and anyone   waiting for the key is woken up.  If the key was already instantiated,   -EBUSY will be returned. ", "void key_put(struct key *key)": "key_put - Discard a reference to a key.   @key: The key to discard a reference from.     Discard a reference to a key, and when all the references are gone, we   schedule the cleanup task to come and pull it out of the tree in process   context at some later time. ", "static key_ref_t __key_create_or_update(key_ref_t keyring_ref,const char *type,const char *description,const void *payload,size_t plen,key_perm_t perm,unsigned long flags,bool allow_update)": "key_create_or_update() and key_create() ", "ret = key_permission(key_ref, KEY_NEED_WRITE);if (ret < 0)goto error;ret = -EEXIST;if (!key->type->update)goto error;down_write(&key->sem);ret = key->type->update(key, prep);if (ret == 0) ": "key_update(key_ref_t key_ref,     struct key_preparsed_payload  prep){struct key  key = key_ref_to_ptr(key_ref);int ret;  need write permission on the key to update it ", "void key_revoke(struct key *key)": "key_revoke - Revoke a key.   @key: The key to be revoked.     Mark a key as being revoked and ask the type to free up its resources.  The   revocation timeout is set and the key and all its links will be   automatically garbage collected after key_gc_delay amount of time if they   are not manually dealt with first. ", "if (awaken)wake_up_bit(&key->flags, KEY_FLAG_USER_CONSTRUCT);return ret;}/** * key_instantiate_and_link - Instantiate a key and link it into the keyring. * @key: The key to instantiate. * @data: The data to use to instantiate the keyring. * @datalen: The length of @data. * @keyring: Keyring to create a link in on success (or NULL). * @authkey: The authorisation token permitting instantiation. * * Instantiate a key that's in the uninstantiated state using the provided data * and, if successful, link it in to the destination keyring if one is * supplied. * * If successful, 0 is returned, the authorisation token is revoked and anyone * waiting for the key is woken up.  If the key was already instantiated, * -EBUSY will be returned. ": "key_invalidate(authkey);if (prep->expiry != TIME64_MAX) {key->expiry = prep->expiry;key_schedule_gc(prep->expiry + key_gc_delay);}}}mutex_unlock(&key_construction_mutex);  wake up anyone waiting for a key to be constructed ", "int generic_key_instantiate(struct key *key, struct key_preparsed_payload *prep)": "generic_key_instantiate - Simple instantiation of a key from preparsed data   @key: The key to be instantiated   @prep: The preparsed data to load.     Instantiate a key from preparsed data.  We assume we can just copy the data   in directly and clear the old pointers.     This can be pointed to directly by the key type instantiate op pointer. ", "int register_key_type(struct key_type *ktype)": "register_key_type - Register a type of key.   @ktype: The new key type.     Register a new key type.     Returns 0 on success or -EEXIST if a type of this name already exists. ", "void unregister_key_type(struct key_type *ktype)": "unregister_key_type - Unregister a type of key.   @ktype: The key type.     Unregister a key type and mark all the extant keys of this type as dead.   Those keys of this type are then destroyed to get rid of their payloads and   they and their links will be garbage collected as soon as possible. ", "void complete_request_key(struct key *authkey, int error)": "complete_request_key - Complete the construction of a key.   @authkey: The authorisation key.   @error: The success or failute of the construction.     Complete the attempt to construct a key.  The key will be negated   if an error is indicated.  The authorisation key will be revoked   unconditionally. ", "struct key *request_key_and_link(struct key_type *type, const char *description, struct key_tag *domain_tag, const void *callout_info, size_t callout_len, void *aux, struct key *dest_keyring, unsigned long flags)": "wait_for_key_construction() should be used to wait for that to complete. ", "struct key *request_key_tag(struct key_type *type,    const char *description,    struct key_tag *domain_tag,    const char *callout_info)": "request_key_tag - Request a key and wait for construction   @type: Type of key.   @description: The searchable description of the key.   @domain_tag: The domain in which the key operates.   @callout_info: The data to pass to the instantiation upcall (or NULL).     As for request_key_and_link() except that it does not add the returned key   to a keyring if found, new keys are always allocated in the user's quota,   the callout_info must be a NUL-terminated string and no auxiliary data can   be passed.     Furthermore, it then works as wait_for_key_construction() to wait for the   completion of keys undergoing construction with a non-interruptible wait. ", "struct key *request_key_with_auxdata(struct key_type *type,     const char *description,     struct key_tag *domain_tag,     const void *callout_info,     size_t callout_len,     void *aux)": "request_key_with_auxdata - Request a key with auxiliary data for the upcaller   @type: The type of key we want.   @description: The searchable description of the key.   @domain_tag: The domain in which the key operates.   @callout_info: The data to pass to the instantiation upcall (or NULL).   @callout_len: The length of callout_info.   @aux: Auxiliary data for the upcall.     As for request_key_and_link() except that it does not add the returned key   to a keyring if found and new keys are always allocated in the user's quota.     Furthermore, it then works as wait_for_key_construction() to wait for the   completion of keys undergoing construction with a non-interruptible wait. ", "struct key *request_key_rcu(struct key_type *type,    const char *description,    struct key_tag *domain_tag)": "request_key_rcu - Request key from RCU-read-locked context   @type: The type of key we want.   @description: The name of the key we want.   @domain_tag: The domain in which the key operates.     Request a key from a context that we may not sleep in (such as RCU-mode   pathwalk).  Keys under construction are ignored.     Return a pointer to the found key if successful, -ENOKEY if we couldn't find   a key or some other error if the key found was unsuitable or inaccessible. ", "void ecryptfs_get_versions(int *major, int *minor, int *file_version)": "ecryptfs_get_versions()     Source code taken from the software 'ecryptfs-utils' version 83.   ", "int ecryptfs_fill_auth_tok(struct ecryptfs_auth_tok *auth_tok,   const char *key_desc)": "ecryptfs_fill_auth_tok - fill the ecryptfs_auth_tok structure     Fill the ecryptfs_auth_tok structure with required ecryptfs data.   The source code is inspired to the original function generate_payload()   shipped with the software 'ecryptfs-utils' version 83.   ", "if (bpf_testmod_loop_test(101) > 100)trace_bpf_testmod_test_read(current, &ctx);/* Magic number to enable writable tp ": "bpf_testmod_test_read(struct file  file, struct kobject  kobj,      struct bin_attribute  bin_attr,      char  buf, loff_t off, size_t len){struct bpf_testmod_test_read_ctx ctx = {.buf = buf,.off = off,.len = len,};struct bpf_testmod_struct_arg_1 struct_arg1 = {10};struct bpf_testmod_struct_arg_2 struct_arg2 = {2, 3};struct bpf_testmod_struct_arg_3  struct_arg3;int i = 1;while (bpf_testmod_return_ptr(i))i++;(void)bpf_testmod_test_struct_arg_1(struct_arg2, 1, 4);(void)bpf_testmod_test_struct_arg_2(1, struct_arg2, 4);(void)bpf_testmod_test_struct_arg_3(1, 4, struct_arg2);(void)bpf_testmod_test_struct_arg_4(struct_arg1, 1, 2, 3, struct_arg2);(void)bpf_testmod_test_struct_arg_5();struct_arg3 = kmalloc((sizeof(struct bpf_testmod_struct_arg_3) +sizeof(int)), GFP_KERNEL);if (struct_arg3 != NULL) {struct_arg3->b[0] = 1;(void)bpf_testmod_test_struct_arg_6(struct_arg3);kfree(struct_arg3);}  This is always true. Use the check to make sure the compiler   doesn't remove bpf_testmod_loop_test. ", "}EXPORT_SYMBOL(bpf_testmod_test_write": "bpf_testmod_test_write(struct file  file, struct kobject  kobj,      struct bin_attribute  bin_attr,      char  buf, loff_t off, size_t len){struct bpf_testmod_test_write_ctx ctx = {.buf = buf,.off = off,.len = len,};trace_bpf_testmod_test_write_bare(current, &ctx);return -EIO;   always fail ", "/* * First, the common part. ": "sound_class  which is common to both OSS and ALSA and OSS sound core which  is used OSS or emulation of it. ", " int register_sound_special_device(const struct file_operations *fops, int unit,  struct device *dev)": "register_sound_special_device - register a special sound node  @fops: File operations for the driver  @unit: Unit number to allocate        @dev: device pointer    Allocate a special sound device by minor number from the sound  subsystem.    Return: The allocated number is returned on success. On failure,  a negative error code is returned. ", "int register_sound_mixer(const struct file_operations *fops, int dev)": "register_sound_mixer - register a mixer device  @fops: File operations for the driver  @dev: Unit number to allocate    Allocate a mixer device. Unit is the number of the mixer requested.  Pass -1 to request the next free mixer unit.    Return: On success, the allocated number is returned. On failure,  a negative error code is returned. ", "int register_sound_dsp(const struct file_operations *fops, int dev)": "register_sound_dsp - register a DSP device  @fops: File operations for the driver  @dev: Unit number to allocate    Allocate a DSP device. Unit is the number of the DSP requested.  Pass -1 to request the next free DSP unit.    This function allocates both the audio and dsp device entries together  and will always allocate them as a matching pair - eg dsp3audio3    Return: On success, the allocated number is returned. On failure,  a negative error code is returned. ", "void unregister_sound_special(int unit)": "unregister_sound_special - unregister a special sound device  @unit: unit number to allocate    Release a sound device that was allocated with  register_sound_special(). The unit passed is the return value from  the register function. ", "void unregister_sound_mixer(int unit)": "unregister_sound_mixer - unregister a mixer  @unit: unit number to allocate    Release a sound device that was allocated with register_sound_mixer().  The unit passed is the return value from the register function. ", "void unregister_sound_dsp(int unit)": "unregister_sound_dsp - unregister a DSP device  @unit: unit number to allocate    Release a sound device that was allocated with register_sound_dsp().  The unit passed is the return value from the register function.    Both of the allocated units are released together automatically. ", "ret = snd_pcm_hw_constraint_step(runtime, 0,SNDRV_PCM_HW_PARAM_PERIOD_BYTES, 32);if (ret)return ret;ret = snd_pcm_hw_constraint_step(runtime, 0,SNDRV_PCM_HW_PARAM_BUFFER_BYTES, 32);if (ret)return ret;ret = snd_pcm_hw_constraint_integer(runtime,    SNDRV_PCM_HW_PARAM_PERIODS);if (ret < 0)return ret;return snd_dmaengine_pcm_open(substream, dma_request_slave_channel(asoc_rtd_to_cpu(rtd, 0)->dev,     dma_params->chan_name));}EXPORT_SYMBOL(pxa2xx_pcm_open": "pxa2xx_pcm_open(struct snd_pcm_substream  substream){struct snd_soc_pcm_runtime  rtd = substream->private_data;struct snd_pcm_runtime  runtime = substream->runtime;struct snd_dmaengine_dai_dma_data  dma_params;int ret;runtime->hw = pxa2xx_pcm_hardware;dma_params = snd_soc_dai_get_dma_data(asoc_rtd_to_cpu(rtd, 0), substream);if (!dma_params)return 0;    For mysterious reasons (and despite what the manual says)   playback samples are lost if the DMA count is not a multiple   of the DMA burst size.  Let's add a rule to enforce that. ", "int snd_fw_transaction(struct fw_unit *unit, int tcode,       u64 offset, void *buffer, size_t length,       unsigned int flags)": "snd_fw_transaction - send a request and wait for its completion   @unit: the driver's unit on the target device   @tcode: the transaction code   @offset: the address in the target's address space   @buffer: inputoutput data   @length: length of @buffer   @flags: use %FW_FIXED_GENERATION and add the generation value to attempt the           request only in that generation; use %FW_QUIET to suppress error           messages     Submits an asynchronous request to the target device, and waits for the   response.  The node ID and the current generation are derived from @unit.   On a bus reset or an error, the transaction is retried a few times.   Returns zero on success, or a negative error code. ", "int fw_iso_resources_init(struct fw_iso_resources *r, struct fw_unit *unit)": "fw_iso_resources_init - initializes a &struct fw_iso_resources   @r: the resource manager to initialize   @unit: the device unit for which the resources will be needed     If the device does not support all channel numbers, change @r->channels_mask   after calling this function. ", "void fw_iso_resources_destroy(struct fw_iso_resources *r)": "fw_iso_resources_destroy - destroy a resource manager   @r: the resource manager that is no longer needed ", "int fw_iso_resources_allocate(struct fw_iso_resources *r,      unsigned int max_payload_bytes, int speed)": "fw_iso_resources_free() when the resources are not longer needed. ", "int iso_packets_buffer_init(struct iso_packets_buffer *b, struct fw_unit *unit,    unsigned int count, unsigned int packet_size,    enum dma_data_direction direction)": "iso_packets_buffer_init - allocates the memory for packets   @b: the buffer structure to initialize   @unit: the device at the other end of the stream   @count: the number of packets   @packet_size: the (maximum) size of a packet, in bytes   @direction: %DMA_TO_DEVICE or %DMA_FROM_DEVICE ", "void iso_packets_buffer_destroy(struct iso_packets_buffer *b,struct fw_unit *unit)": "iso_packets_buffer_destroy - frees packet buffer resources   @b: the buffer structure to free   @unit: the device at the other end of the stream ", "int amdtp_stream_init(struct amdtp_stream *s, struct fw_unit *unit,      enum amdtp_stream_direction dir, unsigned int flags,      unsigned int fmt,      amdtp_stream_process_ctx_payloads_t process_ctx_payloads,      unsigned int protocol_size)": "amdtp_stream_init - initialize an AMDTP stream structure   @s: the AMDTP stream to initialize   @unit: the target of the stream   @dir: the direction of stream   @flags: the details of the streaming protocol consist of cip_flags enumeration-constants.   @fmt: the value of fmt field in CIP header   @process_ctx_payloads: callback handler to process payloads of isoc context   @protocol_size: the size to allocate newly for protocol ", "void amdtp_stream_destroy(struct amdtp_stream *s)": "amdtp_stream_destroy - free stream resources   @s: the AMDTP stream to destroy ", "int amdtp_stream_add_pcm_hw_constraints(struct amdtp_stream *s,struct snd_pcm_runtime *runtime)": "amdtp_stream_add_pcm_hw_constraints - add hw constraints for PCM substream   @s:the AMDTP stream, which must be initialized.   @runtime:the PCM substream runtime ", "int amdtp_stream_set_parameters(struct amdtp_stream *s, unsigned int rate,unsigned int data_block_quadlets, unsigned int pcm_frame_multiplier)": "amdtp_stream_set_parameters - set stream parameters   @s: the AMDTP stream to configure   @rate: the sample rate   @data_block_quadlets: the size of a data block in quadlet unit   @pcm_frame_multiplier: the multiplier to compute the number of PCM frames by the number of AMDTP    events.     The parameters must be set before the stream is started, and must not be   changed while the stream is running. ", "unsigned int amdtp_stream_get_max_payload(struct amdtp_stream *s)": "amdtp_stream_get_max_payload - get the stream's packet size   @s: the AMDTP stream     This function must not be called before the stream has been configured   with amdtp_stream_set_parameters(). ", "void amdtp_stream_pcm_prepare(struct amdtp_stream *s)": "amdtp_stream_pcm_prepare - prepare PCM device for running   @s: the AMDTP stream     This function should be called from the PCM device's .prepare callback. ", "tag = FW_ISO_CONTEXT_MATCH_TAG1;if ((s->flags & CIP_EMPTY_WITH_TAG0) || (s->flags & CIP_NO_HEADER))tag |= FW_ISO_CONTEXT_MATCH_TAG0;s->ready_processing = false;err = fw_iso_context_start(s->context, -1, 0, tag);if (err < 0)goto err_pkt_descs;mutex_unlock(&s->mutex);return 0;err_pkt_descs:kfree(s->packet_descs);s->packet_descs = NULL;err_context:if (s->direction == AMDTP_OUT_STREAM) ": "amdtp_stream_update(s);if (s->direction == AMDTP_IN_STREAM) {s->ctx_data.tx.max_ctx_payload_length = max_ctx_payload_size;s->ctx_data.tx.ctx_header_size = ctx_header_size;s->ctx_data.tx.event_starts = false;if (s->domain->replay.enable) { struct fw_iso_context.drop_overflow_headers is false therefore it's possible to cache much unexpectedly.s->ctx_data.tx.cache.size = max_t(unsigned int, s->syt_interval   2,  queue_size   3  2);s->ctx_data.tx.cache.pos = 0;s->ctx_data.tx.cache.descs = kcalloc(s->ctx_data.tx.cache.size,sizeof( s->ctx_data.tx.cache.descs), GFP_KERNEL);if (!s->ctx_data.tx.cache.descs) {err = -ENOMEM;goto err_context;}}} else {static const struct {unsigned int data_block;unsigned int syt_offset;}  entry, initial_state[] = {[CIP_SFC_32000]  = {  4, 3072 },[CIP_SFC_48000]  = {  6, 1024 },[CIP_SFC_96000]  = { 12, 1024 },[CIP_SFC_192000] = { 24, 1024 },[CIP_SFC_44100]  = {  0,   67 },[CIP_SFC_88200]  = {  0,   67 },[CIP_SFC_176400] = {  0,   67 },};s->ctx_data.rx.seq.descs = kcalloc(queue_size, sizeof( s->ctx_data.rx.seq.descs), GFP_KERNEL);if (!s->ctx_data.rx.seq.descs) {err = -ENOMEM;goto err_context;}s->ctx_data.rx.seq.size = queue_size;s->ctx_data.rx.seq.pos = 0;entry = &initial_state[s->sfc];s->ctx_data.rx.data_block_state = entry->data_block;s->ctx_data.rx.syt_offset_state = entry->syt_offset;s->ctx_data.rx.last_syt_offset = TICKS_PER_CYCLE;s->ctx_data.rx.event_count = 0;}if (s->flags & CIP_NO_HEADER)s->tag = TAG_NO_CIP_HEADER;elses->tag = TAG_CIP; NOTE: When operating without hardIRQsoftIRQ, applications tends to call ioctl request for runtime of PCM substream in the interval equivalent to the size of PCM buffer. It could take a round over queue of AMDTP packet descriptors and small loss of history. For safe, keep more 8 elements for the queue, equivalent to 1 ms.descs = kcalloc(s->queue_size + 8, sizeof( descs), GFP_KERNEL);if (!descs) {err = -ENOMEM;goto err_context;}s->packet_descs = descs;INIT_LIST_HEAD(&s->packet_descs_list);for (i = 0; i < s->queue_size; ++i) {INIT_LIST_HEAD(&descs->link);list_add_tail(&descs->link, &s->packet_descs_list);++descs;}s->packet_descs_cursor = list_first_entry(&s->packet_descs_list, struct pkt_desc, link);s->packet_index = 0;do {struct fw_iso_packet params;if (s->direction == AMDTP_IN_STREAM) {err = queue_in_packet(s, &params);} else {bool sched_irq = false;params.header_length = 0;params.payload_length = 0;if (is_irq_target) {sched_irq = !((s->packet_index + 1) %      idle_irq_interval);}err = queue_out_packet(s, &params, sched_irq);}if (err < 0)goto err_pkt_descs;} while (s->packet_index > 0);  NOTE: TAG1 matches CIP. This just affects in stream. ", "static int amdtp_stream_start(struct amdtp_stream *s, int channel, int speed,      unsigned int queue_size, unsigned int idle_irq_interval)": "amdtp_stream_pcm_abort(s);WRITE_ONCE(s->pcm_buffer_pointer, SNDRV_PCM_POS_XRUN);}static snd_pcm_sframes_t compute_pcm_extra_delay(struct amdtp_stream  s, const struct pkt_desc  desc, unsigned int count){unsigned int data_block_count = 0;u32 latest_cycle;u32 cycle_time;u32 curr_cycle;u32 cycle_gap;int i, err;if (count == 0)goto end; Forward to the latest record.for (i = 0; i < count - 1; ++i)desc = amdtp_stream_next_packet_desc(s, desc);latest_cycle = desc->cycle;err = fw_card_read_cycle_time(fw_parent_device(s->unit)->card, &cycle_time);if (err < 0)goto end; Compute cycle count with lower 3 bits of second field and cycle field like timestamp format of 1394 OHCI isochronous context.curr_cycle = compute_ohci_iso_ctx_cycle_count((cycle_time >> 12) & 0x0000ffff);if (s->direction == AMDTP_IN_STREAM) { NOTE: The AMDTP packet descriptor should be for the past isochronous cycle since it corresponds to arrived isochronous packet.if (compare_ohci_cycle_count(latest_cycle, curr_cycle) > 0)goto end;cycle_gap = decrement_ohci_cycle_count(curr_cycle, latest_cycle); NOTE: estimate delay by recent history of arrived AMDTP packets. The estimated value expectedly corresponds to a few packets (0-2) since the packet arrived at the most recent isochronous cycle has been already processed.for (i = 0; i < cycle_gap; ++i) {desc = amdtp_stream_next_packet_desc(s, desc);data_block_count += desc->data_blocks;}} else { NOTE: The AMDTP packet descriptor should be for the future isochronous cycle since it was already scheduled.if (compare_ohci_cycle_count(latest_cycle, curr_cycle) < 0)goto end;cycle_gap = decrement_ohci_cycle_count(latest_cycle, curr_cycle); NOTE: use history of scheduled packets.for (i = 0; i < cycle_gap; ++i) {data_block_count += desc->data_blocks;desc = prev_packet_desc(s, desc);}}end:return data_block_count   s->pcm_frame_multiplier;}static void process_ctx_payloads(struct amdtp_stream  s, const struct pkt_desc  desc, unsigned int count){struct snd_pcm_substream  pcm;int i;pcm = READ_ONCE(s->pcm);s->process_ctx_payloads(s, desc, count, pcm);if (pcm) {unsigned int data_block_count = 0;pcm->runtime->delay = compute_pcm_extra_delay(s, desc, count);for (i = 0; i < count; ++i) {data_block_count += desc->data_blocks;desc = amdtp_stream_next_packet_desc(s, desc);}update_pcm_pointers(s, pcm, data_block_count   s->pcm_frame_multiplier);}}static void process_rx_packets(struct fw_iso_context  context, u32 tstamp, size_t header_length,       void  header, void  private_data){struct amdtp_stream  s = private_data;const struct amdtp_domain  d = s->domain;const __be32  ctx_header = header;const unsigned int events_per_period = d->events_per_period;unsigned int event_count = s->ctx_data.rx.event_count;struct pkt_desc  desc = s->packet_descs_cursor;unsigned int pkt_header_length;unsigned int packets;u32 curr_cycle_time;bool need_hw_irq;int i;if (s->packet_index < 0)return; Calculate the number of packets in buffer and check XRUN.packets = header_length  sizeof( ctx_header);generate_rx_packet_descs(s, desc, ctx_header, packets);process_ctx_payloads(s, desc, packets);if (!(s->flags & CIP_NO_HEADER))pkt_header_length = IT_PKT_HEADER_SIZE_CIP;elsepkt_header_length = 0;if (s == d->irq_target) { At NO_PERIOD_WAKEUP mode, the packets for all ITIR contexts are processed by the tasks of user process operating ALSA PCM character device by calling ioctl(2) with some requests, instead of scheduled hardware IRQ of an IT context.struct snd_pcm_substream  pcm = READ_ONCE(s->pcm);need_hw_irq = !pcm || !pcm->runtime->no_period_wakeup;} else {need_hw_irq = false;}if (trace_amdtp_packet_enabled())(void)fw_card_read_cycle_time(fw_parent_device(s->unit)->card, &curr_cycle_time);for (i = 0; i < packets; ++i) {struct {struct fw_iso_packet params;__be32 header[CIP_HEADER_QUADLETS];} template = { {0}, {0} };bool sched_irq = false;build_it_pkt_header(s, desc->cycle, &template.params, pkt_header_length,    desc->data_blocks, desc->data_block_counter,    desc->syt, i, curr_cycle_time);if (s == s->domain->irq_target) {event_count += desc->data_blocks;if (event_count >= events_per_period) {event_count -= events_per_period;sched_irq = need_hw_irq;}}if (queue_out_packet(s, &template.params, sched_irq) < 0) {cancel_stream(s);return;}desc = amdtp_stream_next_packet_desc(s, desc);}s->ctx_data.rx.event_count = event_count;s->packet_descs_cursor = desc;}static void skip_rx_packets(struct fw_iso_context  context, u32 tstamp, size_t header_length,    void  header, void  private_data){struct amdtp_stream  s = private_data;struct amdtp_domain  d = s->domain;const __be32  ctx_header = header;unsigned int packets;unsigned int cycle;int i;if (s->packet_index < 0)return;packets = header_length  sizeof( ctx_header);cycle = compute_ohci_it_cycle(ctx_header[packets - 1], s->queue_size);s->next_cycle = increment_ohci_cycle_count(cycle, 1);for (i = 0; i < packets; ++i) {struct fw_iso_packet params = {.header_length = 0,.payload_length = 0,};bool sched_irq = (s == d->irq_target && i == packets - 1);if (queue_out_packet(s, &params, sched_irq) < 0) {cancel_stream(s);return;}}}static void irq_target_callback(struct fw_iso_context  context, u32 tstamp, size_t header_length,void  header, void  private_data);static void process_rx_packets_intermediately(struct fw_iso_context  context, u32 tstamp,size_t header_length, void  header, void  private_data){struct amdtp_stream  s = private_data;struct amdtp_domain  d = s->domain;__be32  ctx_header = header;const unsigned int queue_size = s->queue_size;unsigned int packets;unsigned int offset;if (s->packet_index < 0)return;packets = header_length  sizeof( ctx_header);offset = 0;while (offset < packets) {unsigned int cycle = compute_ohci_it_cycle(ctx_header[offset], queue_size);if (compare_ohci_cycle_count(cycle, d->processing_cycle.rx_start) >= 0)break;++offset;}if (offset > 0) {unsigned int length = sizeof( ctx_header)   offset;skip_rx_packets(context, tstamp, length, ctx_header, private_data);if (amdtp_streaming_error(s))return;ctx_header += offset;header_length -= length;}if (offset < packets) {s->ready_processing = true;wake_up(&s->ready_wait);if (d->replay.enable)s->ctx_data.rx.cache_pos = 0;process_rx_packets(context, tstamp, header_length, ctx_header, private_data);if (amdtp_streaming_error(s))return;if (s == d->irq_target)s->context->callback.sc = irq_target_callback;elses->context->callback.sc = process_rx_packets;}}static void process_tx_packets(struct fw_iso_context  context, u32 tstamp, size_t header_length,       void  header, void  private_data){struct amdtp_stream  s = private_data;__be32  ctx_header = header;struct pkt_desc  desc = s->packet_descs_cursor;unsigned int packet_count;unsigned int desc_count;int i;int err;if (s->packet_index < 0)return; Calculate the number of packets in buffer and check XRUN.packet_count = header_length  s->ctx_data.tx.ctx_header_size;desc_count = 0;err = generate_tx_packet_descs(s, desc, ctx_header, packet_count, &desc_count);if (err < 0) {if (err != -EAGAIN) {cancel_stream(s);return;}} else {struct amdtp_domain  d = s->domain;process_ctx_payloads(s, desc, desc_count);if (d->replay.enable)cache_seq(s, desc, desc_count);for (i = 0; i < desc_count; ++i)desc = amdtp_stream_next_packet_desc(s, desc);s->packet_descs_cursor = desc;}for (i = 0; i < packet_count; ++i) {struct fw_iso_packet params = {0};if (queue_in_packet(s, &params) < 0) {cancel_stream(s);return;}}}static void drop_tx_packets(struct fw_iso_context  context, u32 tstamp, size_t header_length,    void  header, void  private_data){struct amdtp_stream  s = private_data;const __be32  ctx_header = header;unsigned int packets;unsigned int cycle;int i;if (s->packet_index < 0)return;packets = header_length  s->ctx_data.tx.ctx_header_size;ctx_header += (packets - 1)   s->ctx_data.tx.ctx_header_size  sizeof( ctx_header);cycle = compute_ohci_cycle_count(ctx_header[1]);s->next_cycle = increment_ohci_cycle_count(cycle, 1);for (i = 0; i < packets; ++i) {struct fw_iso_packet params = {0};if (queue_in_packet(s, &params) < 0) {cancel_stream(s);return;}}}static void process_tx_packets_intermediately(struct fw_iso_context  context, u32 tstamp,size_t header_length, void  header, void  private_data){struct amdtp_stream  s = private_data;struct amdtp_domain  d = s->domain;__be32  ctx_header;unsigned int packets;unsigned int offset;if (s->packet_index < 0)return;packets = header_length  s->ctx_data.tx.ctx_header_size;offset = 0;ctx_header = header;while (offset < packets) {unsigned int cycle = compute_ohci_cycle_count(ctx_header[1]);if (compare_ohci_cycle_count(cycle, d->processing_cycle.tx_start) >= 0)break;ctx_header += s->ctx_data.tx.ctx_header_size  sizeof(__be32);++offset;}ctx_header = header;if (offset > 0) {size_t length = s->ctx_data.tx.ctx_header_size   offset;drop_tx_packets(context, tstamp, length, ctx_header, s);if (amdtp_streaming_error(s))return;ctx_header += length  sizeof( ctx_header);header_length -= length;}if (offset < packets) {s->ready_processing = true;wake_up(&s->ready_wait);process_tx_packets(context, tstamp, header_length, ctx_header, s);if (amdtp_streaming_error(s))return;context->callback.sc = process_tx_packets;}}static void drop_tx_packets_initially(struct fw_iso_context  context, u32 tstamp,      size_t header_length, void  header, void  private_data){struct amdtp_stream  s = private_data;struct amdtp_domain  d = s->domain;__be32  ctx_header;unsigned int count;unsigned int events;int i;if (s->packet_index < 0)return;count = header_length  s->ctx_data.tx.ctx_header_size; Attempt to detect any event in the batch of packets.events = 0;ctx_header = header;for (i = 0; i < count; ++i) {unsigned int payload_quads =(be32_to_cpu( ctx_header) >> ISO_DATA_LENGTH_SHIFT)  sizeof(__be32);unsigned int data_blocks;if (s->flags & CIP_NO_HEADER) {data_blocks = payload_quads  s->data_block_quadlets;} else {__be32  cip_headers = ctx_header + IR_CTX_HEADER_DEFAULT_QUADLETS;if (payload_quads < CIP_HEADER_QUADLETS) {data_blocks = 0;} else {payload_quads -= CIP_HEADER_QUADLETS;if (s->flags & CIP_UNAWARE_SYT) {data_blocks = payload_quads  s->data_block_quadlets;} else {u32 cip1 = be32_to_cpu(cip_headers[1]); NODATA packet can includes any data blocks but they are not available as event.if ((cip1 & CIP_NO_DATA) == CIP_NO_DATA)data_blocks = 0;elsedata_blocks = payload_quads  s->data_block_quadlets;}}}events += data_blocks;ctx_header += s->ctx_data.tx.ctx_header_size  sizeof(__be32);}drop_tx_packets(context, tstamp, header_length, header, s);if (events > 0)s->ctx_data.tx.event_starts = true; Decide the cycle count to begin processing content of packet in IR contexts.{unsigned int stream_count = 0;unsigned int event_starts_count = 0;unsigned int cycle = UINT_MAX;list_for_each_entry(s, &d->streams, list) {if (s->direction == AMDTP_IN_STREAM) {++stream_count;if (s->ctx_data.tx.event_starts)++event_starts_count;}}if (stream_count == event_starts_count) {unsigned int next_cycle;list_for_each_entry(s, &d->streams, list) {if (s->direction != AMDTP_IN_STREAM)continue;next_cycle = increment_ohci_cycle_count(s->next_cycle,d->processing_cycle.tx_init_skip);if (cycle == UINT_MAX ||    compare_ohci_cycle_count(next_cycle, cycle) > 0)cycle = next_cycle;s->context->callback.sc = process_tx_packets_intermediately;}d->processing_cycle.tx_start = cycle;}}}static void process_ctxs_in_domain(struct amdtp_domain  d){struct amdtp_stream  s;list_for_each_entry(s, &d->streams, list) {if (s != d->irq_target && amdtp_stream_running(s))fw_iso_context_flush_completions(s->context);if (amdtp_streaming_error(s))goto error;}return;error:if (amdtp_stream_running(d->irq_target))cancel_stream(d->irq_target);list_for_each_entry(s, &d->streams, list) {if (amdtp_stream_running(s))cancel_stream(s);}}static void irq_target_callback(struct fw_iso_context  context, u32 tstamp, size_t header_length,void  header, void  private_data){struct amdtp_stream  s = private_data;struct amdtp_domain  d = s->domain;process_rx_packets(context, tstamp, header_length, header, private_data);process_ctxs_in_domain(d);}static void irq_target_callback_intermediately(struct fw_iso_context  context, u32 tstamp,size_t header_length, void  header, void  private_data){struct amdtp_stream  s = private_data;struct amdtp_domain  d = s->domain;process_rx_packets_intermediately(context, tstamp, header_length, header, private_data);process_ctxs_in_domain(d);}static void irq_target_callback_skip(struct fw_iso_context  context, u32 tstamp,     size_t header_length, void  header, void  private_data){struct amdtp_stream  s = private_data;struct amdtp_domain  d = s->domain;bool ready_to_start;skip_rx_packets(context, tstamp, header_length, header, private_data);process_ctxs_in_domain(d);if (d->replay.enable && !d->replay.on_the_fly) {unsigned int rx_count = 0;unsigned int rx_ready_count = 0;struct amdtp_stream  rx;list_for_each_entry(rx, &d->streams, list) {struct amdtp_stream  tx;unsigned int cached_cycles;if (rx->direction != AMDTP_OUT_STREAM)continue;++rx_count;tx = rx->ctx_data.rx.replay_target;cached_cycles = calculate_cached_cycle_count(tx, 0);if (cached_cycles > tx->ctx_data.tx.cache.size  2)++rx_ready_count;}ready_to_start = (rx_count == rx_ready_count);} else {ready_to_start = true;} Decide the cycle count to begin processing content of packet in IT contexts. All of IT contexts are expected to start and get callback when reaching here.if (ready_to_start) {unsigned int cycle = s->next_cycle;list_for_each_entry(s, &d->streams, list) {if (s->direction != AMDTP_OUT_STREAM)continue;if (compare_ohci_cycle_count(s->next_cycle, cycle) > 0)cycle = s->next_cycle;if (s == d->irq_target)s->context->callback.sc = irq_target_callback_intermediately;elses->context->callback.sc = process_rx_packets_intermediately;}d->processing_cycle.rx_start = cycle;}} This is executed one time. For in-stream, first packet has come. For out-stream, prepared to transmit first packet.static void amdtp_stream_first_callback(struct fw_iso_context  context,u32 tstamp, size_t header_length,void  header, void  private_data){struct amdtp_stream  s = private_data;struct amdtp_domain  d = s->domain;if (s->direction == AMDTP_IN_STREAM) {context->callback.sc = drop_tx_packets_initially;} else {if (s == d->irq_target)context->callback.sc = irq_target_callback_skip;elsecontext->callback.sc = skip_rx_packets;}context->callback.sc(context, tstamp, header_length, header, s);}     amdtp_stream_start - start transferring packets   @s: the AMDTP stream to start   @channel: the isochronous channel on the bus   @speed: firewire speed code   @queue_size: The number of packets in the queue.   @idle_irq_interval: the interval to queue packet during initial state.     The stream cannot be started until it has been configured with   amdtp_stream_set_parameters() and it must be started before any PCM or MIDI   device can be started. ", "buf[1] = 0xff;/* UNIT ": "avc_general_set_sig_fmt(struct fw_unit  unit, unsigned int rate,    enum avc_general_plug_dir dir,    unsigned short pid){unsigned int sfc;u8  buf;bool flag;int err;flag = false;for (sfc = 0; sfc < CIP_SFC_COUNT; sfc++) {if (amdtp_rate_table[sfc] == rate) {flag = true;break;}}if (!flag)return -EINVAL;buf = kzalloc(8, GFP_KERNEL);if (buf == NULL)return -ENOMEM;buf[0] = 0x00;  AVC CONTROL ", "buf[1] = 0xff;/* Unit ": "avc_general_get_sig_fmt(struct fw_unit  unit, unsigned int  rate,    enum avc_general_plug_dir dir,    unsigned short pid){unsigned int sfc;u8  buf;int err;buf = kzalloc(8, GFP_KERNEL);if (buf == NULL)return -ENOMEM;buf[0] = 0x01;  AVC STATUS ", "if ((subunit_type == 0x1E) || (subunit_id == 5))return -EINVAL;buf = kzalloc(8, GFP_KERNEL);if (buf == NULL)return -ENOMEM;buf[0] = 0x01;/* AV/C STATUS ": "avc_general_get_plug_info(struct fw_unit  unit, unsigned int subunit_type,      unsigned int subunit_id, unsigned int subfunction,      u8 info[AVC_PLUG_INFO_BUF_BYTES]){u8  buf;int err;  extended subunit in spec.4.2 is not supported ", "err = -ENOSYS;else if (buf[0] == 0x0a) /* REJECTED ": "fcp_avc_transaction(unit, buf, 8, buf, 8,  BIT(1) | BIT(2) | BIT(3) | BIT(4) | BIT(5));if (err < 0);else if (err < 8)err = -EIO;else if (buf[0] == 0x08)   NOT IMPLEMENTED ", "void fcp_bus_reset(struct fw_unit *unit)": "fcp_bus_reset - inform the target handler about a bus reset   @unit: the unit that might be used by fcp_avc_transaction()     This function must be called from the driver's .update handler to inform   the FCP transaction handler that a bus reset has happened.  Any pending FCP   transactions are retried. ", "int cmp_connection_init(struct cmp_connection *c,struct fw_unit *unit,enum cmp_direction direction,unsigned int pcr_index)": "cmp_connection_init - initializes a connection manager   @c: the connection manager to initialize   @unit: a unit of the target device   @direction: input or output   @pcr_index: the index of the iPCRoPCR on the target device ", "int cmp_connection_check_used(struct cmp_connection *c, bool *used)": "cmp_connection_check_used - check connection is already esablished or not   @c: the connection manager to be checked   @used: the pointer to store the result of checking the connection ", "void cmp_connection_destroy(struct cmp_connection *c)": "cmp_connection_destroy - free connection manager resources   @c: the connection manager ", "int cmp_connection_establish(struct cmp_connection *c)": "cmp_connection_establish - establish a connection to the target   @c: the connection manager     This function establishes a point-to-point connection from the local   computer to the target by allocating isochronous resources (channel and   bandwidth) and setting the target's inputoutput plug control register.   When this function succeeds, the caller is responsible for starting   transmitting packets. ", "int cmp_connection_update(struct cmp_connection *c)": "cmp_connection_update - update the connection after a bus reset   @c: the connection manager     This function must be called from the driver's .update handler to   reestablish any connection that might have been active.     Returns zero on success, or a negative error code.  On an error, the   connection is broken and the caller must stop transmitting iso packets. ", "void cmp_connection_break(struct cmp_connection *c)": "cmp_connection_break - break the connection to the target   @c: the connection manager     This function deactives the connection in the target's inputoutput plug   control register, and frees the isochronous resources of the connection.   Before calling this function, the caller should cease transmitting packets. ", " timeout = 3;u16 wTmp;/* void *DAQD; ": "snd_msnd_DARQ(struct snd_msnd  chip, int bank){int  size, n,", "DAPQ_tail = readw(chip->DAPQ + JQS_wTail);while (DAPQ_tail != readw(chip->DAPQ + JQS_wHead) || start) ": "snd_msnd_DAPQ(struct snd_msnd  chip, int start){u16DAPQ_tail;intprotect = start, nbanks = 0;void__iomem  DAQD;static int play_banks_submitted;  unsigned long flags;spin_lock_irqsave(&chip->lock, flags); not necessary ", "msleep(1);snd_printdd(\"(1) jiffies = %lu\\n\", jiffies);/* check condition up to 250 ms ": "snd_wss_mce_down(struct snd_wss  chip){unsigned long flags;unsigned long end_time;int timeout;int hw_mask = WSS_HW_CS4231_MASK | WSS_HW_CS4232_MASK | WSS_HW_AD1848;snd_wss_busy_wait(chip);#ifdef CONFIG_SND_DEBUGif (wss_inb(chip, CS4231P(REGSEL)) & CS4231_INIT)snd_printk(KERN_DEBUG \"mce_down [0x%lx] - \"   \"auto calibration time out (0)\\n\",   (long)CS4231P(REGSEL));#endifspin_lock_irqsave(&chip->reg_lock, flags);chip->mce_bit &= ~CS4231_MCE;timeout = wss_inb(chip, CS4231P(REGSEL));wss_outb(chip, CS4231P(REGSEL), chip->mce_bit | (timeout & 0x1f));spin_unlock_irqrestore(&chip->reg_lock, flags);if (timeout == 0x80)snd_printk(KERN_DEBUG \"mce_down [0x%lx]: \"   \"serious init problem - codec still busy\\n\",   chip->port);if ((timeout & CS4231_MCE) == 0 || !(chip->hardware & hw_mask))return;    Wait for (possible -- during init auto-calibration may not be set)   calibration process to start. Needs up to 5 sample periods on AD1848   which at the slowest possible rate of 5.5125 kHz means 907 us. ", "chip->capture_substream->runtime->overrange++;}EXPORT_SYMBOL(snd_wss_overrange": "snd_wss_overrange(struct snd_wss  chip){unsigned long flags;unsigned char res;spin_lock_irqsave(&chip->reg_lock, flags);res = snd_wss_in(chip, CS4231_TEST_INIT);spin_unlock_irqrestore(&chip->reg_lock, flags);if (res & (0x08 | 0x02))  detect overrange only above 0dB; may be user selectable? ", "status = CS4231_PLAYBACK_IRQ;elsestatus = snd_wss_in(chip, CS4231_IRQ_STATUS);if (status & CS4231_TIMER_IRQ) ": "snd_wss_interrupt(int irq, void  dev_id){struct snd_wss  chip = dev_id;unsigned char status;if (chip->hardware & WSS_HW_AD1848_MASK)  pretend it was the only possible irq for AD1848 ", "snd_wss_thinkpad_twiddle(chip, 1);}/* global setup ": "snd_wss_create(struct snd_card  card,      unsigned long port,      unsigned long cport,      int irq, int dma1, int dma2,      unsigned short hardware,      unsigned short hwshare,      struct snd_wss   rchip){struct snd_wss  chip;int err;err = snd_wss_new(card, hardware, hwshare, &chip);if (err < 0)return err;chip->irq = -1;chip->dma1 = -1;chip->dma2 = -1;chip->res_port = devm_request_region(card->dev, port, 4, \"WSS\");if (!chip->res_port) {snd_printk(KERN_ERR \"wss: can't grab port 0x%lx\\n\", port);return -EBUSY;}chip->port = port;if ((long)cport >= 0) {chip->res_cport = devm_request_region(card->dev, cport, 8,      \"CS4232 Control\");if (!chip->res_cport) {snd_printk(KERN_ERR\"wss: can't grab control port 0x%lx\\n\", cport);return -ENODEV;}}chip->cport = cport;if (!(hwshare & WSS_HWSHARE_IRQ))if (devm_request_irq(card->dev, irq, snd_wss_interrupt, 0,     \"WSS\", (void  ) chip)) {snd_printk(KERN_ERR \"wss: can't grab IRQ %d\\n\", irq);return -EBUSY;}chip->irq = irq;card->sync_irq = chip->irq;if (!(hwshare & WSS_HWSHARE_DMA1) &&    snd_devm_request_dma(card->dev, dma1, \"WSS - 1\")) {snd_printk(KERN_ERR \"wss: can't grab DMA1 %d\\n\", dma1);return -EBUSY;}chip->dma1 = dma1;if (!(hwshare & WSS_HWSHARE_DMA2) && dma1 != dma2 && dma2 >= 0 &&    snd_devm_request_dma(card->dev, dma2, \"WSS - 2\")) {snd_printk(KERN_ERR \"wss: can't grab DMA2 %d\\n\", dma2);return -EBUSY;}if (dma1 == dma2 || dma2 < 0) {chip->single_dma = 1;chip->dma2 = chip->dma1;} elsechip->dma2 = dma2;if (hardware == WSS_HW_THINKPAD) {chip->thinkpad_flag = 1;chip->hardware = WSS_HW_DETECT;   reset ", "pcm->private_data = chip;pcm->info_flags = 0;if (chip->single_dma)pcm->info_flags |= SNDRV_PCM_INFO_HALF_DUPLEX;if (chip->hardware != WSS_HW_INTERWAVE)pcm->info_flags |= SNDRV_PCM_INFO_JOINT_DUPLEX;strcpy(pcm->name, snd_wss_chip_id(chip));snd_pcm_set_managed_buffer_all(pcm, SNDRV_DMA_TYPE_DEV, chip->card->dev,       64*1024, chip->dma1 > 3 || chip->dma2 > 3 ? 128*1024 : 64*1024);chip->pcm = pcm;return 0;}EXPORT_SYMBOL(snd_wss_pcm": "snd_wss_pcm(struct snd_wss  chip, int device){struct snd_pcm  pcm;int err;err = snd_pcm_new(chip->card, \"WSS\", device, 1, 1, &pcm);if (err < 0)return err;snd_pcm_set_ops(pcm, SNDRV_PCM_STREAM_PLAYBACK, &snd_wss_playback_ops);snd_pcm_set_ops(pcm, SNDRV_PCM_STREAM_CAPTURE, &snd_wss_capture_ops);  global setup ", "spin_lock_irqsave(&chip->reg_lock, flags);if (!(chip->hardware & WSS_HW_AD1848_MASK)) ": "snd_wss_timer_resolution(struct snd_timer  timer){struct snd_wss  chip = snd_timer_chip(timer);if (chip->hardware & WSS_HW_CS4236B_MASK)return 14467;elsereturn chip->image[CS4231_PLAYBK_FORMAT] & 1 ? 9969 : 9920;}static int snd_wss_timer_start(struct snd_timer  timer){unsigned long flags;unsigned int ticks;struct snd_wss  chip = snd_timer_chip(timer);spin_lock_irqsave(&chip->reg_lock, flags);ticks = timer->sticks;if ((chip->image[CS4231_ALT_FEATURE_1] & CS4231_TIMER_ENABLE) == 0 ||    (unsigned char)(ticks >> 8) != chip->image[CS4231_TIMER_HIGH] ||    (unsigned char)ticks != chip->image[CS4231_TIMER_LOW]) {chip->image[CS4231_TIMER_HIGH] = (unsigned char) (ticks >> 8);snd_wss_out(chip, CS4231_TIMER_HIGH,    chip->image[CS4231_TIMER_HIGH]);chip->image[CS4231_TIMER_LOW] = (unsigned char) ticks;snd_wss_out(chip, CS4231_TIMER_LOW,    chip->image[CS4231_TIMER_LOW]);snd_wss_out(chip, CS4231_ALT_FEATURE_1,    chip->image[CS4231_ALT_FEATURE_1] |    CS4231_TIMER_ENABLE);}spin_unlock_irqrestore(&chip->reg_lock, flags);return 0;}static int snd_wss_timer_stop(struct snd_timer  timer){unsigned long flags;struct snd_wss  chip = snd_timer_chip(timer);spin_lock_irqsave(&chip->reg_lock, flags);chip->image[CS4231_ALT_FEATURE_1] &= ~CS4231_TIMER_ENABLE;snd_wss_out(chip, CS4231_ALT_FEATURE_1,    chip->image[CS4231_ALT_FEATURE_1]);spin_unlock_irqrestore(&chip->reg_lock, flags);return 0;}static void snd_wss_init(struct snd_wss  chip){unsigned long flags;snd_wss_calibrate_mute(chip, 1);snd_wss_mce_down(chip);#ifdef SNDRV_DEBUG_MCEsnd_printk(KERN_DEBUG \"init: (1)\\n\");#endifsnd_wss_mce_up(chip);spin_lock_irqsave(&chip->reg_lock, flags);chip->image[CS4231_IFACE_CTRL] &= ~(CS4231_PLAYBACK_ENABLE |    CS4231_PLAYBACK_PIO |    CS4231_RECORD_ENABLE |    CS4231_RECORD_PIO |    CS4231_CALIB_MODE);chip->image[CS4231_IFACE_CTRL] |= CS4231_AUTOCALIB;snd_wss_out(chip, CS4231_IFACE_CTRL, chip->image[CS4231_IFACE_CTRL]);spin_unlock_irqrestore(&chip->reg_lock, flags);snd_wss_mce_down(chip);#ifdef SNDRV_DEBUG_MCEsnd_printk(KERN_DEBUG \"init: (2)\\n\");#endifsnd_wss_mce_up(chip);spin_lock_irqsave(&chip->reg_lock, flags);chip->image[CS4231_IFACE_CTRL] &= ~CS4231_AUTOCALIB;snd_wss_out(chip, CS4231_IFACE_CTRL, chip->image[CS4231_IFACE_CTRL]);snd_wss_out(chip,    CS4231_ALT_FEATURE_1, chip->image[CS4231_ALT_FEATURE_1]);spin_unlock_irqrestore(&chip->reg_lock, flags);snd_wss_mce_down(chip);#ifdef SNDRV_DEBUG_MCEsnd_printk(KERN_DEBUG \"init: (3) - afei = 0x%x\\n\",   chip->image[CS4231_ALT_FEATURE_1]);#endifspin_lock_irqsave(&chip->reg_lock, flags);snd_wss_out(chip, CS4231_ALT_FEATURE_2,    chip->image[CS4231_ALT_FEATURE_2]);spin_unlock_irqrestore(&chip->reg_lock, flags);snd_wss_mce_up(chip);spin_lock_irqsave(&chip->reg_lock, flags);snd_wss_out(chip, CS4231_PLAYBK_FORMAT,    chip->image[CS4231_PLAYBK_FORMAT]);spin_unlock_irqrestore(&chip->reg_lock, flags);snd_wss_mce_down(chip);#ifdef SNDRV_DEBUG_MCEsnd_printk(KERN_DEBUG \"init: (4)\\n\");#endifsnd_wss_mce_up(chip);spin_lock_irqsave(&chip->reg_lock, flags);if (!(chip->hardware & WSS_HW_AD1848_MASK))snd_wss_out(chip, CS4231_REC_FORMAT,    chip->image[CS4231_REC_FORMAT]);spin_unlock_irqrestore(&chip->reg_lock, flags);snd_wss_mce_down(chip);snd_wss_calibrate_mute(chip, 0);#ifdef SNDRV_DEBUG_MCEsnd_printk(KERN_DEBUG \"init: (5)\\n\");#endif}static int snd_wss_open(struct snd_wss  chip, unsigned int mode){unsigned long flags;mutex_lock(&chip->open_mutex);if ((chip->mode & mode) ||    ((chip->mode & WSS_MODE_OPEN) && chip->single_dma)) {mutex_unlock(&chip->open_mutex);return -EAGAIN;}if (chip->mode & WSS_MODE_OPEN) {chip->mode |= mode;mutex_unlock(&chip->open_mutex);return 0;}  ok. now enable and ack CODEC IRQ ", "if (chip->hardware & WSS_HW_AD1848_MASK)count = 11;/* There is no loopback on OPTI93X ": "snd_wss_mixer(struct snd_wss  chip){struct snd_card  card;unsigned int idx;int err;int count = ARRAY_SIZE(snd_wss_controls);if (snd_BUG_ON(!chip || !chip->pcm))return -EINVAL;card = chip->card;strcpy(card->mixername, chip->pcm->name);  Use only the first 11 entries on AD1848 ", "elsegus->mix_cntrl_reg &= ~0x08;/* disable latches ": "snd_gf1_delay(gus);if (latches)gus->mix_cntrl_reg |= 0x08;  enable latches ", "static int snd_gus_free(struct snd_gus_card *gus)": "snd_gus_use_dec(struct snd_gus_card   gus){module_put(gus->card->module);}static int snd_gus_joystick_info(struct snd_kcontrol  kcontrol, struct snd_ctl_elem_info  uinfo){uinfo->type = SNDRV_CTL_ELEM_TYPE_INTEGER;uinfo->count = 1;uinfo->value.integer.min = 0;uinfo->value.integer.max = 31;return 0;}static int snd_gus_joystick_get(struct snd_kcontrol  kcontrol, struct snd_ctl_elem_value  ucontrol){struct snd_gus_card  gus = snd_kcontrol_chip(kcontrol);ucontrol->value.integer.value[0] = gus->joystick_dac & 31;return 0;}static int snd_gus_joystick_put(struct snd_kcontrol  kcontrol, struct snd_ctl_elem_value  ucontrol){struct snd_gus_card  gus = snd_kcontrol_chip(kcontrol);unsigned long flags;int change;unsigned char nval;nval = ucontrol->value.integer.value[0] & 31;spin_lock_irqsave(&gus->reg_lock, flags);change = gus->joystick_dac != nval;gus->joystick_dac = nval;snd_gf1_write8(gus, SNDRV_GF1_GB_JOYSTICK_DAC_LEVEL, gus->joystick_dac);spin_unlock_irqrestore(&gus->reg_lock, flags);return change;}static const struct snd_kcontrol_new snd_gus_joystick_control = {.iface = SNDRV_CTL_ELEM_IFACE_CARD,.name = \"Joystick Speed\",.info = snd_gus_joystick_info,.get = snd_gus_joystick_get,.put = snd_gus_joystick_put};static void snd_gus_init_control(struct snd_gus_card  gus){if (!gus->ace_flag)snd_ctl_add(gus->card, snd_ctl_new1(&snd_gus_joystick_control, gus));}    ", "}static int snd_gus_init_dma_irq(struct snd_gus_card * gus, int latches)": "snd_gf1_peek(gus, 0L) != 0xaa || snd_gf1_peek(gus, 1L) != 0x55) {snd_printk(KERN_ERR \"plain GF1 card at 0x%lx without onboard DRAM?\\n\", gus->gf1.port);return -ENOMEM;}for (idx = 1, d = 0xab; idx < 4; idx++, d++) {local = idx << 18;snd_gf1_poke(gus, local, d);snd_gf1_poke(gus, local + 1, d + 1);if (snd_gf1_peek(gus, local) != d ||    snd_gf1_peek(gus, local + 1) != d + 1 ||    snd_gf1_peek(gus, 0L) != 0xaa)break;}#if 1gus->gf1.memory = idx << 18;#elsegus->gf1.memory = 256   1024;#endiffor (l = 0, local = gus->gf1.memory; l < 4; l++, local -= 256   1024) {gus->gf1.mem_alloc.banks_8[l].address =    gus->gf1.mem_alloc.banks_8[l].size = 0;gus->gf1.mem_alloc.banks_16[l].address = l << 18;gus->gf1.mem_alloc.banks_16[l].size = local > 0 ? 256   1024 : 0;}gus->gf1.mem_alloc.banks_8[0].size = gus->gf1.memory;return 0;  some memory were detected ", "gus->gf1.reg_page = GUSP(gus, GF1PAGE);gus->gf1.reg_regsel = GUSP(gus, GF1REGSEL);gus->gf1.reg_data8 = GUSP(gus, GF1DATAHIGH);gus->gf1.reg_data16 = GUSP(gus, GF1DATALOW);gus->gf1.reg_irqstat = GUSP(gus, IRQSTAT);gus->gf1.reg_dram = GUSP(gus, DRAM);gus->gf1.reg_timerctrl = GUSP(gus, TIMERCNTRL);gus->gf1.reg_timerdata = GUSP(gus, TIMERDATA);/* allocate resources ": "snd_gus_create(struct snd_card  card,   unsigned long port,   int irq, int dma1, int dma2,   int timer_dev,   int voices,   int pcm_channels,   int effect,   struct snd_gus_card   rgus){struct snd_gus_card  gus;int err;static const struct snd_device_ops ops = {.dev_free =snd_gus_dev_free,}; rgus = NULL;gus = kzalloc(sizeof( gus), GFP_KERNEL);if (gus == NULL)return -ENOMEM;spin_lock_init(&gus->reg_lock);spin_lock_init(&gus->voice_alloc);spin_lock_init(&gus->active_voice_lock);spin_lock_init(&gus->event_lock);spin_lock_init(&gus->dma_lock);spin_lock_init(&gus->pcm_volume_level_lock);spin_lock_init(&gus->uart_cmd_lock);mutex_init(&gus->dma_mutex);gus->gf1.irq = -1;gus->gf1.dma1 = -1;gus->gf1.dma2 = -1;gus->card = card;gus->gf1.port = port;  fill register variables for speedup ", "EXPORT_SYMBOL(snd_gf1_delay);EXPORT_SYMBOL(snd_gf1_write8);EXPORT_SYMBOL(snd_gf1_look8);EXPORT_SYMBOL(snd_gf1_write16);EXPORT_SYMBOL(snd_gf1_look16);EXPORT_SYMBOL(snd_gf1_i_write8);EXPORT_SYMBOL(snd_gf1_i_look8);EXPORT_SYMBOL(snd_gf1_i_look16);EXPORT_SYMBOL(snd_gf1_dram_addr);EXPORT_SYMBOL(snd_gf1_write_addr);EXPORT_SYMBOL(snd_gf1_poke);EXPORT_SYMBOL(snd_gf1_peek);  /* gus_reset.c ": "snd_gus_initialize(struct snd_gus_card  gus){int err;if (!gus->interwave) {err = snd_gus_check_version(gus);if (err < 0) {snd_printk(KERN_ERR \"version check failed\\n\");return err;}err = snd_gus_detect_memory(gus);if (err < 0)return err;}err = snd_gus_init_dma_irq(gus, 1);if (err < 0)return err;snd_gf1_start(gus);gus->initialized = 1;return 0;}    gus_io.c ", "static int snd_gus_detect_memory(struct snd_gus_card * gus)": "snd_gus_interrupt, 0, \"GUS GF1\", (void  ) gus)) {snd_printk(KERN_ERR \"gus: can't grab irq %d\\n\", irq);snd_gus_free(gus);return -EBUSY;}gus->gf1.irq = irq;card->sync_irq = irq;if (request_dma(dma1, \"GUS - 1\")) {snd_printk(KERN_ERR \"gus: can't grab DMA1 %d\\n\", dma1);snd_gus_free(gus);return -EBUSY;}gus->gf1.dma1 = dma1;if (dma2 >= 0 && dma1 != dma2) {if (request_dma(dma2, \"GUS - 2\")) {snd_printk(KERN_ERR \"gus: can't grab DMA2 %d\\n\", dma2);snd_gus_free(gus);return -EBUSY;}gus->gf1.dma2 = dma2;} else {gus->gf1.dma2 = gus->gf1.dma1;gus->equal_dma = 1;}gus->timer_dev = timer_dev;if (voices < 14)voices = 14;if (voices > 32)voices = 32;if (pcm_channels < 0)pcm_channels = 0;if (pcm_channels > 8)pcm_channels = 8;pcm_channels++;pcm_channels &= ~1;gus->gf1.effect = effect ? 1 : 0;gus->gf1.active_voices = voices;gus->gf1.pcm_channels = pcm_channels;gus->gf1.volume_ramp = 25;gus->gf1.smooth_pan = 1;err = snd_device_new(card, SNDRV_DEV_LOWLEVEL, gus, &ops);if (err < 0) {snd_gus_free(gus);return err;} rgus = gus;return 0;}     Memory detection routine for plain GF1 soundcards ", "spin_lock_irqsave(&chip->reg_lock, flags);if (snd_sbdsp_reset(chip) < 0) ": "snd_sbdsp_reset(struct snd_sb  chip){int i;outb(1, SBP(chip, RESET));udelay(10);outb(0, SBP(chip, RESET));udelay(30);for (i = BUSY_LOOPS; i; i--)if (inb(SBP(chip, DATA_AVAIL)) & 0x80) {if (inb(SBP(chip, READ)) == 0xaa)return 0;elsebreak;}snd_printdd(\"%s [0x%lx] failed...\\n\", __func__, chip->port);return -ENODEV;}static int snd_sbdsp_version(struct snd_sb   chip){unsigned int result;snd_sbdsp_command(chip, SB_DSP_GET_VERSION);result = (short) snd_sbdsp_get_byte(chip) << 8;result |= (short) snd_sbdsp_get_byte(chip);return result;}static int snd_sbdsp_probe(struct snd_sb   chip){int version;int major, minor;char  str;unsigned long flags;     initialization sequence ", "dma16 = -1;} else if (snd_devm_request_dma(card->dev, dma16,\"SoundBlaster - 16bit\")) ": "snd_sbdsp_create(struct snd_card  card,     unsigned long port,     int irq,     irq_handler_t irq_handler,     int dma8,     int dma16,     unsigned short hardware,     struct snd_sb   r_chip){struct snd_sb  chip;int err;if (snd_BUG_ON(!r_chip))return -EINVAL; r_chip = NULL;chip = devm_kzalloc(card->dev, sizeof( chip), GFP_KERNEL);if (!chip)return -ENOMEM;spin_lock_init(&chip->reg_lock);spin_lock_init(&chip->open_lock);spin_lock_init(&chip->midi_input_lock);spin_lock_init(&chip->mixer_lock);chip->irq = -1;chip->dma8 = -1;chip->dma16 = -1;chip->port = port;if (devm_request_irq(card->dev, irq, irq_handler,     (hardware == SB_HW_ALS4000 ||      hardware == SB_HW_CS5530) ?     IRQF_SHARED : 0,     \"SoundBlaster\", (void  ) chip)) {snd_printk(KERN_ERR \"sb: can't grab irq %d\\n\", irq);return -EBUSY;}chip->irq = irq;card->sync_irq = chip->irq;if (hardware == SB_HW_ALS4000)goto __skip_allocation;chip->res_port = devm_request_region(card->dev, port, 16,     \"SoundBlaster\");if (!chip->res_port) {snd_printk(KERN_ERR \"sb: can't grab port 0x%lx\\n\", port);return -EBUSY;}#ifdef CONFIG_ISAif (dma8 >= 0 && snd_devm_request_dma(card->dev, dma8,      \"SoundBlaster - 8bit\")) {snd_printk(KERN_ERR \"sb: can't grab DMA8 %d\\n\", dma8);return -EBUSY;}chip->dma8 = dma8;if (dma16 >= 0) {if (hardware != SB_HW_ALS100 && (dma16 < 5 || dma16 > 7)) {  no duplex ", "if (chip->hardware != SB_HW_JAZZ16)break;fallthrough;case SB_MODE_PLAYBACK_8:substream = chip->playback_substream;if (chip->playback_format == SB_DSP_OUTPUT)    snd_sb8_playback_trigger(substream, SNDRV_PCM_TRIGGER_START);snd_pcm_period_elapsed(substream);break;case SB_MODE_CAPTURE_16:if (chip->hardware != SB_HW_JAZZ16)break;fallthrough;case SB_MODE_CAPTURE_8:substream = chip->capture_substream;if (chip->capture_format == SB_DSP_INPUT)    snd_sb8_capture_trigger(substream, SNDRV_PCM_TRIGGER_START);snd_pcm_period_elapsed(substream);break;}return IRQ_HANDLED;}static snd_pcm_uframes_t snd_sb8_playback_pointer(struct snd_pcm_substream *substream)": "snd_sb8dsp_interrupt(struct snd_sb  chip){struct snd_pcm_substream  substream;snd_sb_ack_8bit(chip);switch (chip->mode) {case SB_MODE_PLAYBACK_16:  ok.. playback is active ", "// printk(KERN_DEBUG \"codec->irq=%i, codec->dma8=%i, codec->dma16=%i\\n\", chip->irq, chip->dma8, chip->dma16);spin_lock_irqsave(&chip->mixer_lock, flags);mpureg = snd_sbmixer_read(chip, SB_DSP4_MPUSETUP) & ~0x06;spin_unlock_irqrestore(&chip->mixer_lock, flags);switch (chip->irq) ": "snd_sb16dsp_configure(struct snd_sb   chip){unsigned long flags;unsigned char irqreg = 0, dmareg = 0, mpureg;unsigned char realirq, realdma, realmpureg;  note: mpu register should be present only on SB16 Vibra soundcards ", "static snd_pcm_uframes_t snd_sb16_playback_pointer(struct snd_pcm_substream *substream)": "snd_sb16dsp_interrupt(int irq, void  dev_id){struct snd_sb  chip = dev_id;unsigned char status;int ok;spin_lock(&chip->mixer_lock);status = snd_sbmixer_read(chip, SB_DSP4_IRQSTATUS);spin_unlock(&chip->mixer_lock);if ((status & SB_IRQTYPE_MPUIN) && chip->rmidi_callback)chip->rmidi_callback(irq, chip->rmidi->private_data);if (status & SB_IRQTYPE_8BIT) {ok = 0;if (chip->mode & SB_MODE_PLAYBACK_8) {snd_pcm_period_elapsed(chip->playback_substream);snd_sb16_csp_update(chip);ok++;}if (chip->mode & SB_MODE_CAPTURE_8) {snd_pcm_period_elapsed(chip->capture_substream);ok++;}spin_lock(&chip->reg_lock);if (!ok)snd_sbdsp_command(chip, SB_DSP_DMA8_OFF);snd_sb_ack_8bit(chip);spin_unlock(&chip->reg_lock);}if (status & SB_IRQTYPE_16BIT) {ok = 0;if (chip->mode & SB_MODE_PLAYBACK_16) {snd_pcm_period_elapsed(chip->playback_substream);snd_sb16_csp_update(chip);ok++;}if (chip->mode & SB_MODE_CAPTURE_16) {snd_pcm_period_elapsed(chip->capture_substream);ok++;}spin_lock(&chip->reg_lock);if (!ok)snd_sbdsp_command(chip, SB_DSP_DMA16_OFF);snd_sb_ack_16bit(chip);spin_unlock(&chip->reg_lock);}return IRQ_HANDLED;}  ", "emu->last_reg = reg;}outw((unsigned short)val, port); /* Send data ": "snd_emu8000_poke(struct snd_emu8000  emu, unsigned int port, unsigned int reg, unsigned int val){unsigned long flags;spin_lock_irqsave(&emu->reg_lock, flags);if (reg != emu->last_reg) {outw((unsigned short)reg, EMU8000_PTR(emu));   Set register ", "emu->last_reg = reg;}res = inw(port);/* Read data ": "snd_emu8000_peek(struct snd_emu8000  emu, unsigned int port, unsigned int reg){unsigned short res;unsigned long flags;spin_lock_irqsave(&emu->reg_lock, flags);if (reg != emu->last_reg) {outw((unsigned short)reg, EMU8000_PTR(emu));   Set register ", "emu->last_reg = reg;}outw((unsigned short)val, port); /* Send low word of data ": "snd_emu8000_poke_dw(struct snd_emu8000  emu, unsigned int port, unsigned int reg, unsigned int val){unsigned long flags;spin_lock_irqsave(&emu->reg_lock, flags);if (reg != emu->last_reg) {outw((unsigned short)reg, EMU8000_PTR(emu));   Set register ", "emu->last_reg = reg;}low = inw(port);/* Read low word of data ": "snd_emu8000_peek_dw(struct snd_emu8000  emu, unsigned int port, unsigned int reg){unsigned short low;unsigned int res;unsigned long flags;spin_lock_irqsave(&emu->reg_lock, flags);if (reg != emu->last_reg) {outw((unsigned short)reg, EMU8000_PTR(emu));   Set register ", "EMU8000_CCCA_WRITE(emu, ch, 0x06000000 | right_bit);else   /* DMA read ": "snd_emu8000_dma_chan(struct snd_emu8000  emu, int ch, int mode){unsigned right_bit = (mode & EMU8000_RAM_RIGHT) ? 0x01000000 : 0;mode &= EMU8000_RAM_MODE_MASK;if (mode == EMU8000_RAM_CLOSE) {EMU8000_CCCA_WRITE(emu, ch, 0);EMU8000_DCYSUSV_WRITE(emu, ch, 0x807F);return;}EMU8000_DCYSUSV_WRITE(emu, ch, 0x80);EMU8000_VTFT_WRITE(emu, ch, 0);EMU8000_CVCF_WRITE(emu, ch, 0);EMU8000_PTRX_WRITE(emu, ch, 0x40000000);EMU8000_CPF_WRITE(emu, ch, 0x40000000);EMU8000_PSST_WRITE(emu, ch, 0);EMU8000_CSL_WRITE(emu, ch, 0);if (mode == EMU8000_RAM_WRITE)   DMA write ", "snd_emu8000_write_wait(emu);/* * Detect first 512 KiB.  If a write succeeds at the beginning of a * 512 KiB page we assume that the whole page is there. ": "snd_emu8000_init_fm(emu);   This must really be here and not 2 lines back even ", " voidsnd_emu8000_update_chorus_mode(struct snd_emu8000 *emu)": "snd_emu8000_load_chorus_fx(struct snd_emu8000  emu, int mode, const void __user  buf, long len){struct soundfont_chorus_fx rec;if (mode < SNDRV_EMU8000_CHORUS_PREDEFINED || mode >= SNDRV_EMU8000_CHORUS_NUMBERS) {snd_printk(KERN_WARNING \"invalid chorus mode %d for uploading\\n\", mode);return -EINVAL;}if (len < (long)sizeof(rec) || copy_from_user(&rec, buf, sizeof(rec)))return -EFAULT;chorus_parm[mode] = rec;chorus_defined[mode] = 1;return 0;} exported", " voidsnd_emu8000_update_reverb_mode(struct snd_emu8000 *emu)": "snd_emu8000_load_reverb_fx(struct snd_emu8000  emu, int mode, const void __user  buf, long len){struct soundfont_reverb_fx rec;if (mode < SNDRV_EMU8000_REVERB_PREDEFINED || mode >= SNDRV_EMU8000_REVERB_NUMBERS) {snd_printk(KERN_WARNING \"invalid reverb mode %d for uploading\\n\", mode);return -EINVAL;}if (len < (long)sizeof(rec) || copy_from_user(&rec, buf, sizeof(rec)))return -EFAULT;reverb_parm[mode] = rec;reverb_defined[mode] = 1;return 0;} exported", "static const unsigned short bass_parm[12][3] = ": "snd_emu8000_update_equalizer(emu);snd_emu8000_update_chorus_mode(emu);snd_emu8000_update_reverb_mode(emu);} ----------------------------------------------------------------   BassTreble Equalizer  ----------------------------------------------------------------", "p->ops.csp_use = snd_sb_csp_use;p->ops.csp_unuse = snd_sb_csp_unuse;p->ops.csp_autoload = snd_sb_csp_autoload;p->ops.csp_start = snd_sb_csp_start;p->ops.csp_stop = snd_sb_csp_stop;p->ops.csp_qsound_transfer = snd_sb_csp_qsound_transfer;mutex_init(&p->access_mutex);sprintf(hw->name, \"CSP v%d.%d\", (version >> 4), (version & 0x0f));hw->iface = SNDRV_HWDEP_IFACE_SB16CSP;hw->private_data = p;hw->private_free = snd_sb_csp_free;/* operators - only write/ioctl ": "snd_sb_csp_new(struct snd_sb  chip, int device, struct snd_hwdep    rhwdep){struct snd_sb_csp  p;int version;int err;struct snd_hwdep  hw;if (rhwdep) rhwdep = NULL;if (csp_detect(chip, &version))return -ENODEV;err = snd_hwdep_new(chip->card, \"SB16-CSP\", device, &hw);if (err < 0)return err;p = kzalloc(sizeof( p), GFP_KERNEL);if (!p) {snd_device_free(chip->card, hw);return -ENOMEM;}p->chip = chip;p->version = version;  CSP operators ", "udelay(10);outb(0, ES1688P(chip, RESET));udelay(30);for (i = 0; i < 1000 && !(inb(ES1688P(chip, DATA_AVAIL)) & 0x80); i++);if (inb(ES1688P(chip, READ)) != 0xaa) ": "snd_es1688_mixer_write(struct snd_es1688  chip,    unsigned char reg, unsigned char data){outb(reg, ES1688P(chip, MIXER_ADDR));udelay(10);outb(data, ES1688P(chip, MIXER_DATA));udelay(10);}static unsigned char snd_es1688_mixer_read(struct snd_es1688  chip, unsigned char reg){unsigned char result;outb(reg, ES1688P(chip, MIXER_ADDR));udelay(10);result = inb(ES1688P(chip, MIXER_DATA));udelay(10);return result;}int snd_es1688_reset(struct snd_es1688  chip){int i;outb(3, ES1688P(chip, RESET));  valid only for ESS chips, SB -> 1 ", "err = snd_device_new(card, SNDRV_DEV_LOWLEVEL, chip, &ops);exit:if (err)snd_es1688_free(chip);return err;}static const struct snd_pcm_ops snd_es1688_playback_ops = ": "snd_es1688_create(struct snd_card  card,      struct snd_es1688  chip,      unsigned long port,      unsigned long mpu_port,      int irq,      int mpu_irq,      int dma8,      unsigned short hardware){static const struct snd_device_ops ops = {.dev_free =snd_es1688_dev_free,};                                int err;if (chip == NULL)return -ENOMEM;chip->irq = -1;chip->dma8 = -1;chip->hardware = ES1688_HW_UNDEF;chip->res_port = request_region(port + 4, 12, \"ES1688\");if (chip->res_port == NULL) {snd_printk(KERN_ERR \"es1688: can't grab port 0x%lx\\n\", port + 4);err = -EBUSY;goto exit;}err = request_irq(irq, snd_es1688_interrupt, 0, \"ES1688\", (void  ) chip);if (err < 0) {snd_printk(KERN_ERR \"es1688: can't grab IRQ %d\\n\", irq);goto exit;}chip->irq = irq;card->sync_irq = chip->irq;err = request_dma(dma8, \"ES1688\");if (err < 0) {snd_printk(KERN_ERR \"es1688: can't grab DMA8 %d\\n\", dma8);goto exit;}chip->dma8 = dma8;spin_lock_init(&chip->reg_lock);spin_lock_init(&chip->mixer_lock);chip->port = port;mpu_port &= ~0x000f;if (mpu_port < 0x300 || mpu_port > 0x330)mpu_port = 0;chip->mpu_port = mpu_port;chip->mpu_irq = mpu_irq;chip->hardware = hardware;err = snd_es1688_probe(chip);if (err < 0)goto exit;err = snd_es1688_init(chip, 1);if (err < 0)goto exit;  Register device ", "static int snd_es1688_info_mux(struct snd_kcontrol *kcontrol, struct snd_ctl_elem_info *uinfo)": "snd_es1688_pcm(struct snd_card  card, struct snd_es1688  chip, int device){struct snd_pcm  pcm;int err;err = snd_pcm_new(card, \"ESx688\", device, 1, 1, &pcm);if (err < 0)return err;snd_pcm_set_ops(pcm, SNDRV_PCM_STREAM_PLAYBACK, &snd_es1688_playback_ops);snd_pcm_set_ops(pcm, SNDRV_PCM_STREAM_CAPTURE, &snd_es1688_capture_ops);pcm->private_data = chip;pcm->info_flags = SNDRV_PCM_INFO_HALF_DUPLEX;strcpy(pcm->name, snd_es1688_chip_id(chip));chip->pcm = pcm;snd_pcm_set_managed_buffer_all(pcm, SNDRV_DMA_TYPE_DEV, card->dev,       64 1024, 64 1024);return 0;}     MIXER part ", "ssp_mask |= BIT(epnt->virtual_bus_id);}epnt = (struct nhlt_endpoint *)((u8 *)epnt + epnt->length);}return ssp_mask;}EXPORT_SYMBOL(intel_nhlt_ssp_endpoint_mask": "intel_nhlt_ssp_endpoint_mask(struct nhlt_acpi_table  nhlt, u8 device_type){struct nhlt_endpoint  epnt;int ssp_mask = 0;int i;if (!nhlt || (device_type != NHLT_DEVICE_BT && device_type != NHLT_DEVICE_I2S))return 0;epnt = (struct nhlt_endpoint  )nhlt->desc;for (i = 0; i < nhlt->endpoint_count; i++) {if (epnt->linktype == NHLT_LINK_SSP && epnt->device_type == device_type) {  for SSP the virtual bus id is the SSP port ", "if (epnt->linktype == NHLT_LINK_SSP &&    epnt->device_type == NHLT_DEVICE_I2S &&    epnt->virtual_bus_id == ssp_num) ": "intel_nhlt_ssp_mclk_mask(struct nhlt_acpi_table  nhlt, int ssp_num){struct nhlt_endpoint  epnt;struct nhlt_fmt  fmt;struct nhlt_fmt_cfg  cfg;int mclk_mask = 0;int i, j;if (!nhlt)return 0;epnt = (struct nhlt_endpoint  )nhlt->desc;for (i = 0; i < nhlt->endpoint_count; i++) {  we only care about endpoints connected to an audio codec over SSP ", "uart_interrupt_tx(mpu);}/** * snd_mpu401_uart_interrupt - generic MPU401-UART interrupt handler * @irq: the irq number * @dev_id: mpu401 instance * * Processes the interrupt for MPU401-UART i/o. * * Return: %IRQ_HANDLED if the interrupt was handled. %IRQ_NONE otherwise. ": "snd_mpu401_uart_interrupt(struct snd_mpu401  mpu){unsigned long flags;if (mpu->info_flags & MPU401_INFO_INPUT) {spin_lock_irqsave(&mpu->input_lock, flags);if (test_bit(MPU401_MODE_BIT_INPUT, &mpu->mode))snd_mpu401_uart_input_read(mpu);elsesnd_mpu401_uart_clear_rx(mpu);spin_unlock_irqrestore(&mpu->input_lock, flags);}if (! (mpu->info_flags & MPU401_INFO_TX_IRQ))  ok. for better Tx performance try do some output   when input is done ", "irqreturn_t snd_mpu401_uart_interrupt_tx(int irq, void *dev_id)": "snd_mpu401_uart_interrupt_tx - generic MPU401-UART transmit irq handler   @irq: the irq number   @dev_id: mpu401 instance     Processes the interrupt for MPU401-UART output.     Return: %IRQ_HANDLED if the interrupt was handled. %IRQ_NONE otherwise. ", "int snd_mpu401_uart_new(struct snd_card *card, int device,unsigned short hardware,unsigned long port,unsigned int info_flags,int irq,struct snd_rawmidi ** rrawmidi)": "snd_mpu401_uart_new - create an MPU401-UART instance   @card: the card instance   @device: the device index, zero-based   @hardware: the hardware type, MPU401_HW_XXXX   @port: the base address of MPU401 port   @info_flags: bitflags MPU401_INFO_XXX   @irq: the ISA irq number, -1 if not to be allocated   @rrawmidi: the pointer to store the new rawmidi instance     Creates a new MPU-401 instance.     Note that the rawmidi instance is returned on the rrawmidi argument,   not the mpu401 instance itself.  To access to the mpu401 instance,   cast from rawmidi->private_data (with struct snd_mpu401 magic-cast).     Return: Zero if successful, or a negative error code. ", "/* create the devices if not built yet ": "snd_vx_setup_firmware(struct vx_core  chip){static const char   const fw_files[VX_TYPE_NUMS][4] = {[VX_TYPE_BOARD] = {NULL, \"x1_1_vx2.xlx\", \"bd56002.boot\", \"l_1_vx2.d56\",},[VX_TYPE_V2] = {NULL, \"x1_2_v22.xlx\", \"bd563v2.boot\", \"l_1_v22.d56\",},[VX_TYPE_MIC] = {NULL, \"x1_2_v22.xlx\", \"bd563v2.boot\", \"l_1_v22.d56\",},[VX_TYPE_VXPOCKET] = {\"bx_1_vxp.b56\", \"x1_1_vxp.xlx\", \"bd563s3.boot\", \"l_1_vxp.d56\"},[VX_TYPE_VXP440] = {\"bx_1_vp4.b56\", \"x1_1_vp4.xlx\", \"bd563s3.boot\", \"l_1_vp4.d56\"},};int i, err;for (i = 0; i < 4; i++) {char path[32];const struct firmware  fw;if (! fw_files[chip->type][i])continue;sprintf(path, \"vx%s\", fw_files[chip->type][i]);if (request_firmware(&fw, path, chip->dev)) {snd_printk(KERN_ERR \"vx: can't load firmware %s\\n\", path);return -ENOENT;}err = chip->ops->load_dsp(chip, i, fw);if (err < 0) {release_firmware(fw);return err;}if (i == 1)chip->chip_status |= VX_STAT_XILINX_LOADED;#ifdef CONFIG_PMchip->firmware[i] = fw;#elserelease_firmware(fw);#endif}  ok, we reached to the last one ", "int snd_vx_load_boot_image(struct vx_core *chip, const struct firmware *boot)": "snd_vx_load_boot_image - boot up the xilinx interface   @chip: VX core instance   @boot: the boot record to load ", "irqreturn_t snd_vx_threaded_irq_handler(int irq, void *dev)": "snd_vx_threaded_irq_handler - threaded irq handler ", "irqreturn_t snd_vx_irq_handler(int irq, void *dev)": "snd_vx_irq_handler - interrupt handler   @irq: irq number   @dev: VX core instance ", "int snd_vx_dsp_boot(struct vx_core *chip, const struct firmware *boot)": "snd_vx_dsp_boot - load the DSP boot   @chip: VX core instance   @boot: firmware data ", "int snd_vx_dsp_load(struct vx_core *chip, const struct firmware *dsp)": "snd_vx_dsp_load - load the DSP image   @chip: VX core instance   @dsp: firmware data ", "struct vx_core *snd_vx_create(struct snd_card *card,      const struct snd_vx_hardware *hw,      const struct snd_vx_ops *ops,      int extra_size)": "snd_vx_create - constructor for struct vx_core   @card: card instance   @hw: hardware specific record   @ops: VX ops pointer   @extra_size: extra byte size to allocate appending to chip     this function allocates the instance and prepare for the hardware   initialization.     The object is managed via devres, and will be automatically released.     return the instance pointer if successful, NULL in error. ", "snd_opl4_enable_opl4(opl4);snd_opl4_create_mixer(opl4);snd_opl4_create_proc(opl4);#if IS_ENABLED(CONFIG_SND_SEQUENCER)opl4->seq_client = -1;if (opl4->hardware < OPL3_HW_OPL4_ML)snd_opl4_create_seq_dev(opl4, seq_device);#endifif (ropl3)*ropl3 = opl3;if (ropl4)*ropl4 = opl4;return 0;}EXPORT_SYMBOL(snd_opl4_create": "snd_opl4_create_seq_dev(struct snd_opl4  opl4, int seq_device){opl4->seq_dev_num = seq_device;if (snd_seq_device_new(opl4->card, seq_device, SNDRV_SEQ_DEV_ID_OPL4,       sizeof(struct snd_opl4  ), &opl4->seq_dev) >= 0) {strcpy(opl4->seq_dev->name, \"OPL4 Wavetable\"); (struct snd_opl4   )SNDRV_SEQ_DEVICE_ARGPTR(opl4->seq_dev) = opl4;opl4->seq_dev->private_data = opl4;opl4->seq_dev->private_free = snd_opl4_seq_dev_free;}return 0;}#endifstatic void snd_opl4_free(struct snd_opl4  opl4){snd_opl4_free_proc(opl4);release_and_free_resource(opl4->res_fm_port);release_and_free_resource(opl4->res_pcm_port);kfree(opl4);}static int snd_opl4_dev_free(struct snd_device  device){struct snd_opl4  opl4 = device->device_data;snd_opl4_free(opl4);return 0;}int snd_opl4_create(struct snd_card  card,    unsigned long fm_port, unsigned long pcm_port,    int seq_device,    struct snd_opl3   ropl3, struct snd_opl4   ropl4){struct snd_opl4  opl4;struct snd_opl3  opl3;int err;static const struct snd_device_ops ops = {.dev_free = snd_opl4_dev_free};if (ropl3) ropl3 = NULL;if (ropl4) ropl4 = NULL;opl4 = kzalloc(sizeof( opl4), GFP_KERNEL);if (!opl4)return -ENOMEM;opl4->res_fm_port = request_region(fm_port, 8, \"OPL4 FM\");opl4->res_pcm_port = request_region(pcm_port, 8, \"OPL4 PCMMIX\");if (!opl4->res_fm_port || !opl4->res_pcm_port) {snd_printk(KERN_ERR \"opl4: can't grab ports 0x%lx, 0x%lx\\n\", fm_port, pcm_port);snd_opl4_free(opl4);return -EBUSY;}opl4->card = card;opl4->fm_port = fm_port;opl4->pcm_port = pcm_port;spin_lock_init(&opl4->reg_lock);mutex_init(&opl4->access_mutex);err = snd_opl4_detect(opl4);if (err < 0) {snd_opl4_free(opl4);snd_printd(\"OPL4 chip not detected at %#lx%#lx\\n\", fm_port, pcm_port);return err;}err = snd_device_new(card, SNDRV_DEV_CODEC, opl4, &ops);if (err < 0) {snd_opl4_free(opl4);return err;}err = snd_opl3_create(card, fm_port, fm_port + 2, opl4->hardware, 1, &opl3);if (err < 0) {snd_device_free(card, opl4);return err;}  opl3 initialization disabled opl4, so reenable ", "opl3->command(opl3, OPL3_LEFT | OPL3_REG_PERCUSSION, 0x00);switch (opl3->hardware & OPL3_HW_MASK) ": "snd_opl3_init(struct snd_opl3  opl3){if (! opl3->command) {printk(KERN_ERR \"snd_opl3_init: command not defined!\\n\");return -EINVAL;}opl3->command(opl3, OPL3_LEFT | OPL3_REG_TEST, OPL3_ENABLE_WAVE_SELECT);  Melodic mode ", "case OPL3_HW_OPL3_SV:case OPL3_HW_OPL3_CS:case OPL3_HW_OPL3_FM801:opl3->command = &snd_opl3_command;break;default:opl3->command = &snd_opl2_command;err = snd_opl3_detect(opl3);if (err < 0) ": "snd_opl3_create(struct snd_card  card,    unsigned long l_port,    unsigned long r_port,    unsigned short hardware,    int integrated,    struct snd_opl3    ropl3){struct snd_opl3  opl3;int err; ropl3 = NULL;err = snd_opl3_new(card, hardware, &opl3);if (err < 0)return err;if (! integrated) {opl3->res_l_port = request_region(l_port, 2, \"OPL23 (left)\");if (!opl3->res_l_port) {snd_printk(KERN_ERR \"opl3: can't grab left port 0x%lx\\n\", l_port);snd_device_free(card, opl3);return -EBUSY;}if (r_port != 0) {opl3->res_r_port = request_region(r_port, 2, \"OPL23 (right)\");if (!opl3->res_r_port) {snd_printk(KERN_ERR \"opl3: can't grab right port 0x%lx\\n\", r_port);snd_device_free(card, opl3);return -EBUSY;}}}opl3->l_port = l_port;opl3->r_port = r_port;switch (opl3->hardware) {  some hardware doesn't support timers ", "err = snd_hwdep_new(card, \"OPL2/OPL3\", device, &hw);if (err < 0) ": "snd_opl3_hwdep_new(struct snd_opl3   opl3,       int device, int seq_device,       struct snd_hwdep    rhwdep){struct snd_hwdep  hw;struct snd_card  card = opl3->card;int err;if (rhwdep) rhwdep = NULL;  create hardware dependent device (direct FM) ", "/* ------------------------": "snd_opl3_regmap[MAX_OPL2_VOICES][4] ={   OP1   OP2   OP3   OP4", "/* offsets for SBI params ": "snd_opl3_load_patch(opl3, inst.prog, inst.bank, type,  inst.name, inst.extension,  inst.data);if (err < 0)break;result += sizeof(inst);count -= sizeof(inst);}return result > 0 ? result : err;}    Patch management ", "struct fm_patch *snd_opl3_find_patch(struct snd_opl3 *opl3, int prog, int bank,     int create_patch)": "snd_opl3_find_patch(opl3, prog, bank, 1);if (!patch)return -ENOMEM;patch->type = type;for (i = 0; i < 2; i++) {patch->inst.op[i].am_vib = data[AM_VIB + i];patch->inst.op[i].ksl_level = data[KSL_LEVEL + i];patch->inst.op[i].attack_decay = data[ATTACK_DECAY + i];patch->inst.op[i].sustain_release = data[SUSTAIN_RELEASE + i];patch->inst.op[i].wave_select = data[WAVE_SELECT + i];}patch->inst.feedback_connection[0] = data[CONNECTION];if (type == FM_PATCH_OPL3) {for (i = 0; i < 2; i++) {patch->inst.op[i+2].am_vib =data[OFFSET_4OP + AM_VIB + i];patch->inst.op[i+2].ksl_level =data[OFFSET_4OP + KSL_LEVEL + i];patch->inst.op[i+2].attack_decay =data[OFFSET_4OP + ATTACK_DECAY + i];patch->inst.op[i+2].sustain_release =data[OFFSET_4OP + SUSTAIN_RELEASE + i];patch->inst.op[i+2].wave_select =data[OFFSET_4OP + WAVE_SELECT + i];}patch->inst.feedback_connection[1] =data[OFFSET_4OP + CONNECTION];}if (ext) {patch->inst.echo_delay = ext[0];patch->inst.echo_atten = ext[1];patch->inst.chorus_spread = ext[2];patch->inst.trnsps = ext[3];patch->inst.fix_dur = ext[4];patch->inst.modes = ext[5];patch->inst.fix_key = ext[6];}if (name)strscpy(patch->name, name, sizeof(patch->name));return 0;}EXPORT_SYMBOL(snd_opl3_load_patch);    find a patch with the given program and bank numbers, returns its pointer   if no matching patch is found and create_patch is set, it creates a   new patch object. ", "int snd_opl3_release(struct snd_hwdep * hw, struct file *file)": "snd_opl3_reset(opl3);return 0;case SNDRV_DM_FM_IOCTL_PLAY_NOTE:#ifdef CONFIG_SND_OSSEMULcase SNDRV_DM_FM_OSS_IOCTL_PLAY_NOTE:#endif{struct snd_dm_fm_note note;if (copy_from_user(&note, argp, sizeof(struct snd_dm_fm_note)))return -EFAULT;return snd_opl3_play_note(opl3, &note);}case SNDRV_DM_FM_IOCTL_SET_VOICE:#ifdef CONFIG_SND_OSSEMULcase SNDRV_DM_FM_OSS_IOCTL_SET_VOICE:#endif{struct snd_dm_fm_voice voice;if (copy_from_user(&voice, argp, sizeof(struct snd_dm_fm_voice)))return -EFAULT;return snd_opl3_set_voice(opl3, &voice);}case SNDRV_DM_FM_IOCTL_SET_PARAMS:#ifdef CONFIG_SND_OSSEMULcase SNDRV_DM_FM_OSS_IOCTL_SET_PARAMS:#endif{struct snd_dm_fm_params params;if (copy_from_user(&params, argp, sizeof(struct snd_dm_fm_params)))return -EFAULT;return snd_opl3_set_params(opl3, &params);}case SNDRV_DM_FM_IOCTL_SET_MODE:#ifdef CONFIG_SND_OSSEMULcase SNDRV_DM_FM_OSS_IOCTL_SET_MODE:#endifreturn snd_opl3_set_mode(opl3, (int) arg);case SNDRV_DM_FM_IOCTL_SET_CONNECTION:#ifdef CONFIG_SND_OSSEMULcase SNDRV_DM_FM_OSS_IOCTL_SET_OPL:#endifreturn snd_opl3_set_connection(opl3, (int) arg);#ifdef OPL3_SUPPORT_SYNTHcase SNDRV_DM_FM_IOCTL_CLEAR_PATCHES:snd_opl3_clear_patches(opl3);return 0;#endif#ifdef CONFIG_SND_DEBUGdefault:snd_printk(KERN_WARNING \"unknown IOCTL: 0x%x\\n\", cmd);#endif}return -ENOTTY;}    close the device ", "list_move_tail(&blk->mapped_order_link,       &emu->mapped_order_link_head);spin_unlock_irqrestore(&emu->memblk_lock, flags);return 0;}err = map_memblk(emu, blk);if (err < 0) ": "snd_emu10k1_memblk_map(struct snd_emu10k1  emu, struct snd_emu10k1_memblk  blk){int err;int size;struct list_head  p,  nextp;struct snd_emu10k1_memblk  deleted;unsigned long flags;spin_lock_irqsave(&emu->memblk_lock, flags);if (blk->mapped_page >= 0) {  update order link ", "int snd_emu10k1_alloc_pages_maybe_wider(struct snd_emu10k1 *emu, size_t size,struct snd_dma_buffer *dmab)": "snd_emu10k1_synth_free(emu, blk);}    allocate DMA pages, widening the allocation if necessary     See the comment above snd_emu10k1_detect_iommu() in emu10k1_main.c why   this might be needed.     If you modify this function check whether __synth_free_pages() also needs   changes. ", "continue;}if (type != EMU10K1_SYNTH && emu->get_synth_voice) ": "snd_emu10k1_voice_alloc(struct snd_emu10k1  emu, int type, int count, int channels,    struct snd_emu10k1_pcm  epcm, struct snd_emu10k1_voice   rvoice){unsigned long flags;int result;if (snd_BUG_ON(!rvoice))return -EINVAL;if (snd_BUG_ON(!count))return -EINVAL;if (snd_BUG_ON(!channels))return -EINVAL;spin_lock_irqsave(&emu->voice_lock, flags);for (int got = 0; got < channels; ) {result = voice_alloc(emu, type, count, epcm, &rvoice[got]);if (result == 0) {got++; dev_dbg(emu->card->dev, \"voice alloc - %i, %i of %i\\n\",        rvoice[got - 1]->number, got, want);", "int snd_ice1712_akm4xxx_build_controls(struct snd_ice1712 *ice)": "snd_ice1712_akm4xxx_free(struct snd_ice1712  ice){unsigned int akidx;if (ice->akm == NULL)return;for (akidx = 0; akidx < ice->akm_codecs; akidx++) {struct snd_akm4xxx  ak = &ice->akm[akidx];kfree((void )ak->private_value[0]);}kfree(ice->akm);}    build AK4xxx controls ", "void snd_trident_start_voice(struct snd_trident * trident, unsigned int voice)": "snd_trident_start_voice(struct snd_trident   trident, unsigned int voice)    Description: Start a voice, any channel 0 thru 63.                 This routine automatically handles the fact that there are                 more than 32 channels available.    Parameters : voice - Voice number 0 thru n.                 trident - pointer to target device class for 4DWave.    Return Value: None.  ---------------------------------------------------------------------------", "void snd_trident_stop_voice(struct snd_trident * trident, unsigned int voice)": "snd_trident_stop_voice(struct snd_trident   trident, unsigned int voice)    Description: Stop a voice, any channel 0 thru 63.                 This routine automatically handles the fact that there are                 more than 32 channels available.    Parameters : voice - Voice number 0 thru n.                 trident - pointer to target device class for 4DWave.    Return Value: None.  ---------------------------------------------------------------------------", "void snd_trident_write_voice_regs(struct snd_trident * trident,  struct snd_trident_voice * voice)": "snd_trident_write_voice_regs     Description: This routine will complete and write the 5 hardware channel                registers to hardware.     Parameters:  trident - pointer to target device class for 4DWave.                voice - synthesizer voice structure                Each register field.    ---------------------------------------------------------------------------", "static int snd_trident_hw_params(struct snd_pcm_substream *substream, struct snd_pcm_hw_params *hw_params)": "snd_trident_free_voice(trident, evoice);voice->extra = evoice = NULL;}}return 0;} ---------------------------------------------------------------------------   snd_trident_hw_params     Description: Set the hardware parameters for the playback device.     Parameters:  substream  - PCM substream classhw_params  - hardware parameters     Returns:     Error status    ---------------------------------------------------------------------------", "int snd_ac97_set_rate(struct snd_ac97 *ac97, int reg, unsigned int rate)": "snd_ac97_set_rate - change the rate of the given inputoutput.   @ac97: the ac97 instance   @reg: the register to change   @rate: the sample rate to set     Changes the rate of the given inputoutput on the codec.   If the codec doesn't support VAR, the rate must be 48000 (except   for SPDIF).     The valid registers are AC97_PCM_MIC_ADC_RATE,   AC97_PCM_FRONT_DAC_RATE, AC97_PCM_LR_ADC_RATE.   AC97_PCM_SURR_DAC_RATE and AC97_PCM_LFE_DAC_RATE are accepted   if the codec supports them.   AC97_SPDIF is accepted as a pseudo register to modify the SPDIF   status bits.     Return: Zero if successful, or a negative error code on failure. ", "int snd_ac97_pcm_assign(struct snd_ac97_bus *bus,unsigned short pcms_count,const struct ac97_pcm *pcms)": "snd_ac97_pcm_assign - assign AC97 slots to given PCM streams   @bus: the ac97 bus instance   @pcms_count: count of PCMs to be assigned   @pcms: PCMs to be assigned     It assigns available AC97 slots for given PCMs. If none or only   some slots are available, pcm->xxx.slots and pcm->xxx.rslots[] members   are reduced and might be zero.     Return: Zero if successful, or a negative error code on failure. ", "int snd_ac97_pcm_open(struct ac97_pcm *pcm, unsigned int rate,      enum ac97_pcm_cfg cfg, unsigned short slots)": "snd_ac97_pcm_open - opens the given AC97 pcm   @pcm: the ac97 pcm instance   @rate: rate in Hz, if codec does not support VRA, this value must be 48000Hz   @cfg: output stream characteristics   @slots: a subset of allocated slots (snd_ac97_pcm_assign) for this pcm     It locks the specified slots and sets the given rate to AC97 registers.     Return: Zero if successful, or a negative error code on failure. ", "int snd_ac97_pcm_close(struct ac97_pcm *pcm)": "snd_ac97_pcm_close(pcm);return err;}EXPORT_SYMBOL(snd_ac97_pcm_open);     snd_ac97_pcm_close - closes the given AC97 pcm   @pcm: the ac97 pcm instance     It frees the locked AC97 slots.     Return: Zero. ", "int snd_ac97_pcm_double_rate_rules(struct snd_pcm_runtime *runtime)": "snd_ac97_pcm_double_rate_rules - set double rate constraints   @runtime: the runtime of the ac97 front playback pcm     Installs the hardware constraint rules to prevent using double rates and   more than two channels at the same time.     Return: Zero if successful, or a negative error code on failure. ", "void snd_ac97_write(struct snd_ac97 *ac97, unsigned short reg, unsigned short value)": "snd_ac97_write - write a value on the given register   @ac97: the ac97 instance   @reg: the register to change   @value: the value to set     Writes a value on the given register.  This will invoke the write   callback directly after the register check.   This function doesn't change the register cache unlike   #snd_ca97_write_cache(), so use this only when you don't want to   reflect the change to the suspendresume state. ", "unsigned short snd_ac97_read(struct snd_ac97 *ac97, unsigned short reg)": "snd_ac97_read - read a value from the given register      @ac97: the ac97 instance   @reg: the register to read     Reads a value from the given register.  This will invoke the read   callback directly after the register check.     Return: The read value. ", "void snd_ac97_write_cache(struct snd_ac97 *ac97, unsigned short reg, unsigned short value)": "snd_ac97_write_cache - write a value on the given register and update the cache   @ac97: the ac97 instance   @reg: the register to change   @value: the value to set     Writes a value on the given register and updates the register   cache.  The cached values are used for the cached-read and the   suspendresume. ", "int snd_ac97_update(struct snd_ac97 *ac97, unsigned short reg, unsigned short value)": "snd_ac97_update - update the value on the given register   @ac97: the ac97 instance   @reg: the register to change   @value: the value to set     Compares the value with the register cache and updates the value   only when the value is changed.     Return: 1 if the value is changed, 0 if no change, or a negative   code on failure. ", "int snd_ac97_update_bits(struct snd_ac97 *ac97, unsigned short reg, unsigned short mask, unsigned short value)": "snd_ac97_update_bits - update the bits on the given register   @ac97: the ac97 instance   @reg: the register to change   @mask: the bit-mask to change   @value: the value to set     Updates the masked-bits on the given register only when the value   is changed.     Return: 1 if the bits are changed, 0 if no change, or a negative   code on failure. ", "const char *snd_ac97_get_short_name(struct snd_ac97 *ac97)": "snd_ac97_get_short_name - retrieve codec name   @ac97: the codec instance     Return: The short identifying name of the codec. ", "return snd_ac97_free(ac97);}static int snd_ac97_try_volume_mix(struct snd_ac97 * ac97, int reg)": "snd_ac97_bus_free(struct snd_ac97_bus  bus){if (bus) {snd_ac97_bus_proc_done(bus);kfree(bus->pcms);if (bus->private_free)bus->private_free(bus);kfree(bus);}return 0;}static int snd_ac97_bus_dev_free(struct snd_device  device){struct snd_ac97_bus  bus = device->device_data;return snd_ac97_bus_free(bus);}static int snd_ac97_free(struct snd_ac97  ac97){if (ac97) {#ifdef CONFIG_SND_AC97_POWER_SAVEcancel_delayed_work_sync(&ac97->power_work);#endifsnd_ac97_proc_done(ac97);if (ac97->bus)ac97->bus->codec[ac97->num] = NULL;if (ac97->private_free)ac97->private_free(ac97);kfree(ac97);}return 0;}static int snd_ac97_dev_free(struct snd_device  device){struct snd_ac97  ac97 = device->device_data;snd_ac97_powerdown(ac97);   for avoiding click noises during shut down ", "/* AD claims to remove this control from AD1887, although spec v2.2 does not allow this ": "snd_ac97_mixer_build(struct snd_ac97   ac97){struct snd_card  card = ac97->bus->card;struct snd_kcontrol  kctl;int err;unsigned int idx;unsigned char max;  build master controls ", "int snd_ac97_update_power(struct snd_ac97 *ac97, int reg, int powerup)": "snd_ac97_update_power - update the powerdown register   @ac97: the codec instance   @reg: the rate register, e.g. AC97_PCM_FRONT_DAC_RATE   @powerup: non-zero when power up the part     Update the AC97 powerdown register bits of the given part.     Return: Zero. ", "void snd_ac97_suspend(struct snd_ac97 *ac97)": "snd_ac97_suspend - General suspend function for AC97 codec   @ac97: the ac97 instance     Suspends the codec, power down the chip. ", "static void snd_ac97_powerdown(struct snd_ac97 *ac97)": "snd_ac97_resume(). ", "int snd_ac97_tune_hardware(struct snd_ac97 *ac97,   const struct ac97_quirk *quirk, const char *override)": "snd_ac97_tune_hardware - tune up the hardware   @ac97: the ac97 instance   @quirk: quirk list   @override: explicit quirk value (overrides the list if non-NULL)     Do some workaround for each pci device, such as renaming of the   headphone (true line-out) control as \"Master\".   The quirk-list must be terminated with a zero-filled entry.     Return: Zero if successful, or a negative error code on failure. ", "void hpi_send_recv(struct hpi_message *phm, struct hpi_response *phr)": "hpi_send_recv_f(struct hpi_message  phm, struct hpi_response  phr,struct file  file){if ((phm->adapter_index >= HPI_MAX_ADAPTERS)&& (phm->object != HPI_OBJ_SUBSYSTEM))phr->error = HPI_ERROR_INVALID_OBJ_INDEX;elsehpi_send_recv_ex(phm, phr, file);}  This is called from hpifunc.c functions, called by ALSA   (or other kernel process) In this case there is no file descriptor   available for the message cache code ", "if (oxygen_ac97_wait(chip, OXYGEN_AC97_INT_WRITE_DONE) >= 0 &&    ++succeeded >= 2) ": "oxygen_write_ac97(struct oxygen  chip, unsigned int codec,       unsigned int index, u16 data){unsigned int count, succeeded;u32 reg;reg = data;reg |= index << OXYGEN_AC97_REG_ADDR_SHIFT;reg |= OXYGEN_AC97_REG_DIR_WRITE;reg |= codec << OXYGEN_AC97_REG_CODEC_SHIFT;succeeded = 0;for (count = 5; count > 0; --count) {udelay(5);oxygen_write32(chip, OXYGEN_AC97_REGS, reg);  require two \"completed\" writes, just to be sure ", "if (value == last_read)return value;last_read = value;/* * Invert the register value bits to make sure that two * consecutive unsuccessful reads do not return the same * value. ": "oxygen_read_ac97(struct oxygen  chip, unsigned int codec,     unsigned int index){unsigned int count;unsigned int last_read = UINT_MAX;u32 reg;reg = index << OXYGEN_AC97_REG_ADDR_SHIFT;reg |= OXYGEN_AC97_REG_DIR_READ;reg |= codec << OXYGEN_AC97_REG_CODEC_SHIFT;for (count = 5; count > 0; --count) {udelay(5);oxygen_write32(chip, OXYGEN_AC97_REGS, reg);udelay(10);if (oxygen_ac97_wait(chip, OXYGEN_AC97_INT_READ_DONE) >= 0) {u16 value = oxygen_read16(chip, OXYGEN_AC97_REGS);  we require two consecutive reads of the same value ", "oxygen_write8(chip, OXYGEN_SPI_DATA1, data);oxygen_write8(chip, OXYGEN_SPI_DATA2, data >> 8);if (control & OXYGEN_SPI_DATA_LENGTH_3)oxygen_write8(chip, OXYGEN_SPI_DATA3, data >> 16);oxygen_write8(chip, OXYGEN_SPI_CONTROL, control);return oxygen_wait_spi(chip);}EXPORT_SYMBOL(oxygen_write_spi": "oxygen_write_spi(struct oxygen  chip, u8 control, unsigned int data){    We need to wait AFTER initiating the SPI transaction,   otherwise read operations will not work. ", "msleep(1);oxygen_write8(chip, OXYGEN_2WIRE_MAP, map);oxygen_write8(chip, OXYGEN_2WIRE_DATA, data);oxygen_write8(chip, OXYGEN_2WIRE_CONTROL,      device | OXYGEN_2WIRE_DIR_WRITE);}EXPORT_SYMBOL(oxygen_write_i2c": "oxygen_write_i2c(struct oxygen  chip, u8 device, u8 map, u8 data){  should not need more than about 300 us ", "_write_uart(chip, 1, MPU401_ENTER_UART);}EXPORT_SYMBOL(oxygen_reset_uart": "oxygen_reset_uart(struct oxygen  chip){_write_uart(chip, 1, MPU401_RESET);msleep(1);   wait for ACK ", "static const unsigned int reg_values[5] = ": "oxygen_update_dac_routing(struct oxygen  chip){  DAC 0: front, DAC 1: surround, DAC 2: centerLFE, DAC 3: back ", "down_write(&umidi->disc_rwsem);spin_lock_irq(&umidi->disc_lock);umidi->disconnected = 1;spin_unlock_irq(&umidi->disc_lock);up_write(&umidi->disc_rwsem);del_timer_sync(&umidi->error_timer);for (i = 0; i < MIDI_MAX_ENDPOINTS; ++i) ": "snd_usbmidi_disconnect(struct list_head  p){struct snd_usb_midi  umidi;unsigned int i, j;umidi = list_entry(p, struct snd_usb_midi, list);    an URB's completion handler may start the timer and   a timer may submit an URB. To reliably break the cycle   a flag under lock must be used ", "snd_rawmidi_proceed(substream);return;}queue_work(system_highpri_wq, &port->ep->work);}}static void snd_usbmidi_output_drain(struct snd_rawmidi_substream *substream)": "snd_usbmidi_input_start(&umidi->list);}static int substream_open(struct snd_rawmidi_substream  substream, int dir,  int open){struct snd_usb_midi  umidi = substream->rmidi->private_data;struct snd_kcontrol  ctl;down_read(&umidi->disc_rwsem);if (umidi->disconnected) {up_read(&umidi->disc_rwsem);return open ? -ENODEV : 0;}mutex_lock(&umidi->mutex);if (open) {if (!umidi->opened[0] && !umidi->opened[1]) {if (umidi->roland_load_ctl) {ctl = umidi->roland_load_ctl;ctl->vd[0].access |=SNDRV_CTL_ELEM_ACCESS_INACTIVE;snd_ctl_notify(umidi->card,       SNDRV_CTL_EVENT_MASK_INFO, &ctl->id);update_roland_altsetting(umidi);}}umidi->opened[dir]++;if (umidi->opened[1])snd_usbmidi_input_start(&umidi->list);} else {umidi->opened[dir]--;if (!umidi->opened[1])snd_usbmidi_input_stop(&umidi->list);if (!umidi->opened[0] && !umidi->opened[1]) {if (umidi->roland_load_ctl) {ctl = umidi->roland_load_ctl;ctl->vd[0].access &=~SNDRV_CTL_ELEM_ACCESS_INACTIVE;snd_ctl_notify(umidi->card,       SNDRV_CTL_EVENT_MASK_INFO, &ctl->id);}}}mutex_unlock(&umidi->mutex);up_read(&umidi->disc_rwsem);return 0;}static int snd_usbmidi_output_open(struct snd_rawmidi_substream  substream){struct snd_usb_midi  umidi = substream->rmidi->private_data;struct usbmidi_out_port  port = NULL;int i, j;for (i = 0; i < MIDI_MAX_ENDPOINTS; ++i)if (umidi->endpoints[i].out)for (j = 0; j < 0x10; ++j)if (umidi->endpoints[i].out->ports[j].substream == substream) {port = &umidi->endpoints[i].out->ports[j];break;}if (!port)return -ENXIO;substream->runtime->private_data = port;port->state = STATE_UNKNOWN;return substream_open(substream, 0, 1);}static int snd_usbmidi_output_close(struct snd_rawmidi_substream  substream){struct usbmidi_out_port  port = substream->runtime->private_data;cancel_work_sync(&port->ep->work);return substream_open(substream, 0, 0);}static void snd_usbmidi_output_trigger(struct snd_rawmidi_substream  substream,       int up){struct usbmidi_out_port  port =(struct usbmidi_out_port  )substream->runtime->private_data;port->active = up;if (up) {if (port->ep->umidi->disconnected) {  gobble up remaining bytes to prevent wait in   snd_rawmidi_drain_output ", "memset(endpoints, 0, sizeof(endpoints));switch (quirk ? quirk->type : QUIRK_MIDI_STANDARD_INTERFACE) ": "__snd_usbmidi_create(struct snd_card  card, struct usb_interface  iface, struct list_head  midi_list, const struct snd_usb_audio_quirk  quirk, unsigned int usb_id, unsigned int  num_rawmidis){struct snd_usb_midi  umidi;struct snd_usb_midi_endpoint_info endpoints[MIDI_MAX_ENDPOINTS];int out_ports, in_ports;int i, err;umidi = kzalloc(sizeof( umidi), GFP_KERNEL);if (!umidi)return -ENOMEM;umidi->dev = interface_to_usbdev(iface);umidi->card = card;umidi->iface = iface;umidi->quirk = quirk;umidi->usb_protocol_ops = &snd_usbmidi_standard_ops;if (num_rawmidis)umidi->next_midi_device =  num_rawmidis;spin_lock_init(&umidi->disc_lock);init_rwsem(&umidi->disc_rwsem);mutex_init(&umidi->mutex);if (!usb_id)usb_id = USB_ID(le16_to_cpu(umidi->dev->descriptor.idVendor),       le16_to_cpu(umidi->dev->descriptor.idProduct));umidi->usb_id = usb_id;timer_setup(&umidi->error_timer, snd_usbmidi_error_timer, 0);  detect the endpoint(s) to use ", "void snd_util_memhdr_free(struct snd_util_memhdr *hdr)": "snd_util_memhdr_new(int memsize){struct snd_util_memhdr  hdr;hdr = kzalloc(sizeof( hdr), GFP_KERNEL);if (hdr == NULL)return NULL;hdr->size = memsize;mutex_init(&hdr->block_mutex);INIT_LIST_HEAD(&hdr->block);return hdr;}    free a memory manager ", "while ((p = hdr->block.next) != &hdr->block) ": "snd_util_memhdr_free(struct snd_util_memhdr  hdr){struct list_head  p;if (!hdr)return;  release all blocks ", "units = size;if (units & 1)units++;if (units > hdr->size)return NULL;/* look for empty block ": "__snd_util_mem_alloc(struct snd_util_memhdr  hdr, int size){struct snd_util_memblk  blk;unsigned int units, prev_offset;struct list_head  p;if (snd_BUG_ON(!hdr || size <= 0))return NULL;  word alignment ", "int snd_util_mem_free(struct snd_util_memhdr *hdr, struct snd_util_memblk *blk)": "__snd_util_mem_free(struct snd_util_memhdr  hdr, struct snd_util_memblk  blk){list_del(&blk->list);hdr->nblocks--;hdr->used -= blk->size;kfree(blk);}    free a memory block (with mutex) ", "struct snd_util_memblk *__snd_util_memblk_new(struct snd_util_memhdr *hdr, unsigned int units,      struct list_head *prev)": "__snd_util_memblk_new(hdr, units, p->prev);}    create a new memory block with the given size   the block is linked next to prev ", "memset(&sf_cb, 0, sizeof(sf_cb));sf_cb.private_data = emu;if (emu->ops.sample_new)sf_cb.sample_new = sf_sample_new;if (emu->ops.sample_free)sf_cb.sample_free = sf_sample_free;if (emu->ops.sample_reset)sf_cb.sample_reset = sf_sample_reset;emu->sflist = snd_sf_new(&sf_cb, emu->memhdr);if (emu->sflist == NULL)return -ENOMEM;err = snd_emux_init_hwdep(emu);if (err < 0)return err;snd_emux_init_voices(emu);snd_emux_init_seq(emu, card, index);#if IS_ENABLED(CONFIG_SND_SEQUENCER_OSS)snd_emux_init_seq_oss(emu);#endifsnd_emux_init_virmidi(emu, card);snd_emux_proc_init(emu, card, index);return 0;}EXPORT_SYMBOL(snd_emux_register": "snd_emux_register(struct snd_emux  emu, struct snd_card  card, int index, char  name){int err;struct snd_sf_callback sf_cb;if (snd_BUG_ON(!emu->hw || emu->max_voices <= 0))return -EINVAL;if (snd_BUG_ON(!card || !name))return -EINVAL;emu->card = card;emu->name = kstrdup(name, GFP_KERNEL);emu->voices = kcalloc(emu->max_voices, sizeof(struct snd_emux_voice),      GFP_KERNEL);if (emu->name == NULL || emu->voices == NULL)return -ENOMEM;  create soundfont list ", "v = (log_tbl[s + 1] * low + log_tbl[s] * (0x100 - low)) >> 8;v -= offset;v = (v * ratio) >> 16;v += (24 - bit) * ratio;return v;}EXPORT_SYMBOL(snd_sf_linear_to_log": "snd_sf_linear_to_log(unsigned int amount, int offset, int ratio){int v;int s, low, bit;if (amount < 2)return 0;for (bit = 0; ! (amount & 0x80000000L); bit++)amount <<= 1;s = (amount >> 24) & 0x7f;low = (amount >> 16) & 0xff;  linear approximation by lower 8 bit ", "emu->use_time = 0;spin_unlock_irqrestore(&emu->voice_lock, flags);}EXPORT_SYMBOL(snd_emux_terminate_all": "snd_emux_terminate_all(struct snd_emux  emu){int i;struct snd_emux_voice  vp;unsigned long flags;spin_lock_irqsave(&emu->voice_lock, flags);for (i = 0; i < emu->max_voices; i++) {vp = &emu->voices[i];if (STATE_IS_PLAYING(vp->state))terminate_voice(emu, vp, 0);if (vp->state == SNDRV_EMUX_ST_OFF) {if (emu->ops.free_voice)emu->ops.free_voice(vp);if (emu->ops.reset)emu->ops.reset(emu, i);}vp->time = 0;}  initialize allocation time ", "struct snd_ac97 *snd_soc_alloc_ac97_component(struct snd_soc_component *component)": "snd_soc_alloc_ac97_component() - Allocate new a AC'97 device   @component: The COMPONENT for which to create the AC'97 device     Allocated a new snd_ac97 device and intializes it, but does not yet register   it. The caller is responsible to either call device_add(&ac97->dev) to   register the device, or to call put_device(&ac97->dev) to free the device.     Returns: A snd_ac97 device or a PTR_ERR in case of an error. ", "mt8192_afe_gpio_request(dev, false, MT8192_DAI_ADDA, 0);mt8192_afe_gpio_request(dev, false, MT8192_DAI_ADDA, 1);return 0;}EXPORT_SYMBOL(mt8192_afe_gpio_init": "mt8192_afe_gpio_init(struct device  dev){int i, ret;aud_pinctrl = devm_pinctrl_get(dev);if (IS_ERR(aud_pinctrl)) {ret = PTR_ERR(aud_pinctrl);dev_err(dev, \"%s(), ret %d, cannot get aud_pinctrl!\\n\",__func__, ret);return ret;}for (i = 0; i < ARRAY_SIZE(aud_gpios); i++) {aud_gpios[i].gpioctrl = pinctrl_lookup_state(aud_pinctrl,     aud_gpios[i].name);if (IS_ERR(aud_gpios[i].gpioctrl)) {ret = PTR_ERR(aud_gpios[i].gpioctrl);dev_dbg(dev, \"%s(), pinctrl_lookup_state %s fail, ret %d\\n\",__func__, aud_gpios[i].name, ret);} else {aud_gpios[i].gpio_prepare = true;}}mt8192_afe_gpio_select(dev, MT8192_AFE_GPIO_CLK_MOSI_ON);  gpio status init ", "int fsl_asoc_get_dma_channel(struct device_node *ssi_np,     const char *name,     struct snd_soc_dai_link *dai,     unsigned int *dma_channel_id,     unsigned int *dma_id)": "fsl_asoc_get_dma_channel - determine the dma channel for a SSI node     @ssi_np: pointer to the SSI device tree node   @name: name of the phandle pointing to the dma channel   @dai: ASoC DAI link pointer to be filled with platform_name   @dma_channel_id: dma channel id to be returned   @dma_id: dma id to be returned     This function determines the dma and channel id for given SSI node.  It   also discovers the platform_name for the ASoC DAI link. ", "void fsl_asoc_get_pll_clocks(struct device *dev, struct clk **pll8k_clk,     struct clk **pll11k_clk)": "fsl_asoc_get_pll_clocks - get two PLL clock source     @dev: device pointer   @pll8k_clk: PLL clock pointer for 8kHz   @pll11k_clk: PLL clock pointer for 11kHz     This function get two PLL clock source ", "void fsl_asoc_reparent_pll_clocks(struct device *dev, struct clk *clk,  struct clk *pll8k_clk,  struct clk *pll11k_clk, u64 ratio)": "fsl_asoc_reparent_pll_clocks - set clock parent if necessary     @dev: device pointer   @clk: root clock pointer   @pll8k_clk: PLL clock pointer for 8kHz   @pll11k_clk: PLL clock pointer for 11kHz   @ratio: target requency for root clock     This function set root clock parent according to the target ratio ", "if (!pci->revision)return 0;for (i = 0; i < ARRAY_SIZE(config_table); i++, table++) ": "snd_amd_acp_find_config(struct pci_dev  pci){const struct config_entry  table = config_table;u16 device = pci->device;int i;  Do not enable FLAGS on older platforms with Rev id zero ", "if (sdev->basefw.fw)return 0;fw_filename = kasprintf(GFP_KERNEL, \"%s/%s\",plat_data->fw_filename_prefix,plat_data->fw_filename);if (!fw_filename)return -ENOMEM;ret = request_firmware(&sdev->basefw.fw, fw_filename, sdev->dev);if (ret < 0) ": "snd_sof_load_firmware_raw(struct snd_sof_dev  sdev){struct snd_sof_pdata  plat_data = sdev->pdata;const char  fw_filename;ssize_t ext_man_size;int ret;  Don't request firmware again if firmware is already requested ", "ret = sdev->ipc->ops->fw_loader->validate(sdev);if (ret < 0) ": "snd_sof_load_firmware_memcpy(struct snd_sof_dev  sdev){int ret;ret = snd_sof_load_firmware_raw(sdev);if (ret < 0)return ret;  make sure the FW header and file is valid ", "sdev->dbg_dump_printed = false;sdev->ipc_dump_printed = false;/* create read-only fw_version debugfs to store boot version info ": "snd_sof_run_firmware(struct snd_sof_dev  sdev){int ret;init_waitqueue_head(&sdev->boot_wait);  (re-)enable dsp dump ", "release_firmware(sdev->basefw.fw);sdev->basefw.fw = NULL;}EXPORT_SYMBOL(snd_sof_fw_unload": "snd_sof_fw_unload(struct snd_sof_dev  sdev){  TODO: support module unloading at runtime ", "__iowrite32_copy(dest, src, m);if (n) ": "sof_block_write(struct snd_sof_dev  sdev, enum snd_sof_fw_blk_type blk_type,    u32 offset, void  src, size_t size){int bar = snd_sof_dsp_get_bar_index(sdev, blk_type);const u8  src_byte = src;void __iomem  dest;u32 affected_mask;u32 tmp;int m, n;if (bar < 0)return bar;dest = sdev->bar[bar] + offset;m = size  4;n = size % 4;  __iowrite32_copy use 32bit size values so divide by 4 ", "sdev->system_suspend_target = SOF_SUSPEND_S3;/* * if the firmware is crashed or boot failed then we try to aim for S3 * to reboot the firmware ": "snd_sof_prepare(struct device  dev){struct snd_sof_dev  sdev = dev_get_drvdata(dev);const struct sof_dev_desc  desc = sdev->pdata->desc;  will suspend to S3 by default ", "msg.extension = SOF_IPC4_GLB_PIPE_STATE_EXT_MULTI;/* ipc_size includes the count and the pipeline IDs for the number of pipelines ": "sof_ipc4_set_pipeline_state(sdev, trigger_list->pipeline_ids[0], state);primary = state;primary |= SOF_IPC4_MSG_TYPE_SET(SOF_IPC4_GLB_SET_PIPELINE_STATE);primary |= SOF_IPC4_MSG_DIR(SOF_IPC4_MSG_REQUEST);primary |= SOF_IPC4_MSG_TARGET(SOF_IPC4_FW_GEN_MSG);msg.primary = primary;  trigger multiple pipelines with a single IPC ", "void snd_sof_dsp_update_bits_forced(struct snd_sof_dev *sdev, u32 bar,    u32 offset, u32 mask, u32 value)": "snd_sof_dsp_update_bits_forced_unlocked(struct snd_sof_dev  sdev, u32 bar,     u32 offset, u32 mask, u32 value){unsigned int old, new;u32 ret;ret = snd_sof_dsp_read(sdev, bar, offset);old = ret;new = (old & ~mask) | (value & mask);snd_sof_dsp_write(sdev, bar, offset, new);}  This is for registers bits with attribute RWC ", "void snd_sof_dsp_panic(struct snd_sof_dev *sdev, u32 offset, bool non_recoverable)": "snd_sof_dsp_panic - handle a received DSP panic message   @sdev: Pointer to the device's sdev   @offset: offset of panic information   @non_recoverable: the panic is fatal, no recovery will be done by the caller ", "bool sof_debug_check_flag(int mask)": "sof_debug_check_flag - check if a given flag(s) is set in sof_core_debug   @mask: Flag or combination of flags to check     Returns true if all bits set in mask is also set in sof_core_debug, otherwise   false ", "void sof_print_oops_and_stack(struct snd_sof_dev *sdev, const char *level,      u32 panic_code, u32 tracep_code, void *oops,      struct sof_ipc_panic_info *panic_info,      void *stack, size_t stack_words)": "sof_print_oops_and_stack - Handle the printing of DSP oops and stack trace   @sdev: Pointer to the device's sdev   @level: prink log level to use for the printing   @panic_code: the panic code   @tracep_code: tracepoint code   @oops: Pointer to DSP specific oops data   @panic_info: Pointer to the received panic information message   @stack: Pointer to the call stack data   @stack_words: Number of words in the stack data     helper to be called from .dbg_dump callbacks. No error code is   provided, it's left as an exercise for the caller of .dbg_dump   (typically IPC or loader) ", "sdev->dev = dev;/* initialize default DSP power state ": "snd_sof_device_probe(struct device  dev, struct snd_sof_pdata  plat_data){struct snd_sof_dev  sdev;int ret;sdev = devm_kzalloc(dev, sizeof( sdev), GFP_KERNEL);if (!sdev)return -ENOMEM;  initialize sof device ", "sof_unregister_clients(sdev);/* * Unregister machine driver. This will unbind the snd_card which * will remove the component driver and unload the topology * before freeing the snd_card. ": "snd_sof_device_remove(struct device  dev){struct snd_sof_dev  sdev = dev_get_drvdata(dev);struct snd_sof_pdata  pdata = sdev->pdata;int ret;if (IS_ENABLED(CONFIG_SND_SOC_SOF_PROBE_WORK_QUEUE))cancel_work_sync(&sdev->probe_work);    Unregister any registered client device first before IPC and debugfs   to allow client drivers to be removed cleanly ", "u32 idx = (5 * i) >> 1;u32 pfn = snd_sgbuf_get_addr(dmab, i * PAGE_SIZE) >> PAGE_SHIFT;u8 *pg_table;pg_table = (u8 *)(page_table + idx);/* * pagetable compression: * byte 0     byte 1     byte 2     byte 3     byte 4     byte 5 * ___________pfn 0__________ __________pfn 1___________  _pfn 2... * .... ....  .... ....  .... ....  .... ....  .... ....  .... * It is created by: * 1. set current location to 0, PFN index i to 0 * 2. put pfn[i] at current location in Little Endian byte order * 3. calculate an intermediate value as *    x = (pfn[i+1] << 4) | (pfn[i] & 0xf) * 4. put x at offset (current location + 2) in LE byte order * 5. increment current location by 5 bytes, increment i by 2 * 6. continue to (2) ": "snd_sof_create_page_table(struct device  dev,      struct snd_dma_buffer  dmab,      unsigned char  page_table, size_t size){int i, pages;pages = snd_sgbuf_aligned_pages(size);dev_dbg(dev, \"generating page table for %p size 0x%zx pages %d\\n\",dmab->area, size, pages);for (i = 0; i < pages; i++) {    The number of valid address bits for each page is 20.   idx determines the byte position within page_table   where the current page's address is stored   in the compressed page_table.   This can be calculated by multiplying the page number by 2.5. ", "if (!sdev->ipc_dump_printed)dev_info(sdev->dev, \"Attempting to prevent DSP from entering D3 state to preserve context\\n\");pm_runtime_get_if_in_use(sdev->dev);}/* dump vital information to the logs ": "snd_sof_handle_fw_exception(struct snd_sof_dev  sdev, const char  msg){if (IS_ENABLED(CONFIG_SND_SOC_SOF_DEBUG_RETAIN_DSP_CONTEXT) ||    sof_debug_check_flag(SOF_DBG_RETAIN_CTX)) {  should we prevent DSP entering D3 ? ", "pm_runtime_set_autosuspend_delay(dev, SND_SOF_SUSPEND_DELAY_MS);pm_runtime_use_autosuspend(dev);pm_runtime_mark_last_busy(dev);pm_runtime_set_active(dev);pm_runtime_enable(dev);}int sof_of_probe(struct platform_device *pdev)": "sof_of_probe_complete(struct device  dev){  allow runtime_pm ", "snd_sof_device_remove(&pdev->dev);return 0;}EXPORT_SYMBOL(sof_of_remove": "sof_of_remove(struct platform_device  pdev){pm_runtime_disable(&pdev->dev);  call sof helper for DSP hardware remove ", "if (!sdev->msg) ": "snd_sof_ipc_get_reply(struct snd_sof_dev  sdev){    Sometimes, there is unexpected reply ipc arriving. The reply   ipc belongs to none of the ipcs sent from driver.   In this case, the driver must ignore the ipc. ", "msg->ipc_complete = true;wake_up(&msg->waitq);}EXPORT_SYMBOL(snd_sof_ipc_reply": "snd_sof_ipc_reply(struct snd_sof_dev  sdev, u32 msg_id){struct snd_sof_ipc_msg  msg = &sdev->ipc->msg;if (msg->ipc_complete) {dev_dbg(sdev->dev,\"no reply expected, received 0x%x, will be ignored\",msg_id);return;}  wake up and return the error if we have waiters on this message ? ", "msg->ipc_complete = true;init_waitqueue_head(&msg->waitq);switch (sdev->pdata->ipc_type) ": "snd_sof_ipc_init(struct snd_sof_dev  sdev){struct snd_sof_ipc  ipc;struct snd_sof_ipc_msg  msg;const struct sof_ipc_ops  ops;ipc = devm_kzalloc(sdev->dev, sizeof( ipc), GFP_KERNEL);if (!ipc)return NULL;mutex_init(&ipc->tx_mutex);ipc->sdev = sdev;msg = &ipc->msg;  indicate that we aren't sending a message ATM ", "mutex_lock(&ipc->tx_mutex);ipc->disable_ipc_tx = true;mutex_unlock(&ipc->tx_mutex);if (ipc->ops->exit)ipc->ops->exit(sdev);}EXPORT_SYMBOL(snd_sof_ipc_free": "snd_sof_ipc_free(struct snd_sof_dev  sdev){struct snd_sof_ipc  ipc = sdev->ipc;if (!ipc)return;  disable sending of ipc's ", "if (--swidget->use_count)return 0;pipe_widget = swidget->spipe->pipe_widget;/* reset route setup status for all routes that contain this widget ": "sof_widget_free_unlocked(struct snd_sof_dev  sdev,    struct snd_sof_widget  swidget){const struct sof_ipc_tplg_ops  tplg_ops = sof_ipc_get_ops(sdev, tplg);struct snd_sof_widget  pipe_widget;int err = 0;int ret;if (!swidget->private)return 0;trace_sof_widget_free(swidget);  only free when use_count is 0 ", "if (!swidget->private)return 0;trace_sof_widget_setup(swidget);/* widget already set up ": "sof_widget_setup_unlocked(struct snd_sof_dev  sdev,     struct snd_sof_widget  swidget){const struct sof_ipc_tplg_ops  tplg_ops = sof_ipc_get_ops(sdev, tplg);bool use_count_decremented = false;int ret;  skip if there is no private data ", "if (!spcm->stream[dir].d0i3_compatible)return false;d0i3_compatible_active = true;}}return d0i3_compatible_active;}EXPORT_SYMBOL(snd_sof_dsp_only_d0i3_compatible_stream_active": "snd_sof_dsp_only_d0i3_compatible_stream_active(struct snd_sof_dev  sdev){struct snd_pcm_substream  substream;struct snd_sof_pcm  spcm;bool d0i3_compatible_active = false;int dir;list_for_each_entry(spcm, &sdev->pcm_list, list) {for_each_pcm_streams(dir) {substream = spcm->stream[dir].substream;if (!substream || !substream->runtime)continue;    substream->runtime being not NULL indicates   that the stream is open. No need to check the   stream state. ", "mach = snd_sof_machine_select(sdev);if (mach) ": "sof_machine_check(struct snd_sof_dev  sdev){struct snd_sof_pdata  sof_pdata = sdev->pdata;const struct sof_dev_desc  desc = sof_pdata->desc;struct snd_soc_acpi_mach  mach;if (!IS_ENABLED(CONFIG_SND_SOC_SOF_FORCE_NOCODEC_MODE)) {const struct snd_sof_of_mach  of_mach;if (IS_ENABLED(CONFIG_SND_SOC_SOF_NOCODEC_DEBUG_SUPPORT) &&    sof_debug_check_flag(SOF_DBG_FORCE_NOCODEC))goto nocodec;  find machine ", "plat_data->pdev_mach =platform_device_register_data(sdev->dev, drv_name,      PLATFORM_DEVID_NONE, mach, size);if (IS_ERR(plat_data->pdev_mach))return PTR_ERR(plat_data->pdev_mach);dev_dbg(sdev->dev, \"created machine %s\\n\",dev_name(&plat_data->pdev_mach->dev));return 0;}EXPORT_SYMBOL(sof_machine_register": "sof_machine_register(struct snd_sof_dev  sdev, void  pdata){struct snd_sof_pdata  plat_data = pdata;const char  drv_name;const void  mach;int size;drv_name = plat_data->machine->drv_name;mach = plat_data->machine;size = sizeof( plat_data->machine);  register machine driver, pass machine info as pdata ", "void snd_sof_pcm_period_elapsed(struct snd_pcm_substream *substream)": "snd_sof_pcm_period_elapsed_work(struct work_struct  work){struct snd_sof_pcm_stream  sps =container_of(work, struct snd_sof_pcm_stream,     period_elapsed_work);snd_pcm_period_elapsed(sps->substream);}void snd_sof_pcm_init_elapsed_work(struct work_struct  work){ INIT_WORK(work, snd_sof_pcm_period_elapsed_work);}    sof pcm period elapse, this could be called at irq thread context. ", "if (!dai) ": "sof_pcm_dai_link_fixup(struct snd_soc_pcm_runtime  rtd, struct snd_pcm_hw_params  params){struct snd_interval  rate = hw_param_interval(params,SNDRV_PCM_HW_PARAM_RATE);struct snd_interval  channels = hw_param_interval(params,SNDRV_PCM_HW_PARAM_CHANNELS);struct snd_mask  fmt = hw_param_mask(params, SNDRV_PCM_HW_PARAM_FORMAT);struct snd_soc_component  component =snd_soc_rtdcom_lookup(rtd, SOF_AUDIO_PCM_DRV_NAME);struct snd_sof_dai  dai =snd_sof_find_dai(component, (char  )rtd->dai_link->name);struct snd_sof_dev  sdev = snd_soc_component_get_drvdata(component);const struct sof_ipc_pcm_ops  pcm_ops = sof_ipc_get_ops(sdev, pcm);  no topology exists for this BE, try a common configuration ", "if (!stream)return -ESTRPIPE;posn_offset = stream->posn_offset;} else ": "sof_ipc_msg_data(struct snd_sof_dev  sdev,     struct snd_sof_pcm_stream  sps,     void  p, size_t sz){if (!sps || !sdev->stream_box.size) {snd_sof_dsp_mailbox_read(sdev, sdev->dsp_box.offset, p, sz);} else {size_t posn_offset;if (sps->substream) {struct sof_stream  stream = sps->substream->runtime->private_data;  The stream might already be closed ", "if (posn_offset > sdev->stream_box.size ||    posn_offset % sizeof(struct sof_ipc_stream_posn) != 0)return -EINVAL;posn_offset += sdev->stream_box.offset;if (sps->substream) ": "sof_set_stream_data_offset(struct snd_sof_dev  sdev,       struct snd_sof_pcm_stream  sps,       size_t posn_offset){  check if offset is overflow or it is not aligned ", "substream->runtime->private_data = stream;/* align to DMA minimum transfer size ": "sof_stream_pcm_open(struct snd_sof_dev  sdev,struct snd_pcm_substream  substream){struct sof_stream  stream = kmalloc(sizeof( stream), GFP_KERNEL);if (!stream)return -ENOMEM;  binding pcm substream to hda stream ", "void mtk_adsp_dump(struct snd_sof_dev *sdev, u32 flags)": "mtk_adsp_dump() - This function is called when a panic message is   received from the firmware.   @sdev: SOF device   @flags: parameter not used but required by ops prototype ", "void imx8_dump(struct snd_sof_dev *sdev, u32 flags)": "imx8_dump() - This function is called when a panic message is   received from the firmware.   @sdev: SOF device   @flags: parameter not used but required by ops prototype ", "if (verb == 0x7ff) /* special case ": "hda_to_sdw(unsigned int nid, unsigned int verb, unsigned int payload,       unsigned int  sdw_addr_h, unsigned int  sdw_data_h,       unsigned int  sdw_addr_l, unsigned int  sdw_data_l){unsigned int offset_h, offset_l, e_verb;if (((verb & 0xff) != 0) || verb == 0xf00) {   12 bits command ", "cfg->btn_high[i] = 500000;else/* Micro to Milli Volts ": "wcd_dt_parse_mbhc_data(struct device  dev, struct wcd_mbhc_config  cfg){struct device_node  np = dev->of_node;int ret, i, microvolt;if (of_property_read_bool(np, \"qcom,hphl-jack-type-normally-closed\"))cfg->hphl_swh = false;elsecfg->hphl_swh = true;if (of_property_read_bool(np, \"qcom,ground-jack-type-normally-closed\"))cfg->gnd_swh = false;elsecfg->gnd_swh = true;ret = of_property_read_u32(np, \"qcom,mbhc-headset-vthreshold-microvolt\",   &microvolt);if (ret)dev_dbg(dev, \"missing qcom,mbhc-hs-mic-max-vthreshold--microvolt in dt node\\n\");elsecfg->hs_thr = microvolt1000;ret = of_property_read_u32(np, \"qcom,mbhc-headphone-vthreshold-microvolt\",   &microvolt);if (ret)dev_dbg(dev, \"missing qcom,mbhc-hs-mic-min-vthreshold-microvoltentry\\n\");elsecfg->hph_thr = microvolt1000;ret = of_property_read_u32_array(np, \"qcom,mbhc-buttons-vthreshold-microvolt\", &cfg->btn_high[0], WCD_MBHC_DEF_BUTTONS);if (ret)dev_err(dev, \"missing qcom,mbhc-buttons-vthreshold-microvolt entry\\n\");for (i = 0; i < WCD_MBHC_DEF_BUTTONS; i++) {if (ret)   default voltage ", "if (mbhc->mbhc_cb->hph_pull_up_control_v2)mbhc->mbhc_cb->hph_pull_up_control_v2(component,      HS_PULLUP_I_DEFAULT);else if (mbhc->mbhc_cb->hph_pull_up_control)mbhc->mbhc_cb->hph_pull_up_control(component, I_DEFAULT);elsewcd_mbhc_write_field(mbhc, WCD_MBHC_HS_L_DET_PULL_UP_CTRL, 3);wcd_mbhc_write_field(mbhc, WCD_MBHC_HPHL_PLUG_TYPE, mbhc->cfg->hphl_swh);wcd_mbhc_write_field(mbhc, WCD_MBHC_GND_PLUG_TYPE, mbhc->cfg->gnd_swh);wcd_mbhc_write_field(mbhc, WCD_MBHC_SW_HPH_LP_100K_TO_GND, 1);if (mbhc->cfg->gnd_det_en && mbhc->mbhc_cb->mbhc_gnd_det_ctrl)mbhc->mbhc_cb->mbhc_gnd_det_ctrl(component, true);wcd_mbhc_write_field(mbhc, WCD_MBHC_HS_L_DET_PULL_UP_COMP_CTRL, 1);wcd_mbhc_write_field(mbhc, WCD_MBHC_L_DET_EN, 1);/* Insertion debounce set to 96ms ": "wcd_mbhc_initialise(struct wcd_mbhc  mbhc){struct snd_soc_component  component = mbhc->component;int ret;ret = pm_runtime_get_sync(component->dev);if (ret < 0 && ret != -EACCES) {dev_err_ratelimited(component->dev,    \"pm_runtime_get_sync failed in %s, ret %d\\n\",    __func__, ret);pm_runtime_put_noidle(component->dev);return ret;}mutex_lock(&mbhc->lock);  enable HS detection ", "rate = clk_get_rate(adau1372->mclk);switch (rate) ": "adau1372_probe(struct device  dev, struct regmap  regmap,   void ( switch_mode)(struct device  dev)){struct adau1372  adau1372;unsigned int clk_ctrl;unsigned long rate;int ret;if (IS_ERR(regmap))return PTR_ERR(regmap);adau1372 = devm_kzalloc(dev, sizeof( adau1372), GFP_KERNEL);if (!adau1372)return -ENOMEM;adau1372->mclk = devm_clk_get(dev, \"mclk\");if (IS_ERR(adau1372->mclk))return PTR_ERR(adau1372->mclk);adau1372->pd_gpio = devm_gpiod_get_optional(dev, \"powerdown\", GPIOD_OUT_HIGH);if (IS_ERR(adau1372->pd_gpio))return PTR_ERR(adau1372->pd_gpio);adau1372->regmap = regmap;adau1372->switch_mode = switch_mode;adau1372->dev = dev;adau1372->rate_constraints.list = adau1372_rates;adau1372->rate_constraints.count = ARRAY_SIZE(adau1372_rates);adau1372->rate_constraints.mask = ADAU1372_RATE_MASK_TDM2;dev_set_drvdata(dev, adau1372);    The datasheet says that the internal MCLK always needs to run at   12.288MHz. Automatically choose a valid configuration from the   external clock. ", "aic3x->gpio_reset = devm_gpiod_get(dev, \"reset\",GPIOD_ASIS | GPIOD_FLAGS_BIT_NONEXCLUSIVE);ret = PTR_ERR_OR_ZERO(aic3x->gpio_reset);if (ret)return ret;aic3x->shared_reset = true;}gpiod_set_consumer_name(aic3x->gpio_reset, \"tlv320aic3x reset\");for (i = 0; i < ARRAY_SIZE(aic3x->supplies); i++)aic3x->supplies[i].supply = aic3x_supply_names[i];ret = devm_regulator_bulk_get(dev, ARRAY_SIZE(aic3x->supplies),      aic3x->supplies);if (ret) ": "aic3x_probe(struct device  dev, struct regmap  regmap, kernel_ulong_t driver_data){struct aic3x_priv  aic3x;struct aic3x_setup_data  ai3x_setup;struct device_node  np = dev->of_node;int ret, i;u32 value;aic3x = devm_kzalloc(dev, sizeof(struct aic3x_priv), GFP_KERNEL);if (!aic3x)return -ENOMEM;aic3x->regmap = regmap;if (IS_ERR(aic3x->regmap)) {ret = PTR_ERR(aic3x->regmap);return ret;}regcache_cache_only(aic3x->regmap, true);dev_set_drvdata(dev, aic3x);if (np) {ai3x_setup = devm_kzalloc(dev, sizeof( ai3x_setup), GFP_KERNEL);if (!ai3x_setup)return -ENOMEM;if (of_property_read_u32_array(np, \"ai3x-gpio-func\",ai3x_setup->gpio_func, 2) >= 0) {aic3x->setup = ai3x_setup;}if (!of_property_read_u32(np, \"ai3x-micbias-vg\", &value)) {switch (value) {case 1 :aic3x->micbias_vg = AIC3X_MICBIAS_2_0V;break;case 2 :aic3x->micbias_vg = AIC3X_MICBIAS_2_5V;break;case 3 :aic3x->micbias_vg = AIC3X_MICBIAS_AVDDV;break;default :aic3x->micbias_vg = AIC3X_MICBIAS_OFF;dev_err(dev, \"Unsuitable MicBias voltage \"\"found in DT\\n\");}} else {aic3x->micbias_vg = AIC3X_MICBIAS_OFF;}}aic3x->model = driver_data;aic3x->gpio_reset = devm_gpiod_get_optional(dev, \"reset\",    GPIOD_OUT_HIGH);ret = PTR_ERR_OR_ZERO(aic3x->gpio_reset);if (ret) {if (ret != -EBUSY)return ret;    Apparently there are setups where the codec is sharing   its reset line. Try to get it non-exclusively, although   the utility of this is unclear: how do we make sure that   resetting one chip will not disturb the others that share   the same line? ", "if (aic3x->gpio_reset && !aic3x->shared_reset)gpiod_set_value(aic3x->gpio_reset, 1);}EXPORT_SYMBOL(aic3x_remove": "aic3x_remove(struct device  dev){struct aic3x_priv  aic3x = dev_get_drvdata(dev);  Leave the codec in reset state ", "int wsa_macro_set_spkr_mode(struct snd_soc_component *component, int mode)": "wsa_macro_set_spkr_mode - Configures speaker compander and smartboost   settings based on speaker mode.     @component: codec instance   @mode: Indicates speaker configuration mode.     Returns 0 on success or -EINVAL on error. ", "rc = regmap_update_bits(priv->regmap, PCM3060_REG64,PCM3060_REG_MRST, 0);if (rc) ": "pcm3060_probe(struct device  dev){int rc;struct pcm3060_priv  priv = dev_get_drvdata(dev);  soft reset ", " /* Record capability notes 30/01/2001:  * At present these observations apply only to pmac LL driver (the only one  * that can do record, at present).  However, if other LL drivers for machines  * with record are added they may apply.  *  * The fragment parameters for the record and play channels are separate.  * However, if the driver is opened O_RDWR there is no way (in the current OSS  * API) to specify their values independently for the record and playback  * channels.  Since the only common factor between the input & output is the  * sample rate (on pmac) it should be possible to open /dev/dspX O_WRONLY and  * /dev/dspY O_RDONLY.  The input & output channels could then have different  * characteristics (other than the first that sets sample rate claiming the  * right to set it for ever).  As it stands, the format, channels, number of  * bits & sample rate are assumed to be common.  In the future perhaps these  * should be the responsibility of the LL driver - and then if a card really  * does not share items between record & playback they can be specified  * separately.": "dmasounddmasound_core.c        OSSFree compatible Atari TTFalcon and Amiga DMA sound driver for    Linuxm68k    Extended to support Power Macintosh for Linuxppc by Paul Mackerras      (c) 1995 by Michael Schlueter & Michael Marte      Michael Schlueter (michael@duck.syd.de) did the basic structure of the VFS    interface and the u-law to signed byte conversion.      Michael Marte (marte@informatik.uni-muenchen.de) did the sound queue,    devmixer, devsndstat and complemented the VFS interface. He would like    to thank:      - Michael Schlueter for initial ideas and documentation on the MFP and  the DMA sound hardware.      - Therapy? for their CD 'Troublegum' which really made me rock.      devsndstat is based on code by Hannu Savolainen, the author of the    VoxWare family of drivers.      This file is subject to the terms and conditions of the GNU General Public    License.  See the file COPYING in the main directory of this archive    for more details.      History:    1995825First release    1995902Roman Hodek:    - Fixed atari_stram_alloc() call, the timer      programming and several race conditions  1995914Roman Hodek:    - After some discussion with Michael Schlueter,      revised the interrupt disabling    - Slightly speeded up U8->S8 translation by using      long operations where possible    - Added 4:3 interpolation for devaudio    1995920Torsten Scherer:    - Fixed a bug in sq_write and changed devaudio      converting to play at 12517Hz instead of 6258Hz.    1995923Torsten Scherer:    - Changed sq_interrupt() and sq_play() to pre-program      the DMA for another frame while there's still one      running. This allows the IRQ response to be      arbitrarily delayed and playing will still continue.    19951014Guenther Kelleter, Torsten Scherer:    - Better support for Falcon audio (the Falcon doesn't      raise an IRQ at the end of a frame, but at the      beginning instead!). uses 'if (codec_dma)' in lots      of places to simply switch between Falcon and TT      code.    19951106Torsten Scherer:    - Started introducing a hardware abstraction scheme      (may perhaps also serve for Amigas?)    - Can now play samples at almost all frequencies by      means of a more generalized expand routine    - Takes a good deal of care to cut data only at      sample sizes    - Buffer size is now a kernel runtime option    - Implemented fsync() & several minor improvements  Guenther Kelleter:    - Useful hints and bug fixes    - Cross-checked it for Falcons    199639Geert Uytterhoeven:    - Support added for Amiga, A-law, 16-bit little      endian.    - Unification to driverssounddmasound.c.    199646Martin Mitchell:    - Updated to 1.3 kernel.    1996613       Topi Kanerva:    - Fixed things that were broken (mainly the amiga      14-bit routines)    - devsndstat shows now the real hardware frequency    - The lowpass filter is disabled by default now    1996925Geert Uytterhoeven:    - Modularization    1998610Andreas Schwab:    - Converted to use sound_core    19991228Richard Zidlicky:    - Added support for Q40    2000227Geert Uytterhoeven:    - Clean up and split the code into 4 parts:        o dmasound_core: machine-independent code        o dmasound_atari: Atari TT and Falcon support        o dmasound_awacs: Apple PowerMac support        o dmasound_paula: Amiga support    2000325Geert Uytterhoeven:    - Integration of dmasound_q40    - Small clean ups    20010126 [1.0] Iain Sandoe    - make devsndstat show revision & edition info.    - since dmasound.mach.sq_setup() can fail on pmac      its type has been changed to int and the returns      are checked.     [1.1]  - stop missing translations from being called.  20010208 [1.2]  - remove unused translation tables & move machine-      specific tables to low-level.    - return correct info. for SNDCTL_DSP_GETFMTS.     [1.3]  - implement SNDCTL_DSP_GETCAPS fully.     [1.4]  - make devsndstat text length usage deterministic.    - make devsndstat call to low-level      dmasound.mach.state_info() pass max space to ll driver.    - tidy startup banners and output info.     [1.5]  - tidy up a little (removed some unused #defines in      dmasound.h)    - fix up HAS_RECORD conditionalisation.    - add record code in places it is missing...    - change buf-sizes to bytes to allow < 1kb for pmac      if user param entry is < 256 the value is taken to      be in kb > 256 is taken to be in bytes.    - make default bufffrag params conditional on      machine to allow smaller values for pmac.    - made the ioctls, read & write comply with the OSS      rules on setting params.    - added parsing of _setup() params for record.  20010404 [1.6]  - fix bug where sample rates higher than maximum were      being reported as OK.    - fix open() to return -EBUSY as per OSS doc. when      audio is in use - this is independent of O_NOBLOCK.    - fix bug where SNDCTL_DSP_POST was blocking. ", "/* Set default settings. ": "dmasound_init(void){int res ;if (irq_installed)return -EBUSY;  Set up sound queue, devaudio and devdsp. ", "/* FIXME: other than in the most naive of cases there is no sense in these *  buffers being other than powers of two.  This is not checked yet. ": "dmasound_deinit(void){if (irq_installed) {sound_silence();dmasound.mach.irqcleanup();irq_installed = 0;}write_sq_release_buffers();if (mixer_unit >= 0)unregister_sound_mixer(mixer_unit);if (state_unit >= 0)unregister_sound_special(state_unit);if (sq_unit >= 0)unregister_sound_dsp(sq_unit);}static int __maybe_unused dmasound_setup(char  str){int ints[6], size;str = get_options(str, ARRAY_SIZE(ints), ints);  check the bootstrap parameter for \"dmasound=\" ", "#ifdef DEBUG_DMASOUNDprintk(\"dmasound_core: tried to sq_setup a locked queue\\n\") ;#endifreturn -EINVAL ;}sq->locked = 1 ; /* don't think we have a race prob. here _check_ ": "dmasound_write_sq;static void sq_reset_output(void) ;static int sq_allocate_buffers(struct sound_queue  sq, int num, int size){int i;if (sq->buffers)return 0;sq->numBufs = num;sq->bufSize = size;sq->buffers = kmalloc_array (num, sizeof(char  ), GFP_KERNEL);if (!sq->buffers)return -ENOMEM;for (i = 0; i < num; i++) {sq->buffers[i] = dmasound.mach.dma_alloc(size, GFP_KERNEL);if (!sq->buffers[i]) {while (i--)dmasound.mach.dma_free(sq->buffers[i], size);kfree(sq->buffers);sq->buffers = NULL;return -ENOMEM;}}return 0;}static void sq_release_buffers(struct sound_queue  sq){int i;if (sq->buffers) {for (i = 0; i < sq->numBufs; i++)dmasound.mach.dma_free(sq->buffers[i], sq->bufSize);kfree(sq->buffers);sq->buffers = NULL;}}static int sq_setup(struct sound_queue  sq){int ( setup_func)(void) = NULL;int hard_frame ;if (sq->locked) {   are we already set? - and not changeable ", "module_param(writeBufSize, int, 0);MODULE_LICENSE(\"GPL\");static int sq_unit = -1;static int mixer_unit = -1;static int state_unit = -1;static int irq_installed;/* control over who can modify resources shared between play/record ": "dmasound_catchRadius = 0;module_param(dmasound_catchRadius, int, 0);static unsigned int numWriteBufs = DEFAULT_N_BUFFERS;module_param(numWriteBufs, int, 0);static unsigned int writeBufSize = DEFAULT_BUFF_SIZE ;  in bytes ", "char dmasound_alaw2dma8[] = ": "dmasound_ulaw2dma8[] = {-126,-122,-118,-114,-110,-106,-102,-98,-94,-90,-86,-82,-78,-74,-70,-66,-63,-61,-59,-57,-55,-53,-51,-49,-47,-45,-43,-41,-39,-37,-35,-33,-31,-30,-29,-28,-27,-26,-25,-24,-23,-22,-21,-20,-19,-18,-17,-16,-16,-15,-15,-14,-14,-13,-13,-12,-12,-11,-11,-10,-10,-9,-9,-8,-8,-8,-7,-7,-7,-7,-6,-6,-6,-6,-5,-5,-5,-5,-4,-4,-4,-4,-4,-4,-3,-3,-3,-3,-3,-3,-3,-3,-2,-2,-2,-2,-2,-2,-2,-2,-2,-2,-2,-2,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,0,125,121,117,113,109,105,101,97,93,89,85,81,77,73,69,65,62,60,58,56,54,52,50,48,46,44,42,40,38,36,34,32,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,15,15,14,14,13,13,12,12,11,11,10,10,9,9,8,8,7,7,7,6,6,6,6,5,5,5,5,4,4,4,4,3,3,3,3,3,3,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};  8 bit A-law ", "    /*     *  Visible symbols for modules     ": "dmasound_alaw2dma8[] = {-22,-21,-24,-23,-18,-17,-20,-19,-30,-29,-32,-31,-26,-25,-28,-27,-11,-11,-12,-12,-9,-9,-10,-10,-15,-15,-16,-16,-13,-13,-14,-14,-86,-82,-94,-90,-70,-66,-78,-74,-118,-114,-126,-122,-102,-98,-110,-106,-43,-41,-47,-45,-35,-33,-39,-37,-59,-57,-63,-61,-51,-49,-55,-53,-2,-2,-2,-2,-2,-2,-2,-2,-2,-2,-2,-2,-2,-2,-2,-2,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-6,-6,-6,-6,-5,-5,-5,-5,-8,-8,-8,-8,-7,-7,-7,-7,-3,-3,-3,-3,-3,-3,-3,-3,-4,-4,-4,-4,-4,-4,-4,-4,21,20,23,22,17,16,19,18,29,28,31,30,25,24,27,26,10,10,11,11,8,8,9,9,14,14,15,15,12,12,13,13,86,82,94,90,70,66,78,74,118,114,126,122,102,98,110,106,43,41,47,45,35,33,39,37,59,57,63,61,51,49,55,53,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,5,5,5,4,4,4,4,7,7,7,7,6,6,6,6,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3};#endif   HAS_8BIT_TABLES ", "  CS8427_SWCLK | CS8427_TCBLDIR,  /* CS8427_REG_CONTROL2: hold last valid audio sample, RMCK=256*Fs,     normal stereo operation ": "snd_cs8427_init(struct snd_i2c_bus  bus,    struct snd_i2c_device  device){static unsigned char initvals1[] = {  CS8427_REG_CONTROL1 | CS8427_REG_AUTOINC,    CS8427_REG_CONTROL1: RMCK to OMCK, valid PCM audio, disable mutes,     TCBL=output ", "tea->regs[TEA6330T_SADDR_FADER] = 0x3f;tea->regs[TEA6330T_SADDR_AUDIO_SWITCH] = equalizer ? 0 : TEA6330T_EQN;/* initialize mixer ": "snd_tea6330t_update_mixer(struct snd_card  card,      struct snd_i2c_bus  bus,      int equalizer, int fader){struct snd_i2c_device  device;struct tea6330t  tea;const struct snd_kcontrol_new  knew;unsigned int idx;int err;u8 default_treble, default_bass;unsigned char bytes[7];tea = kzalloc(sizeof( tea), GFP_KERNEL);if (tea == NULL)return -ENOMEM;err = snd_i2c_device_create(bus, \"TEA6330T\", TEA6330T_ADDR, &device);if (err < 0) {kfree(tea);return err;}tea->device = device;tea->bus = bus;tea->equalizer = equalizer;tea->fader = fader;device->private_data = tea;device->private_free = snd_tea6330_free;snd_i2c_lock(bus);  turn fader off and handle equalizer ", "if (atomic_dec_and_test(&chip->wq_processing))schedule_delayed_work(&chip->work, HZ / 10);}EXPORT_SYMBOL(snd_ak4114_reinit": "snd_ak4114_reinit(struct ak4114  chip){if (atomic_inc_return(&chip->wq_processing) == 1)cancel_delayed_work_sync(&chip->work);mutex_lock(&chip->reinit_mutex);ak4114_init_regs(chip);mutex_unlock(&chip->reinit_mutex);  bring up statistics  event queing ", "schedule_delayed_work(&ak4114->work, HZ / 10);return 0;}EXPORT_SYMBOL(snd_ak4114_build": "snd_ak4114_build(struct ak4114  ak4114,     struct snd_pcm_substream  ply_substream,     struct snd_pcm_substream  cap_substream){struct snd_kcontrol  kctl;unsigned int idx;int err;if (snd_BUG_ON(!cap_substream))return -EINVAL;ak4114->playback_substream = ply_substream;ak4114->capture_substream = cap_substream;for (idx = 0; idx < AK4114_CONTROLS; idx++) {kctl = snd_ctl_new1(&snd_ak4114_iec958_controls[idx], ak4114);if (kctl == NULL)return -ENOMEM;if (strstr(kctl->id.name, \"Playback\")) {if (ply_substream == NULL) {snd_ctl_free_one(kctl);ak4114->kctls[idx] = NULL;continue;}kctl->id.device = ply_substream->pcm->device;kctl->id.subdevice = ply_substream->number;} else {kctl->id.device = cap_substream->pcm->device;kctl->id.subdevice = cap_substream->number;}err = snd_ctl_add(ak4114->card, kctl);if (err < 0)return err;ak4114->kctls[idx] = kctl;}snd_ak4114_proc_init(ak4114);  trigger workq ", "res = external_rate(rcs1);if (!(flags & AK4114_CHECK_NO_RATE) && runtime && runtime->rate != res) ": "snd_ak4114_check_rate_and_errors(struct ak4114  ak4114, unsigned int flags){struct snd_pcm_runtime  runtime = ak4114->capture_substream ? ak4114->capture_substream->runtime : NULL;unsigned long _flags;int res = 0;unsigned char rcs0, rcs1;unsigned char c0, c1;rcs1 = reg_read(ak4114, AK4114_REG_RCS1);if (flags & AK4114_CHECK_NO_STAT)goto __rate;rcs0 = reg_read(ak4114, AK4114_REG_RCS0);spin_lock_irqsave(&ak4114->lock, _flags);if (rcs0 & AK4114_PAR)ak4114->errors[AK4114_PARITY_ERRORS]++;if (rcs1 & AK4114_V)ak4114->errors[AK4114_V_BIT_ERRORS]++;if (rcs1 & AK4114_CCRC)ak4114->errors[AK4114_CCRC_ERRORS]++;if (rcs1 & AK4114_QCRC)ak4114->errors[AK4114_QCRC_ERRORS]++;c0 = (ak4114->rcs0 & (AK4114_QINT | AK4114_CINT | AK4114_PEM | AK4114_AUDION | AK4114_AUTO | AK4114_UNLCK)) ^                     (rcs0 & (AK4114_QINT | AK4114_CINT | AK4114_PEM | AK4114_AUDION | AK4114_AUTO | AK4114_UNLCK));c1 = (ak4114->rcs1 & 0xf0) ^ (rcs1 & 0xf0);ak4114->rcs0 = rcs0 & ~(AK4114_QINT | AK4114_CINT);ak4114->rcs1 = rcs1;spin_unlock_irqrestore(&ak4114->lock, _flags);ak4114_notify(ak4114, rcs0, rcs1, c0, c1);if (ak4114->change_callback && (c0 | c1) != 0)ak4114->change_callback(ak4114, c0, c1);      __rate:  compare rate ", "cancel_delayed_work_sync(&chip->work);}EXPORT_SYMBOL(snd_ak4114_suspend": "snd_ak4114_suspend(struct ak4114  chip){atomic_inc(&chip->wq_processing);   don't schedule new work ", "bytes[0] = PT2258_CMD_RESET;snd_i2c_lock(pt->i2c_bus);if (snd_i2c_sendbytes(pt->i2c_dev, bytes, 1) != 1)goto __error;snd_i2c_unlock(pt->i2c_bus);/* mute all channels ": "snd_pt2258_reset(struct snd_pt2258  pt){unsigned char bytes[2];int i;  reset chip ", "snd_akm4xxx_set(ak, chip, reg, val);ak->ops.unlock(ak, chip);}EXPORT_SYMBOL(snd_akm4xxx_write": "snd_akm4xxx_write(struct snd_akm4xxx  ak, int chip, unsigned char reg,       unsigned char val){ak->ops.lock(ak, chip);ak->ops.write(ak, chip, reg, val);  save the data ", "break;case SND_AK4355:ak435X_reset(ak, state);break;case SND_AK4358:ak435X_reset(ak, state);break;case SND_AK4381:ak4381_reset(ak, state);break;default:break;}}EXPORT_SYMBOL(snd_akm4xxx_reset": "snd_akm4xxx_reset(struct snd_akm4xxx  ak, int state){switch (ak->type) {case SND_AK4524:case SND_AK4528:case SND_AK4620:ak4524_reset(ak, state);break;case SND_AK4529:  FIXME: needed for ak4529? ", "0x01, 0x00, /* 1: ADC/DAC reset ": "snd_akm4xxx_init(struct snd_akm4xxx  ak){static const unsigned char inits_ak4524[] = {0x00, 0x07,   0: all power up ", "cancel_delayed_work_sync(&chip->work);}EXPORT_SYMBOL(snd_ak4113_suspend": "snd_ak4113_suspend(struct ak4113  chip){atomic_inc(&chip->wq_processing);   don't schedule new work ", "reg_write(chip, AK4117_REG_PWRDN, 0);udelay(200);/* release reset, but leave powerdown ": "snd_ak4117_reinit(chip);chip->rcs0 = reg_read(chip, AK4117_REG_RCS0) & ~(AK4117_QINT | AK4117_CINT | AK4117_STC);chip->rcs1 = reg_read(chip, AK4117_REG_RCS1);chip->rcs2 = reg_read(chip, AK4117_REG_RCS2);err = snd_device_new(card, SNDRV_DEV_CODEC, chip, &ops);if (err < 0)goto __fail;if (r_ak4117) r_ak4117 = chip;return 0;      __fail:snd_ak4117_free(chip);return err;}void snd_ak4117_reg_write(struct ak4117  chip, unsigned char reg, unsigned char mask, unsigned char val){if (reg >= 5)return;reg_write(chip, reg, (chip->regmap[reg] & ~mask) | val);}void snd_ak4117_reinit(struct ak4117  chip){unsigned char old = chip->regmap[AK4117_REG_PWRDN], reg;del_timer(&chip->timer);chip->init = 1;  bring the chip to reset state and powerdown state ", "if (c1 & 0x0f)snd_ctl_notify(ak4117->card, SNDRV_CTL_EVENT_MASK_VALUE, &ak4117->kctls[4]->id);if ((c1 & AK4117_PEM) | (c0 & AK4117_CINT))snd_ctl_notify(ak4117->card, SNDRV_CTL_EVENT_MASK_VALUE, &ak4117->kctls[6]->id);if (c0 & AK4117_QINT)snd_ctl_notify(ak4117->card, SNDRV_CTL_EVENT_MASK_VALUE, &ak4117->kctls[8]->id);if (c0 & AK4117_AUDION)snd_ctl_notify(ak4117->card, SNDRV_CTL_EVENT_MASK_VALUE, &ak4117->kctls[9]->id);if (c1 & AK4117_NPCM)snd_ctl_notify(ak4117->card, SNDRV_CTL_EVENT_MASK_VALUE, &ak4117->kctls[10]->id);if (c1 & AK4117_DTSCD)snd_ctl_notify(ak4117->card, SNDRV_CTL_EVENT_MASK_VALUE, &ak4117->kctls[11]->id);if (ak4117->change_callback && (c0 | c1) != 0)ak4117->change_callback(ak4117, c0, c1);      __rate:/* compare rate ": "snd_ak4117_check_rate_and_errors(struct ak4117  ak4117, unsigned int flags){struct snd_pcm_runtime  runtime = ak4117->substream ? ak4117->substream->runtime : NULL;unsigned long _flags;int res = 0;unsigned char rcs0, rcs1, rcs2;unsigned char c0, c1;rcs1 = reg_read(ak4117, AK4117_REG_RCS1);if (flags & AK4117_CHECK_NO_STAT)goto __rate;rcs0 = reg_read(ak4117, AK4117_REG_RCS0);rcs2 = reg_read(ak4117, AK4117_REG_RCS2); printk(KERN_DEBUG \"AK IRQ: rcs0 = 0x%x, rcs1 = 0x%x, rcs2 = 0x%x\\n\", rcs0, rcs1, rcs2);spin_lock_irqsave(&ak4117->lock, _flags);if (rcs0 & AK4117_PAR)ak4117->errors[AK4117_PARITY_ERRORS]++;if (rcs0 & AK4117_V)ak4117->errors[AK4117_V_BIT_ERRORS]++;if (rcs2 & AK4117_CCRC)ak4117->errors[AK4117_CCRC_ERRORS]++;if (rcs2 & AK4117_QCRC)ak4117->errors[AK4117_QCRC_ERRORS]++;c0 = (ak4117->rcs0 & (AK4117_QINT | AK4117_CINT | AK4117_STC | AK4117_AUDION | AK4117_AUTO | AK4117_UNLCK)) ^                     (rcs0 & (AK4117_QINT | AK4117_CINT | AK4117_STC | AK4117_AUDION | AK4117_AUTO | AK4117_UNLCK));c1 = (ak4117->rcs1 & (AK4117_DTSCD | AK4117_NPCM | AK4117_PEM | 0x0f)) ^             (rcs1 & (AK4117_DTSCD | AK4117_NPCM | AK4117_PEM | 0x0f));ak4117->rcs0 = rcs0 & ~(AK4117_QINT | AK4117_CINT | AK4117_STC);ak4117->rcs1 = rcs1;ak4117->rcs2 = rcs2;spin_unlock_irqrestore(&ak4117->lock, _flags);if (rcs0 & AK4117_PAR)snd_ctl_notify(ak4117->card, SNDRV_CTL_EVENT_MASK_VALUE, &ak4117->kctls[0]->id);if (rcs0 & AK4117_V)snd_ctl_notify(ak4117->card, SNDRV_CTL_EVENT_MASK_VALUE, &ak4117->kctls[1]->id);if (rcs2 & AK4117_CCRC)snd_ctl_notify(ak4117->card, SNDRV_CTL_EVENT_MASK_VALUE, &ak4117->kctls[2]->id);if (rcs2 & AK4117_QCRC)snd_ctl_notify(ak4117->card, SNDRV_CTL_EVENT_MASK_VALUE, &ak4117->kctls[3]->id);  rate change ", "follower->info = follower_info;follower->get = follower_get;follower->put = follower_put;if (follower->vd[0].access & SNDRV_CTL_ELEM_ACCESS_TLV_CALLBACK)follower->tlv.c = follower_tlv_cmd;follower->private_data = srec;follower->private_free = follower_free;list_add_tail(&srec->list, &master_link->followers);return 0;}EXPORT_SYMBOL(_snd_ctl_add_follower": "_snd_ctl_add_follower(struct snd_kcontrol  master,  struct snd_kcontrol  follower,  unsigned int flags){struct link_master  master_link = snd_kcontrol_chip(master);struct link_follower  srec;srec = kzalloc(struct_size(srec, follower.vd, follower->count),       GFP_KERNEL);if (!srec)return -ENOMEM;srec->kctl = follower;srec->follower =  follower;memcpy(srec->follower.vd, follower->vd, follower->count   sizeof( follower->vd));srec->master = master_link;srec->flags = flags;  override callbacks ", "struct snd_kcontrol *snd_ctl_make_virtual_master(char *name, const unsigned int *tlv)": "snd_ctl_make_virtual_master - Create a virtual master control   @name: name string of the control element to create   @tlv: optional TLV int array for dB information     Creates a virtual master control with the given name string.     After creating a vmaster element, you can add the follower controls   via snd_ctl_add_follower() or snd_ctl_add_follower_uncached().     The optional argument @tlv can be used to specify the TLV information   for dB scale of the master control.  It should be a single element   with #SNDRV_CTL_TLVT_DB_SCALE, #SNDRV_CTL_TLV_DB_MINMAX or   #SNDRV_CTL_TLVT_DB_MINMAX_MUTE type, and should be the max 0dB.     Return: The created control element, or %NULL for errors (ENOMEM). ", "int copy_to_user_fromio(void __user *dst, const volatile void __iomem *src, size_t count)": "copy_to_user_fromio - copy data from mmio-space to user-space   @dst: the destination pointer on user-space   @src: the source pointer on mmio   @count: the data size to copy in bytes     Copies the data from mmio-space to user-space.     Return: Zero if successful, or non-zero on failure. ", "int copy_from_user_toio(volatile void __iomem *dst, const void __user *src, size_t count)": "copy_from_user_toio - copy data from user-space to mmio-space   @dst: the destination pointer on mmio-space   @src: the source pointer on user-space   @count: the data size to copy in bytes     Copies the data from user-space to mmio-space.     Return: Zero if successful, or non-zero on failure. ", "void snd_pcm_set_ops(struct snd_pcm *pcm, int direction,     const struct snd_pcm_ops *ops)": "snd_pcm_set_ops - set the PCM operators   @pcm: the pcm instance   @direction: stream direction, SNDRV_PCM_STREAM_XXX   @ops: the operator table     Sets the given PCM operators to the pcm instance. ", "void snd_pcm_set_sync(struct snd_pcm_substream *substream)": "snd_pcm_set_sync - set the PCM sync id   @substream: the pcm substream     Sets the PCM sync identifier for the card. ", "int snd_interval_refine(struct snd_interval *i, const struct snd_interval *v)": "snd_interval_refine - refine the interval value of configurator   @i: the interval value to refine   @v: the interval value to refer to     Refines the interval value with the reference value.   The interval is changed to the range satisfying both intervals.   The interval status (min, max, integer, etc.) are evaluated.     Return: Positive if the value is changed, zero if it's not changed, or a   negative error code. ", "int snd_interval_ratnum(struct snd_interval *i,unsigned int rats_count, const struct snd_ratnum *rats,unsigned int *nump, unsigned int *denp)": "snd_interval_ratnum - refine the interval value   @i: interval to refine   @rats_count: number of ratnum_t    @rats: ratnum_t array   @nump: pointer to store the resultant numerator   @denp: pointer to store the resultant denominator     Return: Positive if the value is changed, zero if it's not changed, or a   negative error code. ", "int snd_interval_list(struct snd_interval *i, unsigned int count,      const unsigned int *list, unsigned int mask)": "snd_interval_list - refine the interval value from the list   @i: the interval value to refine   @count: the number of elements in the list   @list: the value list   @mask: the bit-mask to evaluate     Refines the interval value from the list.   When mask is non-zero, only the elements corresponding to bit 1 are   evaluated.     Return: Positive if the value is changed, zero if it's not changed, or a   negative error code. ", "int snd_interval_ranges(struct snd_interval *i, unsigned int count,const struct snd_interval *ranges, unsigned int mask)": "snd_interval_ranges - refine the interval value from the list of ranges   @i: the interval value to refine   @count: the number of elements in the list of ranges   @ranges: the ranges list   @mask: the bit-mask to evaluate     Refines the interval value from the list of ranges.   When mask is non-zero, only the elements corresponding to bit 1 are   evaluated.     Return: Positive if the value is changed, zero if it's not changed, or a   negative error code. ", "int snd_pcm_hw_rule_add(struct snd_pcm_runtime *runtime, unsigned int cond,int var,snd_pcm_hw_rule_func_t func, void *private,int dep, ...)": "snd_pcm_hw_rule_add - add the hw-constraint rule   @runtime: the pcm runtime instance   @cond: condition bits   @var: the variable to evaluate   @func: the evaluation function   @private: the private data pointer passed to function   @dep: the dependent variables     Return: Zero if successful, or a negative error code on failure. ", "int snd_pcm_hw_constraint_mask64(struct snd_pcm_runtime *runtime, snd_pcm_hw_param_t var, u_int64_t mask)": "snd_pcm_hw_constraint_mask64 - apply the given bitmap mask constraint   @runtime: PCM runtime instance   @var: hw_params variable to apply the mask   @mask: the 64bit bitmap mask     Apply the constraint of the given bitmap mask to a 64-bit mask parameter.     Return: Zero if successful, or a negative error code on failure. ", "int snd_pcm_hw_constraint_integer(struct snd_pcm_runtime *runtime, snd_pcm_hw_param_t var)": "snd_pcm_hw_constraint_integer - apply an integer constraint to an interval   @runtime: PCM runtime instance   @var: hw_params variable to apply the integer constraint     Apply the constraint of integer to an interval parameter.     Return: Positive if the value is changed, zero if it's not changed, or a   negative error code. ", "int snd_pcm_hw_constraint_minmax(struct snd_pcm_runtime *runtime, snd_pcm_hw_param_t var, unsigned int min, unsigned int max)": "snd_pcm_hw_constraint_minmax - apply a minmax range constraint to an interval   @runtime: PCM runtime instance   @var: hw_params variable to apply the range   @min: the minimal value   @max: the maximal value      Apply the minmax range constraint to an interval parameter.     Return: Positive if the value is changed, zero if it's not changed, or a   negative error code. ", "int snd_pcm_hw_constraint_list(struct snd_pcm_runtime *runtime,       unsigned int cond,       snd_pcm_hw_param_t var,       const struct snd_pcm_hw_constraint_list *l)": "snd_pcm_hw_constraint_list  list = rule->private;return snd_interval_list(hw_param_interval(params, rule->var), list->count, list->list, list->mask);}     snd_pcm_hw_constraint_list - apply a list of constraints to a parameter   @runtime: PCM runtime instance   @cond: condition bits   @var: hw_params variable to apply the list constraint   @l: list      Apply the list of constraints to an interval parameter.     Return: Zero if successful, or a negative error code on failure. ", "int snd_pcm_hw_constraint_ranges(struct snd_pcm_runtime *runtime, unsigned int cond, snd_pcm_hw_param_t var, const struct snd_pcm_hw_constraint_ranges *r)": "snd_pcm_hw_constraint_ranges  r = rule->private;return snd_interval_ranges(hw_param_interval(params, rule->var),   r->count, r->ranges, r->mask);}     snd_pcm_hw_constraint_ranges - apply list of range constraints to a parameter   @runtime: PCM runtime instance   @cond: condition bits   @var: hw_params variable to apply the list of range constraints   @r: ranges     Apply the list of range constraints to an interval parameter.     Return: Zero if successful, or a negative error code on failure. ", "int snd_pcm_hw_constraint_ratnums(struct snd_pcm_runtime *runtime,   unsigned int cond,  snd_pcm_hw_param_t var,  const struct snd_pcm_hw_constraint_ratnums *r)": "snd_pcm_hw_constraint_ratnums  r = rule->private;unsigned int num = 0, den = 0;int err;err = snd_interval_ratnum(hw_param_interval(params, rule->var),  r->nrats, r->rats, &num, &den);if (err >= 0 && den && rule->var == SNDRV_PCM_HW_PARAM_RATE) {params->rate_num = num;params->rate_den = den;}return err;}     snd_pcm_hw_constraint_ratnums - apply ratnums constraint to a parameter   @runtime: PCM runtime instance   @cond: condition bits   @var: hw_params variable to apply the ratnums constraint   @r: struct snd_ratnums constriants     Return: Zero if successful, or a negative error code on failure. ", "int snd_pcm_hw_constraint_ratdens(struct snd_pcm_runtime *runtime,   unsigned int cond,  snd_pcm_hw_param_t var,  const struct snd_pcm_hw_constraint_ratdens *r)": "snd_pcm_hw_constraint_ratdens  r = rule->private;unsigned int num = 0, den = 0;int err = snd_interval_ratden(hw_param_interval(params, rule->var),  r->nrats, r->rats, &num, &den);if (err >= 0 && den && rule->var == SNDRV_PCM_HW_PARAM_RATE) {params->rate_num = num;params->rate_den = den;}return err;}     snd_pcm_hw_constraint_ratdens - apply ratdens constraint to a parameter   @runtime: PCM runtime instance   @cond: condition bits   @var: hw_params variable to apply the ratdens constraint   @r: struct snd_ratdens constriants     Return: Zero if successful, or a negative error code on failure. ", "int snd_pcm_hw_constraint_msbits(struct snd_pcm_runtime *runtime,  unsigned int cond, unsigned int width, unsigned int msbits)": "snd_pcm_hw_constraint_msbits - add a hw constraint msbits rule   @runtime: PCM runtime instance   @cond: condition bits   @width: sample bits width   @msbits: msbits width     This constraint will set the number of most significant bits (msbits) if a   sample format with the specified width has been select. If width is set to 0   the msbits will be set for any sample format with a width larger than the   specified msbits.     Return: Zero if successful, or a negative error code on failure. ", "int snd_pcm_hw_constraint_step(struct snd_pcm_runtime *runtime,       unsigned int cond,       snd_pcm_hw_param_t var,       unsigned long step)": "snd_pcm_hw_constraint_step - add a hw constraint step rule   @runtime: PCM runtime instance   @cond: condition bits   @var: hw_params variable to apply the step constraint   @step: step size     Return: Zero if successful, or a negative error code on failure. ", "int snd_pcm_hw_constraint_pow2(struct snd_pcm_runtime *runtime,       unsigned int cond,       snd_pcm_hw_param_t var)": "snd_pcm_hw_constraint_pow2 - add a hw constraint power-of-2 rule   @runtime: PCM runtime instance   @cond: condition bits   @var: hw_params variable to apply the power-of-2 constraint     Return: Zero if successful, or a negative error code on failure. ", "int snd_pcm_hw_rule_noresample(struct snd_pcm_runtime *runtime,       unsigned int base_rate)": "snd_pcm_hw_rule_noresample_func(struct snd_pcm_hw_params  params,   struct snd_pcm_hw_rule  rule){unsigned int base_rate = (unsigned int)(uintptr_t)rule->private;struct snd_interval  rate;rate = hw_param_interval(params, SNDRV_PCM_HW_PARAM_RATE);return snd_interval_list(rate, 1, &base_rate, 0);}     snd_pcm_hw_rule_noresample - add a rule to allow disabling hw resampling   @runtime: PCM runtime instance   @base_rate: the rate at which the hardware does not resample     Return: Zero if successful, or a negative error code on failure. ", "int snd_pcm_hw_param_value(const struct snd_pcm_hw_params *params,   snd_pcm_hw_param_t var, int *dir)": "snd_pcm_hw_param_value - return @params field @var value   @params: the hw_params instance   @var: parameter to retrieve   @dir: pointer to the direction (-1,0,1) or %NULL     Return: The value for field @var if it's fixed in configuration space   defined by @params. -%EINVAL otherwise. ", "int snd_pcm_hw_param_first(struct snd_pcm_substream *pcm,    struct snd_pcm_hw_params *params,    snd_pcm_hw_param_t var, int *dir)": "snd_pcm_hw_param_first(struct snd_pcm_hw_params  params,   snd_pcm_hw_param_t var){int changed;if (hw_is_mask(var))changed = snd_mask_refine_first(hw_param_mask(params, var));else if (hw_is_interval(var))changed = snd_interval_refine_first(hw_param_interval(params, var));elsereturn -EINVAL;if (changed > 0) {params->cmask |= 1 << var;params->rmask |= 1 << var;}return changed;}     snd_pcm_hw_param_first - refine config space and return minimum value   @pcm: PCM instance   @params: the hw_params instance   @var: parameter to retrieve   @dir: pointer to the direction (-1,0,1) or %NULL     Inside configuration space defined by @params remove from @var all   values > minimum. Reduce configuration space accordingly.     Return: The minimum, or a negative error code on failure. ", "int snd_pcm_hw_param_last(struct snd_pcm_substream *pcm,   struct snd_pcm_hw_params *params,  snd_pcm_hw_param_t var, int *dir)": "snd_pcm_hw_param_last(struct snd_pcm_hw_params  params,  snd_pcm_hw_param_t var){int changed;if (hw_is_mask(var))changed = snd_mask_refine_last(hw_param_mask(params, var));else if (hw_is_interval(var))changed = snd_interval_refine_last(hw_param_interval(params, var));elsereturn -EINVAL;if (changed > 0) {params->cmask |= 1 << var;params->rmask |= 1 << var;}return changed;}     snd_pcm_hw_param_last - refine config space and return maximum value   @pcm: PCM instance   @params: the hw_params instance   @var: parameter to retrieve   @dir: pointer to the direction (-1,0,1) or %NULL     Inside configuration space defined by @params remove from @var all   values < maximum. Reduce configuration space accordingly.     Return: The maximum, or a negative error code on failure. ", "int snd_pcm_lib_ioctl(struct snd_pcm_substream *substream,      unsigned int cmd, void *arg)": "snd_pcm_lib_ioctl_reset(struct snd_pcm_substream  substream,   void  arg){struct snd_pcm_runtime  runtime = substream->runtime;unsigned long flags;snd_pcm_stream_lock_irqsave(substream, flags);if (snd_pcm_running(substream) &&    snd_pcm_update_hw_ptr(substream) >= 0)runtime->status->hw_ptr %= runtime->buffer_size;else {runtime->status->hw_ptr = 0;runtime->hw_ptr_wrap = 0;}snd_pcm_stream_unlock_irqrestore(substream, flags);return 0;}static int snd_pcm_lib_ioctl_channel_info(struct snd_pcm_substream  substream,  void  arg){struct snd_pcm_channel_info  info = arg;struct snd_pcm_runtime  runtime = substream->runtime;int width;if (!(runtime->info & SNDRV_PCM_INFO_MMAP)) {info->offset = -1;return 0;}width = snd_pcm_format_physical_width(runtime->format);if (width < 0)return width;info->offset = 0;switch (runtime->access) {case SNDRV_PCM_ACCESS_MMAP_INTERLEAVED:case SNDRV_PCM_ACCESS_RW_INTERLEAVED:info->first = info->channel   width;info->step = runtime->channels   width;break;case SNDRV_PCM_ACCESS_MMAP_NONINTERLEAVED:case SNDRV_PCM_ACCESS_RW_NONINTERLEAVED:{size_t size = runtime->dma_bytes  runtime->channels;info->first = info->channel   size   8;info->step = width;break;}default:snd_BUG();break;}return 0;}static int snd_pcm_lib_ioctl_fifo_size(struct snd_pcm_substream  substream,       void  arg){struct snd_pcm_hw_params  params = arg;snd_pcm_format_t format;int channels;ssize_t frame_size;params->fifo_size = substream->runtime->hw.fifo_size;if (!(substream->runtime->hw.info & SNDRV_PCM_INFO_FIFO_IN_FRAMES)) {format = params_format(params);channels = params_channels(params);frame_size = snd_pcm_format_size(format, channels);if (frame_size > 0)params->fifo_size = frame_size;}return 0;}     snd_pcm_lib_ioctl - a generic PCM ioctl callback   @substream: the pcm substream instance   @cmd: ioctl command   @arg: ioctl argument     Processes the generic ioctl commands for PCM.   Can be passed as the ioctl callback for PCM ops.     Return: Zero if successful, or a negative error code on failure. ", "void snd_pcm_period_elapsed_under_stream_lock(struct snd_pcm_substream *substream)": "snd_pcm_period_elapsed_under_stream_lock() - update the status of runtime for the next period  under acquired lock of PCM substream.   @substream: the instance of pcm substream.     This function is called when the batch of audio data frames as the same size as the period of   buffer is already processed in audio data transmission.     The call of function updates the status of runtime with the latest position of audio data   transmission, checks overrun and underrun over buffer, awaken user processes from waiting for   available audio data frames, sampling audio timestamp, and performs stop or drain the PCM   substream according to configured threshold.     The function is intended to use for the case that PCM driver operates audio data frames under   acquired lock of PCM substream; e.g. in callback of any operation of &snd_pcm_ops in process   context. In any interrupt context, it's preferrable to use ``snd_pcm_period_elapsed()`` instead   since lock of PCM substream should be acquired in advance.     Developer should pay enough attention that some callbacks in &snd_pcm_ops are done by the call of   function:     - .pointer - to retrieve current position of audio data transmission by frame count or XRUN state.   - .trigger - with SNDRV_PCM_TRIGGER_STOP at XRUN or DRAINING state.   - .get_time_info - to retrieve audio time stamp if needed.     Even if more than one periods have elapsed since the last call, you have to call this only once. ", "if (!is_playback &&    runtime->state == SNDRV_PCM_STATE_PREPARED &&    size >= runtime->start_threshold) ": "__snd_pcm_lib_xfer(struct snd_pcm_substream  substream,     void  data, bool interleaved,     snd_pcm_uframes_t size, bool in_kernel){struct snd_pcm_runtime  runtime = substream->runtime;snd_pcm_uframes_t xfer = 0;snd_pcm_uframes_t offset = 0;snd_pcm_uframes_t avail;pcm_copy_f writer;pcm_transfer_f transfer;bool nonblock;bool is_playback;int err;err = pcm_sanity_check(substream);if (err < 0)return err;is_playback = substream->stream == SNDRV_PCM_STREAM_PLAYBACK;if (interleaved) {if (runtime->access != SNDRV_PCM_ACCESS_RW_INTERLEAVED &&    runtime->channels > 1)return -EINVAL;writer = interleaved_copy;} else {if (runtime->access != SNDRV_PCM_ACCESS_RW_NONINTERLEAVED)return -EINVAL;writer = noninterleaved_copy;}if (!data) {if (is_playback)transfer = fill_silence;elsereturn -EINVAL;} else if (in_kernel) {if (substream->ops->copy_kernel)transfer = substream->ops->copy_kernel;elsetransfer = is_playback ?default_write_copy_kernel : default_read_copy_kernel;} else {if (substream->ops->copy_user)transfer = (pcm_transfer_f)substream->ops->copy_user;elsetransfer = is_playback ?default_write_copy : default_read_copy;}if (size == 0)return 0;nonblock = !!(substream->f_flags & O_NONBLOCK);snd_pcm_stream_lock_irq(substream);err = pcm_accessible_state(runtime);if (err < 0)goto _end_unlock;runtime->twake = runtime->control->avail_min ? : 1;if (runtime->state == SNDRV_PCM_STATE_RUNNING)snd_pcm_update_hw_ptr(substream);    If size < start_threshold, wait indefinitely. Another   thread may start capture ", "int snd_card_new(struct device *parent, int idx, const char *xid,    struct module *module, int extra_size,    struct snd_card **card_ret)": "snd_card_new - create and initialize a soundcard structure    @parent: the parent device object    @idx: card index (address) [0 ... (SNDRV_CARDS-1)]    @xid: card identification (ASCII string)    @module: top level module for locking    @extra_size: allocate this extra size after the main soundcard structure    @card_ret: the pointer to store the created card instance      The function allocates snd_card instance via kzalloc with the given    space for the driver to use freely.  The allocated struct is stored    in the given card_ret pointer.      Return: Zero if successful or a negative error code. ", "void snd_card_disconnect(struct snd_card *card)": "snd_card_disconnect - disconnect all APIs from the file-operations (user space)    @card: soundcard structure      Disconnects all APIs from the file-operations (user space).      Return: Zero, otherwise a negative error code.      Note: The current implementation replaces all active file->f_op with special          dummy file operations (they do nothing except release). ", "void snd_card_free_when_closed(struct snd_card *card)": "snd_card_free_when_closed - Disconnect the card, free it later eventually   @card: soundcard structure     Unlike snd_card_free(), this function doesn't try to release the card   resource immediately, but tries to disconnect at first.  When the card   is still in use, the function returns before freeing the resources.   The card resources will be freed when the refcount gets to zero.     Return: zero if successful, or a negative error code ", "int snd_devm_card_new(struct device *parent, int idx, const char *xid,      struct module *module, size_t extra_size,      struct snd_card **card_ret)": "snd_card_register(), the very first devres action to call snd_card_free()   is added automatically.  In that way, the resource disconnection is assured   at first, then released in the expected order.     If an error happens at the probe before snd_card_register() is called and   there have been other devres resources, you'd need to free the card manually   via snd_card_free() call in the error; otherwise it may lead to UAF due to   devres call orders.  You can use snd_card_free_on_error() helper for   handling it more easily.     Return: zero if successful, or a negative error code ", "if (!*id || !strncmp(id, \"card\", 4)) ": "snd_card_set_id_no_lock(struct snd_card  card, const char  src,    const char  nid){int len, loops;bool is_default = false;char  id;copy_valid_id_string(card, src, nid);id = card->id; again:  use \"Default\" for obviously invalid strings   (\"card\" conflicts with proc directories) ", "  int snd_component_add(struct snd_card *card, const char *component)": "snd_component_add - add a component string    @card: soundcard structure    @component: the component id string      This function adds the component id string to the supported list.    The component can be referred from the alsa-lib.      Return: Zero otherwise a negative error code. ", "int snd_card_file_add(struct snd_card *card, struct file *file)": "snd_card_file_add - add the file to the file list of the card    @card: soundcard structure    @file: file pointer      This function adds the file to the file linked-list of the card.    This linked-list is used to keep tracking the connection state,    and to avoid the release of busy resources by hotplug.      Return: zero or a negative error code. ", "int snd_card_file_remove(struct snd_card *card, struct file *file)": "snd_card_file_remove - remove the file from the file list    @card: soundcard structure    @file: file pointer      This function removes the file formerly added to the card via    snd_card_file_add() function.    If all files are removed and snd_card_free_when_closed() was    called beforehand, it processes the pending release of    resources.      Return: Zero or a negative error code. ", "int snd_power_wait(struct snd_card *card)": "snd_power_wait - wait until the card gets powered up (old form)   @card: soundcard structure     Wait until the card gets powered up to SNDRV_CTL_POWER_D0 state.     Return: Zero if successful, or a negative error code. ", "int snd_hwdep_new(struct snd_card *card, char *id, int device,  struct snd_hwdep **rhwdep)": "snd_hwdep_new - create a new hwdep instance   @card: the card instance   @id: the id string   @device: the device index (zero-based)   @rhwdep: the pointer to store the new hwdep instance     Creates a new hwdep instance with the given index on the card.   The callbacks (hwdep->ops) must be set on the returned instance   after this call manually by the caller.     Return: Zero if successful, or a negative error code on failure. ", "void snd_dma_program(unsigned long dma,     unsigned long addr, unsigned int size,                     unsigned short mode)": "snd_dma_program - program an ISA DMA transfer   @dma: the dma number   @addr: the physical address of the buffer   @size: the DMA transfer size   @mode: the DMA transfer mode, DMA_MODE_XXX     Programs an ISA DMA transfer for the given buffer. ", "void snd_dma_disable(unsigned long dma)": "snd_dma_disable - stop the ISA DMA transfer   @dma: the dma number     Stops the ISA DMA transfer. ", "unsigned int snd_dma_pointer(unsigned long dma, unsigned int size)": "snd_dma_pointer - return the current pointer to DMA transfer buffer in bytes   @dma: the dma number   @size: the dma transfer size     Return: The current pointer in DMA transfer buffer in bytes. ", "int snd_info_get_line(struct snd_info_buffer *buffer, char *line, int len)": "snd_info_get_line - read one line from the procfs buffer   @buffer: the procfs buffer   @line: the buffer to store   @len: the max. buffer size     Reads one line from the buffer and stores the string.     Return: Zero if successful, or 1 if error or EOF. ", "const char *snd_info_get_str(char *dest, const char *src, int len)": "snd_info_get_str - parse a string token   @dest: the buffer to store the string token   @src: the original string   @len: the max. length of token - 1     Parses the original string and copy a token to the given   string buffer.     Return: The updated pointer of the original string so that   it can be used for the next call. ", "int snd_info_card_create(struct snd_card *card)": "snd_info_register(entry) < 0) {snd_info_free_entry(entry);return NULL;}return entry;}static struct snd_info_entry  snd_info_create_entry(const char  name, struct snd_info_entry  parent,      struct module  module);int __init snd_info_init(void){snd_proc_root = snd_info_create_entry(\"asound\", NULL, THIS_MODULE);if (!snd_proc_root)return -ENOMEM;snd_proc_root->mode = S_IFDIR | 0555;snd_proc_root->p = proc_mkdir(\"asound\", NULL);if (!snd_proc_root->p)goto error;#ifdef CONFIG_SND_OSSEMULsnd_oss_root = create_subdir(THIS_MODULE, \"oss\");if (!snd_oss_root)goto error;#endif#if IS_ENABLED(CONFIG_SND_SEQUENCER)snd_seq_root = create_subdir(THIS_MODULE, \"seq\");if (!snd_seq_root)goto error;#endifif (snd_info_version_init() < 0 ||    snd_minor_info_init() < 0 ||    snd_minor_info_oss_init() < 0 ||    snd_card_info_init() < 0 ||    snd_info_minor_register() < 0)goto error;return 0; error:snd_info_free_entry(snd_proc_root);return -ENOMEM;}int __exit snd_info_done(void){snd_info_free_entry(snd_proc_root);return 0;}static void snd_card_id_read(struct snd_info_entry  entry,     struct snd_info_buffer  buffer){struct snd_card  card = entry->private_data;snd_iprintf(buffer, \"%s\\n\", card->id);}    create a card proc file   called from init.c ", "static struct snd_info_entry *snd_info_create_entry(const char *name, struct snd_info_entry *parent,      struct module *module)": "snd_info_create_card_entry().     Return: The pointer of the new instance, or %NULL on failure. ", "const struct snd_pci_quirk *snd_pci_quirk_lookup_id(u16 vendor, u16 device,const struct snd_pci_quirk *list)": "snd_pci_quirk_lookup_id - look up a PCI SSID quirk list   @vendor: PCI SSV id   @device: PCI SSD id   @list: quirk list, terminated by a null entry     Look through the given quirk list and finds a matching entry   with the same PCI SSID.  When subdevice is 0, all subdevice   values may match.     Returns the matched entry pointer, or NULL if nothing matched. ", "queue_autoload_drivers();#endif}EXPORT_SYMBOL(snd_seq_autoload_init": "snd_seq_autoload_init(void){atomic_dec(&snd_seq_in_init);#ifdef CONFIG_SND_SEQUENCER_MODULE  initial autoload only when snd-seq is a module ", "#include <linux/device.h>#include <linux/init.h>#include <linux/module.h>#include <sound/core.h>#include <sound/info.h>#include <sound/seq_device.h>#include <sound/seq_kernel.h>#include <sound/initval.h>#include <linux/kmod.h>#include <linux/slab.h>#include <linux/mutex.h>MODULE_AUTHOR(\"Takashi Iwai <tiwai@suse.de>\");MODULE_DESCRIPTION(\"ALSA sequencer device management\");MODULE_LICENSE(\"GPL\");/* * bus definition ": "snd_seq_device_new().  This is an entry pointer to communicate   with the sequencer device \"driver\", which is involved with the   actual part to communicate with the sequencer core.   Each sequencer device entry has an id string and the corresponding   driver with the same id is loaded when required.  For example,   lowlevel codes to access emu8000 chip on sbawe card are included in   emu8000-synth module.  To activate this module, the hardware   resources like io port are passed via snd_seq_device argument. ", "if (tid->dev_sclass <= SNDRV_TIMER_SCLASS_NONE ||    tid->dev_sclass > SNDRV_TIMER_SCLASS_OSS_SEQUENCER) ": "snd_timer_open(struct snd_timer_instance  timeri,   struct snd_timer_id  tid,   unsigned int slave_id){struct snd_timer  timer;struct device  card_dev_to_put = NULL;int err;mutex_lock(&register_mutex);if (tid->dev_class == SNDRV_TIMER_CLASS_SLAVE) {  open a slave instance ", "int snd_timer_open(struct snd_timer_instance *timeri,   struct snd_timer_id *tid,   unsigned int slave_id)": "snd_timer_close_locked(struct snd_timer_instance  timeri,   struct device   card_devp_to_put);    open a timer instance   when opening a master, the slave id must be here given. ", "} else ": "snd_pcm_lib_free_pages(substream);}if (substream->dma_buffer.area != NULL &&    substream->dma_buffer.bytes >= size) {dmab = &substream->dma_buffer;   use the pre-allocated buffer ", "spin_lock_irq(&timer->lock);while (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) ": "snd_timer_stop(timeri);if (timer) {timer->num_instances--;  wait, until the active callback is finished ", "if (!(timeri->flags & SNDRV_TIMER_IFLG_PAUSED))return -EINVAL;if (timeri->flags & SNDRV_TIMER_IFLG_SLAVE)return snd_timer_start_slave(timeri, false);elsereturn snd_timer_start1(timeri, false, 0);}EXPORT_SYMBOL(snd_timer_continue": "snd_timer_continue(struct snd_timer_instance  timeri){  timer can continue only after pause ", "resolution = snd_timer_hw_resolution(timer);/* loop for all active instances * Here we cannot use list_for_each_entry because the active_list of a * processed instance is relinked to done_list_head before the callback * is called. ": "snd_timer_interrupt(struct snd_timer   timer, unsigned long ticks_left){struct snd_timer_instance  ti,  ts,  tmp;unsigned long resolution;struct list_head  ack_list_head;unsigned long flags;bool use_work = false;if (timer == NULL)return;if (timer->card && timer->card->shutdown) {snd_timer_clear_callbacks(timer, &timer->ack_list_head);return;}spin_lock_irqsave(&timer->lock, flags);  remember the current resolution ", "if (card != NULL) ": "snd_timer_new(struct snd_card  card, char  id, struct snd_timer_id  tid,  struct snd_timer   rtimer){struct snd_timer  timer;int err;static const struct snd_device_ops ops = {.dev_free = snd_timer_dev_free,.dev_register = snd_timer_dev_register,.dev_disconnect = snd_timer_dev_disconnect,};if (snd_BUG_ON(!tid))return -EINVAL;if (tid->dev_class == SNDRV_TIMER_CLASS_CARD ||    tid->dev_class == SNDRV_TIMER_CLASS_PCM) {if (WARN_ON(!card))return -EINVAL;}if (rtimer) rtimer = NULL;timer = kzalloc(sizeof( timer), GFP_KERNEL);if (!timer)return -ENOMEM;timer->tmr_class = tid->dev_class;timer->card = card;timer->tmr_device = tid->device;timer->tmr_subdevice = tid->subdevice;if (id)strscpy(timer->id, id, sizeof(timer->id));timer->sticks = 1;INIT_LIST_HEAD(&timer->device_list);INIT_LIST_HEAD(&timer->open_list_head);INIT_LIST_HEAD(&timer->active_list_head);INIT_LIST_HEAD(&timer->ack_list_head);INIT_LIST_HEAD(&timer->sack_list_head);spin_lock_init(&timer->lock);INIT_WORK(&timer->task_work, snd_timer_work);timer->max_instances = 1000;   default limit per timer ", "list_for_each_entry(ts, &ti->slave_active_head, active_list)if (ts->ccallback)ts->ccallback(ts, event, &tstamp, resolution);}/* start/continue a master timer ": "snd_timer_notify1(struct snd_timer_instance  ti, int event){struct snd_timer  timer = ti->timer;unsigned long resolution = 0;struct snd_timer_instance  ts;struct timespec64 tstamp;if (timer_tstamp_monotonic)ktime_get_ts64(&tstamp);elsektime_get_real_ts64(&tstamp);if (snd_BUG_ON(event < SNDRV_TIMER_EVENT_START ||       event > SNDRV_TIMER_EVENT_PAUSE))return;if (timer &&    (event == SNDRV_TIMER_EVENT_START ||     event == SNDRV_TIMER_EVENT_CONTINUE))resolution = snd_timer_hw_resolution(timer);if (ti->ccallback)ti->ccallback(ti, event, &tstamp, resolution);if (ti->flags & SNDRV_TIMER_IFLG_SLAVE)return;if (timer == NULL)return;if (timer->hw.flags & SNDRV_TIMER_HW_SLAVE)return;event += 10;   convert to SNDRV_TIMER_EVENT_MXXX ", "snd_dma_free_pages(runtime->dma_buffer_p);kfree(runtime->dma_buffer_p);}snd_compr_set_runtime_buffer(stream, NULL);return 0;}EXPORT_SYMBOL(snd_compr_free_pages": "snd_compr_free_pages(struct snd_compr_stream  stream){struct snd_compr_runtime  runtime;if (snd_BUG_ON(!(stream) || !(stream)->runtime))return -EINVAL;runtime = stream->runtime;if (runtime->dma_area == NULL)return 0;if (runtime->dma_buffer_p != &stream->dma_buffer) {  It's a newly allocated buffer. Release it now. ", "int snd_dma_alloc_dir_pages(int type, struct device *device,    enum dma_data_direction dir, size_t size,    struct snd_dma_buffer *dmab)": "snd_dma_alloc_dir_pages - allocate the buffer area according to the given  type and direction   @type: the DMA buffer type   @device: the device pointer   @dir: DMA direction   @size: the buffer size to allocate   @dmab: buffer allocation record to store the allocated data     Calls the memory-allocator function for the corresponding   buffer type.     Return: Zero if the buffer with the given size is allocated successfully,   otherwise a negative value on error. ", "int snd_dma_alloc_pages_fallback(int type, struct device *device, size_t size, struct snd_dma_buffer *dmab)": "snd_dma_alloc_pages_fallback - allocate the buffer area according to the given type with fallback   @type: the DMA buffer type   @device: the device pointer   @size: the buffer size to allocate   @dmab: buffer allocation record to store the allocated data     Calls the memory-allocator function for the corresponding   buffer type.  When no space is left, this function reduces the size and   tries to allocate again.  The size actually allocated is stored in   res_size argument.     Return: Zero if the buffer with the given size is allocated successfully,   otherwise a negative value on error. ", "void snd_dma_free_pages(struct snd_dma_buffer *dmab)": "snd_dma_free_pages - release the allocated buffer   @dmab: the buffer allocation record to release     Releases the allocated buffer via snd_dma_alloc_pages(). ", "int snd_dma_buffer_mmap(struct snd_dma_buffer *dmab,struct vm_area_struct *area)": "snd_dma_buffer_mmap - perform mmap of the given DMA buffer   @dmab: buffer allocation information   @area: VM area information     Return: zero if successful, or a negative error code ", "dma_addr_t snd_sgbuf_get_addr(struct snd_dma_buffer *dmab, size_t offset)": "snd_sgbuf_get_addr - return the physical address at the corresponding offset   @dmab: buffer allocation information   @offset: offset in the ring buffer     Return: the physical address ", "struct page *snd_sgbuf_get_page(struct snd_dma_buffer *dmab, size_t offset)": "snd_sgbuf_get_page - return the physical page at the corresponding offset   @dmab: buffer allocation information   @offset: offset in the ring buffer     Return: the page pointer ", "unsigned int snd_sgbuf_get_chunk_size(struct snd_dma_buffer *dmab,      unsigned int ofs, unsigned int size)": "snd_sgbuf_get_chunk_size - compute the max chunk size with continuous pages  on sg-buffer   @dmab: buffer allocation information   @ofs: offset in the ring buffer   @size: the requested size     Return: the chunk size ", "int snd_device_new(struct snd_card *card, enum snd_device_type type,   void *device_data, const struct snd_device_ops *ops)": "snd_device_new - create an ALSA device component   @card: the card instance   @type: the device type, SNDRV_DEV_XXX   @device_data: the data pointer of this device   @ops: the operator table     Creates a new device component for the given data pointer.   The device will be assigned to the card and managed together   by the card.     The data pointer plays a role as the identifier, too, so the   pointer address must be unique and unchanged.     Return: Zero if successful, or a negative error code on failure. ", "list_del(&dev->list);__snd_device_disconnect(dev);if (dev->ops->dev_free) ": "snd_device_free(struct snd_device  dev){  unlink ", "int snd_device_register(struct snd_card *card, void *device_data)": "snd_device_register(struct snd_device  dev){if (dev->state == SNDRV_DEV_BUILD) {if (dev->ops->dev_register) {int err = dev->ops->dev_register(dev);if (err < 0)return err;}dev->state = SNDRV_DEV_REGISTERED;}return 0;}     snd_device_register - register the device   @card: the card instance   @device_data: the data pointer to register     Registers the device which was already created via   snd_device_new().  Usually this is called from snd_card_register(),   but it can be called later if any new devices are created after   invocation of snd_card_register().     Return: Zero if successful, or a negative error code on failure or if the   device not found. ", "if (minor < 0)return minor;preg = kmalloc(sizeof(struct snd_minor), GFP_KERNEL);if (preg == NULL)return -ENOMEM;preg->type = type;preg->card = card ? card->number : -1;preg->device = dev;preg->f_ops = f_ops;preg->private_data = private_data;preg->card_ptr = card;mutex_lock(&sound_oss_mutex);snd_oss_minors[minor] = preg;minor_unit = SNDRV_MINOR_OSS_DEVICE(minor);switch (minor_unit) ": "snd_register_oss_device(int type, struct snd_card  card, int dev,    const struct file_operations  f_ops, void  private_data){int minor = snd_oss_kernel_minor(type, card, dev);int minor_unit;struct snd_minor  preg;int cidx = SNDRV_MINOR_OSS_CARD(minor);int track2 = -1;int register1 = -1, register2 = -1;struct device  carddev = snd_card_get_device_link(card);if (card && card->number >= SNDRV_MINOR_OSS_DEVICES)return 0;   ignore silently ", "unregister_sound_special(minor);if (track2 >= 0)unregister_sound_special(track2);kfree(mptr);return 0;}EXPORT_SYMBOL(snd_unregister_oss_device": "snd_unregister_oss_device(int type, struct snd_card  card, int dev){int minor = snd_oss_kernel_minor(type, card, dev);int cidx = SNDRV_MINOR_OSS_CARD(minor);int track2 = -1;struct snd_minor  mptr;if (card && card->number >= SNDRV_MINOR_OSS_DEVICES)return 0;if (minor < 0)return minor;mutex_lock(&sound_oss_mutex);mptr = snd_oss_minors[minor];if (mptr == NULL) {mutex_unlock(&sound_oss_mutex);return -ENOENT;}switch (SNDRV_MINOR_OSS_DEVICE(minor)) {case SNDRV_MINOR_OSS_PCM:track2 = SNDRV_MINOR_OSS(cidx, SNDRV_MINOR_OSS_AUDIO);break;case SNDRV_MINOR_OSS_MIDI:track2 = SNDRV_MINOR_OSS(cidx, SNDRV_MINOR_OSS_DMMIDI);break;case SNDRV_MINOR_OSS_MIDI1:track2 = SNDRV_MINOR_OSS(cidx, SNDRV_MINOR_OSS_DMMIDI1);break;}if (track2 >= 0)snd_oss_minors[track2] = NULL;snd_oss_minors[minor] = NULL;mutex_unlock(&sound_oss_mutex);  call unregister_sound_special() outside sound_oss_mutex;   otherwise may deadlock, as it can trigger the release of a card ", "void snd_request_card(int card)": "snd_request_card - try to load the card module   @card: the card number     Tries to load the module \"snd-card-X\" for the given card number   via request_module.  Returns immediately if already loaded. ", "void *snd_lookup_minor_data(unsigned int minor, int type)": "snd_lookup_minor_data - get user data of a registered device   @minor: the minor number   @type: device type (SNDRV_DEVICE_TYPE_XXX)     Checks that a minor device with the specified type is registered, and returns   its user data pointer.     This function increments the reference counter of the card instance   if an associated instance with the given minor number and type is found.   The caller must call snd_card_unref() appropriately later.     Return: The user data pointer if the specified device is found. %NULL   otherwise. ", "int snd_register_device(int type, struct snd_card *card, int dev,const struct file_operations *f_ops,void *private_data, struct device *device)": "snd_register_device - Register the ALSA device file for the card   @type: the device type, SNDRV_DEVICE_TYPE_XXX   @card: the card instance   @dev: the device index   @f_ops: the file operations   @private_data: user pointer for f_ops->open()   @device: the device to register     Registers an ALSA device file for the given card.   The operators have to be set in reg parameter.     Return: Zero if successful, or a negative error code on failure. ", "int snd_unregister_device(struct device *dev)": "snd_unregister_device - unregister the device on the given card   @dev: the device instance     Unregisters the device file already registered via   snd_register_device().     Return: Zero if successful, or a negative error code on failure. ", "int snd_pcm_format_signed(snd_pcm_format_t format)": "snd_pcm_format_signed - Check the PCM format is signed linear   @format: the format to check     Return: 1 if the given PCM format is signed linear, 0 if unsigned   linear, and a negative error code for non-linear formats. ", "int snd_pcm_format_unsigned(snd_pcm_format_t format)": "snd_pcm_format_unsigned - Check the PCM format is unsigned linear   @format: the format to check     Return: 1 if the given PCM format is unsigned linear, 0 if signed   linear, and a negative error code for non-linear formats. ", "int snd_pcm_format_linear(snd_pcm_format_t format)": "snd_pcm_format_linear - Check the PCM format is linear   @format: the format to check     Return: 1 if the given PCM format is linear, 0 if not. ", "int snd_pcm_format_little_endian(snd_pcm_format_t format)": "snd_pcm_format_little_endian - Check the PCM format is little-endian   @format: the format to check     Return: 1 if the given PCM format is little-endian, 0 if   big-endian, or a negative error code if endian not specified. ", "int snd_pcm_format_big_endian(snd_pcm_format_t format)": "snd_pcm_format_big_endian - Check the PCM format is big-endian   @format: the format to check     Return: 1 if the given PCM format is big-endian, 0 if   little-endian, or a negative error code if endian not specified. ", "int snd_pcm_format_width(snd_pcm_format_t format)": "snd_pcm_format_width - return the bit-width of the format   @format: the format to check     Return: The bit-width of the format, or a negative error code   if unknown format. ", "int snd_pcm_format_physical_width(snd_pcm_format_t format)": "snd_pcm_format_physical_width - return the physical bit-width of the format   @format: the format to check     Return: The physical bit-width of the format, or a negative error code   if unknown format. ", "ssize_t snd_pcm_format_size(snd_pcm_format_t format, size_t samples)": "snd_pcm_format_size - return the byte size of samples on the given format   @format: the format to check   @samples: sampling rate     Return: The byte size of the given samples for the format, or a   negative error code if unknown format. ", "const unsigned char *snd_pcm_format_silence_64(snd_pcm_format_t format)": "snd_pcm_format_silence_64 - return the silent data in 8 bytes array   @format: the format to check     Return: The format pattern to fill or %NULL if error. ", "int snd_pcm_format_set_silence(snd_pcm_format_t format, void *data, unsigned int samples)": "snd_pcm_format_set_silence - set the silence data on the buffer   @format: the PCM format   @data: the buffer pointer   @samples: the number of samples to set silence     Sets the silence data on the buffer for the given samples.     Return: Zero if successful, or a negative error code on failure. ", "int snd_pcm_hw_limit_rates(struct snd_pcm_hardware *hw)": "snd_pcm_hw_limit_rates - determine rate_minrate_max fields   @hw: the pcm hw instance     Determines the rate_min and rate_max fields from the rates bits of   the given hw.     Return: Zero if successful. ", "unsigned int snd_pcm_rate_to_rate_bit(unsigned int rate)": "snd_pcm_rate_to_rate_bit - converts sample rate to SNDRV_PCM_RATE_xxx bit   @rate: the sample rate to convert     Return: The SNDRV_PCM_RATE_xxx flag that corresponds to the given rate, or   SNDRV_PCM_RATE_KNOT for an unknown rate. ", "unsigned int snd_pcm_rate_bit_to_rate(unsigned int rate_bit)": "snd_pcm_rate_bit_to_rate - converts SNDRV_PCM_RATE_xxx bit to sample rate   @rate_bit: the rate bit to convert     Return: The sample rate that corresponds to the given SNDRV_PCM_RATE_xxx flag   or 0 for an unknown rate bit. ", "static DECLARE_RWSEM(snd_pcm_link_rwsem);void snd_pcm_group_init(struct snd_pcm_group *group)": "snd_pcm_hw_refine_old_user(struct snd_pcm_substream  substream,      struct snd_pcm_hw_params_old __user   _oparams);static int snd_pcm_hw_params_old_user(struct snd_pcm_substream  substream,      struct snd_pcm_hw_params_old __user   _oparams);#endifstatic int snd_pcm_open(struct file  file, struct snd_pcm  pcm, int stream);    ", "int snd_pcm_stop(struct snd_pcm_substream *substream, snd_pcm_state_t state)": "snd_pcm_stop - try to stop all running streams in the substream group   @substream: the PCM substream instance   @state: PCM state after stopping the stream     The state of each stream is then changed to the given state unconditionally.     Return: Zero if successful, or a negative error code. ", "int snd_pcm_suspend_all(struct snd_pcm *pcm)": "snd_pcm_suspend_all - trigger SUSPEND to all substreams in the given pcm   @pcm: the PCM instance     After this call, all streams are changed to SUSPENDED state.     Return: Zero if successful (or @pcm is %NULL), or a negative error code. ", "if (substream->managed_buffer_alloc &&    substream->dma_buffer.dev.need_sync)substream->runtime->hw.info |= SNDRV_PCM_INFO_EXPLICIT_SYNC;*rsubstream = substream;return 0; error:snd_pcm_release_substream(substream);return err;}EXPORT_SYMBOL(snd_pcm_open_substream": "snd_pcm_open_substream(struct snd_pcm  pcm, int stream,   struct file  file,   struct snd_pcm_substream   rsubstream){struct snd_pcm_substream  substream;int err;err = snd_pcm_attach_substream(pcm, stream, file, &substream);if (err < 0)return err;if (substream->ref_count > 1) { rsubstream = substream;return 0;}err = snd_pcm_hw_constraints_init(substream);if (err < 0) {pcm_dbg(pcm, \"snd_pcm_hw_constraints_init failed\\n\");goto error;}err = substream->ops->open(substream);if (err < 0)goto error;substream->hw_opened = 1;err = snd_pcm_hw_constraints_complete(substream);if (err < 0) {pcm_dbg(pcm, \"snd_pcm_hw_constraints_complete failed\\n\");goto error;}  automatically set EXPLICIT_SYNC flag in the managed mode whenever   the DMA buffer requires it ", "int snd_pcm_kernel_ioctl(struct snd_pcm_substream *substream, unsigned int cmd, void *arg)": "snd_pcm_kernel_ioctl - Execute PCM ioctl in the kernel-space   @substream: PCM substream   @cmd: IOCTL cmd   @arg: IOCTL argument     The function is provided primarily for OSS layer and USB gadget drivers,   and it allows only the limited set of ioctls (hw_params, sw_params,   prepare, start, drain, drop, forward).     Return: zero if successful, or a negative error code ", "int snd_pcm_lib_mmap_iomem(struct snd_pcm_substream *substream,   struct vm_area_struct *area)": "snd_pcm_lib_mmap_iomem - Default PCM data mmap function for IO mem   @substream: PCM substream   @area: VMA     When your hardware uses the iomapped pages as the hardware buffer and   wants to mmap it, pass this function as mmap pcm_ops.  Note that this   is supposed to work only on limited architectures.     Return: zero if successful, or a negative error code ", "/** * snd_pcm_lib_default_mmap - Default PCM data mmap function * @substream: PCM substream * @area: VMA * * This is the default mmap handler for PCM data.  When mmap pcm_ops is NULL, * this function is invoked implicitly. * * Return: zero if successful, or a negative error code ": "snd_pcm_mmap_data_fault(struct vm_fault  vmf){struct snd_pcm_substream  substream = vmf->vma->vm_private_data;struct snd_pcm_runtime  runtime;unsigned long offset;struct page   page;size_t dma_bytes;if (substream == NULL)return VM_FAULT_SIGBUS;runtime = substream->runtime;offset = vmf->pgoff << PAGE_SHIFT;dma_bytes = PAGE_ALIGN(runtime->dma_bytes);if (offset > dma_bytes - PAGE_SIZE)return VM_FAULT_SIGBUS;if (substream->ops->page)page = substream->ops->page(substream, offset);else if (!snd_pcm_get_dma_buf(substream))page = virt_to_page(runtime->dma_area + offset);elsepage = snd_sgbuf_get_page(snd_pcm_get_dma_buf(substream), offset);if (!page)return VM_FAULT_SIGBUS;get_page(page);vmf->page = page;return 0;}static const struct vm_operations_struct snd_pcm_vm_ops_data = {.open =snd_pcm_mmap_data_open,.close =snd_pcm_mmap_data_close,};static const struct vm_operations_struct snd_pcm_vm_ops_data_fault = {.open =snd_pcm_mmap_data_open,.close =snd_pcm_mmap_data_close,.fault =snd_pcm_mmap_data_fault,};    mmap the DMA buffer on RAM ", "int snd_pcm_create_iec958_consumer_default(u8 *cs, size_t len)": "snd_pcm_create_iec958_consumer_default - create default consumer format IEC958 channel status   @cs: channel status buffer, at least four bytes   @len: length of channel status buffer     Create the consumer format channel status data in @cs of maximum size   @len. When relevant, the configuration-dependant bits will be set as   unspecified.     Drivers should then call einter snd_pcm_fill_iec958_consumer() or   snd_pcm_fill_iec958_consumer_hw_params() to replace these unspecified   bits by their actual values.     Drivers may wish to tweak the contents of the buffer after creation.     Returns: length of buffer, or negative error code if something failed. ", "int snd_pcm_create_iec958_consumer_hw_params(struct snd_pcm_hw_params *params,     u8 *cs, size_t len)": "snd_pcm_create_iec958_consumer_hw_params - create IEC958 channel status   @params: the hw_params instance for extracting rate and sample format   @cs: channel status buffer, at least four bytes   @len: length of channel status buffer     Create the consumer format channel status data in @cs of maximum size   @len corresponding to the parameters of the PCM runtime @runtime.     Drivers may wish to tweak the contents of the buffer after creation.     Returns: length of buffer, or negative error code if something failed. ", "void snd_pcm_lib_preallocate_free_for_all(struct snd_pcm *pcm)": "snd_pcm_lib_preallocate_free_for_all - release all pre-allocated buffers on the pcm   @pcm: the pcm instance     Releases all the pre-allocated buffers on the given pcm. ", "void snd_pcm_lib_preallocate_pages(struct snd_pcm_substream *substream,  int type, struct device *data,  size_t size, size_t max)": "snd_pcm_lib_preallocate_pages - pre-allocation for the given DMA type   @substream: the pcm substream instance   @type: DMA type (SNDRV_DMA_TYPE_ )   @data: DMA type dependent data   @size: the requested pre-allocation size in bytes   @max: the max. allowed pre-allocation size     Do pre-allocation for the given DMA buffer type. ", "void snd_pcm_lib_preallocate_pages_for_all(struct snd_pcm *pcm,  int type, void *data,  size_t size, size_t max)": "snd_pcm_lib_preallocate_pages_for_all - pre-allocation for continuous memory type (all substreams)   @pcm: the pcm instance   @type: DMA type (SNDRV_DMA_TYPE_ )   @data: DMA type dependent data   @size: the requested pre-allocation size in bytes   @max: the max. allowed pre-allocation size     Do pre-allocation to all substreams of the given pcm for the   specified DMA type. ", "int snd_pcm_set_managed_buffer(struct snd_pcm_substream *substream, int type,struct device *data, size_t size, size_t max)": "snd_pcm_set_managed_buffer - set up buffer management for a substream   @substream: the pcm substream instance   @type: DMA type (SNDRV_DMA_TYPE_ )   @data: DMA type dependent data   @size: the requested pre-allocation size in bytes   @max: the max. allowed pre-allocation size     Do pre-allocation for the given DMA buffer type, and set the managed   buffer allocation mode to the given substream.   In this mode, PCM core will allocate a buffer automatically before PCM   hw_params ops call, and release the buffer after PCM hw_free ops call   as well, so that the driver doesn't need to invoke the allocation and   the release explicitly in its callback.   When a buffer is actually allocated before the PCM hw_params call, it   turns on the runtime buffer_changed flag for drivers changing their hw   parameters accordingly.     When @size is non-zero and @max is zero, this tries to allocate for only   the exact buffer size without fallback, and may return -ENOMEM.   Otherwise, the function tries to allocate smaller chunks if the allocation   fails.  This is the behavior of snd_pcm_set_fixed_buffer().     When both @size and @max are zero, the function only sets up the buffer   for later dynamic allocations. It's used typically for buffers with   SNDRV_DMA_TYPE_VMALLOC type.     Upon successful buffer allocation and setup, the function returns 0.     Return: zero if successful, or a negative error code ", "int snd_pcm_set_managed_buffer_all(struct snd_pcm *pcm, int type,   struct device *data,   size_t size, size_t max)": "snd_pcm_set_managed_buffer_all - set up buffer management for all substreams  for all substreams   @pcm: the pcm instance   @type: DMA type (SNDRV_DMA_TYPE_ )   @data: DMA type dependent data   @size: the requested pre-allocation size in bytes   @max: the max. allowed pre-allocation size     Do pre-allocation to all substreams of the given pcm for the specified DMA   type and size, and set the managed_buffer_alloc flag to each substream.     Return: zero if successful, or a negative error code ", "int snd_pcm_lib_malloc_pages(struct snd_pcm_substream *substream, size_t size)": "snd_pcm_lib_malloc_pages - allocate the DMA buffer   @substream: the substream to allocate the DMA buffer to   @size: the requested buffer size in bytes     Allocates the DMA buffer on the BUS type given earlier to   snd_pcm_lib_preallocate_xxx_pages().     Return: 1 if the buffer is changed, 0 if not changed, or a negative   code on failure. ", "vfree(runtime->dma_area);}runtime->dma_area = __vmalloc(size, gfp_flags);if (!runtime->dma_area)return -ENOMEM;runtime->dma_bytes = size;return 1;}EXPORT_SYMBOL(_snd_pcm_lib_alloc_vmalloc_buffer": "_snd_pcm_lib_alloc_vmalloc_buffer(struct snd_pcm_substream  substream,      size_t size, gfp_t gfp_flags){struct snd_pcm_runtime  runtime;if (PCM_RUNTIME_CHECK(substream))return -EINVAL;runtime = substream->runtime;if (runtime->dma_area) {if (runtime->dma_bytes >= size)return 0;   already large enough ", "int snd_pcm_lib_free_vmalloc_buffer(struct snd_pcm_substream *substream)": "snd_pcm_lib_free_vmalloc_buffer - free vmalloc buffer   @substream: the substream with a buffer allocated by  snd_pcm_lib_alloc_vmalloc_buffer()     Return: Zero if successful, or a negative error code on failure. ", "struct page *snd_pcm_lib_get_vmalloc_page(struct snd_pcm_substream *substream,  unsigned long offset)": "snd_pcm_lib_get_vmalloc_page - map vmalloc buffer offset to page struct   @substream: the substream with a buffer allocated by  snd_pcm_lib_alloc_vmalloc_buffer()   @offset: offset in the buffer     This function is to be used as the page callback in the PCM ops.     Return: The page struct, or %NULL on failure. ", "int snd_pcm_new_stream(struct snd_pcm *pcm, int stream, int substream_count)": "snd_pcm_new_stream - create a new PCM stream   @pcm: the pcm instance   @stream: the stream direction, SNDRV_PCM_STREAM_XXX   @substream_count: the number of substreams     Creates a new stream for the pcm.   The corresponding stream on the pcm must have been empty before   calling this, i.e. zero must be given to the argument of   snd_pcm_new().     Return: Zero if successful, or a negative error code on failure. ", "int snd_pcm_new_internal(struct snd_card *card, const char *id, int device,int playback_count, int capture_count,struct snd_pcm **rpcm)": "snd_pcm_new_internal - create a new internal PCM instance   @card: the card instance   @id: the id string   @device: the device index (zero based - shared with normal PCMs)   @playback_count: the number of substreams for playback   @capture_count: the number of substreams for capture   @rpcm: the pointer to store the new pcm instance     Creates a new internal PCM instance with no userspace device or procfs   entries. This is used by ASoC Back End PCMs in order to create a PCM that   will only be used internally by kernel drivers. i.e. it cannot be opened   by userspace. It provides existing ASoC components drivers with a substream   and access to any private data.     The pcm operators have to be set afterwards to the new instance   via snd_pcm_set_ops().     Return: Zero if successful, or a negative error code on failure. ", "const char *snd_pcm_format_name(snd_pcm_format_t format)": "snd_pcm_notify_list);#endifstatic int snd_pcm_free(struct snd_pcm  pcm);static int snd_pcm_dev_free(struct snd_device  device);static int snd_pcm_dev_register(struct snd_device  device);static int snd_pcm_dev_disconnect(struct snd_device  device);static struct snd_pcm  snd_pcm_get(struct snd_card  card, int device){struct snd_pcm  pcm;list_for_each_entry(pcm, &snd_pcm_devices, list) {if (pcm->card == card && pcm->device == device)return pcm;}return NULL;}static int snd_pcm_next(struct snd_card  card, int device){struct snd_pcm  pcm;list_for_each_entry(pcm, &snd_pcm_devices, list) {if (pcm->card == card && pcm->device > device)return pcm->device;else if (pcm->card->number > card->number)return -1;}return -1;}static int snd_pcm_add(struct snd_pcm  newpcm){struct snd_pcm  pcm;if (newpcm->internal)return 0;list_for_each_entry(pcm, &snd_pcm_devices, list) {if (pcm->card == newpcm->card && pcm->device == newpcm->device)return -EBUSY;if (pcm->card->number > newpcm->card->number ||(pcm->card == newpcm->card &&pcm->device > newpcm->device)) {list_add(&newpcm->list, pcm->list.prev);return 0;}}list_add_tail(&newpcm->list, &snd_pcm_devices);return 0;}static int snd_pcm_control_ioctl(struct snd_card  card, struct snd_ctl_file  control, unsigned int cmd, unsigned long arg){switch (cmd) {case SNDRV_CTL_IOCTL_PCM_NEXT_DEVICE:{int device;if (get_user(device, (int __user  )arg))return -EFAULT;mutex_lock(&register_mutex);device = snd_pcm_next(card, device);mutex_unlock(&register_mutex);if (put_user(device, (int __user  )arg))return -EFAULT;return 0;}case SNDRV_CTL_IOCTL_PCM_INFO:{struct snd_pcm_info __user  info;unsigned int device, subdevice;int stream;struct snd_pcm  pcm;struct snd_pcm_str  pstr;struct snd_pcm_substream  substream;int err;info = (struct snd_pcm_info __user  )arg;if (get_user(device, &info->device))return -EFAULT;if (get_user(stream, &info->stream))return -EFAULT;if (stream < 0 || stream > 1)return -EINVAL;stream = array_index_nospec(stream, 2);if (get_user(subdevice, &info->subdevice))return -EFAULT;mutex_lock(&register_mutex);pcm = snd_pcm_get(card, device);if (pcm == NULL) {err = -ENXIO;goto _error;}pstr = &pcm->streams[stream];if (pstr->substream_count == 0) {err = -ENOENT;goto _error;}if (subdevice >= pstr->substream_count) {err = -ENXIO;goto _error;}for (substream = pstr->substream; substream;     substream = substream->next)if (substream->number == (int)subdevice)break;if (substream == NULL) {err = -ENXIO;goto _error;}mutex_lock(&pcm->open_mutex);err = snd_pcm_info_user(substream, info);mutex_unlock(&pcm->open_mutex);_error:mutex_unlock(&register_mutex);return err;}case SNDRV_CTL_IOCTL_PCM_PREFER_SUBDEVICE:{int val;if (get_user(val, (int __user  )arg))return -EFAULT;control->preferred_subdevice[SND_CTL_SUBDEV_PCM] = val;return 0;}}return -ENOIOCTLCMD;}#define FORMAT(v) [SNDRV_PCM_FORMAT_##v] = #vstatic const char   const snd_pcm_format_names[] = {FORMAT(S8),FORMAT(U8),FORMAT(S16_LE),FORMAT(S16_BE),FORMAT(U16_LE),FORMAT(U16_BE),FORMAT(S24_LE),FORMAT(S24_BE),FORMAT(U24_LE),FORMAT(U24_BE),FORMAT(S32_LE),FORMAT(S32_BE),FORMAT(U32_LE),FORMAT(U32_BE),FORMAT(FLOAT_LE),FORMAT(FLOAT_BE),FORMAT(FLOAT64_LE),FORMAT(FLOAT64_BE),FORMAT(IEC958_SUBFRAME_LE),FORMAT(IEC958_SUBFRAME_BE),FORMAT(MU_LAW),FORMAT(A_LAW),FORMAT(IMA_ADPCM),FORMAT(MPEG),FORMAT(GSM),FORMAT(SPECIAL),FORMAT(S24_3LE),FORMAT(S24_3BE),FORMAT(U24_3LE),FORMAT(U24_3BE),FORMAT(S20_3LE),FORMAT(S20_3BE),FORMAT(U20_3LE),FORMAT(U20_3BE),FORMAT(S18_3LE),FORMAT(S18_3BE),FORMAT(U18_3LE),FORMAT(U18_3BE),FORMAT(G723_24),FORMAT(G723_24_1B),FORMAT(G723_40),FORMAT(G723_40_1B),FORMAT(DSD_U8),FORMAT(DSD_U16_LE),FORMAT(DSD_U32_LE),FORMAT(DSD_U16_BE),FORMAT(DSD_U32_BE),};     snd_pcm_format_name - Return a name string for the given PCM format   @format: PCM format     Return: the format name string ", "void snd_ctl_notify(struct snd_card *card, unsigned int mask,    struct snd_ctl_elem_id *id)": "snd_ctl_notify - Send notification to user-space for a control change   @card: the card to send notification   @mask: the event mask, SNDRV_CTL_EVENT_    @id: the ctl element id to send notification     This function adds an event record with the given id and mask, appends   to the list and wakes up the user-space for notification.  This can be   called in the atomic context. ", "void snd_ctl_notify_one(struct snd_card *card, unsigned int mask,struct snd_kcontrol *kctl, unsigned int ioff)": "snd_ctl_notify_one - Send notification to user-space for a control change   @card: the card to send notification   @mask: the event mask, SNDRV_CTL_EVENT_    @kctl: the pointer with the control instance   @ioff: the additional offset to the control index     This function calls snd_ctl_notify() and does additional jobs   like LED state changes. ", "struct snd_kcontrol *snd_ctl_new1(const struct snd_kcontrol_new *ncontrol,  void *private_data)": "snd_ctl_new1 - create a control instance from the template   @ncontrol: the initialization record   @private_data: the private data to set     Allocates a new struct snd_kcontrol instance and initialize from the given   template.  When the access field of ncontrol is 0, it's assumed as   READWRITE access. When the count field is 0, it's assumes as one.     Return: The pointer of the newly generated instance, or %NULL on failure. ", "void snd_ctl_free_one(struct snd_kcontrol *kcontrol)": "snd_ctl_free_one - release the control instance   @kcontrol: the control instance     Releases the control instance created via snd_ctl_new()   or snd_ctl_new1().   Don't call this after the control was added to the card. ", "kctl->id.iface = ncontrol->iface;kctl->id.device = ncontrol->device;kctl->id.subdevice = ncontrol->subdevice;if (ncontrol->name) ": "snd_ctl_add(). ", "int snd_ctl_replace(struct snd_card *card, struct snd_kcontrol *kcontrol,    bool add_on_replace)": "snd_ctl_replace - replace the control instance of the card   @card: the card instance   @kcontrol: the control instance to replace   @add_on_replace: add the control if not already added     Replaces the given control.  If the given control does not exist   and the add_on_replace flag is set, the control is added.  If the   control exists, it is destroyed first.     It frees automatically the control which cannot be added or replaced.     Return: Zero if successful, or a negative error code on failure. ", "if (card->last_numid >= UINT_MAX - count)card->last_numid = 0;list_for_each_entry(kctl, &card->controls, list) ": "snd_ctl_remove_numid_conflict(struct snd_card  card,  unsigned int count){struct snd_kcontrol  kctl;  Make sure that the ids assigned to the control do not wrap around ", "int snd_ctl_remove_id(struct snd_card *card, struct snd_ctl_elem_id *id)": "snd_ctl_remove_id - remove the control of the given id and release it   @card: the card instance   @id: the control id to remove     Finds the control instance with the given id, removes it from the   card list and releases it.     Return: 0 if successful, or a negative error code on failure. ", "int snd_ctl_rename_id(struct snd_card *card, struct snd_ctl_elem_id *src_id,      struct snd_ctl_elem_id *dst_id)": "snd_ctl_rename_id - replace the id of a control on the card   @card: the card instance   @src_id: the old id   @dst_id: the new id     Finds the control with the old id from the card, and replaces the   id with the new one.     The function tries to keep the already assigned numid while replacing   the rest.     Note that this function should be used only in the card initialization   phase.  Calling after the card instantiation may cause issues with   user-space expecting persistent numids.     Return: Zero if successful, or a negative error code on failure. ", "/** * snd_ctl_find_numid - find the control instance with the given number-id * @card: the card instance * @numid: the number-id to search * * Finds the control instance with the given number-id from the card. * * The caller must down card->controls_rwsem before calling this function * (if the race condition can happen). * * Return: The pointer of the instance if found, or %NULL if not. * ": "snd_ctl_find_numid_slow(struct snd_card  card, unsigned int numid){struct snd_kcontrol  kctl;list_for_each_entry(kctl, &card->controls, list) {if (kctl->id.numid <= numid && kctl->id.numid + kctl->count > numid)return kctl;}return NULL;}#endif   !CONFIG_SND_CTL_FAST_LOOKUP ", "int snd_ctl_add(struct snd_card *card, struct snd_kcontrol *kcontrol)": "snd_ctl_find_id(card, &id);if (!old) {if (mode == CTL_REPLACE)return -EINVAL;} else {if (mode == CTL_ADD_EXCLUSIVE) {dev_err(card->dev,\"control %i:%i:%i:%s:%i is already present\\n\",id.iface, id.device, id.subdevice, id.name,id.index);return -EBUSY;}err = snd_ctl_remove(card, old);if (err < 0)return err;}if (snd_ctl_find_hole(card, kcontrol->count) < 0)return -ENOMEM;list_add_tail(&kcontrol->list, &card->controls);card->controls_count += kcontrol->count;kcontrol->id.numid = card->last_numid + 1;card->last_numid += kcontrol->count;add_hash_entries(card, kcontrol);for (idx = 0; idx < kcontrol->count; idx++)snd_ctl_notify_one(card, SNDRV_CTL_EVENT_MASK_ADD, kcontrol, idx);return 0;}static int snd_ctl_add_replace(struct snd_card  card,       struct snd_kcontrol  kcontrol,       enum snd_ctl_add_mode mode){int err = -EINVAL;if (! kcontrol)return err;if (snd_BUG_ON(!card || !kcontrol->info))goto error;down_write(&card->controls_rwsem);err = __snd_ctl_add_replace(card, kcontrol, mode);up_write(&card->controls_rwsem);if (err < 0)goto error;return 0; error:snd_ctl_free_one(kcontrol);return err;}     snd_ctl_add - add the control instance to the card   @card: the card instance   @kcontrol: the control instance to add     Adds the control instance created via snd_ctl_new() or   snd_ctl_new1() to the given card. Assigns also an unique   numid used for fast search.     It frees automatically the control which cannot be added.     Return: Zero if successful, or a negative error code on failure.   ", "int snd_ctl_register_ioctl(snd_kctl_ioctl_func_t fcn)": "snd_ctl_register_ioctl(snd_kctl_ioctl_func_t fcn, struct list_head  lists){struct snd_kctl_ioctl  pn;pn = kzalloc(sizeof(struct snd_kctl_ioctl), GFP_KERNEL);if (pn == NULL)return -ENOMEM;pn->fioctl = fcn;down_write(&snd_ioctl_rwsem);list_add_tail(&pn->list, lists);up_write(&snd_ioctl_rwsem);return 0;}     snd_ctl_register_ioctl - register the device-specific control-ioctls   @fcn: ioctl callback function     called from each device manager like pcm.c, hwdep.c, etc.     Return: zero if successful, or a negative error code ", "int snd_ctl_register_ioctl_compat(snd_kctl_ioctl_func_t fcn)": "snd_ctl_register_ioctl_compat - register the device-specific 32bit compat   control-ioctls   @fcn: ioctl callback function     Return: zero if successful, or a negative error code ", "int snd_ctl_unregister_ioctl(snd_kctl_ioctl_func_t fcn)": "snd_ctl_unregister_ioctl(snd_kctl_ioctl_func_t fcn,     struct list_head  lists){struct snd_kctl_ioctl  p;if (snd_BUG_ON(!fcn))return -EINVAL;down_write(&snd_ioctl_rwsem);list_for_each_entry(p, lists, list) {if (p->fioctl == fcn) {list_del(&p->list);up_write(&snd_ioctl_rwsem);kfree(p);return 0;}}up_write(&snd_ioctl_rwsem);snd_BUG();return -EINVAL;}     snd_ctl_unregister_ioctl - de-register the device-specific control-ioctls   @fcn: ioctl callback function to unregister     Return: zero if successful, or a negative error code ", "int snd_ctl_unregister_ioctl_compat(snd_kctl_ioctl_func_t fcn)": "snd_ctl_unregister_ioctl_compat - de-register the device-specific compat   32bit control-ioctls   @fcn: ioctl callback function to unregister     Return: zero if successful, or a negative error code ", "int snd_ctl_boolean_mono_info(struct snd_kcontrol *kcontrol,      struct snd_ctl_elem_info *uinfo)": "snd_ctl_boolean_mono_info - Helper function for a standard boolean info   callback with a mono channel   @kcontrol: the kcontrol instance   @uinfo: info to store     This is a function that can be used as info callback for a standard   boolean control with a single mono channel.     Return: Zero (always successful) ", "int snd_ctl_boolean_stereo_info(struct snd_kcontrol *kcontrol,struct snd_ctl_elem_info *uinfo)": "snd_ctl_boolean_stereo_info - Helper function for a standard boolean info   callback with stereo two channels   @kcontrol: the kcontrol instance   @uinfo: info to store     This is a function that can be used as info callback for a standard   boolean control with stereo two channels.     Return: Zero (always successful) ", "int snd_ctl_enum_info(struct snd_ctl_elem_info *info, unsigned int channels,      unsigned int items, const char *const names[])": "snd_ctl_enum_info - fills the info structure for an enumerated control   @info: the structure to be filled   @channels: the number of the control's channels; often one   @items: the number of control values; also the size of @names   @names: an array containing the names of all control values     Sets all required fields in @info to their appropriate values.   If the control's accessibility is not the default (readable and writable),   the caller has to fill @info->access.     Return: Zero (always successful) ", "int snd_jack_add_new_kctl(struct snd_jack *jack, const char * name, int mask)": "snd_jack_add_new_kctl - Create a new snd_jack_kctl and add it to jack   @jack:  the jack instance which the kctl will attaching to   @name:  the name for the snd_kcontrol object   @mask:  a bitmask of enum snd_jack_type values that can be detected           by this snd_jack_kctl object.     Creates a new snd_kcontrol object and adds it to the jack kctl_list.     Return: Zero if successful, or a negative error code on failure. ", "int snd_jack_new(struct snd_card *card, const char *id, int type, struct snd_jack **jjack, bool initial_kctl, bool phantom_jack)": "snd_jack_new - Create a new jack   @card:  the card instance   @id:    an identifying string for this jack   @type:  a bitmask of enum snd_jack_type values that can be detected by           this jack   @jjack: Used to provide the allocated jack object to the caller.   @initial_kctl: if true, create a kcontrol and add it to the jack list.   @phantom_jack: Don't create a input device for phantom jacks.     Creates a new jack object.     Return: Zero if successful, or a negative error code on failure.   On success @jjack will be initialised. ", "void snd_jack_set_parent(struct snd_jack *jack, struct device *parent)": "snd_jack_set_parent - Set the parent device for a jack     @jack:   The jack to configure   @parent: The device to set as parent for the jack.     Set the parent for the jack devices in the device tree.  This   function is only valid prior to registration of the jack.  If no   parent is configured then the parent device will be the sound card. ", "int snd_jack_set_key(struct snd_jack *jack, enum snd_jack_types type,     int keytype)": "snd_jack_set_key - Set a key mapping on a jack     @jack:    The jack to configure   @type:    Jack report type for this key   @keytype: Input layer key type to be reported     Map a SND_JACK_BTN_  button type to an input layer key, allowing   reporting of keys on accessories via the jack abstraction.  If no   mapping is provided but keys are enabled in the jack type then   BTN_n numeric buttons will be reported.     If jacks are not reporting via the input API this call will have no   effect.     Note that this is intended to be use by simple devices with small   numbers of keys that can be reported.  It is also possible to   access the input device directly - devices with complex input   capabilities on accessories should consider doing this rather than   using this abstraction.     This function may only be called prior to registration of the jack.     Return: Zero if successful, or a negative error code on failure. ", "static const char * const jack_events_name[] = ": "snd_jack_report(jack_kctl->jack, jack_kctl->jack->hw_status_cache);return ret;}static ssize_t jackin_inject_write(struct file  file,   const char __user  from, size_t count, loff_t  ppos){struct snd_jack_kctl  jack_kctl = file->private_data;int ret, err;unsigned long enable;char buf[8] = { 0 };if (!jack_kctl->sw_inject_enable)return -EINVAL;ret = simple_write_to_buffer(buf, sizeof(buf) - 1, ppos, from, count);err = kstrtoul(buf, 0, &enable);if (err)return err;snd_jack_inject_report(jack_kctl, !!enable ? jack_kctl->mask_bits : 0);return ret;}static ssize_t jack_kctl_id_read(struct file  file, char __user  to, size_t count, loff_t  ppos){struct snd_jack_kctl  jack_kctl = file->private_data;char buf[64];int len, ret;len = scnprintf(buf, sizeof(buf), \"%s\\n\", jack_kctl->kctl->id.name);ret = simple_read_from_buffer(to, count, ppos, buf, len);return ret;}  the bit definition is aligned with snd_jack_types in jack.h ", "if (substream->ops->drain)substream->ops->drain(substream);elsemsleep(50);snd_rawmidi_drop_output(substream);}spin_lock_irq(&substream->lock);snd_rawmidi_buffer_unref(runtime);spin_unlock_irq(&substream->lock);return err;}EXPORT_SYMBOL(snd_rawmidi_drain_output": "snd_rawmidi_drain_output(struct snd_rawmidi_substream  substream){int err = 0;long timeout;struct snd_rawmidi_runtime  runtime;spin_lock_irq(&substream->lock);runtime = substream->runtime;if (!substream->opened || !runtime || !runtime->buffer) {err = -EINVAL;} else {snd_rawmidi_buffer_ref(runtime);runtime->drain = 1;}spin_unlock_irq(&substream->lock);if (err < 0)return err;timeout = wait_event_interruptible_timeout(runtime->sleep,(runtime->avail >= runtime->buffer_size),10 HZ);spin_lock_irq(&substream->lock);if (signal_pending(current))err = -ERESTARTSYS;if (runtime->avail < runtime->buffer_size && !timeout) {rmidi_warn(substream->rmidi,   \"rawmidi drain error (avail = %li, buffer_size = %li)\\n\",   (long)runtime->avail, (long)runtime->buffer_size);err = -EIO;}runtime->drain = 0;spin_unlock_irq(&substream->lock);if (err != -ERESTARTSYS) {  we need wait a while to make sure that Tx FIFOs are empty ", "int snd_rawmidi_receive(struct snd_rawmidi_substream *substream,const unsigned char *buffer, int count)": "snd_rawmidi_receive - receive the input data from the device   @substream: the rawmidi substream   @buffer: the buffer pointer   @count: the data size to read     Reads the data from the internal buffer.     Return: The size of read data, or a negative error code on failure. ", "appl_ptr = runtime->appl_ptr;runtime->appl_ptr += count1;runtime->appl_ptr %= runtime->buffer_size;runtime->avail -= count1;if (kernelbuf)memcpy(kernelbuf + result, runtime->buffer + appl_ptr, count1);if (userbuf) ": "snd_rawmidi_kernel_read1(struct snd_rawmidi_substream  substream,     unsigned char __user  userbuf,     unsigned char  kernelbuf, long count){unsigned long flags;long result = 0, count1;struct snd_rawmidi_runtime  runtime = substream->runtime;unsigned long appl_ptr;int err = 0;spin_lock_irqsave(&substream->lock, flags);snd_rawmidi_buffer_ref(runtime);while (count > 0 && runtime->avail) {count1 = runtime->buffer_size - runtime->appl_ptr;if (count1 > count)count1 = count;if (count1 > (int)runtime->avail)count1 = runtime->avail;  update runtime->appl_ptr before unlocking for userbuf ", "int snd_rawmidi_transmit_empty(struct snd_rawmidi_substream *substream)": "snd_rawmidi_transmit_empty - check whether the output buffer is empty   @substream: the rawmidi substream     Return: 1 if the internal output buffer is empty, 0 if not. ", "static int __snd_rawmidi_transmit_peek(struct snd_rawmidi_substream *substream,       unsigned char *buffer, int count)": "snd_rawmidi_transmit_peek - copy data from the internal buffer   @substream: the rawmidi substream   @buffer: the buffer pointer   @count: data size to transfer     This is a variant of snd_rawmidi_transmit_peek() without spinlock. ", "int snd_rawmidi_transmit_peek(struct snd_rawmidi_substream *substream,      unsigned char *buffer, int count)": "snd_rawmidi_transmit_ack() after the transmission is   finished.     Return: The size of copied data, or a negative error code on failure. ", "int snd_rawmidi_proceed(struct snd_rawmidi_substream *substream)": "snd_rawmidi_proceed - Discard the all pending bytes and proceed   @substream: rawmidi substream     Return: the number of discarded bytes ", "int snd_rawmidi_kernel_release(struct snd_rawmidi_file *rfile)": "snd_rawmidi_kernel_write(substream, &buf, 1);}if (snd_rawmidi_drain_output(substream) == -ERESTARTSYS)snd_rawmidi_output_trigger(substream, 0);}snd_rawmidi_buffer_ref_sync(substream);}spin_lock_irq(&substream->lock);substream->opened = 0;substream->append = 0;spin_unlock_irq(&substream->lock);substream->ops->close(substream);if (substream->runtime->private_free)substream->runtime->private_free(substream);snd_rawmidi_runtime_free(substream);put_pid(substream->pid);substream->pid = NULL;rmidi->streams[substream->stream].substream_opened--;}static void rawmidi_release_priv(struct snd_rawmidi_file  rfile){struct snd_rawmidi  rmidi;rmidi = rfile->rmidi;mutex_lock(&rmidi->open_mutex);if (rfile->input) {close_substream(rmidi, rfile->input, 1);rfile->input = NULL;}if (rfile->output) {close_substream(rmidi, rfile->output, 1);rfile->output = NULL;}rfile->rmidi = NULL;mutex_unlock(&rmidi->open_mutex);wake_up(&rmidi->open_wait);}  called from soundcoreseqseq_midi.c ", "int snd_rawmidi_new(struct snd_card *card, char *id, int device,    int output_count, int input_count,    struct snd_rawmidi **rrawmidi)": "snd_rawmidi_set_ops() to set the operators to the new instance.     Return: Zero if successful, or a negative error code on failure. ", "      16,/* may be configurable ": "snd_virmidi_new(struct snd_card  card, int device, struct snd_rawmidi   rrmidi){struct snd_rawmidi  rmidi;struct snd_virmidi_dev  rdev;int err; rrmidi = NULL;err = snd_rawmidi_new(card, \"VirMidi\", device,      16,  may be configurable ", "memset(&portinfo, 0, sizeof(portinfo));portinfo.addr.client = client;strscpy(portinfo.name, portname ? portname : \"Unnamed port\",sizeof(portinfo.name));portinfo.capability = cap;portinfo.type = type;portinfo.kernel = pcbp;portinfo.midi_channels = midi_channels;portinfo.midi_voices = midi_voices;/* Create it ": "snd_seq_event_port_attach(int client,      struct snd_seq_port_callback  pcbp,      int cap, int type, int midi_channels,      int midi_voices, char  portname){struct snd_seq_port_info portinfo;int  ret;  Set up the port ", "ev->type = status_event[ST_SPECIAL + c - 0xf0].event;ev->flags &= ~SNDRV_SEQ_EVENT_LENGTH_MASK;ev->flags |= SNDRV_SEQ_EVENT_LENGTH_FIXED;return ev->type != SNDRV_SEQ_EVENT_NONE;}spin_lock_irqsave(&dev->lock, flags);if ((c & 0x80) &&    (c != MIDI_CMD_COMMON_SYSEX_END || dev->type != ST_SYSEX)) ": "snd_midi_event_encode_byte(struct snd_midi_event  dev, unsigned char c,struct snd_seq_event  ev){bool rc = false;unsigned long flags;if (c >= MIDI_CMD_COMMON_CLOCK) {  real-time event ", "cmd = 0x80 | (type << 4) | (ev->data.note.channel & 0x0f);if (cmd == MIDI_CMD_COMMON_SYSEX) ": "snd_midi_event_decode(struct snd_midi_event  dev, unsigned char  buf, long count,   struct snd_seq_event  ev){unsigned int cmd, type;if (ev->type == SNDRV_SEQ_EVENT_NONE)return -ENOENT;for (type = 0; type < ARRAY_SIZE(status_event); type++) {if (ev->type == status_event[type].event)goto __found;}for (type = 0; type < ARRAY_SIZE(extra_event); type++) {if (ev->type == extra_event[type].event)return extra_event[type].decode(dev, buf, count, ev);}return -ENOENT;      __found:if (type >= ST_SPECIAL)cmd = 0xf0 + (type - ST_SPECIAL);else  data.note.channel and data.control.channel is identical ", "if (ev->type == SNDRV_SEQ_EVENT_NOTE)return;/* Make sure that we don't have a note on that should really be * a note off ": "snd_midi_process_event(const struct snd_midi_op  ops,       struct snd_seq_event  ev,       struct snd_midi_channel_set  chanset){struct snd_midi_channel  chan;void  drv;int dest_channel = 0;if (ev == NULL || chanset == NULL) {pr_debug(\"ALSA: seq_midi_emul: ev or chanbase NULL (snd_midi_process_event)\\n\");return;}if (chanset->channels == NULL)return;if (snd_seq_ev_is_channel_type(ev)) {dest_channel = ev->data.note.channel;if (dest_channel >= chanset->max_channels) {pr_debug(\"ALSA: seq_midi_emul: dest channel is %d, max is %d\\n\",   dest_channel, chanset->max_channels);return;}}chan = chanset->channels + dest_channel;drv  = chanset->private_data;  EVENT_NOTE should be processed before queued ", "chan->gm_rpn_fine_tuning = 0;chan->gm_rpn_coarse_tuning = 0;if (i == 9)chan->drum_channel = 1;elsechan->drum_channel = 0;}}EXPORT_SYMBOL(snd_midi_channel_set_clear": "snd_midi_channel_set_clear(struct snd_midi_channel_set  chset){int i;chset->midi_mode = SNDRV_MIDI_MODE_GM;chset->gs_master_volume = 127;for (i = 0; i < chset->max_channels; i++) {struct snd_midi_channel  chan = chset->channels + i;memset(chan->note, 0, sizeof(chan->note));chan->midi_aftertouch = 0;chan->midi_pressure = 0;chan->midi_program = 0;chan->midi_pitchbend = 0;snd_midi_reset_controllers(chan);chan->gm_rpn_pitch_bend_range = 256;   2 semitones ", "client = seq_create_client1(client_index, 0);if (client == NULL) ": "snd_seq_create_kernel_client(struct snd_card  card, int client_index, const char  name_fmt, ...){struct snd_seq_client  client;va_list args;if (snd_BUG_ON(in_interrupt()))return -EBUSY;if (card && client_index >= SNDRV_SEQ_CLIENTS_PER_CARD)return -EINVAL;if (card == NULL && client_index >= SNDRV_SEQ_GLOBAL_CLIENTS)return -EINVAL;mutex_lock(&register_mutex);if (card) {client_index += SNDRV_SEQ_GLOBAL_CLIENTS+ card->number   SNDRV_SEQ_CLIENTS_PER_CARD;if (client_index >= SNDRV_SEQ_DYNAMIC_CLIENTS_BEGIN)client_index = -1;}  empty write queue as default ", "if (ev->type == SNDRV_SEQ_EVENT_KERNEL_ERROR)return -EINVAL; /* quoted events can't be enqueued ": "snd_seq_kernel_client_enqueue(int client, struct snd_seq_event  ev,  struct file  file, bool blocking){struct snd_seq_client  cptr;int result;if (snd_BUG_ON(!ev))return -EINVAL;if (!snd_seq_ev_is_ump(ev)) {if (ev->type == SNDRV_SEQ_EVENT_NONE)return 0;   ignore this ", "ev->queue = SNDRV_SEQ_QUEUE_DIRECT;ev->source.client = client;if (check_event_type_and_length(ev))return -EINVAL;cptr = snd_seq_client_use_ptr(client);if (cptr == NULL)return -EINVAL;if (!cptr->accept_output)result = -EPERM;elseresult = snd_seq_deliver_event(cptr, ev, atomic, hop);snd_seq_client_unlock(cptr);return result;}EXPORT_SYMBOL(snd_seq_kernel_client_dispatch": "snd_seq_kernel_client_dispatch(int client, struct snd_seq_event   ev,   int atomic, int hop){struct snd_seq_client  cptr;int result;if (snd_BUG_ON(!ev))return -EINVAL;  fill in client number ", "int snd_seq_kernel_client_ctl(int clientid, unsigned int cmd, void *arg)": "snd_seq_kernel_client_ctl - operate a command for a client with data in         kernel space.   @clientid:A numerical ID for a client.   @cmd:An ioctl(2) command for ALSA sequencer operation.   @arg:A pointer to data in kernel space.     Against its name, both kernelapplication client can be handled by this   kernel API. A pointer of 'arg' argument should be in kernel space.     Return: 0 at success. Negative error code at failure. "}